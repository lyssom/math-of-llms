与固定形式的正弦余弦位置编码不同，可学习位置编码（Learned Positional Encoding）将位置编码视为可学习的参数，通过反向传播自动从数据中学习最优的位置表示。这种方法的核心思想是：与其手工设计位置编码的形式，不如让模型自己发现最适合任务的编码方式。可学习位置编码在Transformer架构的早期研究和某些特定任务上取得了良好的性能，是位置编码研究中的重要范式之一。

从数学角度来看，可学习位置编码涉及参数化矩阵的性质分析。我们需要理解：可学习编码能够表示什么样的位置函数？其表达能力与参数量的关系如何？可学习编码的矩阵具有怎样的秩和奇异值结构？这些矩阵性质如何影响训练动态和泛化性能？本节将从线性代数和矩阵分析的角度，系统探讨可学习位置编码的数学原理和性质。

可学习位置编码的数学分析不仅有助于深入理解其工作原理，也为设计新的位置编码方案提供了理论基础。通过比较可学习编码与正弦编码的矩阵性质，我们可以更清晰地看到两种范式的优势和局限性，从而在实际应用中做出更好的选择。

## 6.3.1 可学习位置编码的定义与参数化
可学习位置编码将位置编码矩阵视为可训练的参数。设序列的最大长度为$⁡n_{\max}$，嵌入维度为$d_{\text{model}}$​，则可学习位置编码矩阵定义为：
$$
P \in \mathbb{R}^{n_{\max} \times d_{\text{model}}}, \quad P = \begin{bmatrix} p_1^T \\ p_2^T \\ \vdots \\ p_{n_{\max}}^T \end{bmatrix} \tag{6.3.1}
$$
其中，每个$p_i \in \mathbb{R}^{d_{\text{model}}}$是位置$i$的编码向量，$n_{\max}$​是预设的最大序列长度。编码矩阵$P$中的所有元素都是可学习的参数，在训练过程中通过梯度下降进行优化。

在模型的前向传播中，对于长度为$n$的输入序列，带有位置编码的输入表示为：
$$
X_{\text{pos}} = X + P_{1:n} \tag{6.3.2}
$$
其中，$X \in \mathbb{R}^{n \times d_{\text{model}}}$是内容嵌入矩阵，$P_{1:n} \in \mathbb{R}^{n \times d_{\text{model}}}$是位置编码矩阵的前$n$行。位置编码矩阵的其他行（对应于$n_{\max} > n$的位置）在当前输入中不被使用，但会在处理更长序列时被激活。

与正弦余弦编码不同，可学习位置编码没有固定的数学公式。编码矩阵$P$的具体数值完全由训练数据决定，模型会学习到最能支持任务目标的位置表示。这种灵活性是可学习编码的主要优势，但同时也带来了新的挑战，如过拟合风险、外推困难等。
可学习位置编码的参数量为$n_{\max} \times d_{\text{model}}$​。例如，当$n_{\max} = 4096$，$d_{\text{model}} = 512$时，位置编码的参数量约为$2 \times 10^6$个，这是一个相当可观的参数量。

从参数效率的角度，我们可以分析位置编码的自由度。编码矩阵$P$的秩$\text{rank}(P) \leq \min(n_{\max}, d_{\text{model}})$。如果$⁡d_{\text{model}} < n_{\max}$（这是常见配置），则$\text{rank}(P) \leq d_{\text{model}}$。这意味着，尽管编码矩阵有$n_{\max} \times d_{\text{model}}$个参数，但其有效自由度不超过$d_{\text{model}}$​，因为所有位置编码向量都位于由前$d_{\text{model}}$​个基向量张成的子空间中。

考虑编码矩阵的奇异值分解：$P = U \Sigma V^T$，其中$⁡U \in \mathbb{R}^{n_{\max} \times n_{\max}}$、$\Sigma \in \mathbb{R}^{n_{\max} \times d_{\text{model}}}$、$V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$​。非零奇异值的数量等于矩阵的秩$r \leq d_{\text{model}}$​。设非零奇异值为$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$，则编码矩阵可以表示为：
$$
P = \sum_{k=1}^{r} \sigma_k u_k v_k^T \tag{6.3.3}
$$
其中，$u_k$是$U$的前$r$列，$v_k$​是$V$的前$r$列。这个分解表明，可学习位置编码的有效表示可以用$r$个秩-1矩阵的和来描述，参数量约为$2r \cdot d_{\text{model}} + r \approx O(r \cdot d_{\text{model}})$，远小于原始的$n_{\max} \cdot d_{\text{model}}$​。
可学习位置编码在嵌入空间中形成特定的几何结构。设$P$是学习到的位置编码矩阵，考虑位置编码向量集合$\{p_1, p_2, \ldots, p_{n_{\max}}\} \subset \mathbb{R}^{d_{\text{model}}}$。

这些编码向量的几何分布取决于训练过程中学习到的模式。理想情况下，编码向量应该形成一个能够唯一标识每个位置的配置。从微分几何的角度，我们可以分析编码向量的以下性质。

位置编码向量的质心为：
$$
c = \frac{1}{n_{\max}} \sum_{i=1}^{n_{\max}} p_i \tag{6.3.4}
$$
质心位置反映了编码向量的"中心趋势"。如果编码向量的均值为零（通过适当的初始化和正则化实现），则质心位于原点。

编码向量之间的角度关系可以通过Gram矩阵分析。位置编码的Gram矩阵$G = PP^T \in \mathbb{R}^{n_{\max} \times n_{\max}}$的第$(i, j)$个元素为：
$$
G_{ij} = p_i \cdot p_j \tag{6.3.5}
$$
Gram矩阵编码了所有位置对之间的相似性信息。正定Gram矩阵（$G \succ 0$）意味着所有编码向量线性无关，任意位置都可以被唯一标识。

编码向量的分散程度可以用方差矩阵来度量。设去均值化的编码为$\tilde{P} = P - \mathbf{1}c^T$，其中$\mathbf{1}$是全1向量，则协方差矩阵为：
$$
\Sigma_P = \frac{1}{n_{\max}} \tilde{P}^T \tilde{P} \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}} \tag{6.3.6}
$$
协方差矩阵的特征值表示编码在不同方向上的分散程度，特征向量表示分散的主方向。

## 6.3.2 表达能力分析

可学习位置编码的表达式能力是指它能够表示的位置函数的范围。从函数逼近的角度，位置编码可以视为一个映射$f: \{1, 2, \ldots, n_{\max}\} \rightarrow \mathbb{R}^{d_{\text{model}}}$，将位置索引映射到编码向量。这个映射的复杂度取决于编码矩阵的秩。

设编码矩阵的秩为$r$，则存在$r$个正交基向量$v_1, v_2, \ldots, v_r \in \mathbb{R}^{d_{\text{model}}}$，使得每个位置编码可以表示为：
$$
p_i = \sum_{k=1}^{r} \alpha_{i,k} v_k \tag{6.3.7}
$$
其中，$\alpha_{i,k}$​是位置$i$在基$v_k$​上的坐标。因此，可学习编码的函数空间是维度不超过$r$的线性子空间上的所有函数。

当$r = d_{\text{model}}$（满秩情况）时，编码向量可以张成整个$\mathbb{R}^{d_{\text{model}}}$​空间，表达能力最强。当$r < d_{\text{model}}$​时，编码被限制在某个低维子空间中，表达能力受限。然而，实践中由于$n_{\max} \gg d_{\text{model}}$，编码矩阵通常可以达到满秩$r = d_{\text{model}}$。

与正弦余弦编码相比，可学习编码的表达能力更加灵活。正弦余弦编码的函数空间是固定的（由不同频率的正弦余弦函数张成），而可学习编码的函数空间由训练数据决定，可以适应特定任务的分布。
可学习位置编码在理论上可以逼近正弦余弦位置编码。考虑正弦余弦编码矩阵$P_{\text{sin}} \in \mathbb{R}^{n_{\max} \times d_{\text{model}}}$，其第$i$行第$2m$列元素为$\sin(i / \omega_m)$，第$i$行第$2m+1$列元素为$\cos(i / \omega_m)$。

可学习编码$P_{\text{learn}}$可以通过最小二乘逼近正弦编码：
$$
P_{\text{learn}} = \arg\min_{P} \|P - P_{\text{sin}}\|_F^2 \tag{6.3.8}
$$
由于正弦编码矩阵$P_{\text{sin}}$​的秩不超过$d_{\text{model}}/2$（实际上等于$d_{\text{model}}/2$），最优逼近可以通过保留前$d_{\text{model}}/2$个奇异值和对应的奇异向量来实现。

设$P_{\text{sin}}$​的奇异值分解为$P_{\text{sin}} = U \Sigma V^T$，其中$\Sigma = \text{diag}(\sigma_1, \sigma_2, \ldots, \sigma_r)$，$r = \text{rank}(P_{\text{sin}}) = d_{\text{model}}/2$。最优低秩逼近（秩为$d$的逼近）为：
$$
P_{\text{approx}} = U \text{diag}(\sigma_1, \ldots, \sigma_d) V^T \tag{6.3.9}
$$
当$d = d_{\text{model}}$​时，逼近误差为零。这意味着满秩的可学习编码可以精确表示正弦余弦编码。

然而，逼近的方向很重要。可学习编码学习到的基可能与正弦编码的基完全不同，即使最终能够表示相同的编码向量。这种"等价但不同"的表示可能影响训练动态和最终性能。
可学习位置编码的表达能力虽然灵活，但也有理论上界。首先，编码维度$d_{\text{model}}$​是表达能力的硬上限,编码向量的维度决定了它最多能够区分$n_{\max}$个位置（当编码向量线性无关时）。这与正弦余弦编码的极限相同。其次，编码的有效表达能力受到训练数据分布的影响。如果训练数据中的位置分布不均衡（如某些位置出现频率很低），模型可能无法学习到这些位置的可靠编码。这种采样偏差可能导致模型在某些位置上的表现不佳。第三，编码的表达能力与泛化能力之间存在权衡。高度表达的位置编码可能对训练位置过拟合，难以泛化到未见过的位置。正弦余弦编码的固定结构提供了一种隐式正则化，限制了过拟合的风险。

从信息论的角度，位置编码最多能够编码约$⁡\log_2 n_{\max}$比特的位置信息。设编码向量的维度为$d_{\text{model}}$​，每个维度使用$b$位精度，则总编码容量约为$d_{\text{model}} \cdot b$比特。当$⁡d_{\text{model}} \cdot b \gg \log_2 n_{\max}$时，编码有足够的容量表示所有位置；当$d_{\text{model}} \cdot b \ll \log_2 n_{\text{max}}$​时，编码容量不足，无法唯一区分所有位置。

## 6.3.3 矩阵秩与奇异值分析

位置编码矩阵$P$的秩是影响其性质的关键参数。秩$\text{rank}(P)$衡量了编码向量的线性无关程度，秩越高，编码向量越"分散"，表达能力越强；秩越低，编码向量越"相关"，表达能力越弱。

编码矩阵秩的上界为$\text{rank}(P) \leq \min(n_{\max}, d_{\text{model}})$。由于通常$n_{\max} \gg d_{\text{model}}$，实际的上界为$\text{rank}(P) \leq d_{\text{model}}$​。

在训练过程中，编码矩阵的秩会逐渐增加。初始化的编码矩阵（如随机初始化）通常具有较高的秩（接近$d_{\text{model}}$​）。随着训练的进行，某些奇异值会增大，某些会减小，最终收敛到特定的分布。

设$P$的奇异值分解为$P = U \Sigma V^T$，其中$\Sigma = \text{diag}(\sigma_1, \ldots, \sigma_d)$，$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_d \geq 0$。则矩阵的Frobenius范数为：
$$
\|P\|_F = \sqrt{\sum_{i=1}^{d} \sigma_i^2} \tag{6.3.10}
$$
谱范数为：
$$
\|P\|_2 = \sigma_1 \tag{6.3.11}
$$
条件数为：
$$
\kappa(P) = \frac{\sigma_1}{\sigma_d} \tag{6.3.12}
$$
条件数反映了编码矩阵的数值稳定性。当条件数很大时，编码矩阵接近奇异，数值计算可能不稳定。编码矩阵的奇异值分布揭示了编码的结构特性。考虑以下几种典型的奇异值分布模式。快速衰减分布：$\sigma_1 \gg \sigma_2 \gg \cdots \gg \sigma_r$，前几个奇异值占据了大部分能量。这种分布表明编码主要沿少数几个方向分散，大部分方向上的变化很小。快速衰减的奇异值分布对应于"低秩"结构，编码主要捕获位置的主要变化模式。均匀分布：$\sigma_1 \approx \sigma_2 \approx \cdots \approx \sigma_r$​，所有奇异值相近。这种分布表明编码在各方向上均匀分散，没有明显的主导方向。均匀分布对应于"满秩"结构，编码充分利用了$d_{\text{model}}$​维空间。阶梯分布：$\sigma_1 \approx \sigma_2 \approx \cdots \approx \sigma_k \gg \sigma_{k+1} \approx \cdots \approx \sigma_r \approx 0$。这种分布表明编码存在明显的维度分层，前$k$个维度携带主要信息，后$d-k$个维度几乎不携带信息。
训练良好的可学习位置编码通常表现出快速衰减或阶梯状的奇异值分布。这种分布反映了位置编码的内在结构，位置变化的主要模式可以用少数几个维度捕获，与正弦余弦编码的多尺度频率结构有某种对应关系。
编码矩阵的低秩近似是分析和压缩可学习位置编码的重要工具。给定秩为$r$的编码矩阵$P$，其秩为$k < r$的最佳近似（按Frobenius范数）为：
$$
P_k = \sum_{i=1}^{k} \sigma_i u_i v_i^T \tag{6.3.13}
$$
其中，$\sigma_i, u_i, v_i$​是$P$的奇异值分解中的对应项。近似误差为：
$$
\|P - P_k\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2} \tag{6.3.14}
$$
低秩近似的应用包括：第一，参数压缩，用$P_k$​替代$P$，参数量从$O(n_{\max} \cdot d_{\text{model}})$减少到$O(k \cdot (n_{\max} + d_{\text{model}}))$。第二，计算加速，低秩矩阵的运算更快，因为非零奇异值较少。第三，噪声过滤，小的奇异值可能对应噪声，低秩近似可以去除这些噪声。
实践中，对于典型的位置编码（如$n_{\max} = 4096$，$d_{\text{model}} = 512d$），保留前64-128个奇异值通常能够达到接近原始编码的性能，说明编码的有效秩远低于满秩。
编码矩阵的秩与长度外推能力之间存在重要的关系。外推能力是指模型处理超出训练长度的序列的能力。设训练时使用的最大位置为$n_{\text{train}}$​，推理时需要处理的位置为$n_{\text{test}} > n_{\text{train}}$​。
可学习位置编码在外推时面临的挑战是：位置$n_{\text{test}}$的编码$p_{n_{\text{test}}}$​​在训练时从未见过，模型需要"想象"这个位置应该有什么样的编码。由于编码矩阵只学习了前$n_{\text{train}}$​行的参数，对于$n > n_{\text{train}}$​的行，模型只能使用插值或外推策略。
从秩的角度，如果编码矩阵$P_{1:n_{\text{train}}}$​​的秩较低（接近$n_{\text{train}}$​的某些维度），则新位置的编码可以通过已学习位置的线性组合来构造。例如，如果$P$的秩为$r$，则$p_{n_{\text{test}}}$​​可以表示为前$r$个基向量的线性组合，这些基向量是从训练数据中学习的。
然而，这种基于秩的外推假设了位置编码的平滑性——相邻位置的编码应该相似。如果训练数据支持这种平滑性假设，外推效果会较好；否则，外推可能失败。

## 6.3.4 初始化策略与训练动态

可学习位置编码的初始化对训练动态有重要影响。常用的初始化方法包括均匀初始化、正态初始化和零初始化。

均匀初始化：从均匀分布$U[-a, a]$中采样每个编码元素。设$p_{i,j} \sim U[-a, a]$，则编码向量的期望范数为：
$$
\mathbb{E}[\|p_i\|_2] = \sqrt{\mathbb{E}[\|p_i\|_2^2]} = \sqrt{d_{\text{model}} \cdot \frac{a^2}{3}} \tag{6.3.15}
$$
选择合适的$a$使得初始编码向量具有适当的范数。常用的选择是$a=0.1$或$a = \sqrt{1/d_{\text{model}}}$​​。

正态初始化：从正态分布$N(0, \sigma^2)$中采样。期望范数为：
$$
\mathbb{E}[\|p_i\|_2] = \sqrt{d_{\text{model}} \cdot \sigma^2} \tag{6.3.16}
$$
常用的选择是$\sigma = 0.02$或$\sigma = \sqrt{2/d_{\text{model}}}$（Kaiming初始化）。

零初始化：将所有编码向量初始化为零。这种初始化意味着初始位置编码不提供任何位置信息，位置信息完全由内容嵌入和后续学习提供。零初始化的优点是简单，缺点是训练初期位置信息完全缺失。

从信息论的角度，初始化应该在"不确定性"和"信息量"之间取得平衡。过于随机的初始化可能引入过多噪声，干扰内容信息的学习；过于确定的初始化（如零初始化）可能限制了位置信息的利用。

可学习位置编码的梯度流动是理解训练动态的关键。考虑注意力层的输出关于位置编码$P$的梯度。

设注意力输出为$O = \text{Attention}(X + P)$，损失函数为$\mathcal{L}$。根据链式法则：
$$
\frac{\partial \mathcal{L}}{\partial P} = \frac{\partial \mathcal{L}}{\partial O} \cdot \frac{\partial O}{\partial (X + P)} \cdot \frac{\partial (X + P)}{\partial P} \tag{6.3.17}
$$
其中，$\frac{\partial (X + P)}{\partial P} = I$（单位矩阵），因为$X$不依赖于$P$。$\frac{\partial O}{\partial (X + P)}$是注意力输出关于输入的雅可比矩阵，其性质取决于注意力机制的实现。

在标准Transformer中，注意力计算的雅可比矩阵涉及Softmax的梯度。根据5.1节的分析，Softmax的梯度为：
$$
\frac{\partial \text{Softmax}(z)_i}{\partial z_j} = \begin{cases} \text{Softmax}(z)_i (1 - \text{Softmax}(z)_i) & \text{if } i = j \\ -\text{Softmax}(z)_i \text{Softmax}(z)_j & \text{if } i \neq j \end{cases} \tag{6.3.18}
$$
这个雅可比矩阵的结构影响梯度流动。如果注意力权重分布均匀（所有权重约$1/n$），则梯度会均匀地流向所有位置；如果注意力权重分布集中（少数位置权重较大），则梯度主要流向被"关注"的位置。

可学习位置编码的收敛性分析涉及优化理论。考虑位置编码的优化问题：
$$
\min_P \mathcal{L}(P) \tag{6.3.19}
$$
其中，$\mathcal{L}$是损失函数，$P$是位置编码矩阵。这是一个无约束优化问题，最优解满足梯度为零条件：
$$
\nabla_P \mathcal{L}(P^*) = 0 \tag{6.3.20}
$$
从二阶条件，最优解处的Hessian矩阵应该是半正定的：
$$
\nabla_P^2 \mathcal{L}(P^*) \succeq 0 \tag{6.3.21}
$$
实际上，位置编码的优化景观通常是非凸的，存在多个局部最优解。不同的初始化可能收敛到不同的局部最优，对应于不同的编码配置。

实验观察表明，可学习位置编码通常收敛到某种"平滑"的配置——相邻位置的编码向量相似，与正弦余弦编码的平滑性质一致。这表明优化过程倾向于学习平滑的位置表示，而非过拟合训练位置的特殊模式。

最优编码的性质还取决于任务和数据分布。对于需要精细位置区分的任务（如代码分析），最优编码可能更"尖锐"；对于位置信息不那么重要的任务（如情感分析），最优编码可能更"平坦"。

位置编码的正则化可以影响学习到的编码特性。常用的正则化方法包括L2正则化、差分正则化和谱正则化。

L2正则化在损失函数中添加编码范数的惩罚：
$$
\mathcal{L}_{\text{reg}} = \mathcal{L} + \lambda \|P\|_F^2 \tag{6.3.22}
$$
L2正则化倾向于学习较小的编码值，防止编码向量过大。这可以看作是对编码幅度的约束，影响编码的"强度"。

差分正则化惩罚相邻位置编码的差异：
$$
\mathcal{L}_{\text{reg}} = \mathcal{L} + \lambda \sum_{i=1}^{n_{\max}-1} \|p_{i+1} - p_i\|_2^2 \tag{6.3.23}
$$
差分正则化鼓励学习平滑的位置编码，相邻位置的编码向量相似。这与位置依赖的平滑性假设一致，可以提高外推性能。

谱正则化惩罚小的奇异值或大的条件数：
$$
\mathcal{L}_{\text{reg}} = \mathcal{L} + \lambda \sum_{i=1}^{d} \frac{1}{\sigma_i^2 + \epsilon} \tag{6.3.24}
$$
谱正则化鼓励编码矩阵具有更好的条件数，数值计算更稳定。

## 6.3.5 与正弦编码的比较分析
可学习位置编码与正弦余弦位置编码在数学性质上有显著差异。这些差异可以从多个角度分析。

从参数化角度，正弦余弦编码是确定性的、公式化的，没有可训练参数；可学习编码是数据驱动的、有可训练参数的。参数化的灵活性是可学习编码的主要优势，也是其主要挑战的来源。

从函数空间角度，正弦余弦编码的函数空间是固定的（由特定频率的正弦余弦函数张成），可学习编码的函数空间由训练数据决定。正弦编码的函数空间具有明确的结构（频率分解），可学习编码的函数空间更加灵活但也更加模糊。

从泛化角度，正弦编码天然支持长度外推（因为公式定义在所有整数上）；可学习编码的外推能力取决于学习到的编码结构，如果编码是平滑的则外推较好，否则可能失败。

从计算角度，正弦编码在推理时无需额外计算（编码可以预先计算并缓存）；可学习编码需要存储参数矩阵，但计算开销相同（都是简单的查表）。
通过实证研究，我们可以比较两种编码的表达能力。设任务为序列分类或语言建模，评估指标为验证集准确率或困惑度。

在短序列场景下（训练和测试序列长度相近），可学习位置编码通常与正弦余弦编码表现相当，有时略优。这是因为可学习编码可以适应训练数据的特定分布，学习到对任务最有帮助的位置表示。

在长序列场景下（测试序列长度显著超过训练序列长度），正弦余弦编码通常表现更好。这是因为正弦编码天然支持外推，而可学习编码可能在外推时表现不佳。

在特定领域任务上（如代码、生物序列），可学习编码可能更优，因为这些领域的"位置依赖"可能不符合正弦余弦编码的假设，可学习编码可以自动发现领域特定的位置模式。

从实验结果来看，两种编码各有优劣，选择取决于具体任务和需求。实践中，也可以尝试结合两种编码（如在可学习编码上叠加正弦编码），以兼取两者的优势。
混合位置编码结合了可学习编码和正弦编码的优点。一种常见的设计是将正弦编码和可学习编码相加：
$$
P_{\text{hybrid}} = P_{\text{sin}} + P_{\text{learn}} \tag{6.3.25}
$$
其中，$P_{\text{sin}}$是固定正弦编码，$P_{\text{learn}}$​是可学习编码残差。这种设计的直觉是：正弦编码提供了"基础"位置表示，可学习编码在此基础上学习"残差"调整。

从数学角度，混合编码的函数空间是正弦编码函数空间与可学习编码函数空间的和集。由于正弦编码的秩为$d_{\text{model}}/2$，可学习编码的秩最多为$d_{\text{model}}$​，混合编码的秩最多为$d_{\text{model}}$​（因为维度限制），与单独使用可学习编码相同。

混合编码的训练策略可以是：先预训练正弦编码（固定），再微调可学习残差；或者端到端联合训练两者。联合训练允许可学习编码调整正弦编码的影响程度，可能学习到最优的组合。

另一种混合设计是使用可学习编码替代正弦编码的某些频率成分。例如，低频成分使用正弦编码（提供稳定的全局位置信息），高频成分使用可学习编码（适应任务的局部位置模式）。这种设计可以看作是"固定+可学习"的分层混合。

## 6.3.6 参数效率与实践考量
可学习位置编码的参数效率是实际应用中的重要考量。参数量为$n_{\max} \times d_{\text{model}}$​，对于大型模型（如$n_{\max} = 8192$，$d_{\text{model}} = 4096$），参数量约为$3.3 \times 10^7$，占总参数量的比例不可忽略（假设总参数量为$10^9$，占比约3%）。

从参数效率的角度，我们可以考虑以下优化策略。

低秩参数化：将编码矩阵$P$参数化为低秩分解$P = UV^T$，其中$U \in \mathbb{R}^{n_{\max} \times r}$，$V \in \mathbb{R}^{d_{\text{model}} \times r}$，$r \ll \min(n_{\max}, d_{\text{model}})$。参数量从$O(n_{\max} \cdot d_{\text{model}})$减少到$O(r \cdot (n_{\max} + d_{\text{model}}))$。当$r \ll d_{\text{model}}$​时，参数节省显著。

共享位置编码：在多层之间共享同一个位置编码矩阵，而不是每层独立学习。设Transformer有$L$层，共享编码将参数量从$L \cdot n_{\max} \cdot d_{\text{model}}$减少到$n_{\max} \cdot d_{\text{model}}$​。

分层位置编码：使用不同分辨率的位置编码在不同层共享。低层使用细粒度编码，高层使用粗粒度编码，可以减少总参数量同时保持各层所需的信息分辨率。

可学习位置编码的存储和计算优化是实际部署中的重要问题。

存储优化方面，位置编码矩阵$P$通常以半精度（FP16或BF16）存储，以减少内存占用。对于超长序列（如$n_{\max} = 100000$），可以考虑压缩存储（如使用低秩分解存储$U$和$V$而非完整$P$）。

计算优化方面，位置编码的前向传播是简单的查表操作，计算开销很小。然而，在推理时，对于超长序列的位置编码，可以考虑增量计算，只计算新增位置的编码，已有的编码从缓存读取。

内存布局优化方面，位置编码矩阵通常按行主序存储（每行是一个位置的编码），以便于按位置查询。在GPU上，可以使用常量内存（Constant Memory）存储位置编码，因为它们在推理过程中不变。
可学习位置编码的性能对超参数选择敏感。关键的超参数包括$n_{\max}$（最大序列长度）、初始化方法、初始化尺度等。

$⁡n_{\max}$​的选择需要权衡内存开销和泛化能力。较大的$n_{\max}$​允许处理更长的序列，但也增加参数量和过拟合风险。通常，$n_{\max}$应该设置为略大于训练时的最大序列长度，以提供一定的余量。

初始化尺度的选择影响训练初期的梯度流动。太大的初始化可能导致训练不稳定，太小的初始化可能限制位置信息的作用。实践中，初始编码范数与内容嵌入范数相当是合理的选择。

学习率的选择方面，位置编码的学习率可以与模型其他部分相同，也可以使用不同的学习率（如warmup或更高的学习率）。某些实践表明，给予位置编码稍高的学习率可以加速其学习。

## 6.3.7 本节小结

本节系统地分析了可学习位置编码的矩阵性质。我们首先定义了可学习位置编码的参数化形式，分析了其参数空间和自由度，揭示了编码矩阵的秩不超过嵌入维度$d_{\text{model}}$​这一重要性质。

在表达能力分析部分，我们探讨了可学习编码能够表示的函数空间，分析了它与正弦余弦编码的逼近关系，以及表达能力与泛化能力之间的权衡。

在矩阵分析部分，我们详细推导了编码矩阵的秩、奇异值分布、低秩近似等性质。奇异值分布揭示了编码的结构特性，训练良好的编码通常表现出快速衰减或阶梯状的奇异值分布，这与正弦编码的多尺度频率结构有某种对应关系。

在训练动态分析部分，我们讨论了初始化策略、梯度流动、收敛性和正则化对编码的影响。这些分析有助于理解可学习编码如何从数据中学习最优的位置表示。

最后，我们将可学习编码与正弦编码进行了比较，分析了两种范式的数学差异和适用场景，讨论了混合位置编码的设计，以及参数效率和实践考量。

通过本节的学习，读者应该能够从矩阵分析的角度深入理解可学习位置编码的性质，认识到其在灵活性与泛化性之间的权衡，以及在实际应用中的优化策略。可学习位置编码作为一种重要的位置编码范式，与正弦余弦编码各有优劣，选择取决于具体任务需求和约束条件。下一节，我们将从群论的角度详细推导旋转位置编码（RoPE）的数学原理。