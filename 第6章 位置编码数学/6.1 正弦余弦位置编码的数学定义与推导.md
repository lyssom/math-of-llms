# 第6章 位置编码数学
## 6.1 正弦余弦位置编码的数学定义与推导
在深度学习序列建模的发展历程中，如何让模型感知序列中元素的位置信息一直是一个核心问题。循环神经网络（RNN）通过其递归结构隐式地携带了位置信息，序列中的每个元素依次被处理，位置信息自然地编码在隐藏状态的更新过程中。然而，这种隐式编码方式存在明显的局限性：信息必须沿着序列逐步传递，导致长程依赖难以建模，且计算难以并行化。

Transformer架构的出现从根本上改变了这一局面。注意力机制允许序列中任意两个位置之间直接建立联系，实现了真正的并行计算。但这种并行化的代价是位置信息的丢失，纯注意力机制是置换不变的（Permutation Invariant），即改变输入序列的顺序不会改变注意力输出的顺序，这意味着模型无法区分"狗咬人"和"人咬狗"这样语义完全不同的句子。位置编码（Positional Encoding）的引入正是为了解决这一根本性问题：通过向输入嵌入添加位置相关的信息，使注意力机制能够感知序列中各元素的绝对或相对位置，从而正确处理词序承载的语法和语义信息。

位置编码的设计需要满足几个基本的数学要求。第一，唯一性：不同的位置应当具有不同的编码向量，使得模型能够唯一标识每个位置。第二，可区分性：位置编码应当能够表达位置之间的距离和相对关系，使得语义上与位置相关的依赖能够被建模。第三，泛化性：编码应当能够处理训练时未见过的序列长度，实现长度外推（Length Extrapolation）。第四，兼容性：位置编码不应干扰内容信息的表示，应当与内容编码保持适当的数学关系。第五，计算效率：编码的生成和计算应当高效，不显著增加模型的整体计算负担。

正弦余弦位置编码（Sinusoidal Positional Encoding）是Transformer原始论文《Attention Is All You Need》中提出的位置编码方案，也是最经典的编码方法之一。这种编码利用不同频率的正弦和余弦函数来唯一标识序列中的每个位置，具有优雅的数学形式和良好的理论性质。本节将从数学角度系统推导和解释正弦余弦位置编码的定义、性质及其设计原理，为深入理解位置编码的数学本质奠定坚实基础。

### 6.1.1 正弦余弦位置编码的数学定义

正弦余弦位置编码的核心思想是为序列中的每个位置$i$生成一个$d$维的编码向量，该向量的各维度由不同频率的正弦和余弦函数值组成。设序列长度为$n$，嵌入维度为$d_{\text{model}}$​，位置$i$的编码向量$p_i \in \mathbb{R}^{d_{\text{model}}}$​定义为：
$$
p_{i, 2m} = \sin\left(\frac{i}{10000^{2m/d_{\text{model}}}}\right), \quad p_{i, 2m+1} = \cos\left(\frac{i}{10000^{2m/d_{\text{model}}}}\right) \tag{6.1.1}
$$
其中，$i \in \{0, 1, 2, \ldots, n-1\}$表示序列中的位置索引（从0开始或从1开始，取决于具体实现），$m \in \{0, 1, 2, \ldots, d_{\text{model}}/2 - 1\}$表示维度索引，$10000$是一个经验性选择的常数基准。编码向量的偶数维度（索引为$2m$）由正弦函数给出，奇数维度（索引为$2m+1$）由余弦函数给出，两者成对出现。

将公式（6.1.1）写成更紧凑的形式，定义角频率$\omega_m$为：
$$
\omega_m = \frac{1}{10000^{2m/d_{\text{model}}}} \tag{6.1.2}
$$
则位置编码可以表示为：
$$
p_i = \begin{bmatrix} \sin(\omega_0 \cdot i) & \cos(\omega_0 \cdot i) &  \cdots & \sin(\omega_{d_{\text{model}}/2-1} \cdot i) & \cos(\omega_{d_{\text{model}}/2-1} \cdot i) \end{bmatrix}^T \tag{6.1.3}
$$
角频率$\omega_m$​随维度$m$呈指数级变化。对于低维度（小的$m$），$\omega_m$较小，对应长周期（Low Frequency）；对于高维度（大的$m$），$\omega_m$​较大，对应短周期（High Frequency）。这种设计使得低维度能够编码位置的粗粒度信息（整体位置、远距离关系），高维度能够编码位置的细粒度信息（精确位置、近距离关系）。

位置编码需要与输入嵌入相结合，共同作为注意力层的输入。设输入嵌入矩阵为$X \in \mathbb{R}^{n \times d_{\text{model}}}$，其中$X_{i}$​表示位置$i$的内容嵌入向量。位置编码矩阵为$P \in \mathbb{R}^{n \times d_{\text{model}}}$，其中$P_{i}$​表示位置$i$的位置编码向量。两者通过逐元素相加（Element-wise Addition）结合：
$$
X_{\text{pos}} = X + P \tag{6.1.4}
$$
其中，$X_{\text{pos}} \in \mathbb{R}^{n \times d_{\text{model}}}$是带有位置信息的输入表示，可作为后续注意力层的输入。这种结合方式的数学意义在于：位置编码被"注入"到内容嵌入中，随后的Query、Key、Value投影会同时考虑内容信息和位置信息。

选择逐元素相加而非拼接（Concatenation）的原因有以下几点。首先，从维度角度，相加保持了输入维度不变，无需额外的投影层来调整维度。其次，从信息融合角度，相加是一种线性融合方式，保留了内容编码和位置编码的相对重要性，由后续的线性变换自动学习。第三，从梯度流动角度，相加操作允许梯度同时流回内容编码和位置编码，两者都能得到有效的更新。

值得注意的是，位置编码的生成是确定性的，不参与梯度更新。在原始Transformer的实现中，位置编码在模型初始化时生成，并在整个训练过程中保持固定。这种设计使得位置编码成为模型的"先验知识"，而非可学习的参数。后续的研究表明，可学习的位置编码在某些场景下能够取得更好的性能，但正弦余弦编码因其简洁性和良好的外推能力仍被广泛使用。

正弦余弦位置编码中的各个参数都经过精心设计，背后蕴含着深刻的数学直觉和实践经验。理解这些参数的选择动机对于深入掌握编码原理至关重要。

基准频率$10000$的选择是一个关键的设计决策。这个值的选择基于以下考量：第一，$10000$足够大，使得对于合理的序列长度（如$n \leq 10000$），最高频率$\omega_{d_{\text{model}}/2-1}$​对应的周期仍然大于序列长度，不会出现"折叠"现象。第二，$10000$的对数（$\ln 10000 \approx 9.21$）足够"平滑"，使得不同维度之间的频率跨度合理，既能覆盖足够多的不同尺度，又不会过于密集或稀疏。第三，这个值是通过实验调优确定的，在机器翻译等基准任务上表现良好。

维度配对的设计（正弦与余弦成对出现）也有明确的数学动机。正弦和余弦是相位相差$\pi/2$的同一频率振动，它们的组合可以表示任意相位的同频率振动。从线性代数的角度，正弦和余弦构成了一组正交基，可以唯一表示该频率分量的任意相位。更重要的是，正弦和余弦的组合使得编码具有"可逆性"：给定编码向量，可以唯一恢复位置信息。

指数频率分布$\omega_m = 10000^{-2m/d_{\text{model}}}$​的几何直觉是：不同维度对应不同"尺度"的位置信息。低维度（$m$接近0）对应低频振动，周期很长，能够编码位置的整体结构；高维度（$m$接近$d_{\text{model}}/2-1$）对应高频振动，周期很短，能够精确区分相近的位置。这种多尺度的编码类似于小波变换（Wavelet Transform）的思想，能够同时捕获全局和局部的位置特征。

### 6.1.2 编码的唯一性与可辨识性

位置编码的基本要求是不同位置具有不同的编码向量。数学上，我们需要证明：对于任意两个不同的位置$i$和$j$（$i \neq j$），对应的编码向量$p_i$​和$p_j$​是不同的，即$p_i \neq p_j$。

假设$p_i = p_j$​，即对于所有维度$k \in \{0, 1, \ldots, d_{\text{model}}-1\}$，有$p_{i,k} = p_{j,k}$​。考虑维度$2m$和$2m+1$的配对：
$$
\begin{align}
\sin\left(\frac{i}{10000^{2m/d_{\text{model}}}}\right) = \sin\left(\frac{j}{10000^{2m/d_{\text{model}}}}\right), \\ \quad \cos\left(\frac{i}{10000^{2m/d_{\text{model}}}}\right) = \cos\left(\frac{j}{10000^{2m/d_{\text{model}}}}\right) \tag{6.1.5}
\end{align}
$$
正弦函数和余弦函数在区间$[0, \pi/2]$上是单射的。因此，由正弦等式可得：
$$
\frac{i}{10000^{2m/d_{\text{model}}}} = \frac{j}{10000^{2m/d_{\text{model}}}} \quad \text{or} \quad \frac{i}{10000^{2m/d_{\text{model}}}} = \pi - \frac{j}{10000^{2m/d_{\text{model}}}} \tag{6.1.6}
$$
第一种情况直接给出$i=j$，与假设矛盾。第二种情况给出：
$$
\frac{i + j}{10000^{2m/d_{\text{model}}}} = \pi \tag{6.1.7}
$$
由于$m$可以取任意值（从0到$d_{\text{model}}/2-1$），左边可以是任意小的数（当$m$大时），而$\pi$是固定的常数。因此，等式不可能对所有$m$同时成立，矛盾。

更简单的不等式分析：对于任意$i \neq j$，存在某个维度$m$使得相位差$\Delta\theta = \frac{|i - j|}{10000^{2m/d_{\text{model}}}}$​不等于$2k\pi$（$k$为整数）。此时，正弦和余弦的值必然不同。由于编码包含多个维度（$d_{\text{model}}$个），总能在至少一个维度上区分任意两个不同的位置。

位置编码之间的相似度（通常通过点积或余弦相似度度量）与位置之间的距离存在特定的数学关系。这种关系是位置编码能够表达位置信息的核心。
考虑位置$i$和位置$j$的编码向量$p_i$​和$p_j$​。它们的点积为：
$$
p_i \cdot p_j = \sum_{m=0}^{d_{\text{model}}/2-1} \left[ \sin\left(\frac{i}{\omega_m}\right)\sin\left(\frac{j}{\omega_m}\right) + \cos\left(\frac{i}{\omega_m}\right)\cos\left(\frac{j}{\omega_m}\right) \right] \tag{6.1.8}
$$
其中，$\omega_m = 10000^{2m/d_{\text{model}}}$。利用三角恒等式$\cos(a - b) = \cos a \cos b + \sin a \sin b$，可以简化为：
$$
p_i \cdot p_j = \sum_{m=0}^{d_{\text{model}}/2-1} \cos\left(\frac{i - j}{\omega_m}\right) \tag{6.1.9}
$$
这个结果具有深刻的数学意义：位置编码的点积只依赖于位置差$\Delta = i - j$，与绝对位置$i$和$j$无关。这意味着位置编码隐式地编码了相对位置信息，位置$i$和位置$j$之间的相似度只取决于它们的距离，而非它们在序列中的具体位置。

当位置差$\Delta$较小时（相邻或相近的位置），所有频率成分的相位差都较小，$\cos(\Delta/\omega_m)$都接近1，因此点积较大。当位置差$\Delta$较大时（相距很远的位置），低频成分的相位差仍然较小（$\cos$接近1），但高频成分的相位差可能达到或超过$\pi$，$⁡\cos$可能为负或较小。因此，点积总体上随位置距离增加而减小，呈现出某种"距离衰减"的趋势。

从几何角度来看，编码向量可以视为高维空间中的点。相近位置的编码向量在高维空间中相距较近，远距离位置的编码向量相距较远。这种几何结构使得注意力机制能够自然地学习到与位置距离相关的注意力模式，距离近的位置倾向于有更高的注意力权重。

正弦余弦位置编码在高维空间中形成特定的几何结构。理解这种几何结构有助于理解位置编码如何影响注意力机制的行为。

考虑位置编码向量的范数。对于任意位置$i$：
$$
\|p_i\|_2^2 = \sum_{k=0}^{d_{\text{model}}-1} p_{i,k}^2 = \sum_{m=0}^{d_{\text{model}}/2-1} \left[ \sin^2\left(\frac{i}{\omega_m}\right) + \cos^2\left(\frac{i}{\omega_m}\right) \right] = \sum_{m=0}^{d_{\text{model}}/2-1} 1 = \frac{d_{\text{model}}}{2} \tag{6.1.10}
$$
因此，所有位置编码向量的范数都相等，$\|p_i\|_2 = \sqrt{d_{\text{model}}/2}$。这意味着编码向量都位于高维球面上，是一个等范数向量集合。

从线性代数的角度，编码向量集合$\{p_1, p_2, \ldots, p_n\}$张成一个$n$维的子空间（如果$n \leq d_{\text{model}}$）或$d_{\text{model}}$​维的空间（如果$n > d_{\text{model}}$​）。这些向量之间存在特定的角度关系，由公式（6.1.9）给出的点积决定。

编码向量的分布具有某种"均匀性"：随着位置$i$的增加，编码向量在球面上均匀分布。这是因为正弦和余弦函数的周期性导致编码向量不会总是聚集在球面的某个区域，而是遍历整个球面。这种均匀分布确保了不同位置之间的区分性，同时也使得位置信息能够在后续的线性变换中被有效利用。

### 6.1.3 与注意力分数的交互分析
位置编码通过影响Query和Key的计算来间接影响注意力权重。设输入嵌入为$X$，位置编码为$P$，则带有位置信息的输入表示为$X_{\text{pos}} = X + P$。Query、Key、Value的计算分别为：
$$
Q = (X + P)W_Q, \quad K = (X + P)W_K, \quad V = (X + P)W_V \tag{6.1.11}
$$
其中，$W_Q, W_K, W_V$​是可学习的投影矩阵。注意力分数矩阵为：
$$
S = \frac{QK^T}{\sqrt{d_k}} = \frac{(X + P)W_Q W_K^T (X + P)^T}{\sqrt{d_k}} \tag{6.1.12}
$$
展开后得到四项：
$$
S = \frac{1}{\sqrt{d_k}} \left( XW_Q W_K^T X^T + XW_Q W_K^T P^T + PW_Q W_K^T X^T + PW_Q W_K^T P^T \right) \tag{6.1.13}
$$
各项的物理意义如下：第一项$XW_Q W_K^T X^T$是原始内容之间的注意力分数，与位置无关，编码语义相似性。第二项$XW_Q W_K^T P^T$是内容与位置之间的交叉项，编码"什么内容出现在什么位置"的信息。第三项$PW_Q W_K^T X^T$是位置与内容之间的交叉项，与第二项转置相关。第四项$PW_Q W_K^T P^T$是纯粹的位置-位置交互，编码位置之间的几何关系。

注意力权重矩阵是Softmax作用于$S$的结果。位置编码通过上述四项影响最终的注意力权重分布，使得模型能够学习内容相关的依赖（第一项）、位置相关的依赖（第四项）、以及内容与位置的交互（第二、三项）。

位置编码$P$通过投影矩阵$W_Q$和$W_K$​进入注意力计算，这个过程可以理解为位置信息的"投影"和"融合"。设位置投影为：
$$
Q_P = PW_Q, \quad K_P = PW_K \tag{6.1.14}
$$
则注意力分数可以重新写为：
$$
S_{ij} = \frac{1}{\sqrt{d_k}} \left( q_i^{X} \cdot k_j^{X} + q_i^{X} \cdot k_j^{P} + q_i^{P} \cdot k_j^{X} + q_i^{P} \cdot k_j^{P} \right) \tag{6.1.15}
$$
其中，$q_i^{X} = X_i W_Q$是位置$i$的内容Query，$q_i^{P} = P_i W_Q$是位置$i$的位置Query，$k_j^{X} = X_j W_K$是位置$j$的内容Key，$k_j^{P} = P_j W_K$​是位置$j$的位置Key。位置投影$Q_P$​和$K_P$​可以预先计算（因为$P$是固定的），也可以与内容投影同时计算。从优化的角度，位置信息和内容信息共享投影矩阵$W_Q$和$W_K$​，使得模型能够学习位置与内容之间的复杂交互关系。

位置投影的一个重要性质是：它不随输入序列的内容变化而变化，只与位置本身有关。这意味着对于相同的输入序列（相同的内容嵌入），不同的位置编码会导致不同的注意力分布；从另一个角度，对于不同的输入序列，相同位置的位置投影是相同的。这种"位置独立性"使得位置信息能够被有效学习和泛化。

在解码器（Decoder）的自注意力层中，需要使用因果掩码（Causal Mask）来防止位置$i$关注位置$j>i$的信息，确保自回归生成时的"因果性"。掩码操作与位置编码的结合有两种常见方式。

第一种是掩码作用于注意力分数。对于位置$i$，掩码将位置$j>i$的注意力分数设为$-\infty$，使得Softmax后的注意力权重为0：
$$
S_{ij}^{\text{masked}} = \begin{cases} S_{ij} & \text{if } j \leq i \\ -\infty & \text{if } j > i \end{cases} \tag{6.1.16}
$$
位置编码本身不参与掩码操作，其值保持不变。这种方式下，位置编码仍然提供了绝对位置信息，即使某些位置被掩码遮挡。

第二种是掩码同时作用于位置编码。在某些实现中，被掩码的位置的位置编码被设为0或忽略。这种方式进一步确保了被掩码位置的信息完全不会泄露。

因果位置编码（Casual Positional Encoding）是针对解码器设计的位置编码变体。在标准的正弦余弦编码中，位置编码是双向的，位置$i$的编码与位置$j$的编码是对称的。因果位置编码通过某种方式打破这种对称性，使得位置$i$只能"看到"位置$\leq i$的信息。这可以通过修改正弦编码的定义（使用非对称的频率或相位）或使用可学习的因果位置编码来实现。

### 6.1.4 数值示例与可视化

为了更直观地理解正弦余弦位置编码，我们给出具体的数值示例。设嵌入维度$d_{\text{model}} = 8$（为简化展示，维度较小），基准频率为10000，则各维度对应的频率参数$\omega_m$为：

对于$m=0$：$\omega_0 = 10000^{0/4} = 1$，$\sin(i)$和$\cos(i)$  
对于$m=1$：$\omega_1 = 10000^{2/4} = 100$，$\sin(i/100)$和$\cos(i/100)$  
对于$m=2$：$\omega_2 = 10000^{4/4} = 10000$，$\sin(i/10000)$和$\cos(i/10000)$  
对于$m=3$：$\omega_3 = 10000^{6/4} = 1000000$，$\sin(i/1000000)$和$\cos(i/1000000)$

考虑位置$i = 0, 1, 2$的编码向量：

位置0：$p_0 = [0, 1, 0, 1, 0, 1, 0, 1]$
位置1：$p_1 = [\sin(1), \cos(1), \sin(0.01), \cos(0.01), \sin(0.0001), \cos(0.0001), \sin(0.000001), \cos(0.000001)]$
位置2：$p_2 = [\sin(2), \cos(2), \sin(0.02), \cos(0.02), \sin(0.0002), \cos(0.0002), \sin(0.000002), \cos(0.000002)]$

从这个示例可以看出：对于低维度（$m=0$），位置0的编码为$(0,1)$，位置1的编码为$(\sin(1), \cos(1)) \approx (0.84, 0.54)$，位置2的编码为$(\sin(2), \cos(2)) \approx (0.91, -0.42)$，这些值随位置变化显著。对于高维度（$m=2,3$），由于除以了很大的数，相位变化很小，编码接近$(0, 1)$（即位置0的编码）。

这种数值分布反映了多尺度编码的特性：低维度捕获位置的粗粒度变化，高维度捕获位置的精细变化。在实际应用中，维度通常较大（如$d_{\text{model}} = 512$或4096），会有更多不同尺度的频率成分。

根据公式（6.1.9），位置编码的点积只依赖于位置差$\Delta = i - j$。我们通过数值计算来验证这一性质。

设$d_{\text{model}} = 8$，计算不同位置对的点积：

位置0与位置1的点积：$$
\begin{align}
p_0 \cdot p_1 &= \sum_{m=0}^{3} \cos(1/\omega_m) \\& \approx \cos(1) + \cos(0.01) + \cos(0.0001) + \cos(0.000001) \\& \approx 0.54 + 1.00 + 1.00 + 1.00 = 3.54
\tag{6.1.17}
\end{align}
$$
位置0与位置2的点积：
$$
\begin{align}
p_0 \cdot p_2 & = \sum_{m=0}^{3} \cos(2/\omega_m) \\& \approx \cos(2) + \cos(0.02) + \cos(0.0002) + \cos(0.000002) \\& \approx -0.42 + 1.00 + 1.00 + 1.00 = 2.58
\tag{6.1.18}\end{align}
$$位置1与位置2的点积：$$
\begin{align}
p_1 \cdot p_2 & = \sum_{m=0}^{3} \cos(1/\omega_m) \\& \approx \cos(1) + \cos(0.01) + \cos(0.0001) + \cos(0.000001) \\& \approx 0.54 + 1.00 + 1.00 + 1.00 = 3.54
\tag{6.1.19}\end{align}
$$

注意位置1与位置2的点积和位置0与位置1的点积相等，因为它们的位置差都是1！这个数值验证了公式（6.1.9）的结论：点积只依赖于相对位置$\Delta = i - j$。

进一步观察：当位置差$\Delta$增加时，点积呈现下降趋势（从位置差0时的$p_i \cdot p_i = d_{\text{model}}/2 = 4$，到位置差1时的约3.54，再到位置差2时的约2.58）。这种下降在低维度（高频成分）中更为明显，在高维度（低频成分）中较为平缓。从几何角度，正弦余弦位置编码的高维空间结构可以通过降维可视化来理解。考虑将$d_{\text{model}}$维的编码向量投影到二维平面进行可视化。一种常用的可视化方法是将编码向量视为时间序列，绘制各维度随位置变化的曲线。对于低维度（$m$接近0），正弦和余弦曲线随位置变化剧烈；对于高维度（$m$接近$d_{\text{model}}/2-1$），曲线几乎为常数。这种多尺度的波动模式是位置编码的核心特征。另一种可视化方法是使用主成分分析（PCA）将编码向量投影到前两个主成分。观察发现：编码向量在二维投影空间中形成螺旋或圆形分布，随着位置增加，编码点沿螺旋轨迹移动。这种分布反映了编码向量在高维球面上的均匀性，每个位置对应球面上的一个点，相邻位置的点在球面上相邻。还有一种可视化方法是热力图展示。绘制编码矩阵（位置为行、维度为列）的热力图，可以清晰地看到：低维度区域呈现高频波动，高维度区域呈现低频波动；相邻位置在低维度区域的热力图模式相似度较低，在高维度区域的热力图模式相似度较高。

这些可视化有助于建立对位置编码几何结构的直观理解，也为分析注意力机制如何利用位置信息提供了视觉基础。

### 6.1.5 正弦余弦编码与其他编码的比较

正弦余弦位置编码与可学习位置编码（Learned Positional Encoding）是两种主要的编码范式，它们在数学性质和实际表现上有显著差异。

从参数化角度，正弦余弦编码是确定性的、固定的形式，参数由数学公式确定，不需要训练。可学习位置编码将位置编码视为可学习的参数矩阵$P \in \mathbb{R}^{n_{\max} \times d}$，通过反向传播优化得到。从表达能力角度，可学习编码可以学习任意复杂的位置表示模式，理论上比正弦编码更灵活。

从泛化能力角度，正弦编码的优势在于可以处理任意长度的序列（因为公式定义在所有整数上），具有天然的"长度外推"能力。可学习编码只能处理训练时见过的长度（$⁡n \leq n_{\max}$），对于超出$n_{\max}$​的序列，需要特殊处理（如截断或微调）。

从参数效率角度，正弦编码没有可训练参数，参数效率为100%。可学习编码需要$O(n_{\max} \cdot d)$的参数量，当$⁡n_{\max}$​很大时这是一笔不小的开销。从归纳偏置角度，正弦编码隐含了"位置可以通过不同频率的正弦波表示"的先验，可学习编码没有这种先验。

实验结果表明，正弦编码和可学习编码在不同任务和场景下各有优势。对于需要长度外推的任务（如长文本生成），正弦编码或其变体（如RoPE）通常表现更好。对于任务特定的场景，可学习编码可能取得更好的性能。

正弦余弦编码是一种绝对位置编码（Absolute Positional Encoding），因为它为每个绝对位置$i$生成一个独立的编码向量。相对位置编码（Relative Positional Encoding）则不显式编码绝对位置，而是直接修改注意力计算，使注意力分数依赖于Query和Key之间的相对位置。

绝对位置编码的数学形式为：$p_i = f(i)$，其中$f$是位置到向量的映射。相对位置编码的数学形式为：$S_{ij} = g(q_i, k_j, i - j)$，即注意力分数显式依赖于相对位置$i−j$。

绝对位置编码的优点：实现简单，与标准注意力框架兼容；可以处理任意位置的查询。绝对位置编码的缺点：难以表达相对位置信息（虽然隐含在编码中，但不直接）；外推时可能不稳定。

相对位置编码的优点：显式编码相对位置，与位置距离的直觉一致；通常具有更好的外推性能。相对位置编码的缺点：实现相对复杂，需要修改注意力计算；计算开销可能更大。

RoPE（旋转位置编码）可以视为绝对位置编码和相对位置编码的桥梁：它通过绝对位置编码的形式（每个位置有唯一的编码）实现了相对位置不变性（注意力分数只依赖于相对位置）。

### 6.1.6 本节小结

本节系统地介绍了正弦余弦位置编码的数学定义、推导和性质。我们首先从位置编码的动机出发，解释了为什么纯注意力机制需要位置编码来感知序列顺序。然后详细推导了正弦余弦编码的数学公式，分析了各个参数（基准频率、维度配对、频率分布）的设计原理和数学直觉。

我们深入分析了位置编码的唯一性，证明了不同位置具有不同的编码向量，以及编码向量的点积与相对位置的数学关系。这种"相对位置可表达性"是位置编码能够有效支持位置相关建模的关键。

我们进一步分析了位置编码如何与注意力计算交互：位置编码通过投影进入Query和Key的计算，贡献内容-内容、内容-位置、位置-位置三类交互项，使得模型能够学习多种类型的依赖关系。

通过具体的数值示例，我们展示了编码向量的数值分布、点积与位置差的关系，以及编码空间的几何结构。最后，我们将正弦余弦编码与可学习编码、绝对位置编码与相对位置编码进行了比较，分析了各自的优缺点和适用场景。

正弦余弦位置编码以其简洁的数学形式、良好的理论性质和天然的长度外推能力，成为Transformer架构中的经典设计。理解其数学原理对于深入掌握位置编码的本质、设计新的编码方案、以及优化大语言模型的性能都具有重要意义。下一节，我们将从频率空间的角度进一步分析位置编码的理论基础。