## 6.2 可学习位置编码：参数化偏移与子空间

与固定形式的正弦余弦位置编码不同，可学习位置编码将位置编码视为可训练的参数，通过反向传播自动从数据中学习最优的位置表示。这种方法的核心思想是与其手工设计位置编码的形式，不如让模型自己发现最适合任务的编码方式。可学习位置编码在Transformer架构的早期研究和某些特定任务上取得了良好的性能，是位置编码研究中的重要范式之一。从数学角度来看，可学习位置编码涉及参数化偏移的性质分析，理解其表达能力、秩结构以及与固定编码的本质差异，对于合理应用这一编码范式具有重要意义。

### 6.2.1 可学习位置编码的参数化形式

可学习位置编码将位置编码矩阵视为可训练的参数。设序列的最大长度为$n_{\max}$，嵌入维度为$d_{\text{model}}$，则可学习位置编码矩阵定义为：

$$
P \in \mathbb{R}^{n_{\max} \times d_{\text{model}}}, \quad P = \begin{bmatrix} p_1^T \\ p_2^T \\ \vdots \\ p_{n_{\max}}^T \end{bmatrix} \tag{6.2.1}
$$

其中，每个$p_i \in \mathbb{R}^{d_{\text{model}}}$是位置$i$的编码向量，$n_{\max}$是预设的最大序列长度。编码矩阵$P$中的所有元素都是可学习的参数，在训练过程中通过梯度下降进行优化。这种参数化形式与正弦余弦编码的确定性公式形成鲜明对比：可学习编码没有固定的数学表达式，其具体数值完全由训练数据决定，模型会学习到最能支持任务目标的位置表示。

从参数化偏移的角度来看，可学习位置编码可以被视为对每个位置学习一个独立的偏移向量。与正弦余弦编码中由数学公式决定的相位不同，可学习编码的"相位"是完全自由的。设位置$i$的内容嵌入为$x_i$，则带有位置编码的输入表示为$x_i + p_i$，其中$p_i$是位置$i$学到的偏移量。这种加性组合方式与正弦余弦编码相同，但位置偏移量$p_i$是从数据中学习而非预先定义的。

可学习位置编码的参数量为$n_{\max} \times d_{\text{model}}$。例如，当$n_{\max} = 4096$，$d_{\text{model}} = 512$时，位置编码的参数量约为$2 \times 10^6$个，这是一个相当可观的参数量。这种直接的参数化方式提供了极大的灵活性，但同时也带来了参数效率的问题。尽管有如此多的参数，编码矩阵的有效自由度受到维度的限制。

### 6.2.2 位置嵌入矩阵的秩与表示能力

位置编码矩阵$P$的秩是影响其性质的关键参数。秩$\text{rank}(P)$衡量了编码向量的线性无关程度，秩越高，编码向量越"分散"，表达能力越强；秩越低，编码向量越"相关"，表达能力越弱。编码矩阵秩的上界为$\text{rank}(P) \leq \min(n_{\max}, d_{\text{model}})$，由于通常$n_{\max} \gg d_{\text{model}}$，实际的上界为$\text{rank}(P) \leq d_{\text{model}}$。

考虑编码矩阵的奇异值分解：$P = U \Sigma V^T$，其中$U \in \mathbb{R}^{n_{\max} \times n_{\max}}$、$\Sigma \in \mathbb{R}^{n_{\max} \times d_{\text{model}}}$、$V \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$。非零奇异值的数量等于矩阵的秩$r \leq d_{\text{model}}$。设非零奇异值为$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$，则编码矩阵可以表示为：

$$
P = \sum_{k=1}^{r} \sigma_k u_k v_k^T \tag{6.2.2}
$$

其中，$u_k$是$U$的前$r$列，$v_k$是$V$的前$r$列。这个分解表明，可学习位置编码的有效表示可以用$r$个秩-1矩阵的和来描述，参数量约为$2r \cdot d_{\text{model}} + r \approx O(r \cdot d_{\text{model}})$，远小于原始的$n_{\max} \cdot d_{\text{model}}$。

设编码矩阵的秩为$r$，则存在$r$个正交基向量$v_1, v_2, \ldots, v_r \in \mathbb{R}^{d_{\text{model}}}$，使得每个位置编码可以表示为：

$$
p_i = \sum_{k=1}^{r} \alpha_{i,k} v_k \tag{6.2.3}
$$

其中，$\alpha_{i,k}$是位置$i$在基$v_k$上的坐标。因此，可学习编码的函数空间是维度不超过$r$的线性子空间上的所有函数。当$r = d_{\text{model}}$（满秩情况）时，编码向量可以张成整个$\mathbb{R}^{d_{\text{model}}}$空间，表达能力最强。当$r < d_{\text{model}}$时，编码被限制在某个低维子空间中，表达能力受限。

可学习位置编码的表达能力虽然灵活，但也有理论上界。编码维度$d_{\text{model}}$是表达能力的硬上限，编码向量的维度决定了它最多能够区分$n_{\max}$个位置（当编码向量线性无关时）。编码的有效表达能力还受到训练数据分布的影响，如果训练数据中的位置分布不均衡，模型可能无法学习到某些位置的可靠编码。

### 6.2.3 外推能力差的数学原因

可学习位置编码在外推时面临根本性的困难。设训练时使用的最大位置为$n_{\text{train}}$，推理时需要处理的位置为$n_{\text{test}} > n_{\text{train}}$。可学习位置编码在外推时面临的挑战是位置$n_{\text{test}}$的编码$p_{n_{\text{test}}}$在训练时从未见过，模型需要"想象"这个位置应该有什么样的编码。

从参数化的角度来看，可学习位置编码是一个查表操作：$p_i = P[i, :]$，其中$P$是学习到的参数矩阵。这个映射在训练时只对$i \leq n_{\text{train}}$有定义，对于$i > n_{\text{train}}$，参数矩阵没有对应的行。模型只能使用某种插值或外推策略来估计未见位置的编码。

外推失败的根本原因在于可学习编码缺乏数学结构的约束。正弦余弦编码的公式定义在所有整数上，对于任意位置$i$，编码由$f(i)$唯一定义，不存在"未见"的问题。而可学习编码的每个位置编码都是独立的参数，没有内在的连续性或结构。相邻位置$i$和$i+1$的编码$p_i$和$p_{i+1}$之间没有数学上的必然联系，它们可能完全不同。

从秩的角度分析，如果编码矩阵$P_{1:n_{\text{train}}}$的秩较低，则新位置的编码可以通过已学习位置的线性组合来构造。例如，如果$P$的秩为$r$，则$p_{n_{\text{test}}}$可以表示为前$r$个基向量的线性组合。然而，这种基于秩的外推假设了位置编码的平滑性——相邻位置的编码应该相似。如果训练数据不支持这种平滑性假设，外推效果会较差。

实验观察表明，可学习位置编码通常收敛到某种"平滑"的配置，相邻位置的编码向量相似，与正弦余弦编码的平滑性质一致。这表明优化过程倾向于学习平滑的位置表示。然而，这种平滑性是学习的结果而非结构的保证，不能保证在所有情况下都成立。

### 6.2.4 与绝对位置编码的本质差异

可学习位置编码与正弦余弦编码在数学性质上存在本质差异，这些差异可以从多个角度分析。

从参数化角度，正弦余弦编码是确定性的、公式化的，没有可训练参数；可学习编码是数据驱动的、有可训练参数的。参数化的灵活性是可学习编码的主要优势，也是其主要挑战的来源。正弦编码的参数由数学公式确定，具有良好的数学性质（如相对位置不变性、平滑性、周期性等）；可学习编码的参数由训练数据决定，其性质取决于优化过程。

从函数空间角度，正弦余弦编码的函数空间是固定的（由特定频率的正弦余弦函数张成），可学习编码的函数空间由训练数据决定。正弦编码的函数空间具有明确的结构（频率分解），不同频率成分正交且独立；可学习编码的函数空间更加灵活但也更加模糊，基函数的选择取决于训练数据。

从泛化角度，正弦编码天然支持长度外推，因为公式定义在所有整数上；可学习编码的外推能力取决于学习到的编码结构，如果编码是平滑的则外推较好，否则可能失败。正弦编码的外推能力来源于其数学公式的连续性，可学习编码的外推能力来源于学习的正则化效果。

从信息论角度，两种编码编码位置信息的机制不同。正弦余弦编码通过多个不同频率的正弦波叠加唯一表示每个位置，不同位置在不同频率成分上有不同的相位，这种表示是完备且确定的。可学习编码通过查表方式表示位置，每个位置有一个独立的编码向量，这种表示依赖于训练数据覆盖。

在短序列场景下（训练和测试序列长度相近），可学习位置编码通常与正弦余弦编码表现相当，有时略优。这是因为可学习编码可以适应训练数据的特定分布，学习到对任务最有帮助的位置表示。在长序列场景下（测试序列长度显著超过训练序列长度），正弦余弦编码通常表现更好，因为正弦编码天然支持外推，而可学习编码可能在外推时表现不佳。

### 6.2.5 本节小结

本节从数学角度系统分析了可学习位置编码的参数化形式、秩与表达能力、外推能力的局限性，以及与正弦余弦编码的本质差异。可学习位置编码将位置编码视为可训练的偏移向量，通过反向传播从数据中学习最优的位置表示。其编码矩阵的秩不超过嵌入维度$d_{\text{model}}$，决定了有效表达能力。外推能力差的原因在于可学习编码缺乏内在的数学结构，每个位置的编码是独立参数，无法自然扩展到未见位置。与正弦余弦编码相比，可学习编码在灵活性上有优势，但在泛化性和外推能力上存在不足。选择哪种编码范式应根据具体任务需求和约束条件决定。