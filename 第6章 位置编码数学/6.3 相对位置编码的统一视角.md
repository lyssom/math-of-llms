## 6.3 相对位置编码的统一视角

在深入探讨了正弦余弦位置编码和可学习位置编码之后，我们有必要从更宏观的角度审视位置编码的本质目的。位置编码的根本目标是什么？为什么标准的注意力机制无法感知位置信息？相对位置编码为何成为现代大语言模型的主流选择？本节将从信息编码和注意力计算两个层面，建立理解相对位置编码的统一理论框架，揭示位置信息如何通过影响Query和Key的内积来塑造注意力行为。

相对位置编码的统一视角不仅能够帮助我们理解现有的各种位置编码方案（如RoPE、ALiBi等），还能够指导我们在实际应用中选择和设计合适的位置编码策略。通过建立位置编码与注意力分数计算之间的数学联系，我们将看到位置信息是如何被"编织"进Transformer的计算图中的。

### 6.3.1 为什么注意力机制需要相对位置信息

标准的多头自注意力机制在形式上具有一个重要的数学性质：置换不变性（Permutation Invariance）。设输入序列为$x_1, x_2, \ldots, x_n$，对应的嵌入向量为$v_1, v_2, \ldots, v_n$。注意力输出为：

$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V \tag{6.3.1}
$$

其中，$Q = XW_Q$，$K = XW_K$，$V = XW_V$，$X = [v_1^T; v_2^T; \ldots; v_n^T]$是输入嵌入矩阵。

考虑对输入序列进行置换$\pi$，得到新序列$\{v_{\pi(1)}, v_{\pi(2)}, \ldots, v_{\pi(n)}\}$。置换后的注意力计算为：

$$
\text{Attention}' = \text{Softmax}\left(\frac{Q'K'^T}{\sqrt{d_k}}\right) V' \tag{6.3.2}
$$

其中，$Q'$、$K'$、$V'$是由置换后的嵌入计算得到的。由于矩阵乘法$QK^T$只依赖于嵌入向量的内容和投影矩阵，置换输入序列等价于对$Q$、$K$、$V$的对应行进行置换，最终的注意力输出也会按照相同的置换进行变换。这意味着注意力机制无法区分输入序列的不同排列——"狗咬人"和"人咬狗"将产生完全相同的注意力输出，尽管它们的语义截然不同。

置换不变性对于位置无关的任务（如集合预测、集合分类）可能是优点，但对于大多数序列建模任务却是致命的缺陷。自然语言和代码具有强烈的顺序性，词序承载着关键的语法和语义信息。"I love you"和"You love I"的词汇完全相同，但只有前者是正确的英语表达。类似地，在编程语言中，语句的顺序直接决定了程序的语义。注意力机制需要位置信息来打破置换不变性，从而能够感知和利用序列的顺序结构。

位置编码的引入正是为了解决这一根本性问题。通过向输入嵌入添加位置相关的编码向量，位置信息被注入到Query和Key的计算中，进而影响注意力权重的分布。位置编码的核心目标是使注意力机制能够区分不同位置的词元，并根据位置关系（相邻、远离、相对方向等）调整信息聚合的方式。

相对位置与绝对位置是编码位置信息的两种不同范式。绝对位置编码为每个绝对位置$i$生成一个唯一的编码向量$p_i$，目标是使模型能够识别"当前位置是序列中的第几个位置"。相对位置编码不显式编码绝对位置，而是直接修改注意力计算，使得注意力权重依赖于Query和Key之间的相对位置$\Delta = j - i$，目标是使模型能够识别"两个位置之间的距离和方向"。从信息论的角度，绝对位置编码回答的是"我在哪里"的问题，相对位置编码回答的是"你离我有多远"的问题。

绝对位置编码在某些场景下存在明显的局限性。首先，绝对位置编码难以泛化到训练时未见过的位置，模型无法为超出训练范围的序列长度生成合理的编码。其次，绝对位置编码没有显式建模位置之间的关系，相邻位置之间的编码变化可能不连续，导致模型难以学习平滑的位置依赖模式。第三，绝对位置编码引入的位置偏差可能与内容信息相互干扰，在某些情况下反而降低模型性能。

相对位置编码通过直接建模位置之间的关系，克服了绝对位置编码的这些局限性。相对位置编码的核心优势在于：第一，平移不变性，注意力分数只依赖于相对位置而非绝对位置，使得模型能够学习与位置无关的模式；第二，更好的外推能力，相对位置关系可以在任意长度的序列上定义，不受训练序列长度的限制；第三，更符合语言直觉，许多语言依赖关系与相对位置相关（如主语通常在谓语之前），相对位置编码能够更自然地捕捉这些依赖。

### 6.3.2 相对位移不变性的数学原理

相对位移不变性（Relative Displacement Invariance）是相对位置编码的核心数学性质。理解这一性质需要从注意力分数的分解入手。考虑位置$i$和位置$j$之间的注意力分数：

$$
S_{ij} = \frac{q_i \cdot k_j}{\sqrt{d_k}} \tag{6.3.3}
$$

其中，$q_i = (x_i + p_i)W_Q$是位置$i$的Query向量，$k_j = (x_j + p_j)W_K$是位置$j$的Key向量。展开后得到：

$$
\begin{align*}
& S_{ij} =\\& \frac{1}{\sqrt{d_k}} \left( x_i W_Q W_K^T x_j^T + x_i W_Q W_K^T p_j^T + p_i W_Q W_K^T x_j^T + p_i W_Q W_K^T p_j^T \right) \tag{6.3.4}
\end{align*}
$$

四项分别代表：内容-内容交互、内容-位置交互、位置-内容交互、位置-位置交互。在绝对位置编码下，这四项都依赖于绝对位置$i$和$j$。相对位置编码的目标是重新组织位置信息的编码方式，使得注意力分数只依赖于相对位置$\Delta = j - i$。

考虑位置编码的简化模型：设位置编码为位置的线性函数，即$p_i = R \cdot i$，其中$R \in \mathbb{R}^{d_{\text{model}} \times 1}$是一个方向向量。这种"线性位置编码"虽然过于简化，但可以帮助我们理解相对位置编码的基本思想。将$p_i$和$p_j$代入注意力分数：
$$
\begin{align*}
S_{ij}
&= \frac{1}{\sqrt{d_k}}
\Bigl(
    x_i W_Q W_K^\top x_j^\top
  + x_i W_Q W_K^\top (R j)^\top \\
&\qquad
  + (R i) W_Q W_K^\top x_j^\top
  + (R i) W_Q W_K^\top (R j)^\top
\Bigr)
\tag{6.3.5}
\end{align*}
$$
仔细分析各项：第三项和第二项可以合并为与$i \cdot j$和$i + j$相关的项。相对位置编码的设计目标是消除对绝对位置的显式依赖，使$S_{ij}$只依赖于$\Delta = j - i$。

相对位移不变性的形式化定义如下。设注意力分数矩阵为$S \in \mathbb{R}^{n \times n}$，如果存在一个函数$f$使得$S_{ij} = f(q_i, k_j, j - i)$，则称注意力机制具有相对位移不变性。这意味着给定相同的Query向量、Key向量和相对位置，无论这两个位置在序列中的绝对位置如何，注意力分数都是相同的。

从群论的角度理解，相对位移不变性对应于序列平移群（Translation Group）在注意力计算中的作用。设平移算子$T_\tau$将位置$i$映射到$i + \tau$，则相对位移不变性要求：

$$
S_{ij} = S_{i+\tau, j+\tau} \quad \forall \tau \in \mathbb{Z} \tag{6.3.6}
$$

这个性质表明，注意力模式是平移等变的——将整个序列平移后，注意力矩阵也进行相同的平移。相对位移不变性使得模型能够学习与位置无关的依赖模式，相同的语法结构（如主语-谓语关系）在序列的不同位置能够被一致地识别。

RoPE（旋转位置编码）是实现相对位移不变性的经典方法。RoPE的核心思想是将Query和Key向量表示为复平面上的旋转，通过内积的旋转不变性来实现相对位置编码。设位置$i$的嵌入向量为$v_i$，RoPE首先对$v_i$应用旋转矩阵：

$$
\text{RoPE}(v_i, \theta_m) = \begin{bmatrix} \cos(i\theta_m) & -\sin(i\theta_m) \\ \sin(i\theta_m) & \cos(i\theta_m) \end{bmatrix} \begin{bmatrix} v_{i, 2m} \\ v_{i, 2m+1} \end{bmatrix} \tag{6.3.7}
$$

其中，$\theta_m = 10000^{-2m/d_{\text{model}}}$是第$m$个频率的旋转角度。考虑两个位置$i$和$j$的Query和Key向量应用RoPE后的内积：

$$
\begin{align*}
&\langle \text{RoPE}(q_i), \text{RoPE}(k_j) \rangle = \\& \sum_{m=0}^{d_k/2-1} q_{i,m} \cos(i\theta_m) k_{j,m} \cos(j\theta_m) + q_{i,m} \sin(i\theta_m) k_{j,m} \sin(j\theta_m) \tag{6.3.8}
\end{align*}
$$

使用三角恒等式简化后可以发现，内积只依赖于$(i - j)\theta_m$，即相对位置。RoPE的内积不变性为：

$$
\langle \text{RoPE}(q_i), \text{RoPE}(k_j) \rangle = \langle \text{RoPE}(q_0), \text{RoPE}(k_{j-i}) \rangle \tag{6.3.9}
$$

这个性质表明，经过RoPE变换后，Query和Key的内积只依赖于它们的相对位置，实现了相对位移不变性。RoPE的优雅之处在于它通过几何变换（旋转）实现了位置编码，同时保持了注意力的标准计算形式。

### 6.3.3 位置编码如何影响$QK^T$

位置编码通过影响Query和Key的计算来间接影响注意力分数矩阵$QK^T$。理解这一影响机制是设计高效位置编码的关键。设输入嵌入为$X$，位置编码为$P$，则带有位置信息的输入表示为$X_{\text{pos}} = X + P$。Query和Key的计算分别为：

$$
Q = (X + P)W_Q, \quad K = (X + P)W_K \tag{6.3.10}
$$

注意力分数矩阵为：

$$
S = \frac{QK^T}{\sqrt{d_k}} = \frac{(X + P)W_Q W_K^T (X + P)^T}{\sqrt{d_k}} \tag{6.3.11}
$$

展开后得到四项：

$$
\begin{align*}
S
&= \frac{1}{\sqrt{d_k}}
\Bigl(
    X W_Q W_K^\top X^\top
  + X W_Q W_K^\top P^\top \\
&\qquad
  + P W_Q W_K^\top X^\top
  + P W_Q W_K^\top P^\top
\Bigr)
\tag{6.3.12}
\end{align*}
$$

第一项$XW_Q W_K^T X^T$是原始内容之间的注意力分数，与位置无关，编码语义相似性。第二项$XW_Q W_K^T P^T$是内容与位置之间的交叉项，编码"什么内容出现在什么位置"的信息。第三项$PW_Q W_K^T X^T$是位置与内容之间的交叉项。第四项$PW_Q W_K^T P^T$是纯粹的位置-位置交互，编码位置之间的几何关系。

在相对位置编码的设计中，位置编码的引入方式更加精细。以RoPE为例，位置信息不是简单地与内容嵌入相加，而是通过旋转操作注入。设$q_i$是位置$i$的内容Query向量，RoPE变换后的Query为$\tilde{q}_i = \text{RoPE}(q_i)$。类似地，$\tilde{k}_j = \text{RoPE}(k_j)$。注意力分数为：

$$
\begin{align*}
& S_{ij} = \frac{\tilde{q}_i \cdot \tilde{k}_j}{\sqrt{d_k}} = \\& \frac{q_i \cdot k_j + \sum_{m} q_{i,m} k_{j,m} \cos((i-j)\theta_m) \text{交叉项}}{\sqrt{d_k}} \tag{6.3.13}
\end{align*}
$$

位置编码对$QK^T$的影响可以从两个角度分析。从注意力权重的角度，位置编码使得注意力权重不再是纯粹由内容相似度决定的，而是同时考虑了位置关系。这使得模型能够学习到"在特定相对位置的内容更相关"的模式。例如，动词通常与相邻的名词相关，模型可以通过学习这种位置依赖的注意力模式来更好地捕获语法结构。

从信息流的角度，位置编码为注意力机制提供了"方向感"。在纯内容驱动的注意力中，信息流动的方向是不确定的，模型难以区分"之前"和"之后"。位置编码打破了这种对称性，使得注意力机制能够区分信息流动的方向，从而更准确地建模序列中的依赖关系。

位置编码对$QK^T$的影响还体现在注意力模式的多样性上。实验观察表明，Transformer中的不同头表现出不同的注意力模式：某些头专注于局部依赖（相邻位置的交互），某些头专注于长距离依赖，某些头专注于特定类型的语法关系。这种多样化的注意力模式是位置编码与内容信息共同作用的结果。

相对位置编码的一个关键优势是它能够保持注意力计算的效率。设位置编码矩阵为$P \in \mathbb{R}^{n \times d}$，在相对位置编码下，注意力分数的计算仍然可以表示为$QK^T$的形式，只是$Q$和$K$的计算方式有所改变。具体而言，相对位置编码通常将位置信息编码在$Q$和$K$的特定维度上，使得位置相关的项能够与内容项自然地结合。

### 6.3.4 本节小结

本节从统一视角分析了相对位置编码的理论基础。我们首先解释了为什么注意力机制需要位置信息：标准注意力具有置换不变性，无法感知序列的顺序，而位置编码通过向输入嵌入添加位置相关信息来打破这种不变性。相对位置编码相比绝对位置编码具有平移不变性、更好的外推能力和更符合语言直觉的优势。

我们详细推导了相对位移不变性的数学原理，展示了如何通过重新组织位置信息的编码方式使得注意力分数只依赖于相对位置。RoPE通过旋转操作实现了内积的相对位置不变性，是相对位置编码的经典实现。

我们深入分析了位置编码如何通过影响Query和Key的计算来塑造注意力行为。位置编码为注意力机制注入位置信息，使得注意力权重不仅由内容相似度决定，还考虑了位置关系。这种位置感使得Transformer能够准确建模序列中的语法和语义依赖，是其强大表达能力的重要来源。