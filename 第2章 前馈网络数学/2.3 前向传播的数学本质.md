前向传播（Forward Propagation）是神经网络信息流动的基本方式，它描述了输入数据如何逐层变换，最终产生网络输出的过程。从数学角度来看，前向传播本质上是一个复合函数的求值过程：输入数据依次通过每一层的仿射变换和非线性激活，最终映射到输出空间。理解前向传播的数学本质，不仅有助于我们把握神经网络的工作原理，更能为分析网络的表达能力、训练动态和泛化性能奠定理论基础。本节将从复合函数、链式法则、空间变换等多个数学视角，深入剖析前向传播的内在机制。

## 2.3.1 引言：从计算到函数逼近

在2.1节和2.2节中，我们已经从计算单元和结构组织的角度介绍了神经网络的数学描述。前向传播是这些数学描述在实际运行时的具体执行过程。然而，前向传播的意义远不止于"计算"本身——它本质上实现了从输入空间到输出空间的函数映射。当我们设计一个神经网络架构并训练其参数时，我们实际上是在寻找一个能够拟合目标函数的数学表达。

设有一个$L$层的前馈神经网络，其参数集合为$\Theta = \{\mathbf{W}^{(1)}, \mathbf{b}^{(1)}, \ldots, \mathbf{W}^{(L)}, \mathbf{b}^{(L)}\}$。给定输入$\mathbf{x} \in \mathbb{R}^{n_0}$​，网络通过前向传播计算输出$\hat{\mathbf{y}} = f_{\Theta}(\mathbf{x})$，其中$f_{\Theta}: \mathbb{R}^{n_0} \to \mathbb{R}^{n_L}$是由网络参数定义的函数映射。前向传播的核心数学问题可以概括为：给定网络结构和输入，如何高效地计算这个复合函数的输出。

这个问题看似简单——只需要按顺序执行各层的矩阵运算即可。但深入理解前向传播的数学本质，我们会发现它蕴含着丰富的理论内涵：它揭示了神经网络如何通过层次化的非线性变换逼近复杂的函数关系，展现了深度表示学习的数学基础，并为理解网络的表达能力和计算特性提供了理论工具。

## 2.3.2 复合函数视角下的前向传播

前向传播最本质的数学描述是复合函数（Composite Function）的逐层求值。每一层网络定义了一个从其输入空间到输出空间的映射，多个层的组合则构成了这些映射的复合。

**定义2.3.1（层映射）**：设第$\ell$层的输入为$\mathbf{h}^{(\ell-1)} \in \mathbb{R}^{n_{\ell-1}}$，输出为$\mathbf{h}^{(\ell)} \in \mathbb{R}^{n_\ell}$​。该层定义的映射$f^{(\ell)}: \mathbb{R}^{n_{\ell-1}} \to \mathbb{R}^{n_\ell}$​为：
$$
f^{(\ell)}(\mathbf{h}) = \phi^{(\ell)}\left(\mathbf{W}^{(\ell)}\mathbf{h} + \mathbf{b}^{(\ell)}\right)\tag{2.3.1}
$$
其中$\mathbf{W}^{(\ell)} \in \mathbb{R}^{n_\ell \times n_{\ell-1}}$为权重矩阵，$\mathbf{b}^{(\ell)} \in \mathbb{R}^{n_\ell}$​为偏置向量，$\phi^{(\ell)}: \mathbb{R} \to \mathbb{R}$为激活函数（按元素应用）。

**定义2.3.2（复合函数）**：设$f: A \to B$和$g: B \to C$为两个映射，它们的复合$g \circ f: A \to C$定义为：
$$
(g \circ f)(x) = g(f(x)), \quad \forall x \in A\tag{2.3.2}
$$
**定理2.3.1（神经网络作为复合函数）**：$L$层前馈网络定义的函数$f_{\Theta}$​是各层映射的复合：
$$
f_{\Theta} = f^{(L)} \circ f^{(L-1)} \circ \cdots \circ f^{(1)}\tag{2.3.3}
$$
对于任意输入$\mathbf{x}$，有：
$$
f_{\Theta}(\mathbf{x}) = f^{(L)}\left(f^{(L-1)}\left(\cdots f^{(1)}(\mathbf{x}) \cdots\right)\right)\tag{2.3.4}
$$
证明：直接由前向传播的计算顺序可得。设$\mathbf{h}^{(0)} = \mathbf{x}$，根据层映射的定义，$\mathbf{h}^{(1)} = f^{(1)}(\mathbf{h}^{(0)}) = f^{(1)}(\mathbf{x})$。递推地，$\mathbf{h}^{(2)} = f^{(2)}(\mathbf{h}^{(1)}) = f^{(2)}(f^{(1)}(\mathbf{x}))$，依此类推，直到$\mathbf{h}^{(L)} = f^{(L)}(f^{(L-1)}(\cdots f^{(1)}(\mathbf{x}) \cdots))$。这正是复合函数的定义。

这个定理揭示了前向传播的数学本质：它是一个复合函数的逐层求值过程。网络的学习目标——调整参数$\Theta$以使$f_{\Theta}(\mathbf{x})$逼近目标函数——可以理解为寻找能够最好地逼近目标映射的复合函数形式。

**定理2.3.2（复合函数的梯度——链式法则）**：设$y = f(g(x))$，其中$f$和$g$均为可微函数。则$y$对$x$的导数为：
$$
\frac{dy}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}\tag{2.3.5}​
$$
或等价地，使用莱布尼茨记号表示为：
$$
\frac{dy}{dx} = \frac{dy}{dz} \cdot \frac{dz}{dx}, \quad \text{其中 } z = g(x), y = f(z)\tag{2.3.6}
$$
证明：这是微积分中的基本定理。由导数的定义：
$$
\frac{dy}{dx} = \lim_{\Delta x \to 0} \frac{f(g(x + \Delta x)) - f(g(x))}{\Delta x}\tag{2.3.7}​
$$
利用微分中值定理，存在$\xi$介于$g(x)$和$g(x + \Delta x)$之间，使得：
$$
f(g(x + \Delta x)) - f(g(x)) = f'(\xi) \cdot (g(x + \Delta x) - g(x))\tag{2.3.8}
$$
当$\Delta x \to 0$时，$\xi \to g(x)$，故：
$$
\lim_{\Delta x \to 0} \frac{f(g(x + \Delta x)) - f(g(x))}{\Delta x} = f'(g(x)) \cdot g'(x)\tag{2.3.9}
$$
即$\frac{dy}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}$​。

链式法则在反向传播算法中扮演着核心角色，它使得我们能够高效地计算复合函数对底层变量的梯度。在第2.4节中，我们将看到如何利用链式法则推导出反向传播的梯度计算公式。

**定义2.3.3（雅可比矩阵）**：对于映射$\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$，其雅可比矩阵（Jacobian Matrix）$\mathbf{J}_{\mathbf{f}} \in \mathbb{R}^{m \times n}$定义为：
$$
\mathbf{J}_{\mathbf{f}} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \frac{\partial f_m}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix}\tag{2.3.10}​​​​
$$
**定理2.3.3（复合函数的雅可比矩阵）**：设$\mathbf{y} = \mathbf{f}(\mathbf{z})$，$\mathbf{z} = \mathbf{g}(\mathbf{x})$，则$\mathbf{y}$对$\mathbf{x}$的雅可比矩阵为：
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial \mathbf{y}}{\partial \mathbf{z}} \cdot \frac{\partial \mathbf{z}}{\partial \mathbf{x}}\tag{2.3.11}​
$$
其中矩阵乘法的顺序不可交换。

证明：根据矩阵微积分的链式法则，$(\mathbf{y})_i = f_i(\mathbf{z}(\mathbf{x}))$，故：
$$
\frac{\partial (\mathbf{y})_i}{\partial (\mathbf{x})_j} = \sum_k \frac{\partial f_i}{\partial z_k} \cdot \frac{\partial z_k}{\partial x_j}\tag{2.3.12}​​
$$
这正是矩阵乘法的定义，即$\left[\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right]_{ij} = \left[\frac{\partial \mathbf{y}}{\partial \mathbf{z}} \cdot \frac{\partial \mathbf{z}}{\partial \mathbf{x}}\right]_{ij}$。

## 2.3.3 信息流动的几何描述

前向传播不仅是一个代数计算过程，更是一个几何变换过程。从几何角度看，每一层网络执行了一次从输入空间到输出空间的变换，这些变换的复合最终将输入数据映射到所需的输出空间。理解这一几何本质对于分析神经网络的表达能力和特征学习机制至关重要。

**定义2.3.4（仿射变换的几何意义）**：仿射变换$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$在几何上等价于线性变换（由$\mathbf{W}$描述）和平移变换（由$\mathbf{b}$描述）的复合。线性变换$\mathbf{W}$可以分解为旋转、缩放和剪切等基本变换的组合。

**定理2.3.4（线性变换的奇异值分解）**：任意矩阵$\mathbf{W} \in \mathbb{R}^{m \times n}$可以分解为：
$$
\mathbf{W} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^T\tag{2.3.13}
$$
其中$\mathbf{U} \in \mathbb{R}^{m \times m}$和$\mathbf{V} \in \mathbb{R}^{n \times n}$是正交矩阵，$\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$是对角矩阵（其对角元素为非负实数，称为奇异值）。

证明：这是线性代数中的基本定理（奇异值分解定理）。矩阵$\mathbf{W}^T\mathbf{W}$是半正定对称矩阵，可以正交对角化为$\mathbf{V}\mathbf{\Sigma}^2\mathbf{V}^T$，其中$\mathbf{V}$的列向量是$\mathbf{W}^T\mathbf{W}$的特征向量，奇异值为$\mathbf{\Sigma}$对角线上$\mathbf{W}^T\mathbf{W}$特征值的平方根。则$\mathbf{U} = \mathbf{W}\mathbf{V}\mathbf{\Sigma}^+$（$\mathbf{\Sigma}^+$是$\mathbf{\Sigma}$的伪逆）可验证为正交矩阵，且满足分解式。

奇异值分解揭示了线性变换的几何本质：$\mathbf{V}^T$执行旋转（将输入向量旋转到奇异向量方向），$\mathbf{\Sigma}$执行缩放（按奇异值拉伸或压缩各方向），$\mathbf{U}$执行另一个旋转（将结果旋转到输出空间）。因此，一个仿射变换$\mathbf{W}\mathbf{x} + \mathbf{b}$可以理解为：先将输入向量旋转到"主轴方向"，然后在各主轴方向上进行缩放和平移，最后旋转回原坐标系。

**定义2.3.5（激活函数的几何作用）**：激活函数$\phi(\cdot)$在几何上对仿射变换的结果执行了逐元素的非线性"切割"或"折叠"操作。以ReLU激活函数为例，$\phi(z) = \max(0, z)$将输入空间按超平面$\{ \mathbf{z} \mid z = 0 \}$划分为两个区域：$z > 0$的区域保持不变，$z < 0$的区域被压缩到零点。

**定理2.3.5（ReLU激活的几何效果）**：ReLU激活函数将$\mathbb{R}^n$空间按$n$个坐标轴分割成$2^n$个象限（orthant）。在每个象限内，ReLU网络表现为一个线性函数。

证明：ReLU对每个坐标独立操作：$h_i = \max(0, z_i)$。对于固定的符号模式$s_i = \text{sign}(z_i) \in \{-1, 1\}$（当$z_i = 0$时可归入任一模式），有$z_i = s_i |z_i|$，且：
$$
h_i = \begin{cases} z_i & \text{if } s_i = 1 \\ 0 & \text{if } s_i = -1 \end{cases}\tag{2.3.14}​
$$
因此，对于固定的符号模式，网络输出$\mathbf{h}$是输入$\mathbf{z}$的线性函数。由于$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$本身是线性的，故$\mathbf{h}$也是$\mathbf{x}$的线性函数。

这个定理揭示了ReLU网络的一个重要性质：尽管整体上是非线性的，但网络在局部区域（由激活模式定义）上表现为线性函数。这与样条函数（Spline Function）的分段线性逼近有相似之处，只是ReLU网络的"分片"边界是由数据决定的，而非预先指定的。

**定义2.3.6（深度网络的特征空间层次）**：在$L$层网络中，信息经过$L$次非线性变换后到达输出层。每一层的输出可以视为输入数据在该层定义的"特征空间"中的表示。设$\mathbf{h}^{(\ell)}$为第$\ell$层的输出，则$\mathbf{h}^{(\ell)}$可以理解为$\mathbf{x}$在$\mathbb{R}^{n_\ell}$​空间中的表示。

**定理2.3.6（表示空间的层次结构）**：深度网络的表示空间具有层次化的结构：较低层（靠近输入）的表示捕获数据的局部、低级特征（如边缘、纹理），较高层（靠近输出）的表示编码全局、高级语义特征（如物体类别、语句含义）。

证明：这一结论虽然难以给出严格的数学证明，但可以从信息论和函数逼近的角度理解。仿射变换$\mathbf{W}\mathbf{x} + \mathbf{b}$本质上是对输入$\mathbf{x}$的线性投影，不同的权重矩阵定义了不同的投影方向。浅层网络的线性组合较为简单，只能捕获输入的线性可分特征。随着层数增加，非线性激活使得网络能够学习越来越复杂的特征组合，从而捕获更抽象的语义信息。这一观点与深度学习中的"特征层次假说"相一致。

![[learn_msg.drawio.png]]

## 2.3.4 计算图的表示与实现

从前向传播的数学描述到实际代码实现，需要一种中间表示来桥接理论模型和计算程序。计算图（Computational Graph）是这种中间表示的标准形式，它将数学表达式显式地表示为节点和边的有向无环图。


**定义2.3.7（计算图）**：计算图是一个有向无环图（DAG），其中：节点（Node）表示数学运算（如矩阵乘法、加法、激活函数）；边（Edge）表示数据（如张量、标量）的流动；输入节点表示网络的输入数据；输出节点表示网络的最终输出。

**例2.3.1（单层网络的计算图）**：考虑单层全连接网络$\mathbf{h} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})$。其计算图包含以下节点：输入节点$\mathbf{x}$、权重节点$\mathbf{W}$、偏置节点$\mathbf{b}$；矩阵乘法节点$\mathbf{z}_1 = \mathbf{W}\mathbf{x}$；加法节点$\mathbf{z} = \mathbf{z}_1 + \mathbf{b}$；激活节点$\mathbf{h} = \phi(\mathbf{z})$。

**定理2.3.7（计算图的前向遍历）**：前向传播对应于计算图的拓扑排序（Topological Order）遍历：按从输入到输出的顺序依次计算各节点的值。
![[comp_graph.drawio.png]]
证明：计算图是有向无环图，节点的值只依赖于其前驱节点。拓扑排序保证每个节点在其所有前驱节点之后、其后继节点之前被处理。因此，按拓扑排序遍历可以保证计算每个节点时，其所有依赖值已经就绪。 

**定义2.3.8（自动微分）**：自动微分（Automatic Differentiation）是一种利用计算图高效计算导数的技术。与数值微分（利用有限差分近似）和符号微分（利用代数规则求导）不同，自动微分利用链式法则和计算图的结构，可以精确地计算任意复杂函数的导数。

自动微分分为前向模式（Forward Mode）和反向模式（Reverse Mode）两种。反向模式自动微分是深度学习中反向传播算法的理论基础，我们将在第2.4节详细讨论。前向模式自动微分虽然计算效率较低，但在某些场景（如Jacobian矩阵计算）中有其独特优势。

**定理2.3.8（前向模式的微分计算）**：在前向模式自动微分中，我们伴随值（Adjoint Value）$\bar{x}$表示$\frac{\partial f}{\partial x}$​，并按与前向传播相同的顺序计算。

证明：考虑函数$y = f(g(x))$。设$x$的伴随值为$\bar{x}$，中间变量$z=g(x)$的伴随值为$\bar{z}$。根据链式法则$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial x}$​，前向模式计算$\bar{x} = \bar{z} \cdot \frac{\partial g}{\partial x}$​。对于复合函数，这一过程可以与前向传播并行进行。

**定理2.3.9（反向模式的微分计算）**：在反向模式自动微分中，我们计算每个节点对其直接后继的梯度，并按与前向传播相反的顺序传播这些梯度。

证明：反向模式的核心是链式法则$\frac{\partial y}{\partial x} = \sum_{z \in \text{descendants}(x)} \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial x}$​。设$\bar{z} = \frac{\partial y}{\partial z}$​为后向传播的中间值，则$x$的梯度为$\bar{x} = \sum_{z} \bar{z} \cdot \frac{\partial z}{\partial x}$​。按后向顺序计算可以保证每次计算时所需的依赖值已经就绪。

**表2-2：前向模式与反向模式自动微分的比较**

| 特性     | 前向模式            | 反向模式            |
| ------ | --------------- | --------------- |
| 计算顺序   | 与前向传播相同         | 与前向传播相反         |
| 时间复杂度  | $O(n)$（$n$为输入数） | $O(m)$（$m$为输出数） |
| 空间复杂度  | $O(1)$（额外）      | $O(n)$（存储中间值）   |
| 适用场景   | 雅可比矩阵向量积        | 损失函数梯度计算        |
| 深度学习应用 | 有限              | 反向传播的核心         |

## 2.3.5 前向传播的计算复杂度

理解前向传播的计算复杂度对于评估网络效率、设计模型架构和优化计算资源至关重要。

**定义2.3.9（浮点运算数FLOPs）**：浮点运算数（Floating Point Operations）是衡量计算复杂度的标准指标。一次乘法或一次加法计为一次FLOPs。

**定理2.3.10（单层前向传播的FLOPs）**：对于输入$\mathbf{X} \in \mathbb{R}^{B \times n_{in}}$​、权重$\mathbf{W} \in \mathbb{R}^{n_{in} \times n_{out}}$、偏置$\mathbf{b} \in \mathbb{R}^{n_{out}}$的全连接层，单次前向传播的FLOPs为：
$$
\text{FLOPs}_{\text{layer}} = B \cdot (2n_{in}n_{out} + n_{out})\tag{2.3.15}
$$
其中$2n_{in}n_{out}$来自矩阵乘法（每个输出元素需要$n_{in}$次乘法和$n_{in}$​次加法），$n_{out}$​来自偏置加法。

证明：考虑单个输出元素$z_{ij}$​（第$i$个样本的第$j$个输出）：
$$
z_{ij} = \sum_{k=1}^{n_{in}} x_{ik} w_{kj} + b_j\tag{2.3.16}​
$$
这个求和需要$n_{in}$次乘法和$n_{in}-1$次加法，总计约$2n_{in}$​次FLOPs。$B$个样本、$n_{out}$​个输出元素的总FLOPs为$B \cdot n_{out} \cdot 2n_{in} + B \cdot n_{out}$​（偏置加法），即$B \cdot (2n_{in}n_{out} + n_{out})$。

**推论2.3.1（多层网络的FLOPs）**：对于$L$层全连接网络，总FLOPs为各层FLOPs之和：
$$
\text{FLOPs}_{\text{total}} = \sum_{\ell=1}^{L} B \cdot (2n_{\ell-1}n_\ell + n_\ell) = O\left(B \sum_{\ell=1}^{L} n_{\ell-1}n_\ell\right)\tag{2.3.17}
$$
**例2.3.2（GPT规模模型的FLOPs估算）**：考虑一个简化的Transformer层，包含注意力机制和前馈网络。设隐藏维度$d=4096$，注意力头数为32（每头维度$d_k = d_v = 64$），前馈扩展维度4d=16384。单层Transformer的FLOPs约为：

- 注意力计算：$O(B \cdot L \cdot d^2)$（$L$为序列长度）
- 前馈网络：$O(B \cdot L \cdot d^2)$

对于GPT-3规模的模型（$L=96$层，$d=12288$），单次前向传播的FLOPs约为$10^{23}$量级，需要大量GPU并行计算才能在合理时间内完成。

## 2.3.6 前向传播的并行性

现代GPU和TPU等硬件加速器擅长并行计算。前向传播的矩阵运算形式天然具有良好的并行性，可以充分利用硬件的并行计算能力。

**定理2.3.11（样本级并行）**：对于批量输入$\mathbf{X} \in \mathbb{R}^{B \times n_{in}}$，不同样本之间的计算是相互独立的，可以完全并行执行。

证明：考虑全连接层的计算$\mathbf{Z} = \mathbf{X}\mathbf{W}$。$\mathbf{Z}$的第$i$行仅依赖于$\mathbf{X}$的第$i$行和$\mathbf{W}$，与其他行无关。因此，$B$个样本的计算可以分解为$B$个独立的子计算。

**定理2.3.12（层内并行）**：在同一层内，不同输出神经元（同一batch内）的计算也是并行的。

证明：$\mathbf{Z} = \mathbf{X}\mathbf{W}$的第$j$列仅依赖于$\mathbf{W}$的第$j$列和$\mathbf{X}$的所有列。不同列之间的计算不相互依赖，可以并行执行。

**定理2.3.13（模型并行）**：对于超大规模模型，可以将权重矩阵$\mathbf{W}$按列（或行）分割到多个计算设备上，实现模型并行。

证明：将$\mathbf{W} = [\mathbf{W}_1, \mathbf{W}_2]$按列分割，则$\mathbf{X}\mathbf{W} = [\mathbf{X}\mathbf{W}_1, \mathbf{X}\mathbf{W}_2]$。每个子矩阵$\mathbf{W}_i$​可以分配给不同的设备，计算$\mathbf{X}\mathbf{W}_i$​后合并结果。这种分割不会影响计算的数学正确性。

**表4-3：前向传播的并行性层次**

|并行层次|并行粒度|硬件实现|
|---|---|---|
|样本级并行|Batch内的样本|GPU线程块/流多处理器|
|神经元级并行|层内的输出神经元|GPU线程/Warp|
|矩阵运算级并行|矩阵乘法Tile|GPU共享内存/寄存器|
|模型级并行|模型的不同层/部分|多GPU/多节点|

## 2.3.7 本节小结

本节从多个数学视角深入分析了前向传播的本质。前向传播在数学上是一个复合函数的逐层求值过程，它将输入数据依次通过$L$层非线性变换，最终映射到输出空间。本节的核心内容可以概括为以下几点：

第一，前向传播的本质是复合函数的求值。神经网络可以表示为各层映射的复合$f_{\Theta} = f^{(L)} \circ \cdots \circ f^{(1)}$，链式法则是分析这一复合过程的核心数学工具。

第二，前向传播实现了从输入空间到输出空间的几何变换。仿射变换负责旋转、缩放和平移，激活函数负责非线性切割，两者的组合使得网络能够学习复杂的非线性决策边界。

第三，计算图是连接数学理论和工程实现的桥梁。通过构建计算图，可以系统地组织前向传播的计算步骤，并为反向传播的梯度计算提供数据结构支持。

第四，前向传播具有良好的并行性，可以从样本级、神经元级、矩阵运算级和模型级等多个层次进行并行优化。这种并行性是深度学习在大规模数据和模型上高效训练的基础。

理解前向传播的数学本质，为我们进一步学习反向传播算法（第2.4节）和其他深度学习高级主题奠定了坚实的理论基础。下一节我们将讨论损失函数与优化目标，为理解反向传播提供必要的背景知识。