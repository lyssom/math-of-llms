# 第2章 前馈网络数学
## 2.1 神经元的数学模型

神经网络的数学基础建立在对生物神经元的抽象与简化之上。1943年，沃伦·麦肯罗皮层和沃尔特·皮茨提出了人工神经元的数学模型，标志着计算神经科学和人工智能的诞生。本节将从数学建模的角度，系统阐述神经元从生物原型到数学抽象的演化过程，深入分析单层神经元的数学结构。

### 2.1.1 从生物神经元到数学抽象

生物神经元由细胞体、树突、轴突和突触等部分组成。树突接收信号，细胞体整合处理，轴突传递信号。麦肯罗皮层神经元（McCulloch-Pitts Neuron）是最早的数学模型：

**定义2.1.1（麦肯罗皮层神经元模型）**：
$$
\mathbf{y} = \phi\left(\sum_{i=1}^{n} w_i x_i - \theta\right)\tag{2.1.1}
$$
其中$x_i \in \{0, 1\}$为输入信号，$w_i \in \mathbb{R}$为连接权重，$\theta \in \mathbb{R}$为激活阈值，$\phi(\cdot)$为阶跃激活函数。

阶跃激活函数的数学定义为：
$$
\phi(z) = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}\tag{2.1.2}
$$

**定理2.1.1（麦肯罗皮层神经元的表达能力）**：单个麦肯罗皮层神经元可以实现所有的基本逻辑运算：AND、OR、NOT。

由于任意布尔函数都可以由AND、OR、NOT组合表示，单个神经元可以模拟任意单变量布尔函数。这个定理表明，尽管单个神经元结构极其简单，但它已经具备了一定的逻辑推理能力。麦肯罗皮层神经元实际上是线性分类器，将输入空间划分为两个半空间。

### 2.1.2 感知机与仿射变换

感知机是麦肯罗皮层神经元的重要推广，引入了参数学习机制和连续可微的激活函数。

**定义2.1.2（感知机模型）**：设$\mathbf{x} = [x_1, x_2, \ldots, x_n]^T \in \mathbb{R}^n$为输入向量，$\mathbf{w} = [w_1, w_2, \ldots, w_n]^T \in \mathbb{R}^n$为权重向量，$b \in \mathbb{R}$为偏置项。感知机的数学定义为：
$$
\mathbf{z} = \mathbf{w}^T \mathbf{x} + b, \quad \mathbf{y} = \phi(\mathbf{z})\tag{2.1.3}
$$
其中$\phi: \mathbb{R} \to \mathbb{R}$为激活函数，$\mathbf{z}$称为净输入。

表达式$\mathbf{w}^T \mathbf{x} + b$是线性变换与平移的组合，在几何上称为仿射变换（Affine Transformation）。

**定义2.1.3（仿射变换）**：从$\mathbb{R}^n$到$\mathbb{R}^m$的仿射变换定义为：
$$
\mathbf{f}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}\tag{2.1.4}
$$
其中$\mathbf{W} \in \mathbb{R}^{m \times n}$为变换矩阵，$\mathbf{b} \in \mathbb{R}^m$为平移向量。当$\mathbf{b} = \mathbf{0}$时，仿射变换退化为线性变换。

**定义2.1.4（齐次坐标表示）**：将输入向量$\mathbf{x} \in \mathbb{R}^n$扩展为齐次坐标$\tilde{\mathbf{x}} = [x_1, x_2, \ldots, x_n, 1]^T \in \mathbb{R}^{n+1}$，将权重矩阵和偏置合并为扩展权重矩阵：
$$
\tilde{\mathbf{W}} = \begin{bmatrix} \mathbf{W} & \mathbf{b} \end{bmatrix} \in \mathbb{R}^{m \times (n+1)}\tag{2.1.5}
$$
则仿射变换可以统一表示为$\mathbf{z} = \tilde{\mathbf{W}} \tilde{\mathbf{x}}$。齐次坐标表示将仿射变换转化为单一的矩阵乘法运算。

### 2.1.3 激活函数的数学角色

仿射变换$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$本身是线性变换，无论堆叠多少层，复合后的结果仍然是线性变换。激活函数通过引入非线性变换，打破了神经网络的线性瓶颈。

**定理2.1.2（多层线性网络的恒等性）**：设有$L$层的前馈网络，每一层仅进行仿射变换而不应用激活函数，则整个网络等价于一个单一的仿射变换：
$$
\mathbf{h}^{(L)} = \mathbf{W}' \mathbf{h}^{(0)} + \mathbf{b}'\tag{2.1.6}
$$
其中$\mathbf{W}' = \mathbf{W}^{(L)} \mathbf{W}^{(L-1)} \cdots \mathbf{W}^{(1)}$，$\mathbf{b}' = \mathbf{b}^{(L)} + \mathbf{W}^{(L)}\mathbf{b}^{(L-1)} + \cdots$。

这个定理揭示了没有激活函数的神经网络无论多深，其表达能力等价于单层线性变换。激活函数通过引入非线性打破了这一限制。

**定义2.1.5（常用激活函数）**：

（1）Sigmoid函数：$\sigma(z) = \frac{1}{1 + e^{-z}}$

（2）双曲正切函数：$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1$

（3）修正线性单元：$\text{ReLU}(z) = \max(0, z)$

（4）GELU激活函数：$\text{GELU}(z) = z \cdot \Phi(z) = \frac{z}{2} \left[1 + \text{erf}\left(\frac{z}{\sqrt{2}}\right)\right]$

激活函数的设计需要满足：非线性、连续可微、数值稳定性、计算效率。

### 2.1.4 神经元的几何解释

从几何视角来看，单个神经元执行的是一个超平面分类（或回归）的操作。

**定义2.1.6（决策边界）**：对于二分类问题，使用sigmoid激活函数的神经元$\mathbf{y} = \sigma(\mathbf{w}^T \mathbf{x} + b)$的决策边界定义为：
$$
\{\mathbf{x} \mid \mathbf{w}^T \mathbf{x} + b = 0\}\tag{2.1.7}
$$
这是一个超平面，将输入空间划分为两个区域。

**定理2.1.3（神经元的几何分离能力）**：单个神经元（感知机）只能解决线性可分（Linearly Separable）的问题。对于线性不可分的数据集（如异或问题），单个神经元无法找到正确的分类边界。

考虑异或（XOR）问题：点$(0,0)$和$(1,1)$属于类别0，点$(0,1)$和$(1,0)$属于类别1。任何超平面$\mathbf{w}^T \mathbf{x} + b = 0$都只能将平面划分为两个半平面，无法将类别0的点放在一侧而将类别1的点放在另一侧。

![[fig_CH02_neuron_boundary.png]]

**定义2.1.7（神经元的特征空间映射）**：从特征学习的角度来看，神经元$\mathbf{y} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})$可以理解为将输入向量$\mathbf{x}$从原始输入空间$\mathbb{R}^n$映射到新的特征空间$\mathbb{R}^m$的变换。这个变换包含两个步骤：首先通过仿射变换$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$进行线性投影，然后通过激活函数$\phi(\cdot)$进行非线性变换。

### 2.1.5 概率视角下的神经元模型

神经元模型不仅可以从几何角度理解，还可以从概率论的角度进行分析。

**定义2.1.8（伯努利分布与sigmoid激活）**：考虑二分类问题，设$y \in \{0, 1\}$为类别标签。假设给定输入$\mathbf{x}$时，标签$y$服从伯努利分布：
$$
P(y = 1 \mid \mathbf{x}) = p, \quad P(y = 0 \mid \mathbf{x}) = 1 - p\tag{2.1.8}
$$
sigmoid函数可以将神经元的净输入映射为概率值：
$$
p = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}\tag{2.1.9}
$$

**定理2.1.4（sigmoid神经元与伯努利分布的对应）**：sigmoid神经元的输出可以解释为给定输入$\mathbf{x}$时，类别为1的条件概率$P(y = 1 \mid \mathbf{x})$。当$\mathbf{z} \to \infty$时，$\sigma(\mathbf{z}) \to 1$；当$\mathbf{z} \to -\infty$时，$\sigma(\mathbf{z}) \to 0$。

**定义2.1.9（softmax神经元与多项分布）**：对于多分类问题，Softmax函数将$K$个神经元的输出归一化为概率分布：
$$
P(y = k \mid \mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}, \quad \text{where } \mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}\tag{2.1.10}
$$

**定理2.1.5（softmax的数学性质）**：

（1）输出值均为正且和为1：$\sum_{k=1}^{K} \text{softmax}(\mathbf{z})_k = 1$

（2）平移不变性：对任意常数$c$，有$\text{softmax}(\mathbf{z} + c\mathbf{1}) = \text{softmax}(\mathbf{z})$

（3）单调性：若$z_i > z_j$，则$\text{softmax}(\mathbf{z})_i > \text{softmax}(\mathbf{z})_j$

### 2.1.6 神经元模型的矩阵表示

在实际应用中，我们通常需要同时计算多个神经元的输出，这可以通过矩阵运算高效实现。

**定义2.1.10（层操作）**：设输入矩阵$\mathbf{X} \in \mathbb{R}^{N \times d}$包含$N$个样本，设有$m$个神经元的层，其权重矩阵为$\mathbf{W} \in \mathbb{R}^{d \times m}$，偏置向量为$\mathbf{b} \in \mathbb{R}^{m}$。则该层的输出为：
$$
\mathbf{Z} = \mathbf{X}\mathbf{W} + \mathbf{1}_N \mathbf{b}^T, \quad \mathbf{Y} = \phi(\mathbf{Z})\tag{2.1.11}
$$

**定理2.1.6（矩阵运算与批量处理的等价性）**：上述矩阵运算等价于对每个样本单独进行神经元计算后的结果堆叠。

批量处理（Batch Processing）是深度学习训练的标准范式。通过一次矩阵运算同时处理多个样本，GPU的并行计算能力得到充分利用，显著提高了训练效率。

### 2.1.7 本节小结

本节系统地阐述了神经元的数学模型。核心要点包括：

第一，神经元是神经网络的基本计算单元，其数学形式为$\mathbf{y} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})$，包含仿射变换和非线性激活两个组成部分。

第二，没有激活函数的神经网络等价于单层线性变换，无法拟合复杂的非线性函数。激活函数通过引入非线性，赋予神经网络万能逼近能力。

第三，从几何角度看，单个神经元实现了一个超平面分类器，只能处理线性可分问题。从概率角度看，sigmoid和softmax激活函数的输出可以解释为分类概率。

第四，神经元的矩阵表示使得批量处理成为可能，这是深度学习高效训练的关键技术基础。