神经网络（Neural Network）的数学基础建立在对生物神经元（Biological Neuron）的抽象与简化之上。1943年，神经生理学家沃伦·麦肯罗皮层（Warren McCulloch）和数学家沃尔特·皮茨（Walter Pitts）发表了开创性的论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity），首次提出了人工神经元的数学模型，标志着计算神经科学和人工智能的诞生。本节将从数学建模的角度，系统阐述神经元从生物原型到数学抽象的演化过程，深入分析单层神经元的数学结构，并建立与后续章节的联系。

## 2.1.1 从生物神经元到数学抽象

在深入数学细节之前，我们需要首先理解生物神经元的基本结构及其数学抽象过程。生物神经元是一种复杂的细胞系统，由细胞体（Soma）、树突（Dendrites）、轴突（Axon）和突触（Synapse）等部分组成。树突负责接收来自其他神经元的信号，细胞体对这些输入信号进行整合处理，当输入信号的累积效应超过某个阈值时，神经元被激活并通过轴突向其他神经元传递信号。突触是神经元之间传递信息的连接点，其连接强度决定了信号传递的效率。

**定义2.1.1（麦肯罗皮层神经元模型）**：麦肯罗皮层神经元（McCulloch-Pitts Neuron）是一种二值神经元模型，其数学定义为：
$$
\mathbf{y} = \phi\left(\sum_{i=1}^{n} w_i x_i - \theta\right)\tag{2.1.1}
$$
其中$x_i \in \{0, 1\}$为输入信号，$w_i \in \mathbb{R}$为连接权重，$\theta \in \mathbb{R}$为激活阈值，$\phi(\cdot)$为阶跃激活函数，$\mathbf{y} \in \{0, 1\}$为神经元输出。

阶跃激活函数的数学定义为：
$$
\phi(z) = \begin{cases} 1 & \text{if } z \geq 0 \\ 0 & \text{if } z < 0 \end{cases}\tag{2.1.2}
$$
麦肯罗皮层神经元模型虽然简单，却抓住了生物神经元信息处理的核心特征：加权求和信息整合与阈值触发机制。这种抽象使得我们可以利用线性代数和逻辑代数的工具来分析神经网络的行为。值得注意的是，麦肯罗皮层神经元实际上是一个线性分类器，它将输入空间划分为两个半空间，由超平面$sum_{i=1}^{n} w_i x_i = \theta$所界定。

**定理2.1.1（麦肯罗皮层神经元的表达能力）**：单个麦肯罗皮层神经元可以实现所有的基本逻辑运算，包括逻辑与（AND）、逻辑或（OR）和逻辑非（NOT）。

证明：我们通过构造具体的权重和阈值来说明。假设输入为$x_1, x_2 \in \{0, 1\}$。

对于逻辑与运算（$y = x_1 \land x_2$​），设权重$w_1 = 1$、$w_2 = 1$，阈值$\theta = 1.5$。则当且仅当两个输入均为1时，加权和为$2 \geq 1.5$，输出为1；其他情况输出为0。

对于逻辑或运算（$y = x_1 \lor x_2$​），设权重$w_1 = 1$、$w_2 = 1$，阈值$\theta = 0.5$。只要至少有一个输入为1，加权和至少为1，输出为1；只有两个输入均为0时输出为0。

对于逻辑非运算（$y = \neg x_1$），设权重$w_1 = -1$，阈值$\theta = -0.5$。当$x_1 = 0$时，加权和为$0 \geq -0.5$，输出为1；当$x_1 = 1$时，加权和为$-1 < -0.5$，输出为0。

由于任意布尔函数都可以由AND、OR、NOT组合表示，单个神经元可以模拟任意单变量布尔函数。

这个定理表明，尽管单个神经元结构极其简单，但它已经具备了一定的逻辑推理能力。然而，麦肯罗皮层神经元模型存在明显的局限性：它是离散的二值模型，权重和阈值需要人工设定，无法通过数据自动学习。1958年，弗兰克·罗森布拉特（Frank Rosenblatt）提出的感知机（Perceptron）模型克服了这些局限性，引入了可学习的参数和连续可微的激活函数，为神经网络的现代发展奠定了基础。

## 2.1.2 感知机与仿射变换

感知机是麦肯罗皮层神经元的重要推广，其核心创新在于引入了参数学习机制，使得神经元能够从数据中自动学习最优的连接权重。与麦肯罗皮层神经元使用阶跃函数不同，感知机通常采用连续可微的激活函数，这为使用梯度下降等优化算法提供了数学基础。

**定义2.1.2（感知机模型）**：设$\mathbf{x} = [x_1, x_2, \ldots, x_n]^T \in \mathbb{R}^n$为输入向量，$\mathbf{w} = [w_1, w_2, \ldots, w_n]^T \in \mathbb{R}^n$为权重向量，$b \in \mathbb{R}$为偏置项（Bias）。感知机的数学定义为：
$$
\mathbf{z} = \mathbf{w}^T \mathbf{x} + b, \quad \mathbf{y} = \phi(\mathbf{z})\tag{2.1.3}
$$
其中$\phi: \mathbb{R} \to \mathbb{R}$为激活函数，$\mathbf{z}$称为净输入（Net Input），$\mathbf{y} \in \mathbb{R}^m$为输出向量。

表达式$\mathbf{w}^T \mathbf{x} + b$是线性变换与平移的组合，在几何上称为仿射变换（Affine Transformation）。仿射变换是线性变换的推广，它保持了空间的平行性和比例关系，但不要求保持原点不变。

**定义2.1.3（仿射变换）**：从$\mathbb{R}^n$到$\mathbb{R}^m$的仿射变换定义为：
$$
\mathbf{f}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}\tag{2.1.4}
$$
其中$\mathbf{W} \in \mathbb{R}^{m \times n}$为变换矩阵，$\mathbf{b} \in \mathbb{R}^m$为平移向量。当$\mathbf{b} = \mathbf{0}$时，仿射变换退化为线性变换。

**定理2.1.2（仿射变换的几何意义）**：仿射变换$\mathbf{f}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}$在几何上等价于先进行线性变换$\mathbf{W}\mathbf{x}$，再进行平移变换$\mathbf{b}$。对于超平面$\mathbf{w}^T \mathbf{x} + b = 0$，仿射变换将其映射为另一个超平面。

**证明**：考虑超平面$H = \{\mathbf{x} \mid \mathbf{w}^T \mathbf{x} + b = 0\}$。经过线性变换$\mathbf{W}$后，$H$中任意点$\mathbf{x}$变为$\mathbf{W}\mathbf{x}$。设$\mathbf{w}' = (\mathbf{W}^+)^T \mathbf{w}$，其中$\mathbf{W}^+$为$\mathbf{W}$的摩尔-彭罗斯广义逆，则$\mathbf{w}'^T \mathbf{W}\mathbf{x} = \mathbf{w}^T \mathbf{x}$。因此，线性变换后的点集满足$\mathbf{w}'^T (\mathbf{W}\mathbf{x}) + b = \mathbf{w}^T \mathbf{x} + b = 0$，仍为超平面。平移变换$\mathbf{b}$则将整个超平面平移，不改变其超平面性质。

在神经网络的语境下，仿射变换$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$承担着特征提取和信息整合的功能。权重矩阵$\mathbf{W}$决定了不同特征之间的线性组合方式，偏置项$\mathbf{b}$则提供了灵活的阈值调节能力。理解仿射变换的几何本质，对于分析神经网络的能力边界和训练动态至关重要。

**定义2.1.4（齐次坐标表示）**：为了将仿射变换统一表示为线性变换的形式，我们引入齐次坐标（Homogeneous Coordinates）。将输入向量$\mathbf{x} \in \mathbb{R}^n$扩展为齐次坐标$\tilde{\mathbf{x}} = [x_1, x_2, \ldots, x_n, 1]^T \in \mathbb{R}^{n+1}$，将权重矩阵和偏置合并为扩展权重矩阵：

$$
\tilde{\mathbf{W}} = \begin{bmatrix} \mathbf{W} & \mathbf{b} \end{bmatrix} = \begin{bmatrix} w_{11} & w_{12} & \cdots & w_{1n} & b_1 \\ w_{21} & w_{22} & \cdots & w_{2n} & b_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ w_{m1} & w_{m2} & \cdots & w_{mn} & b_m \end{bmatrix} \in \mathbb{R}^{m \times (n+1)}\tag{2.1.5}
$$

则仿射变换可以统一表示为：
$$
\mathbf{z} = \tilde{\mathbf{W}} \tilde{\mathbf{x}}
$$
齐次坐标表示的优势在于它将仿射变换转化为单一的矩阵乘法运算，使得我们可以使用线性代数的标准工具进行分析。在计算机图形学和计算机视觉中，齐次坐标被广泛用于处理平移、旋转、缩放等仿射变换。

## 2.1.3 激活函数的数学角色

仿射变换$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$本身是线性变换，无论堆叠多少层，复合后的结果仍然是线性变换。激活函数（Activation Function）通过引入非线性变换，打破了神经网络的线性瓶颈，使得神经网络能够拟合任意复杂的非线性函数。这是神经网络强大表达能力的关键来源。

**定理2.1.3（多层线性网络的恒等性）**：设有一个$L$层的前馈网络，每一层仅进行仿射变换而不应用激活函数，即$\mathbf{h}^{(l)} = \mathbf{W}^{(l)} \mathbf{h}^{(l-1)} + \mathbf{b}^{(l)}$。则整个网络等价于一个单一的仿射变换：
$$
\mathbf{h}^{(L)} = \mathbf{W}' \mathbf{h}^{(0)} + \mathbf{b}'\tag{2.1.6}
$$
其中$\mathbf{W}' = \mathbf{W}^{(L)} \mathbf{W}^{(L-1)} \cdots \mathbf{W}^{(1)}$，$\mathbf{b}' = \mathbf{b}^{(L)} + \mathbf{W}^{(L)}\mathbf{b}^{(L-1)} + \cdots + \mathbf{W}^{(L)}\cdots\mathbf{W}^{(2)}\mathbf{b}^{(1)}$。

证明：通过数学归纳法证明。当$L=1$时，结论显然成立。假设对于$L−1$层网络，结论成立，即：
$$
\mathbf{h}^{(L-1)} = \mathbf{W}_{L-1}' \mathbf{h}^{(0)} + \mathbf{b}_{L-1}\tag{2.1.7}'
$$
其中$\mathbf{W}_{L-1}' = \mathbf{W}^{(L-1)} \cdots \mathbf{W}^{(1)}$，$\mathbf{b}_{L-1}' = \mathbf{b}^{(L-1)} + \mathbf{W}^{(L-1)}\mathbf{b}^{(L-2)} + \cdots + \mathbf{W}^{(L-1)}\cdots\mathbf{W}^{(2)}\mathbf{b}^{(1)}$。则第$L$层的输出为：
$$
\mathbf{h}^{(L)} = \mathbf{W}^{(L)} \mathbf{h}^{(L-1)} + \mathbf{b}^{(L)} = \mathbf{W}^{(L)} (\mathbf{W}_{L-1}' \mathbf{h}^{(0)} + \mathbf{b}_{L-1}') + \mathbf{b}^{(L)}\tag{2.1.8}
$$
$$= (\mathbf{W}^{(L)} \mathbf{W}_{L-1}') \mathbf{h}^{(0)} + (\mathbf{W}^{(L)} \mathbf{b}_{L-1}' + \mathbf{b}^{(L)})\tag{2.1.9}
$$
这正是$L$层网络的等效仿射变换形式。
这个定理揭示了一个深刻的事实：没有激活函数的神经网络无论多深，其表达能力等价于单层线性变换。这意味着堆叠多层网络不会带来任何额外的表达能力——这是不可能拟合复杂非线性函数的原因。激活函数通过引入非线性打破了这一限制。

**定义2.1.5（常用激活函数）**：以下是深度学习中常用的激活函数及其数学定义：

（1）Sigmoid函数：
$$
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{e^z + 1}\tag{2.1.10}​
$$
（2）双曲正切函数（Tanh）：
$$
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1\tag{2.1.11}
$$
（3）修正线性单元（ReLU）：
$$
\text{ReLU}(z) = \max(0, z)\tag{2.1.12}
$$
（4）GELU激活函数：
$$
\text{GELU}(z) = z \cdot \Phi(z) = \frac{z}{2} \left[1 + \text{erf}\left(\frac{z}{\sqrt{2}}\right)\right]\tag{2.1.13}
$$
其中$\Phi(z)$为标准正态分布的累积分布函数，$\text{erf}(z)$为误差函数。

这些激活函数各有特点，将在第三章进行详细的数学分析。这里我们关注激活函数在神经元模型中的整体角色。从数学角度看，激活函数$\phi(\cdot)$将神经元的净输入$\mathbf{z}$映射到输出$\mathbf{y}$，完成了从输入空间到输出空间的非线性变换。激活函数的设计需要满足以下几个条件：非线性（保证网络的表达能力）、连续可微（保证梯度计算的可操作性）、数值稳定性（避免数值溢出或下溢）、以及计算效率（满足大规模训练的需求）。

## 2.1.4 神经元的几何解释

从几何视角来看，单个神经元执行的是一个超平面分类（或回归）的操作。理解这一几何本质对于分析神经网络的能力和局限性至关重要。

**定义2.1.6（决策边界）**：对于二分类问题，考虑使用sigmoid激活函数的神经元$\mathbf{y} = \sigma(\mathbf{w}^T \mathbf{x} + b)$。决策边界（Decision Boundary）定义为使后验概率等于0.5的输入点集：
$$
{x∣wTx+b=0}\{\mathbf{x} \mid \mathbf{w}^T \mathbf{x} + b = 0\}\tag{2.1.14}
$$
这是一个超平面，将输入空间划分为两个区域：$\mathbf{w}^T \mathbf{x} + b > 0$的区域对应类别1，$\mathbf{w}^T \mathbf{x} + b < 0$的区域对应类别0。

**定理2.1.4（神经元的几何分离能力）**：单个神经元（感知机）只能解决线性可分（Linearly Separable）的问题。对于线性不可分的数据集（如异或问题），单个神经元无法找到正确的分类边界。

证明：考虑异或（XOR）问题，其真值表为：

| $x_1$​ | $x_2$ | $x_1 \oplus x_2$ |
| ------ | ----- | ---------------- |
| 0      | 0     | 0                |
| 0      | 1     | 1                |
| 1      | 0     | 1                |
| 1      | 1     | 0                |

点$(0,0)$和$(1,1)$属于类别0，点$(0,1)$和$(1,0)$属于类别1。任何超平面$\mathbf{w}^T \mathbf{x} + b = 0$都只能将平面划分为两个半平面，无法将类别0的点放在一侧而将类别1的点放在另一侧（因为异或问题的正例点位于对角位置）。因此，单个神经元无法解决异或问题。
![[graph/XORScene_ManimCE_v0.19.1.png]]
这个定理揭示了单层神经网络的根本局限性：它只能处理线性可分的问题。这一局限性直到多层神经网络（多层感知机）的出现才得到解决——通过在隐藏层引入非线性激活函数，多层神经网络可以学习复杂的非线性决策边界，从而解决异或等线性不可分问题。

**定义2.1.7（神经元的特征空间映射）**：从特征学习的角度来看，神经元$\mathbf{y} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})$可以理解为将输入向量$\mathbf{x}$从原始输入空间$\mathbb{R}^n$映射到新的特征空间$\mathbb{R}^m$的变换。这个变换包含两个步骤：首先通过仿射变换$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$进行线性投影，然后通过激活函数$\phi(\cdot)$进行非线性变换。
![[graph/LinearInseparableScene_ManimCE_v0.19.1.png]]

数学上，这个特征映射可以表示为复合函数$\mathbf{f} = \phi \circ \mathcal{A}$，其中$\mathcal{A}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}$是仿射变换算子。特征空间的维度$m$由神经元的数量决定，不同的神经元学习不同的特征表示，多个神经元的组合形成了一个丰富的特征基。

## 2.1.5 概率视角下的神经元模型

神经元模型不仅可以从几何角度理解，还可以从概率论的角度进行分析。这种视角对于理解神经网络的输出解释和损失函数设计具有重要意义。

**定义2.1.8（伯努利分布与sigmoid激活）**：考虑二分类问题，设$y \in \{0, 1\}$为类别标签。假设给定输入$\mathbf{x}$时，标签$y$服从伯努利分布（Bernoulli Distribution）：
$$
P(y = 1 \mid \mathbf{x}) = p, \quad P(y = 0 \mid \mathbf{x}) = 1 - p\tag{2.1.15}
$$
sigmoid函数恰好可以将神经元的净输入映射为概率值：
$$
p = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}\tag{2.1.16}​
$$
**定理2.1.5（sigmoid神经元与伯努利分布的对应）**：sigmoid神经元$\hat{y} = \sigma(\mathbf{w}^T \mathbf{x} + b)$的输出可以解释为给定输入$\mathbf{x}$时，类别为1的条件概率$P(y = 1 \mid \mathbf{x})$。

证明：sigmoid函数的输出范围为$(0, 1)$，满足概率的基本要求。设$\mathbf{z} = \mathbf{w}^T \mathbf{x} + b$，则：
$$
\sigma(\mathbf{z}) = \frac{1}{1 + e^{-\mathbf{z}}} = \frac{e^{\mathbf{z}}}{e^{\mathbf{z}} + 1}\tag{2.1.17}​
$$
当$\mathbf{z} \to \infty$时，$\sigma(\mathbf{z}) \to 1$；当$\mathbf{z} \to -\infty$时，$\sigma(\mathbf{z}) \to 0$。这与概率的边界行为一致。更重要的是，通过选择适当的权重和偏置，sigmoid函数可以拟合任意单调递增的概率函数。

这种概率解释在逻辑回归（Logistic Regression）中得到了充分的应用。逻辑回归模型正是使用sigmoid神经元进行二分类，其损失函数为交叉熵损失（Cross-Entropy Loss），这将在第四章详细讨论。

**定义2.1.9（softmax神经元与多项分布）**：对于多分类问题，设$y \in \{1, 2, \ldots, K\}$为$K$个类别之一。Softmax函数将$K$个神经元的输出归一化为概率分布：
$$
P(y = k \mid \mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}, \quad \text{where } \mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}\tag{2.1.18}
$$
**定理2.1.6（softmax的数学性质）**：Softmax函数具有以下重要性质：

（1）输出值均为正且和为1：$\sum_{k=1}^{K} \text{softmax}(\mathbf{z})_k = 1$。

（2）平移不变性：对任意常数$c$，有$\text{softmax}(\mathbf{z} + c\mathbf{1}) = \text{softmax}(\mathbf{z})$，其中$\mathbf{1}$是全1向量。

（3）单调性：若$z_i > z_j$​，则$\text{softmax}(\mathbf{z})_i > \text{softmax}(\mathbf{z})_jsoftmax(z)$。

证明：对于性质（1），直接计算：
$$
\sum_{k=1}^{K} \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} = \frac{\sum_{k=1}^{K} e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} = 1∑k=1\tag{2.1.19}
$$

对于性质（2）：
$$
\text{softmax}(\mathbf{z} + c\mathbf{1})_k = \frac{e^{z_k + c}}{\sum_{j=1}^{K} e^{z_j + c}} = \frac{e^c e^{z_k}}{e^c \sum_{j=1}^{K} e^{z_j}} = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} = \text{softmax}(\mathbf{z})_k\tag{2.1.20}​
$$
对于性质（3），考虑比值：
$$
\frac{\text{softmax}(\mathbf{z})_i}{\text{softmax}(\mathbf{z})_j} = \frac{e^{z_i} / \sum e^{z_k}}{e^{z_j} / \sum e^{z_k}} = e^{z_i - z_j} > 1 \quad \text{if } z_i > z_j\tag{2.1.21}​
$$
故$\text{softmax}(\mathbf{z})_i > \text{softmax}(\mathbf{z})_j$。

softmax函数是大语言模型（如GPT、BERT等）中最常用的输出激活函数，用于将模型的logits输出转换为概率分布。这与第六章讨论的交叉熵损失有着深刻的联系——softmax和交叉熵的组合在数学上等价于最大似然估计。

## 2.1.6 神经元模型的矩阵表示

在实际应用中，我们通常需要同时计算多个神经元的输出。这可以通过矩阵运算高效地实现。

**定义2.1.10（层操作）**：设输入矩阵$\mathbf{X} \in \mathbb{R}^{N \times d}$包含$N$个样本，每个样本为$d$维向量。设有$m$个神经元的层，其权重矩阵为$\mathbf{W} \in \mathbb{R}^{d \times m}$，偏置向量为$\mathbf{b} \in \mathbb{R}^{m}$。则该层的输出为：
$$
\mathbf{Z} = \mathbf{X}\mathbf{W} + \mathbf{1}_N \mathbf{b}^T, \quad \mathbf{Y} = \phi(\mathbf{Z})\tag{2.1.22}
$$
其中$\mathbf{1}_N \in \mathbb{R}^{N}$是全1向量，$\mathbf{Z} \in \mathbb{R}^{N \times m}$是净输入矩阵，$\mathbf{Y} \in \mathbb{R}^{N \times m}$是输出矩阵。

这种矩阵表示的优势在于它可以利用现代硬件（特别是GPU）进行高度并行化的矩阵运算，大大提高了计算效率。深度学习框架（如PyTorch、TensorFlow）的核心正是通过这种矩阵表示实现高效的神经网络计算。

**定理2.1.7（矩阵运算与批量处理的等价性）**：上述矩阵运算等价于对每个样本单独进行神经元计算后的结果堆叠。

证明：设$\mathbf{X}$的第$i$行为$\mathbf{x}_i \in \mathbb{R}^{1 \times d}$，$\mathbf{Z}$的第$i$行为$\mathbf{z}_i \in \mathbb{R}^{1 \times m}zi​∈R1×m，\mathbf{b}^T$为行向量。则：
$$
\mathbf{z}_i = \mathbf{x}_i \mathbf{W} + \mathbf{b}^T = \sum_{k=1}^{d} x_{ik} \mathbf{w}_k + \mathbf{b}^T\tag{2.1.23}
$$
其中$\mathbf{w}_k$是$\mathbf{W}$的第$k$列。这正是单样本仿射变换的定义。因此，矩阵运算一次性处理了所有$N$个样本，结果等价于逐样本计算后按行堆叠。

批量处理（Batch Processing）是深度学习训练的标准范式。通过一次矩阵运算同时处理多个样本，GPU的并行计算能力得到充分利用，显著提高了训练效率。同时，批量处理还提供了隐式的正则化效果——在计算损失和梯度时，多个样本的信息被平均，这有助于提高模型的泛化能力。

## 2.1.7 本节小结

本节系统地阐述了神经元的数学模型，从麦肯罗皮层神经元的二值模型出发，逐步推广到现代深度学习中常用的连续激活神经元。我们建立了仿射变换的数学框架，证明了激活函数对于突破神经网络线性瓶颈的关键作用，并从几何和概率两个角度分析了神经元的行为。本节的内容可以概括为以下几个核心要点：

第一，神经元是神经网络的基本计算单元，其数学形式为$\mathbf{y} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})$，包含仿射变换和非线性激活两个组成部分。仿射变换进行线性特征组合，偏置项提供灵活的阈值调节。

第二，没有激活函数的神经网络等价于单层线性变换，无法拟合复杂的非线性函数。激活函数通过引入非线性，赋予神经网络万能逼近能力——理论上可以拟合任意连续函数。

第三，从几何角度看，单个神经元实现了一个超平面分类器，只能处理线性可分问题。从概率角度看，sigmoid和softmax激活函数的输出可以解释为分类概率，这为损失函数的设计提供了理论基础。

第四，神经元的矩阵表示使得批量处理成为可能，这是深度学习高效训练的关键技术基础。理解矩阵运算与逐样本计算的一致性，对于理解深度学习框架的工作原理至关重要。

掌握本节的数学基础后，我们将在下一节讨论如何将多个神经元组织成层，以及整个神经网络的矩阵表示形式。