反向传播（Backpropagation）是深度学习中最核心的算法之一，它使得大规模神经网络的端到端训练成为可能。从数学角度来看，反向传播本质上是链式法则在复合函数梯度计算中的高效应用。1986年，杰弗里·辛顿（Geoffrey Hinton）、大卫·鲁梅尔哈特（David Rumelhart）和罗纳德·威廉姆斯（Ronald Williams）在《自然》杂志上发表了关于反向传播算法的开创性论文，证明了通过错误信号的反向传播可以有效训练多层神经网络，从而开启了深度学习的现代时代。

本节将从数学推导的角度，系统阐述反向传播算法的完整理论框架。我们将从链式法则的矩阵形式出发，逐步推导出各层参数的梯度计算公式，并分析反向传播的计算复杂度。最后，我们将讨论反向传播与自动微分的关系，以及实际实现中的数值稳定性问题。

## 2.4.1 引言：为什么需要反向传播

在2.3节中，我们讨论了前向传播的数学本质：输入数据通过多层非线性变换，最终产生网络输出。然而，前向传播只是完成了"推理"的过程——它告诉我们给定当前参数下网络的输出是什么，但并没有告诉我们如何调整参数以改善输出。

神经网络的训练目标是最小化某个损失函数$L(\hat{\mathbf{y}}, \mathbf{y})$，其中$\hat{\mathbf{y}} = f_{\Theta}(\mathbf{x})$是网络预测，$\mathbf{y}$是真实标签。参数优化需要计算损失函数对各参数的梯度：
$$
\frac{\partial L}{\partial \mathbf{W}^{(\ell)}}, \quad \frac{\partial L}{\partial \mathbf{b}^{(\ell)}}, \quad \forall \ell = 1, \ldots, L\tag{2.4.1}
$$
由于神经网络是一个深度复合函数，直接对复合函数求导会导致计算量爆炸。反向传播通过巧妙地利用链式法则和动态规划思想，将梯度计算的时间复杂度从指数级降低到线性级，使得训练深层网络成为可能。

**问题设定**：设有一个$L$层前馈网络，第$\ell$层的计算定义为：
$$
\mathbf{z}^{(\ell)} = \mathbf{W}^{(\ell)}\mathbf{h}^{(\ell-1)} + \mathbf{b}^{(\ell)}, \quad \mathbf{h}^{(\ell)} = \phi^{(\ell)}(\mathbf{z}^{(\ell)})\tag{2.4.2}
$$
其中$\mathbf{h}^{(0)} = \mathbf{x}$为输入，$\mathbf{h}^{(L)} = \hat{\mathbf{y}}$​为输出。损失函数$L(\hat{\mathbf{y}}, \mathbf{y})$度量预测与真实标签之间的差异。我们的目标是计算$\frac{\partial L}{\partial \mathbf{W}^{(\ell)}}$和$\frac{\partial L}{\partial \mathbf{b}^{(\ell)}}$对所有层$\ell$。

## 2.4.2 链式法则的矩阵形式

反向传播的核心数学工具是链式法则。在多层神经网络中，我们需要将链式法则从标量情形推广到向量和矩阵情形。

**定理2.4.1（标量链式法则）**：设$y = f(u)$，$u = g(x)$，则：
$$
\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}\tag{2.4.3}​
$$
证明：这是微积分基本定理的直接推论，已在2.3节中证明。

**定义2.4.1（梯度与雅可比矩阵）**：设标量函数$f: \mathbb{R}^n \to \mathbb{R}$，其梯度为行向量：
$$
\nabla_{\mathbf{x}} f = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right]\tag{2.4.4}
$$
对于向量值函数$\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$，其雅可比矩阵为：
$$
\mathbf{J}_{\mathbf{f}} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\ \vdots & \ddots & \vdots \\ \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n} \end{bmatrix} \in \mathbb{R}^{m \times n}\tag{2.4.5}
$$
**定理2.4.2（向量链式法则）**：设$\mathbf{z} = \mathbf{g}(\mathbf{y})$，$\mathbf{y} = \mathbf{f}(\mathbf{x})$，则：
$$
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}}\tag{2.4.6}
$$
其中矩阵乘法的顺序不可交换。

证明：考虑$\mathbf{z}$的第$i$个分量$z_i$​和$\mathbf{x}$的第$j$个分量$x_j$​：
$$
\left[\frac{\partial \mathbf{z}}{\partial \mathbf{x}}\right]_{ij} = \frac{\partial z_i}{\partial x_j} = \sum_{k=1}^{m} \frac{\partial z_i}{\partial y_k} \cdot \frac{\partial y_k}{\partial x_j} = \left[\frac{\partial \mathbf{z}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}}\right]_{ij}[∂x∂z​]\tag{2.4.7}
$$
这正是矩阵乘法的定义。

**定理2.4.3（标量对矩阵的链式法则）**：设$L$为标量损失函数，$\mathbf{z} = \mathbf{g}(\mathbf{W})$，其中$\mathbf{W} \in \mathbb{R}^{m \times n}$为矩阵变量。则$L$对$\mathbf{W}$的梯度为：
$$
\frac{\partial L}{\partial \mathbf{W}} = \frac{\partial L}{\partial \mathbf{z}} \cdot \frac{\partial \mathbf{z}}{\partial \mathbf{W}} = \left(\frac{\partial L}{\partial \mathbf{z}}\right)^T \cdot \mathbf{x}^T\tag{2.4.8}
$$
其中假设$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$，$\mathbf{x}$为输入向量。

证明：考虑梯度矩阵的元素$(\frac{\partial L}{\partial \mathbf{W}})_{ij} = \frac{\partial L}{\partial w_{ij}}$​。由链式法则：
$$
\frac{\partial L}{\partial w_{ij}} = \sum_{k=1}^{m} \frac{\partial L}{\partial z_k} \cdot \frac{\partial z_k}{\partial w_{ij}}\tag{2.4.9}
$$
其中$z_k = \sum_{l=1}^{n} w_{kl} x_l + b_k$​，故$\frac{\partial z_k}{\partial w_{ij}} = \delta_{ki} x_j$（$\delta_{ki}$为克罗内克函数）。代入得：
$$
\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial z_i} \cdot x_j = \left[\frac{\partial L}{\partial \mathbf{z}}\right]_i \cdot x_j\tag{2.4.10}​
$$
将所有元素写为矩阵形式，即$\frac{\partial L}{\partial \mathbf{W}} = \left(\frac{\partial L}{\partial \mathbf{z}}\right)^T \mathbf{x}^T$。

这个定理是反向传播中权重梯度计算的核心公式。它告诉我们：损失函数对权重的梯度可以分解为损失函数对净输入的梯度（称为"误差信号"或"梯度"）与输入向量的外积。

## 2.4.3 误差信号的传播

在反向传播中，我们定义一个关键中间变量——误差信号（Error Signal），它表示损失函数对各层净输入的梯度。这个变量在反向传播过程中起着桥梁作用，将输出层的损失信息传递到输入层。

**定义2.4.2（第$\ell$层误差信号）**：设$\boldsymbol{\delta}^{(\ell)} = \frac{\partial L}{\partial \mathbf{z}^{(\ell)}} \in \mathbb{R}^{n_\ell}$​为第$\ell$层的误差信号，即损失函数对该层净输入的梯度。

误差信号包含了损失函数对该层激活值变化的敏感程度信息。通过理解误差信号的传播机制，我们可以清楚地看到损失信息如何从输出层反向流动到输入层。

**定理2.4.4（输出层误差信号）**：对于输出层（$\ell = L$），误差信号为：
$$
\boldsymbol{\delta}^{(L)} = \frac{\partial L}{\partial \mathbf{z}^{(L)}} = \frac{\partial L}{\partial \mathbf{h}^{(L)}} \odot \phi^{(L)'}(\mathbf{z}^{(L)})\tag{2.4.11}
$$
其中$\odot$表示逐元素乘法（Hadamard积），$\phi^{(L)'}$为第$L$层激活函数的导数。

证明：由链式法则和雅可比矩阵的性质：
$$
\boldsymbol{\delta}^{(L)} = \frac{\partial L}{\partial \mathbf{z}^{(L)}} = \frac{\partial L}{\partial \mathbf{h}^{(L)}} \cdot \frac{\partial \mathbf{h}^{(L)}}{\partial \mathbf{z}^{(L)}}\tag{2.4.12}
$$
由于$\mathbf{h}^{(L)} = \phi^{(L)}(\mathbf{z}^{(L)})$是逐元素函数，雅可比矩阵$\frac{\partial \mathbf{h}^{(L)}}{\partial \mathbf{z}^{(L)}}$​是对角矩阵，其对角元素为$\phi^{(L)'}(\mathbf{z}^{(L)}_i)$。因此，矩阵乘法退化为逐元素乘法：
$$
\frac{\partial L}{\partial \mathbf{h}^{(L)}} \cdot \text{diag}(\phi^{(L)'}(\mathbf{z}^{(L)})) = \frac{\partial L}{\partial \mathbf{h}^{(L)}} \odot \phi^{(L)'}(\mathbf{z}^{(L)})\tag{2.4.13}
$$
**推论2.4.1（常见输出层误差信号）**：对于不同的输出层配置，误差信号有不同的简化形式：

（1）回归任务（恒等激活）：$\boldsymbol{\delta}^{(L)} = \hat{\mathbf{y}} - \mathbf{y}$（均方误差损失）。

（2）二分类任务（Sigmoid激活）：$\boldsymbol{\delta}^{(L)} = \hat{\mathbf{y}} - \mathbf{y}$（交叉熵损失）。

（3）多分类任务（Softmax激活）：$\boldsymbol{\delta}^{(L)} = \hat{\mathbf{y}} - \mathbf{y}$（交叉熵损失），其中$\hat{\mathbf{y}}$​是概率分布。

证明：以二分类为例，损失函数为$L = -[y \log \hat{y} + (1-y)\log(1-\hat{y})]$，其中$\hat{y} = \sigma(z)$。计算梯度：
$$
\frac{\partial L}{\partial z} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} = \left(-\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}\right) \cdot \sigma(z)(1-\sigma(z))\tag{2.4.14}
$$
代入$\hat{y} = \sigma(z)$并化简，可得$\frac{\partial L}{\partial z} = \hat{y} - y$。多分类情况的证明类似。

这个推论揭示了一个重要的简化：对于常见的分类和回归任务，使用交叉熵或均方误差损失时，输出层误差信号与预测误差$\hat{\mathbf{y}} - \mathbf{y}$成正比。这一性质大大简化了反向传播的实现。

**定理2.4.5（隐藏层误差信号传播）**：对于隐藏层（$\ell < L$），误差信号从前一层传播到当前层：
$$
\boldsymbol{\delta}^{(\ell)} = \left(\mathbf{W}^{(\ell+1)}\right)^T \boldsymbol{\delta}^{(\ell+1)} \odot \phi^{(\ell)'}(\mathbf{z}^{(\ell)})\tag{2.4.15}
$$
证明：根据误差信号的定义和链式法则：
$$
\boldsymbol{\delta}^{(\ell)} = \frac{\partial L}{\partial \mathbf{z}^{(\ell)}} = \frac{\partial L}{\partial \mathbf{z}^{(\ell+1)}} \cdot \frac{\partial \mathbf{z}^{(\ell+1)}}{\partial \mathbf{h}^{(\ell)}} \cdot \frac{\partial \mathbf{h}^{(\ell)}}{\partial \mathbf{z}^{(\ell)}}\tag{2.4.16}​
$$
第一项为$\boldsymbol{\delta}^{(\ell+1)}$。第二项中，$\mathbf{z}^{(\ell+1)} = \mathbf{W}^{(\ell+1)}\mathbf{h}^{(\ell)} + \mathbf{b}^{(\ell+1)}$，故$\frac{\partial \mathbf{z}^{(\ell+1)}}{\partial \mathbf{h}^{(\ell)}} = \mathbf{W}^{(\ell+1)}∂h(ℓ)$。第三项为对角矩阵$\text{diag}(\phi^{(\ell)'}(\mathbf{z}^{(\ell)}))$。因此：
$$
\boldsymbol{\delta}^{(\ell)} = \boldsymbol{\delta}^{(\ell+1)} \cdot \mathbf{W}^{(\ell+1)} \odot \phi^{(\ell)'}(\mathbf{z}^{(\ell)})\tag{2.4.17}
$$
注意矩阵乘法的方向：$\boldsymbol{\delta}^{(\ell+1)} \in \mathbb{R}^{n_{\ell+1}}$，$\mathbf{W}^{(\ell+1)} \in \mathbb{R}^{n_{\ell+1} \times n_\ell}$​，故$\boldsymbol{\delta}^{(\ell+1)} \mathbf{W}^{(\ell+1)}$的维度为$\mathbb{R}^{1 \times n_\ell}$​。写成向量形式即：
$$
\boldsymbol{\delta}^{(\ell)} = \left(\mathbf{W}^{(\ell+1)}\right)^T \boldsymbol{\delta}^{(\ell+1)} \odot \phi^{(\ell)'}(\mathbf{z}^{(\ell)})\tag{2.4.18}
$$
这个定理揭示了反向传播的本质机制：误差信号从输出层向输入层逐层传播，每一层通过权重矩阵的转置将误差"路由"到对应的输入维度，然后通过激活函数的导数进行缩放。如果激活函数的导数接近零（饱和区域），误差信号会被衰减，导致梯度消失。

## 2.4.4 参数梯度的完整推导

在获得各层误差信号后，我们可以计算损失函数对各层参数的梯度。这是反向传播的最后一步，也是参数更新的直接输入。

**定理2.4.6（权重梯度）**：第$\ell$层权重的梯度为：
$$
\frac{\partial L}{\partial \mathbf{W}^{(\ell)}} = \boldsymbol{\delta}^{(\ell)} \left(\mathbf{h}^{(\ell-1)}\right)^T\tag{2.4.19}
$$
证明：根据定理2.4.3的直接应用，有：
$$
\frac{\partial L}{\partial \mathbf{W}^{(\ell)}} = \left(\frac{\partial L}{\partial \mathbf{z}^{(\ell)}}\right)^T \left(\mathbf{h}^{(\ell-1)}\right)^T = \boldsymbol{\delta}^{(\ell)} \left(\mathbf{h}^{(\ell-1)}\right)^T\tag{2.4.20}
$$
其中$\boldsymbol{\delta}^{(\ell)}$是行向量还是列向量需要注意维度匹配。

**定理2.4.7（偏置梯度）**：第$\ell$层偏置的梯度为：
$$
\frac{\partial L}{\partial \mathbf{b}^{(\ell)}} = \boldsymbol{\delta}^{(\ell)}\tag{2.4.21}
$$
证明：偏置项$\mathbf{b}$直接加到净输入上，故：
$$
\frac{\partial L}{\partial b^{(\ell)}_j} = \sum_{k} \frac{\partial L}{\partial z^{(\ell)}_k} \cdot \frac{\partial z^{(\ell)}_k}{\partial b^{(\ell)}_j} = \frac{\partial L}{\partial z^{(\ell)}_j} \cdot 1\tag{2.4.22}
$$
因为$\frac{\partial z^{(\ell)}_k}{\partial b^{(\ell)}_j} = \delta_{kj}∂bj(ℓ)​$（克罗内克函数）。因此，梯度向量就是误差信号本身。

**算法2.4.1（反向传播算法）**：综合以上结果，反向传播算法可以描述为：

**输入**：网络参数$\{\mathbf{W}^{(\ell)}, \mathbf{b}^{(\ell)}\}_{\ell=1}^L$​，前向传播保存的中间值$\{\mathbf{z}^{(\ell)}, \mathbf{h}^{(\ell)}\}_{\ell=1}^L$，损失函数$L$，真实标签$\mathbf{y}$。

**输出**：各层参数的梯度$\left\{\frac{\partial L}{\partial \mathbf{W}^{(\ell)}}, \frac{\partial L}{\partial \mathbf{b}^{(\ell)}}\right\}_{\ell=1}^L$​。

**步骤**：

（1）**前向传播**：执行完整的前向传播，保存所有层的$\mathbf{z}^{(\ell)}$和$\mathbf{h}^{(\ell)}$。

（2）**输出层误差**：计算$\boldsymbol{\delta}^{(L)} = \frac{\partial L}{\partial \mathbf{z}^{(L)}}$（使用推论2.4.1的简化形式）。

（3）**反向传播循环**：对$\ell = L, L-1, \ldots, 1$：

- 计算权重梯度：$\frac{\partial L}{\partial \mathbf{W}^{(\ell)}} = \boldsymbol{\delta}^{(\ell)} \left(\mathbf{h}^{(\ell-1)}\right)^T$
- 计算偏置梯度：$\frac{\partial L}{\partial \mathbf{b}^{(\ell)}} = \boldsymbol{\delta}^{(\ell)}$
- 若$\ell > 1$，计算前一层误差：$\boldsymbol{\delta}^{(\ell-1)} = \left(\mathbf{W}^{(\ell)}\right)^T \boldsymbol{\delta}^{(\ell)} \odot \phi^{(\ell-1)'}(\mathbf{z}^{(\ell-1)})$

（4）**返回**：所有参数的梯度。
![[backward.drawio.png]]
这个算法清晰地展示了反向传播的执行流程：先通过前向传播保存必要的中间值，然后从输出层开始，逐层计算误差信号和参数梯度，最后将梯度用于参数更新。

## 2.4.5 批量处理的梯度计算

在实际训练中，我们通常使用小批量随机梯度下降（Mini-Batch Stochastic Gradient Descent）。这需要对上述单样本梯度计算进行扩展，以处理批量输入。

**定义2.4.3（批量前向传播）**：设批量输入$\mathbf{X} \in \mathbb{R}^{B \times n_{in}}$，批量标签$\mathbf{Y} \in \mathbb{R}^{B \times n_{out}}$​（或$\mathbf{Y} \in \mathbb{R}^B$对于分类任务）。批量损失通常定义为各样本损失的平均值：
$$
L_{\text{batch}} = \frac{1}{B} \sum_{i=1}^{B} L_i = \frac{1}{B} \sum_{i=1}^{B} L(\hat{\mathbf{y}}_i, \mathbf{y}_i)\tag{2.4.23}
$$
**定理2.4.8（批量误差信号）**：对于批量输入，第$\ell$层的误差信号矩阵为$\boldsymbol{\Delta}^{(\ell)} \in \mathbb{R}^{B \times n_\ell}$，其第$i$行为样本$i$的误差信号$\boldsymbol{\delta}^{(\ell)}_i$​。输出层误差为：
$$
\boldsymbol{\Delta}^{(L)} = \frac{1}{B} (\hat{\mathbf{Y}} - \mathbf{Y})\tag{2.4.24}
$$
其中$\hat{\mathbf{Y}}$是批量预测矩阵。

证明：批量损失对净输入的梯度为：
$$
\frac{\partial L_{\text{batch}}}{\partial z^{(\ell)}_{ij}} = \frac{1}{B} \frac{\partial L_i}{\partial z^{(\ell)}_{ij}}\tag{2.4.25}
$$
因此，误差信号矩阵需要除以批量大小$B$。对于常见的均方误差和交叉熵损失，输出层误差简化为预测误差的平均值。

**定理2.4.9（批量权重梯度）**：批量情况下，第$\ell$层权重的梯度为：
$$
\frac{\partial L_{\text{batch}}}{\partial \mathbf{W}^{(\ell)}} = \frac{1}{B} \boldsymbol{\Delta}^{(\ell)} \left(\mathbf{H}^{(\ell-1)}\right)^T\tag{2.4.26}
$$
其中$\mathbf{H}^{(\ell-1)} \in \mathbb{R}^{B \times n_{\ell-1}}$是批量隐藏激活矩阵。

证明：考虑批量中每个样本的权重梯度：
$$
\frac{\partial L_{\text{batch}}}{\partial \mathbf{W}^{(\ell)}} = \frac{1}{B} \sum_{i=1}^{B} \frac{\partial L_i}{\partial \mathbf{W}^{(\ell)}} = \frac{1}{B} \sum_{i=1}^{B} \boldsymbol{\delta}^{(\ell)}_i \left(\mathbf{h}^{(\ell-1)}_i\right)^T\tag{2.4.27}
$$
将求和写为矩阵形式，即为$\frac{1}{B} \boldsymbol{\Delta}^{(\ell)} \left(\mathbf{H}^{(\ell-1)}\right)^T$。

**推论2.4.2（批量偏置梯度）**：批量情况下，第$\ell$层偏置的梯度为：
$$
\frac{\partial L_{\text{batch}}}{\partial \mathbf{b}^{(\ell)}}
= \frac{1}{B} \sum_{i=1}^{B} \boldsymbol{\delta}^{(\ell)}_i
= \frac{1}{B}\, \boldsymbol{\Delta}^{(\ell)\top} \mathbf{1}_B\tag{2.4.28}
$$
即误差信号矩阵在批量维度上的平均值。

## 2.4.6 计算复杂度分析

理解反向传播的计算复杂度对于评估训练效率和设计高效算法至关重要。

**定理2.4.10（反向传播的FLOPs）**：对于$L$层全连接网络，单次反向传播的浮点运算数为：
$$
\text{FLOPs}_{\text{backward}} \approx 2 \times \text{FLOPs}_{\text{forward}}\tag{2.4.29}​
$$
证明：反向传播的主要计算包括两部分：

（1）误差信号传播：对于每层$\ell$，计算$\boldsymbol{\delta}^{(\ell)} = \left(\mathbf{W}^{(\ell+1)}\right)^T \boldsymbol{\delta}^{(\ell+1)} \odot \phi^{(\ell)'}(\mathbf{z}^{(\ell)})$。矩阵乘法$\left(\mathbf{W}^{(\ell+1)}\right)^T \boldsymbol{\delta}^{(\ell+1)}$的FLOPs约为$2n_\ell n_{\ell+1}$，逐元素乘法约为$n_\ell$​。

（2）参数梯度计算：计算$\frac{\partial L}{\partial \mathbf{W}^{(\ell)}} = \boldsymbol{\delta}^{(\ell)} \left(\mathbf{h}^{(\ell-1)}\right)^T∂W(ℓ)$的FLOPs约为$2n_{\ell-1} n_\ell$​，计算偏置梯度约为$n_\ell$​。

将各层求和，反向传播的总FLOPs约为前向传播的两倍。
 
**表4-4：前向传播与反向传播的FLOPs比较**

| 计算类型 | 单层FLOPs                     | 主要操作       |
| ---- | --------------------------- | ---------- |
| 前向传播 | $2n_{in}n_{out} + n_{out}$  | 矩阵乘法、激活函数  |
| 误差传播 | $2n_{in}n_{out} + n_{in}$​  | 矩阵乘法、逐元素乘法 |
| 梯度计算 | $2n_{in}n_{out} + n_{out}$​ | 外积计算、聚合    |

**推论2.4.3（训练迭代的总复杂度）**：每次训练迭代（包含前向传播、反向传播和参数更新）的总FLOPs约为前向传播的4倍：一次前向传播加上一次反向传播（约为2倍前向传播），再加上参数更新的少量计算。

## 2.4.7 反向传播与自动微分的关系

反向传播是自动微分（Automatic Differentiation）的一种特殊情况。理解这一关系有助于我们从更高的视角理解梯度的计算原理。

**定义2.4.4（自动微分的分类）**：自动微分主要分为两种模式：

（1）前向模式（Forward Mode）：沿计算图的前向方向累积计算导数。

（2）反向模式（Reverse Mode）：沿计算图的反向方向累积计算导数。

**定理2.4.11（反向传播是反向模式自动微分）**：反向传播算法对应于反向模式自动微分在神经网络中的应用。反向模式自动微分从输出节点开始，沿计算图的边反向传播梯度。对于神经网络，计算图是前向传播构建的 DAG，反向传播正是沿此图的反向边计算各节点对损失的梯度。这与反向模式自动微分的定义完全一致。

**表4-5：前向模式与反向模式的比较**

| 特性     | 前向模式自动微分        | 反向模式自动微分（反向传播）  |
| ------ | --------------- | --------------- |
| 计算方向   | 与前向传播相同         | 与前向传播相反         |
| 适用场景   | 雅可比矩阵向量积        | 标量损失函数的梯度       |
| 时间复杂度  | $O(n)$（$n$为输入数） | $O(m)$（$m$为输出数） |
| 空间复杂度  | $O(1)$（额外）      | $O(n+m)$（存储中间值） |
| 深度学习应用 | 有限              | 反向传播的核心         |

**定义2.4.5（梯度检查）**：梯度检查（Gradient Checking）是一种验证反向传播实现正确性的数值方法。其基本思想是用有限差分近似计算梯度，并与反向传播计算的梯度进行比较：
$$
\frac{\partial L}{\partial w_{ij}} \approx \frac{L(w_{ij} + \epsilon) - L(w_{ij} - \epsilon)}{2\epsilon}\tag{2.4.30}​
$$
如果两种方法计算的梯度足够接近（相对误差小于$10^{-7}$），则可以认为反向传播实现正确。

## 2.4.8 数值稳定性与实现细节

在实际实现反向传播时，需要特别注意数值稳定性问题。深度神经网络的训练涉及大量的梯度计算和参数更新，不当的实现可能导致数值溢出、梯度消失或梯度爆炸等问题。

**定义2.4.6（梯度裁剪）**：梯度裁剪（Gradient Clipping）是一种防止梯度爆炸的技术。常见的裁剪方式包括：

（1）按值裁剪：对梯度向量逐元素限制在$[−c,c]$范围内：
$$
\mathbf{g} \leftarrow \text{clip}(\mathbf{g}, -c, c)\tag{2.4.31}
$$
（2）按范数裁剪：如果梯度范数超过阈值，则按比例缩放：
$$
\mathbf{g} \leftarrow \begin{cases} \mathbf{g} & \text{if } \|\mathbf{g}\| \leq c \\ c \cdot \frac{\mathbf{g}}{\|\mathbf{g}\|} & \text{if } \|\mathbf{g}\| > c \end{cases}\tag{2.4.32}​
$$
**定理2.4.12（梯度裁剪的边界效应）**：梯度裁剪将梯度范数限制在$[0,c]$范围内，确保参数更新不会过大。

证明：对于按范数裁剪，若$\|\mathbf{g}\| \leq c$，则$\|\text{clipped}(\mathbf{g})\| = \|\mathbf{g}\| \leq c$；若$\|\mathbf{g}\| > c$，则$\|c \cdot \frac{\mathbf{g}}{\|\mathbf{g}\|}\| = c \cdot \frac{\|\mathbf{g}\|}{\|\mathbf{g}\|} = c$。因此，裁剪后的梯度范数始终不超过$c$。

**定义2.4.7（混合精度训练中的梯度缩放）**：在混合精度训练中，梯度可能因精度限制而下溢。梯度缩放（Gradient Scaling）通过在反向传播时放大损失值来间接放大梯度，从而避免下溢：
$$
L^\hat{L} = s \cdot L, \quad \mathbf{g}_{\text{true}} = \frac{1}{s} \cdot \mathbf{g}_{\hat{L}}\tag{2.4.33}​
$$
其中$s$是缩放因子，$\mathbf{g}_{\hat{L}}$是缩放后损失的梯度。

**表4-6：常见的数值稳定性问题与解决方案**

|问题|症状|解决方案|
|---|---|---|
|梯度消失|梯度值接近0，训练停滞|ReLU激活、残差连接、批归一化|
|梯度爆炸|梯度值溢出，损失NaN|梯度裁剪、适当的权重初始化|
|激活值溢出|中间值过大或过小|归一化层、数值稳定的softmax实现|
|精度下溢|梯度值在FP16下为0|混合精度训练、梯度缩放|


## 2.4.9 本节小结

本节系统阐述了反向传播算法的数学理论基础，从链式法则的矩阵形式出发，逐步推导出完整的梯度计算公式。本节的核心内容可以概括为以下几点：

第一，反向传播的核心是链式法则的高效应用。通过定义误差信号$\boldsymbol{\delta}^{(\ell)} = \frac{\partial L}{\partial \mathbf{z}^{(\ell)}}$，我们将复杂的复合函数求导问题分解为误差信号的前向传播和参数梯度的局部计算两个步骤。

第二，误差信号从前向传播的最后一层（输出层）开始，通过公式$\boldsymbol{\delta}^{(\ell)} = \left(\mathbf{W}^{(\ell+1)}\right)^T \boldsymbol{\delta}^{(\ell+1)} \odot \phi^{(\ell)'}(\mathbf{z}^{(\ell)})$逐层传播到输入层。这一过程实现了损失信息从输出到输入的逆向流动。

第三，参数梯度可以通过误差信号与前向传播保存的激活值计算：权重梯度为$\frac{\partial L}{\partial \mathbf{W}^{(\ell)}} = \boldsymbol{\delta}^{(\ell)} \left(\mathbf{h}^{(\ell-1)}\right)^T$，偏置梯度为$\frac{\partial L}{\partial \mathbf{b}^{(\ell)}} = \boldsymbol{\delta}^{(\ell)}$。

第四，反向传播的计算复杂度约为前向传播的两倍，但这是实现高效梯度计算的必要代价。通过批量处理，可以将多个样本的梯度计算合并为矩阵运算，充分利用硬件并行性。

第五，数值稳定性是反向传播实现中的关键考虑。梯度裁剪、混合精度训练、梯度缩放等技术都是保证大规模神经网络训练稳定性的重要手段。

掌握本节的数学基础后，读者应能够理解反向传播的工作原理，并有能力实现自己的深度学习框架或深入理解现有框架的底层机制。