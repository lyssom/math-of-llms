## 2.2 神经网络的矩阵形式
在上一节中，我们建立了单个神经元的数学模型，理解了仿射变换与激活函数的组合如何实现从输入到输出的非线性映射。然而，实际的神经网络由大量神经元组成，这些神经元按照层次结构组织。如果我们对每个神经元单独进行数学描述和计算，不仅会使得表达式繁琐冗长，更会错失利用矩阵运算简化计算的机会。本节将系统阐述如何将神经网络表示为矩阵运算的形式，这种表示方法不仅是深度学习框架实现高效计算的理论基础，也是理解反向传播算法的必要前提。

### 2.2.1 引言：从神经元到层

单个神经元的能力是有限的。正如我们在4.1节中看到的，单个感知机只能解决线性可分的问题，对于异或等线性不可分的模式无能为力。这一局限性的根本原因在于，单个神经元只形成了一个超平面决策边界，无论权重如何调整，这个决策边界始终是线性的。为了突破这一限制，我们需要组合多个神经元，形成多层网络结构。

**定义2.2.1（神经网络层）**：设有一组神经元$\{n_1, n_2, \ldots, n_m\}$，它们共享相同的输入$\mathbf{x} \in \mathbb{R}^n$，并各自产生输出$y_1, y_2, \ldots, y_m$​。这组神经元构成神经网络的一个层（Layer），记为：
$$
\mathbf{h} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})\tag{2.2.1}
$$
其中$\mathbf{W} \in \mathbb{R}^{m \times n}$为权重矩阵，$\mathbf{b} \in \mathbb{R}^m$为偏置向量，$\phi(\cdot)$为逐元素应用的激活函数，$\mathbf{h} \in \mathbb{R}^m$为该层的输出向量（也称为隐藏表示或隐藏激活）。

从几何角度看，一层中的$m$个神经元实际上定义了$m$个超平面。这些超平面的组合形成了一个复杂的决策边界，其表达能力远超单个超平面。例如，两个神经元的组合可以解决异或问题：第一个神经元学习"左上-右下"的对角线分类，第二个神经元学习"左下-右上"的对角线分类，两者的输出再通过某种方式组合，就可以实现异或逻辑。

**定理2.2.1（两层网络的异或解）**：使用两层神经网络可以解决异或问题。

证明：考虑一个两层网络，第一层有两个神经元，第二层有一个神经元。设输入为$\mathbf{x} = [x_1, x_2]^T$。设计权重和偏置如下：

第一层神经元1：权重$\mathbf{w}^{(1)}_1 = [1, 1]^T$，偏置$b^{(1)}_1 = -0.5$，激活函数为阈值函数。  
第一层神经元2：权重$\mathbf{w}^{(1)}_2 = [-1, -1]^T$，偏置$b^{(1)}_2 = 0.5$，激活函数为阈值函数。  
第二层：权重$\mathbf{w}^{(2)} = [1, 1]^T$，偏置$b^{(2)} = -0.5$，激活函数为阈值函数。

计算各层的输出：

- 当$\mathbf{x} = [0, 0]$时，第一层输出为$[0,0]$，第二层输出为0。
- 当$\mathbf{x} = [1, 1]$时，第一层输出为$[[0,0]$，第二层输出为0。
- 当$\mathbf{x} = [0, 1]$时，第一层输出为$[1,0]$，第二层输出为1。
- 当$\mathbf{x} = [1, 0]$时，第一层输出为$[0,1]$，第二层输出为1。

这正是异或函数的输出。

这个定理虽然使用了简化的阈值激活函数，但其核心洞见对连续激活函数同样适用：多层网络的表达能力超越了单层网络。通过增加网络的深度，我们可以构建任意复杂的非线性函数——这正是深度学习"深度"二字的含义所在。

层的概念在神经网络架构设计中具有核心地位。现代神经网络通常由以下类型的层组成：全连接层（Dense Layer或Fully Connected Layer），每一层的每个神经元与前一层的所有神经元相连；卷积层（Convolutional Layer），通过卷积核提取局部特征；循环层（Recurrent Layer），具有记忆功能，用于处理序列数据；注意力层（Attention Layer），通过注意力机制聚合信息。在本节中，我们主要关注全连接层，因为它的数学形式最为清晰，是理解其他层类型的基础。

### 2.2.2 单层网络的矩阵表示

全连接层是神经网络中最基本的层类型。在全连接层中，每个输出神经元与所有输入神经元相连，因此得名"全连接"。这种连接模式可以用矩阵乘法简洁地表示。

**定义2.2.2（全连接层）**：设输入向量为$\mathbf{x} \in \mathbb{R}^{n_{in}}$，输出向量为$\mathbf{h} \in \mathbb{R}^{n_{out}}$​。全连接层的数学定义为：
$$
\mathbf{h} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})
$$
其中$\mathbf{W} \in \mathbb{R}^{n_{out} \times n_{in}}$为权重矩阵，$\mathbf{b} \in \mathbb{R}^{n_{out}}$​为偏置向量，$\phi: \mathbb{R} \to \mathbb{R}$为激活函数，在此处按元素应用于向量。

**定义2.2.3（权重矩阵的元素含义）**：权重矩阵$\mathbf{W}$的元素$w_{ji}$​表示第$i$个输入特征与第$j$个输出神经元之间的连接强度，即：
$$
\mathbf{W} = \begin{bmatrix} w_{11} & w_{12} & \cdots & w_{1n_{in}} \\ w_{21} & w_{22} & \cdots & w_{2n_{in}} \\ \vdots & \vdots & \ddots & \vdots \\ w_{n_{out}1} & w_{n_{out}2} & \cdots & w_{n_{out}n_{in}} \end{bmatrix}\tag{2.2.2}​​​​
$$
其中行索引$j$对应输出神经元的索引，列索引$i$对应输入特征的索引。

**定理2.2.2（全连接层的计算展开）**：全连接层的矩阵运算等价于对每个输出神经元进行独立的仿射变换计算。

证明：将矩阵乘法展开为元素形式。对于输出向量的第$j$个元素：
$$
h_j = \phi\left(\sum_{i=1}^{n_{in}} w_{ji} x_i + b_j\right)\tag{2.2.3}
$$
这正是4.1节中定义的单个神经元的数学形式。因此，全连接层的矩阵表示是多个神经元并行计算的紧凑写法。

**定理2.2.3（齐次坐标形式的单层网络）**：使用齐次坐标（Homogeneous Coordinates），全连接层可以统一表示为单一的矩阵乘法。

证明：将输入扩展为齐次坐标$\tilde{\mathbf{x}} = [\mathbf{x}^T, 1]^T \in \mathbb{R}^{n_{in}+1}$，将权重矩阵和偏置合并为扩展权重矩阵$\tilde{\mathbf{W}} \in \mathbb{R}^{n_{out} \times (n_{in}+1)}$：
$$
\begin{align*}
\tilde{\mathbf{W}} &= \begin{bmatrix} \mathbf{W} & \mathbf{b} \end{bmatrix} \\ &= \begin{bmatrix} w_{11} & w_{12} & \cdots & w_{1n_{in}} & b_1 \\ w_{21} & w_{22} & \cdots & w_{2n_{in}} & b_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ w_{n_{out}1} & w_{n_{out}2} & \cdots & w_{n_{out}n_{in}} & b_{n_{out}} \end{bmatrix}\tag{2.2.4}
\end{align*}​​​​
$$
则全连接层可以表示为：
$$
\mathbf{h} = \phi\left(\tilde{\mathbf{W}} \tilde{\mathbf{x}}\right)\tag{2.2.5}
$$
其中$\tilde{\mathbf{W}} \tilde{\mathbf{x}}$的计算结果与$\mathbf{W}\mathbf{x} + \mathbf{b}$完全相同。

齐次坐标表示在理论上具有优雅性，它将仿射变换统一为线性变换的形式。然而在实际实现中，我们通常保持权重矩阵和偏置向量的分离形式，原因在于：第一，偏置向量通常有特殊的初始化策略（如初始化为零），与权重矩阵区别对待；第二，反向传播时对权重和偏置的梯度更新可能使用不同的学习率或正则化系数；第三，现代深度学习框架的优化器（如Adam）可能对权重和偏置维护不同的优化状态。

### 2.2.3 多层网络的矩阵形式

多层神经网络（Multi-Layer Neural Network）由多个全连接层堆叠而成，前一层的输出作为后一层的输入。这种层叠结构通过矩阵运算的复合来表示。

**定义2.2.4（L层前馈网络）**：设有一个$L$层的前馈神经网络，第$\ell$层的输入为$\mathbf{h}^{(\ell-1)}$，输出为$\mathbf{h}^{(\ell)}$，其中$\mathbf{h}^{(0)} = \mathbf{x}$为网络输入，$\mathbf{h}^{(L)} = \hat{\mathbf{y}}$​为网络输出。则各层的计算定义为：
$$
\mathbf{z}^{(\ell)} = \mathbf{W}^{(\ell)} \mathbf{h}^{(\ell-1)} + \mathbf{b}^{(\ell)}, \quad \mathbf{h}^{(\ell)} = \phi^{(\ell)}\left(\mathbf{z}^{(\ell)}\right)\tag{2.2.6}
$$
其中$\ell = 1, 2, \ldots, L$，$\mathbf{W}^{(\ell)}$和$\mathbf{b}^{(\ell)}$分别为第$\ell$层的权重矩阵和偏置向量，$\phi^{(\ell)}$为第$\ell$层的激活函数。

**定理2.2.4（多层网络的复合函数表示）**：$L$层前馈网络的计算可以表示为一系列函数的复合：
$$
\hat{\mathbf{y}} = f^{(L)} \circ f^{(L-1)} \circ \cdots \circ f^{(1)}(\mathbf{x})\tag{2.2.7}
$$
其中每个层映射$f^{(\ell)}: \mathbb{R}^{n_{\ell-1}} \to \mathbb{R}^{n_{\ell}}$​定义为：
$$
f^{(\ell)}(\mathbf{h}) = \phi^{(\ell)}\left(\mathbf{W}^{(\ell)} \mathbf{h} + \mathbf{b}^{(\ell)}\right)\tag{2.2.8}
$$
证明：直接由定义2.2.4的递归形式可得。将$\ell = 1$到$L$的等式依次代入：
$$
\mathbf{h}^{(1)} = f^{(1)}(\mathbf{x})\tag{2.2.9}
$$
$$\mathbf{h}^{(2)} = f^{(2)}(\mathbf{h}^{(1)}) = f^{(2)}(f^{(1)}(\mathbf{x}))\tag{2.2.10}$$
$$\vdots$$$$\hat{\mathbf{y}} = \mathbf{h}^{(L)} = f^{(L)}(f^{(L-1)}(\cdots f^{(1)}(\mathbf{x})\cdots))\tag{2.2.11}
$$
即网络输出是各层映射的复合。

**定义2.2.5（深度网络的层宽度）**：在深度学习中，第$\ell$层的维度$n_\ell$​称为该层的宽度（Width）。网络的深度$L$是层的数量，宽度可以是各层不同的。常见的网络配置包括：窄深网络（Few Narrow Layers），如LSTM通常使用数百维的隐藏状态；宽浅网络（Many Shallow Layers），经典的多层感知机常使用数百到数千个隐藏单元；均匀宽度网络，如ResNet使用64-2048等固定宽度。

**定理2.2.5（输出层的特殊设计）**：在多类分类问题中，输出层通常使用softmax激活函数，其数学形式为：
$$
P(y = k \mid \mathbf{x}) = \frac{e^{z^{(L)}_k}}{\sum_{j=1}^{K} e^{z^{(L)}_j}}, \quad \mathbf{z}^{(L)} = \mathbf{W}^{(L)}\mathbf{h}^{(L-1)} + \mathbf{b}^{(L)}\tag{2.2.12}
$$
其中$K$是类别数量，$\mathbf{z}^{(L)}$称为logits向量。

证明：softmax函数将任意实数向量映射为概率分布，满足非负性和归一性。给定$\mathbf{z}^{(L)}$，softmax的输出可以解释为每个类别的后验概率。这一设计在概率论上等价于多项式逻辑回归，在信息论上与交叉熵损失函数相容（详见第四章）。

![[full_conn.drawio.png]]

### 2.2.4 权重矩阵的性质与约束

权重矩阵是神经网络中需要学习的参数。理解权重矩阵的数学性质对于设计有效的初始化方法、正则化策略和优化算法至关重要。

**定义2.2.6（权重矩阵的范数）**：权重矩阵$\mathbf{W}$的常用范数包括：

（1）弗罗贝尼乌斯范数（Frobenius Norm）：
$$
\|\mathbf{W}\|_F = \sqrt{\sum_{i,j} w_{ij}^2} = \sqrt{\text{tr}(\mathbf{W}^T\mathbf{W})}\tag{2.2.13}​
$$
（2）谱范数（Spectral Norm，即最大奇异值）：  
$$\|\mathbf{W}\|_2 = \sigma_{\max}(\mathbf{W}) = \sqrt{\lambda_{\max}(\mathbf{W}^T\mathbf{W})}\tag{2.2.14}$$
（3）核范数（Nuclear Norm，即奇异值之和）：  
$$\|\mathbf{W}\|_* = \sum_{i} \sigma_i(\mathbf{W})\tag{2.2.15}$$
**定理2.2.6（权重初始化与激活方差）**：为了使信息在网络中有效传播，权重矩阵的初始化应使得各层的激活值具有合适的方差。

证明：考虑单个输入元素$x$通过全连接层的情况。为简化分析，假设输入和偏置的均值为零，且各元素相互独立。则输出元素为：
$$
z_j = \sum_{i=1}^{n_{in}} w_{ji} x_i + b_j\tag{2.2.16}​
$$
假设权重和输入独立同分布，且$E[w_{ji}] = E[x_i] = E[b_j] = 0$，则输出的方差为：
$$
\begin{align*}
Var(xi)\text{Var}(z_j) &= \text{Var}\left(\sum_{i=1}^{n_{in}} w_{ji} x_i\right) \\&= \sum_{i=1}^{n_{in}} \text{Var}(w_{ji}x_i) \\& = \sum_{i=1}^{n_{in}} \text{Var}(w_{ji})\text{Var}(x_i)\tag{2.2.17}
\end{align*}
$$
若权重方差为$\sigma_w^2$，输入方差为$\sigma_x^2$​，则：
$$
\text{Var}(z_j) = n_{in} \sigma_w^2 \sigma_x^2\tag{2.2.18}​
$$
为了保持方差稳定（既不爆炸也不消失），我们希望$n_{in} \sigma_w^2 \approx 1$，即$\sigma_w^2 \approx 1/n_{in}$​。这正是Xavier初始化的理论基础。类似地，对于反向传播的梯度流动，可以推导出$\sigma_w^2 \approx 2/n_{in}$的建议，这启发了他后续的He初始化方法。

**定义2.2.7（常用的权重初始化方法）**：

（1）Xavier初始化：权重从均匀分布$\left(-\sqrt{\frac{6}{n_{in}+n_{out}}}, \sqrt{\frac{6}{n_{in}+n_{out}}}\right)$或正态分布$\left(0, \sqrt{\frac{2}{n_{in}+n_{out}}}\right)$中采样。

（2）He初始化（Kaiming初始化）：权重从正态分布$N(0, \sqrt{2/n_{in}})$或均匀分布$U\left(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}}\right)$中采样。

（3）正交初始化：权重矩阵初始化为正交矩阵，即满足$\mathbf{W}^T\mathbf{W} = \mathbf{I}$，这保证了权重矩阵的谱范数为1。

**定理2.2.7（正交初始化的性质）**：使用正交初始化的权重矩阵具有以下性质：谱范数$\|\mathbf{W}\|_2 = 1$，这意味着信息在传播过程中不会被放大；行向量和列向量都是单位正交的，这保证了各输入特征和输出神经元之间的独立性；在反向传播时，梯度也不会被放大或缩小。

证明：正交矩阵$\mathbf{W}$满足$\mathbf{W}^T\mathbf{W} = \mathbf{I}$。因此，其最大奇异值:$$\sigma_{\max}(\mathbf{W}) = \sqrt{\lambda_{\max}(\mathbf{W}^T\mathbf{W})} = \sqrt{\lambda_{\max}(\mathbf{I})} = 1\tag{2.2.19}$$故谱范数为1。正交矩阵的逆矩阵就是其转置，这使得反向传播的计算具有良好的数值稳定性。

### 2.2.5 批量处理的矩阵运算

在实际训练中，我们通常一次处理多个样本（一个批次，batch）。矩阵表示的优势在此体现得淋漓尽致：批量处理可以自然地通过扩展矩阵维度来实现，无需修改网络结构。

**定义2.2.8（批量输入与输出）**：设批次大小为$B$，输入维度为$n_{in}$​，隐藏层维度$n_{hidden}$​，输出维度为$n_{out}$​。批量输入矩阵$\mathbf{X} \in \mathbb{R}^{B \times n_{in}}$的每一行是一个输入样本，批量输出矩阵$\mathbf{H} \in \mathbb{R}^{B \times n_{hidden}}$的每一行是对应样本的隐藏表示。

**定理2.2.8（批量前向传播的矩阵形式）**：对于批量输入$\mathbf{X}$，全连接层的前向传播为：
$$
\mathbf{Z} = \mathbf{X}\mathbf{W} + \mathbf{1}_B \mathbf{b}^T, \quad \mathbf{H} = \phi(\mathbf{Z})\tag{2.2.20}
$$
其中$\mathbf{Z} \in \mathbb{R}^{B \times n_{hidden}}$​为批量净输入，$\mathbf{1}_B \in \mathbb{R}^{B}$是全1向量，$\phi(\cdot)$按元素应用于整个矩阵。

证明：将$\mathbf{X}$的第$i$行记为$\mathbf{x}_i \in \mathbb{R}^{1 \times n_{in}}$​，$\mathbf{Z}$的第$i$行为$\mathbf{z}_i \in \mathbb{R}^{1 \times n_{hidden}}$，$\mathbf{b}^T$为行向量。则：
$$
\mathbf{z}_i = \mathbf{x}_i \mathbf{W} + \mathbf{b}^T\tag{2.2.21}
$$
这正是单个样本的仿射变换公式。将所有$B$个样本的计算写在一起，即得到矩阵形式的批量运算。
**定义2.2.9（广播运算）**：在上述公式中，$\mathbf{1}_B \mathbf{b}^T$实现了将偏置向量$\mathbf{b}$广播（Broadcast）到整个批次。广播是一种隐式的维度扩展操作，使得标量或低维张量可以自动匹配高维张量的形状进行运算。

**表4-1：批量运算的维度分析**

| 变量                          | 形状                          | 说明     |
| --------------------------- | --------------------------- | ------ |
| $\mathbf{X}$                | $B \times n_{in}$​          | 批量输入   |
| $\mathbf{W}$                | $n_{in} \times n_{hidden}$​ | 权重矩阵   |
| $\mathbf{b}$                | $n_{hidden}$​               | 偏置向量   |
| $\mathbf{1}_B \mathbf{b}^T$ | $B \times n_{hidden}$       | 广播后的偏置 |
| $\mathbf{Z}$                | $B \times n_{hidden}$​      | 批量净输入  |
| $\mathbf{H}$                | $B \times n_{hidden}$​      | 批量隐藏激活 |

### 2.2.6 计算复杂度分析

理解神经网络计算的时间和空间复杂度，对于选择合适的网络架构和优化策略至关重要。

**定理2.2.9（前向传播的计算复杂度）**：对于$L$层全连接网络，输入维度为$n_0$，第$\ell$层维度为$n_\ell$（$\ell = 1, \ldots$）。单次前向传播的浮点运算数（FLOPs）为：
$$
\text{FLOPs}_{\text{forward}} = \sum_{\ell=1}^{L} \left(2n_{\ell-1}n_\ell + n_\ell\right)\tag{2.2.22}
$$
其中$n_{\ell-1}n_\ell$来自矩阵乘法（乘法和加法各$n_{\ell-1}n_\ell$​次），$n_\ell$来自偏置加法。

证明：考虑第$\ell$层的矩阵乘法$\mathbf{X}\mathbf{W}$，其中$\mathbf{X} \in \mathbb{R}^{B \times n_{\ell-1}}$，$\mathbf{W} \in \mathbb{R}^{n_{\ell-1} \times n_\ell}$​。输出矩阵为$\mathbf{Z} \in \mathbb{R}^{B \times n_\ell}$​，每个输出元素的计算需要$n_{\ell-1}$次乘法和$n_{\ell-1}-1$次加法。近似为$n_{\ell-1}n_\ell$​次浮点运算。偏置加法需要$n_\ell$​次加法。对各层求和即得总复杂度。

**推论2.2.1（单样本前向传播复杂度）**：当$B=1$时，单样本前向传播的复杂度为：
$$
\text{FLOPs}_{\text{single}} = \sum_{\ell=1}^{L} \left(2n_{\ell-1}n_\ell + n_\ell\right) \approx 2\sum_{\ell=1}^{L} n_{\ell-1}n_\ell\tag{2.2.23}
$$
**定理2.2.10（参数数量与存储需求）**：全连接网络的参数量（可学习参数数量）为：
$$
\text{Params} = \sum_{\ell=1}^{L} \left(n_{\ell-1}n_\ell + n_\ell\right) = \sum_{\ell=1}^{L} n_\ell(n_{\ell-1} + 1)\tag{2.2.24}
$$
对应的存储空间需求（假设使用32位浮点数）为$4 \times \text{Params}$字节。

证明：每个权重矩阵有$n_{\ell-1}n_\ell$​个参数，每个偏置向量有$n_\ell$​个参数。将各层的参数数量相加即得总参数量。每个32位浮点数占用4字节，故存储空间为参数量乘以4。

**例2.2.1（GPT-3规模模型的参数估算）**：考虑一个简化的GPT-3风格模型（此处以全连接层为例简化计算，实际Transformer还包含注意力层）：隐藏维度$n=12288$，层数$L=96$，前馈维度（FFN扩展后）为$4n=49152$。全连接层的参数量约为：
$$
\begin{align*}
& \text{Params}_{\text{FFN}} \\ &\approx L \times (n \times 4n + 4n \times n) \\ &\approx 96 \times (12288 \times 49152 + 49152 \times 12288) \approx 1.15 \times 10^{11}\tag{2.2.25}
\end{align*}
$$
即约1150亿参数，与GPT-3的实际规模相当。这一数量级的参数需要使用混合精度训练、模型并行等高级技术才能在合理的时间内完成训练。

![[gpt3param.drawio.png]]

### 2.2.7 本节小结

本节系统阐述了神经网络的矩阵表示方法，从单层网络的矩阵形式出发，推广到多层网络的复合表示，并分析了权重矩阵的数学性质和批量处理的计算优势。本节的核心内容可以概括为以下几点：

第一，全连接层可以用紧凑的矩阵形式$\mathbf{h} = \phi(\mathbf{W}\mathbf{x} + \mathbf{b})$表示，其中权重矩阵$\mathbf{W}$编码了层内神经元之间的连接强度。这种表示不仅在数学上简洁优雅，更是深度学习框架实现高效计算的基础。

第二，多层网络通过矩阵运算的复合来表示，每一层的输出作为下一层的输入。网络的信息流可以用复合函数$f = f^{(L)} \circ \cdots \circ f^{(1)}$来描述，网络的深度和宽度决定了其参数规模和表达能力。

第三，权重矩阵的初始化对训练动态有重要影响。Xavier初始化和He初始化等方法基于对激活和梯度方差的理论分析，为实践提供了可靠的初始化策略。正交初始化则通过保持权重矩阵的谱范数为1，确保了信息在网络中的稳定传播。

第四，批量处理可以自然地通过扩展矩阵维度来实现，使得一次矩阵运算同时处理多个样本。这种批量处理方式不仅提高了计算效率，还提供了隐式的正则化效果，是深度学习训练的标准范式。

掌握本节的矩阵表示方法后，下一节我们将深入探讨前向传播的数学本质，理解信息如何在网络中逐层流动和变换，以及这种流动如何实现复杂的函数逼近。