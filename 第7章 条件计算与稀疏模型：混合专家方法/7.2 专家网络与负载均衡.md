专家网络是MoE架构中负责实际计算的核心组件，而负载均衡则是确保MoE模型高效训练的关键挑战。本节将深入探讨专家网络的数学表示、加权组合输出的计算方式、路由概率的统计分析，以及多种负载均衡策略的数学原理。这些内容构成了理解和实现高效MoE系统的理论基础，对于在大规模训练场景中充分发挥MoE架构的潜力至关重要。
### 7.2.1 专家网络的数学表示
专家网络（Expert Networks）是MoE架构中执行具体计算任务的功能模块，其设计直接影响模型的整体表达能力和计算效率。从数学角度看，每个专家可以视为一个从输入空间到输出空间的参数化函数映射，而专家集合则构成了一个函数库，门控机制根据输入内容从中选择合适的函数进行计算。
设输入向量为 $\mathbf{x} \in \mathbb{R}^d$，单个专家 $E_i$ 的数学表示通常采用两层前馈神经网络的形式，这是Transformer架构中前馈网络的标准配置。专家的计算过程可以分解为两个连续的线性变换，中间嵌入一个非线性激活函数：
$$E_i(\mathbf{x}) = \sigma(\mathbf{x}^\top \mathbf{W}_1^{(i)} + \mathbf{b}_1^{(i)}) \mathbf{W}_2^{(i)} + \mathbf{b}_2^{(i)})$$
其中 $\mathbf{W}_1^{(i)} \in \mathbb{R}^{d \times d_e}$ 是第一层权重矩阵，$\mathbf{W}_2^{(i)} \in \mathbb{R}^{d_e \times d}$ 是第二层权重矩阵，$\mathbf{b}_1^{(i)} \in \mathbb{R}^{d_e}$ 和 $\mathbf{b}_2^{(i)} \in \mathbb{R}^d$ 是偏置向量，$d_e$ 是专家网络的隐藏维度，$\sigma$ 是激活函数（如GELU或Swish）。这个表达式也可以等价地写成向量形式：
$$\mathbf{h}_i = \sigma(\mathbf{W}_1^{(i)} \mathbf{x} + \mathbf{b}_1^{(i)})$$
$$E_i(\mathbf{x}) = \mathbf{W}_2^{(i)} \mathbf{h}_i + \mathbf{b}_2^{(i)})$$
其中 $\mathbf{h}_i \in \mathbb{R}^{d_e}$ 是专家的中间隐藏表示。从参数量的角度分析，单个专家的参数量为 $\mathcal{N}_e = d \cdot d_e + d_e \cdot d + d_e + d \approx 2d \cdot d_e$（忽略偏置项的小量贡献）。对于拥有 $K$ 个专家的MoE层，总专家参数量为 $\mathcal{N}_{\text{total}} = K \cdot \mathcal{N}_e$，这可以是同等配置稠密层的 $K$ 倍。
专家集合可以数学上表示为一个映射 $\mathcal{E}: \mathbb{R}^d \to \mathbb{R}^{K \times d}$，将输入向量映射为 $K$ 个专家输出的集合：
$$\mathcal{E}(\mathbf{x}) = (E_1(\mathbf{x}), E_2(\mathbf{x}), \ldots, E_K(\mathbf{x}))$$
这种表示强调了专家之间的并行性：理论上所有 $K$ 个专家都可以同时对输入进行计算，然后由门控机制选择性地组合它们的输出。在实际实现中，由于稀疏激活的存在，只有 $K' \ll K$ 个专家会被实际计算，但这种并行计算的抽象表示对于理解MoE的数学结构仍然很有价值。
专家网络的内部结构设计有多种变体。一种重要的变体是引入专家间的参数共享：在某些层，不同专家共享部分参数，只有特定部分保持独立。例如，可以定义共享专家和独立专家的组合：
$$E_i^{\text{shared}}(\mathbf{x}) = \sigma(\mathbf{x}^\top \mathbf{W}_1^{\text{shared}} + \mathbf{b}_1^{\text{shared}}) \mathbf{W}_2^{\text{shared}} + \mathbf{b}_2^{\text{shared}}$$
$$E_i^{\text{unique}}(\mathbf{x}) = \sigma(\mathbf{x}^\top \mathbf{W}_1^{(i)} + \mathbf{b}_1^{(i)}) \mathbf{W}_2^{(i)} + \mathbf{b}_2^{(i)})$$
$$E_i(\mathbf{x}) = E_i^{\text{shared}}(\mathbf{x}) + E_i^{\text{unique}}(\mathbf{x})$$
这种设计的数学优势在于：共享参数减少了总参数量，降低了过拟合风险，同时保留了专家专业化学习独特模式的能力。从计算角度看，共享部分的计算只需要执行一次，其结果可以被所有专家复用，从而减少了总计算量。
在Transformer架构的具体实现中，MoE层通常替代标准Transformer中的前馈网络层。设Transformer层的输入为 $\mathbf{H} \in \mathbb{R}^{S \times d}$（$S$ 为序列长度，$d$ 为隐藏维度），多头注意力子层的输出为 $\mathbf{H}_{\text{attn}}$，则MoE层的输出为：
$$\mathbf{H}_{\text{MoE}} = \text{MoE-Layer}(\mathbf{H}_{\text{attn}}) = \sum_{i=1}^{K} \mathbf{G}_i \odot E_i(\mathbf{H}_{\text{attn}})$$
其中 $\mathbf{G}_i \in \mathbb{R}^{S \times d}$ 是专家 $i$ 的门控权重矩阵（通过 $\mathbf{G}_i = \mathbf{g}(\mathbf{x}) \otimes \mathbf{1}_d$ 扩展），$\odot$ 表示逐元素 Hadamard 积，$\mathbf{1}_d$ 是 $d$ 维全1向量。这个表达式表明门控权重是针对每个token位置独立计算的，不同位置的token可能被路由到不同的专家。
### 7.2.2 加权组合输出的数学表达
MoE层的最终输出是专家输出的加权组合，这一操作将门控决策与专家计算结果融合，产生最终的模型输出。加权组合的数学形式简洁优雅，体现了条件计算范式的核心理念，同时为后续的梯度传播和参数优化提供了清晰的数学框架。
设专家集合为 $\mathcal{E} = \{E_1, E_2, \ldots, E_K\}$，门控函数为 $G: \mathbb{R}^d \to \mathbb{R}^K$，则MoE层的输出为专家输出的加权和：
$$\mathbf{y} = \sum_{i=1}^{K} g_i(\mathbf{x}) \cdot E_i(\mathbf{x})$$
其中 $g_i(\mathbf{x})$ 是门控函数 $G$ 的第 $i$ 个输出分量，表示专家 $E_i$ 对输入 $\mathbf{x}$ 的权重贡献。这个表达式可以理解为：每个专家对输入的处理结果 $E_i(\mathbf{x})$ 被门控权重 $g_i(\mathbf{x})$ 加权后累加，最终输出是所有专家贡献的线性组合。
在稀疏激活的Top-K路由设置下，加权组合退化为仅对被激活的专家求和。设Top-K路由策略选择的专家索引集合为 $S(\mathbf{x}) = \{i_1, i_2, \ldots, i_{K'}\}$，其中 $K'$ 是激活的专家数量（通常 $K' \ll K$），则输出公式变为：
$$\mathbf{y} = \sum_{i \in S(\mathbf{x})} g_i^{\text{sparse}}(\mathbf{x}) \cdot E_i(\mathbf{x})$$
其中稀疏门控权重 $g_i^{\text{sparse}}(\mathbf{x})$ 满足归一化条件 $\sum_{i \in S(\mathbf{x})} g_i^{\text{sparse}}(\mathbf{x}) = 1$。这种归一化通过在激活专家的权重上重新应用Softmax实现：
$$g_i^{\text{sparse}}(\mathbf{x}) = \frac{\exp(s_i(\mathbf{x}))}{\sum_{j \in S(\mathbf{x})} \exp(s_j(\mathbf{x}))}, \quad i \in S(\mathbf{x})$$
其中 $s_i(\mathbf{x}) = (\mathbf{W}\mathbf{x} + \mathbf{b})_i$ 是专家 $i$ 的原始门控得分，未被激活的专家的 $g_i^{\text{sparse}}(\mathbf{x}) = 0$。
从线性代数的角度审视，加权组合输出可以表示为更紧凑的矩阵运算形式。设专家权重矩阵的集合为 $\{\mathbf{W}_1^{(i)}, \mathbf{W}_2^{(i)}\}_{i=1}^{K}$，门控权重向量为 $\mathbf{g}(\mathbf{x}) = (g_1(\mathbf{x}), \ldots, g_K(\mathbf{x}))$，则对于输入 $\mathbf{x}$，输出可以紧凑地写为：
$$\mathbf{y} = \left(\sum_{i=1}^{K} g_i(\mathbf{x}) \cdot \mathbf{W}_2^{(i)} \cdot \sigma(\mathbf{W}_1^{(i)} \mathbf{x} + \mathbf{b}_1^{(i)})\right) + \sum_{i=1}^{K} g_i(\mathbf{x}) \cdot \mathbf{b}_2^{(i)}$$
这个表达式虽然复杂，但揭示了MoE计算的核心结构：输出是专家输出的凸组合，组合系数由输入决定的门控权重给出。
对于批次输入的处理需要特别考虑。设批次输入为 $\mathbf{X} \in \mathbb{R}^{B \times S \times d}$，其中 $B$ 是批次大小，$S$ 是序列长度。门控函数通常在序列维度上独立地为每个位置计算权重：$\mathbf{G} \in \mathbb{R}^{B \times S \times K}$，其中 $\mathbf{G}_{b,s,:}$ 是位置 $(b, s)$ 的门控权重向量。专家输出也是逐位置计算的：$\mathbf{E}_i(\mathbf{X}) \in \mathbb{R}^{B \times S \times d}$。最终输出为：
$$\mathbf{Y}_{b,s,:} = \sum_{i=1}^{K} \mathbf{G}_{b,s,i} \cdot \mathbf{E}_i(\mathbf{X})_{b,s,:}$$
这种逐位置的路由策略允许序列中不同位置的token激活不同的专家，使得MoE能够处理序列内的异质性。例如，在处理一段包含代码和自然语言的混合文本时，代码token可能被路由到具有编程知识专家，而自然语言token被路由到语言理解专家。
加权组合输出的梯度计算是反向传播的关键。设损失函数为 $\mathcal{L}(\mathbf{y})$，则损失关于门控权重 $g_i(\mathbf{x})$ 和专家参数 $\theta_i$ 的梯度分别为：
$$\frac{\partial \mathcal{L}}{\partial g_i(\mathbf{x})} = \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot E_i(\mathbf{x})$$
$$\frac{\partial \mathcal{L}}{\partial \theta_i} = \frac{\partial \mathcal{L}}{\partial g_i(\mathbf{x})} \cdot \frac{\partial g_i(\mathbf{x})}{\partial \theta_i} + \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial E_i(\mathbf{x})}{\partial \theta_i} \cdot \mathbb{I}(i \in S(\mathbf{x}))$$
其中 $\mathbb{I}(\cdot)$ 是指示函数，表明只有被激活的专家才会接收到梯度来更新参数。这两个梯度公式指导了MoE模型的端到端优化：门控网络通过第一个公式学习更好的路由决策，专家网络通过第二个公式学习更好的特征变换。
### 7.2.3 路由概率与专家分配
路由概率是门控机制的输出结果，它决定了每个输入被分配给各个专家的概率。在数学上，路由概率是门控得分的归一化结果，反映了专家对当前输入的"适合程度"。理解路由概率的计算、分布和专家分配的模式，对于掌握MoE的训练动态和优化策略至关重要。
路由得分的计算是路由概率的基础。设门控网络的原始输出为 $\mathbf{s}(\mathbf{x}) = (s_1(\mathbf{x}), s_2(\mathbf{x}), \ldots, s_K(\mathbf{x})) = \mathbf{W}\mathbf{x} + \mathbf{b}$，其中 $\mathbf{s}(\mathbf{x}) \in \mathbb{R}^K$ 是每个专家的原始得分。路由得分的分布和特性直接影响专家分配的模式。门控网络的参数 $\mathbf{W}$ 和 $\mathbf{b}$ 在训练过程中不断调整，以学习将不同类型的输入路由到最适合的专家。
路由概率的归一化通过Softmax函数实现：
$$p_i(\mathbf{x}) = \frac{\exp(s_i(\mathbf{x}))}{\sum_{j=1}^{K} \exp(s_j(\mathbf{x}))}$$
Softmax归一化确保了路由概率满足概率分布的基本性质：$p_i(\mathbf{x}) \geq 0$ 且 $\sum_{i=1}^{K} p_i(\mathbf{x}) = 1$。Softmax函数的核心特性是它将任意实数向量转换为有效的概率分布，同时保持原始得分的相对顺序，得分最高的专家仍然获得最高的概率。然而，Softmax的归一化也意味着每个专家的路由概率不仅取决于自己的得分，还取决于其他专家的得分：当某个专家的得分显著提高时，其他所有专家的路由概率都会相应降低。
在Top-K稀疏路由中，专家分配由路由概率的大小决定，但实际参与计算的专家是概率最高的 $K'$ 个。设 $S(\mathbf{x}) = \{i_1, i_2, \ldots, i_{K'}\}$ 是Top-K选择的专家索引，则专家 $i$ 被选中的概率（在随机初始化或期望意义上）为：
$$P_i = \mathbb{P}(I_i = 1) = \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \mathbb{I}(p_i(\mathbf{x}) \text{ 属于最高的 } K' \text{ 个概率}) \right]$$
其中 $I_i \in \{0, 1\}$ 是专家 $i$ 是否被激活的指示随机变量。专家分配概率 $P_i$ 反映了在长期运行中专家 $i$ 被使用的频率，是衡量负载均衡的重要指标。
理想的路由应该使得专家分配尽可能均匀，即 $P_i \approx 1/K$ 对所有 $i$ 成立。设专家分配概率的平均值为 $\bar{P} = \frac{1}{K} \sum_{i=1}^{K} P_i = K'/K$，则分配概率的方差为：
$$\text{Var}(P) = \frac{1}{K} \sum_{i=1}^{K} (P_i - \bar{P})^2$$
方差的取值范围为 $[0, ((K-1)/K)^2]$。当 $\text{Var}(P) = 0$ 时，所有专家被均匀使用，这是理想的负载均衡状态；当 $\text{Var}(P)$ 接近最大值时，一个专家被使用 $(K-1)/K$ 的时间，其他专家几乎不被使用，这是最严重的负载不均衡。
路由概率的条件期望也是重要的分析工具。给定专家 $i$ 被激活的条件下，其他专家被激活的条件概率为：
$$P(I_j = 1 | I_i = 1) = \frac{\mathbb{P}(I_i = 1, I_j = 1)}{P_i}$$
这个条件概率反映了专家之间的协同或竞争关系。如果 $P(I_j = 1 | I_i = 1) > P_j$，说明专家 $i$ 和 $j$ 倾向于同时被选中，它们可能在处理相似的输入类型；如果 $P(I_j = 1 | I_i = 1) < P_j$，说明专家 $i$ 和 $j$ 倾向于互斥，它们可能分别处理不同类型的输入。
专家分配的时间动态也是需要关注的方面。在训练过程中，专家分配概率 $P_i(t)$ 随训练步数 $t$ 变化。理想的训练动态是：初期 $P_i(t)$ 接近均匀（随机初始化导致），随着训练进行，某些专家逐渐专业化，$P_i(t)$ 开始分化但最终收敛到一个相对均衡的分布。如果 $P_i(t)$ 在训练早期就快速收敛到极端不均衡的状态，可能表明门控网络陷入了局部最优，需要引入正则化来促进专家多样化。
### 7.2.4 负载均衡辅助损失
负载均衡是MoE训练中最核心的挑战之一。如果门控网络将大多数输入路由到少数专家，会导致大部分专家参数得不到充分训练，浪费了模型的容量潜力。为了解决这个问题，研究者提出了多种负载均衡辅助损失函数，通过正则化门控网络来促进专家的均匀使用。
最简单的负载均衡损失是基于专家分配频率的变分损失。设 $f_i$ 是专家 $i$ 在当前批次中被选中的频率（即批内分配比例）：
$$f_i = \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbb{I}(i \in S(\mathbf{x}))$$
其中 $\mathcal{B}$ 是当前批次，$|\mathcal{B}|$ 是批次大小，$\mathbb{I}(\cdot)$ 是指示函数。$f_i$ 表示在当前批次中，专家 $i$ 被激活的样本比例。理想情况下，我们希望 $f_i \approx 1/K$ 对所有 $i$ 成立，即每个专家处理相同比例的输入。
基于 $f_i$ 的负载均衡损失可以定义为交叉熵形式：
$$\mathcal{L}_{\text{balance}} = -\sum_{i=1}^{K} \frac{1}{K} \log f_i = \log K - \frac{1}{K} \sum_{i=1}^{K} \log f_i$$
这个损失在 $f_i = 1/K$ 时最小（值为0），当 $f_i$ 偏离均匀分布时增大。然而，这个损失存在问题：当 $f_i = 0$ 时，$\log f_i$ 趋向负无穷，导致损失爆炸。因此，实际中通常使用更稳定的变体：
$$\mathcal{L}_{\text{balance}} = K \cdot \sum_{i=1}^{K} f_i \cdot \log(K \cdot f_i)$$
这个损失在 $f_i = 1/K$ 时最小，在 $f_i$ 接近0时也有定义（极限为0），在 $f_i$ 过大时增大。数学上，这是 $f_i$ 与均匀分布的KL散度的 $K$ 倍：$\mathcal{L}_{\text{balance}}= \mathrm{KL}\!\left(\mathrm{Uniform} \,\|\, \mathbf{f}\right)$。
另一种常用的负载均衡损失是平方偏差损失：
$$\mathcal{L}_{\text{balance}}^{\text{squared}} = \sum_{i=1}^{K} \left( f_i - \frac{1}{K} \right)^2$$
这个损失对极端不均衡的惩罚更大，因为它对偏离均值的平方进行累加。设 $f_i = 1/K + \delta_i$，则 $\sum_{i=1}^{K} \delta_i = 0$，平方损失为 $\sum_{i=1}^{K} \delta_i^2$。当一个专家的 $f_i$ 显著偏离 $1/K$ 时，$\delta_i$ 很大，平方损失会急剧增大。
更精细的负载均衡损失同时考虑专家分配频率 $f_i$ 和路由概率 $P_i$（平均路由概率）：
$$\mathcal{L}_{\text{balance}} = \sum_{i=1}^{K} f_i \cdot P_i$$
这个损失的目标是使得 $f_i$ 和 $P_i$ 尽可能接近，从而确保专家的长期使用频率与其被选中的概率一致。设 $P_i$ 是专家 $i$ 的平均路由概率（定义为 $P_i = \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} p_i(\mathbf{x})$），则 $\mathcal{L}_{\text{balance}} = \sum_{i=1}^{K} f_i P_i$ 在 $f_i = P_i$ 时最小。注意到 $\sum_{i=1}^{K} f_i = 1$ 和 $\sum_{i=1}^{K} P_i = K'$，可以推导：
$$\sum_{i=1}^{K} f_i P_i \geq \frac{1}{K} \sum_{i=1}^{K} P_i = \frac{K'}{K}$$
当 $f_i$ 与 $P_i$ 成正比时等号成立，这表明在给定 $P_i$ 的情况下，$f_i$ 应该与 $P_i$ 匹配以最小化损失。
在实际训练中，负载均衡损失通常乘以一个权重系数 $\lambda$ 后与主损失（如交叉熵损失）相加：
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{main}} + \lambda \cdot \mathcal{L}_{\text{balance}}$$
权重 $\lambda$ 的选择需要在负载均衡和主任务性能之间取得平衡：$\lambda$ 太小，负载均衡效果不明显；$\lambda$ 太大，可能影响主任务的优化，甚至导致主任务性能下降。实践中，$\lambda$ 通常设置为可学习的参数或使用退火策略：在早期训练阶段使用较大的 $\lambda$ 促进专家分化，在后期训练阶段减小 $\lambda$ 让模型专注于主任务优化。
## 7.2.5 防止专家坍缩的数学原理
专家坍缩（Expert Collapse）是MoE训练中的典型失效模式，指门控网络将几乎所有输入都路由到少数几个专家，导致其他专家几乎不被激活，浪费了大部分模型容量。理解专家坍缩的数学机制和防止策略，对于训练有效的MoE模型至关重要。
专家坍缩的数学本质是门控网络陷入了不良的局部最优。设门控网络的原始得分为 $\mathbf{s}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}$，如果门控网络学习到一组参数使得对于大多数输入 $\mathbf{x}$，少数专家的得分显著高于其他专家，就会导致路由决策的极度不均衡。数学上，这种状态可以用专家分配概率的集中度来刻画。设 $P_{(1)} \geq P_{(2)} \geq \cdots \geq P_{(K)}$ 是排序后的专家分配概率，则集中度可以定义为前 $m$ 个专家的累积概率：
$$C_m = \sum_{i=1}^{m} P_{(i)}$$
在专家坍缩状态下，$C_1 \approx K'$（第一个专家承担几乎所有负载），$C_2 \approx K'$（前两个专家承担几乎所有负载），依此类推。而在理想的均衡状态下，$C_m \approx m \cdot K'/K$。
专家坍缩的发生可以用梯度消失的机制来解释。设专家 $j$ 在当前批次中从未被激活，则 $f_j = 0$。在负载均衡损失 $\mathcal{L}_{\text{balance}} = \sum_{i=1}^{K} f_i P_i$ 中，$f_j = 0$ 使得该项贡献为零。然而，更重要的是，专家 $j$ 不会接收到任何梯度来调整其门控参数 $\mathbf{W}^{(j)}$ 和 $\mathbf{b}^{(j)}$，因为 $p_j(\mathbf{x})$ 不参与任何非零权重的计算。这种正反馈机制导致专家 $j$ 的门控参数保持不变，无法学习改进路由决策。
负载均衡损失通过双重正则化机制防止专家坍缩。对于高频专家（$f_i$ 较大），虽然 $f_i \cdot P_i$ 项增大，但由于 $P_i$ 通常不会同步增大（路由概率受Softmax归一化限制），损失会惩罚这种不平衡。对于低频专家（$f_i$ 较小或为零），损失项 $f_i \cdot P_i$ 很小或为零，但 $P_i$ 本身可能较大（如果专家的专业得分高但没有被激活），这会鼓励门控网络增加对该专家的路由。
数学上，可以分析负载均衡损失的梯度对专家分配的影响。设 $P_i$ 是专家 $i$ 的平均路由概率，$f_i$ 是实际分配频率，损失 $\mathcal{L}_{\text{balance}} = \sum_{i=1}^{K} f_i P_i$ 关于 $P_i$ 的梯度为：
$$\frac{\partial \mathcal{L}_{\text{balance}}}{\partial P_i} = f_i$$
这意味着，如果专家 $i$ 的实际分配频率 $f_i$ 很高，其路由概率 $P_i$ 应该降低以最小化损失；反之，如果 $f_i$ 很低，$P_i$ 应该升高。负载均衡损失因此推动 $P_i$ 向 $f_i$ 的方向调整，促进专家使用的均衡化。
防止专家坍缩的其他策略包括：噪声加入（在门控得分中注入高斯噪声，增加路由决策的随机性）、专家容量限制（限制每个专家最多处理的样本数量，超出容量的样本被重新路由）、以及辅助专家（如引入"虚拟专家"来分担负载）。这些策略从不同角度打破了专家坍缩的正反馈机制，使得训练能够收敛到更均衡的解。
## 7.2.6 Importance Weight机制
Importance Weight（重要性权重）是另一种负载均衡的正则化方法，其核心思想是通过最小化路由概率的方差来强制各专家获得均衡负载。与直接惩罚分配频率偏离的损失函数不同，Importance Weight机制从统计方差的角度来量化负载不均衡，并通过梯度下降来最小化这个方差。
设 $P_i = \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} p_i(\mathbf{x})$ 是专家 $i$ 在当前批次中的平均路由概率，其中 $p_i(\mathbf{x})$ 是输入 $\mathbf{x}$ 被路由到专家 $i$ 的Softmax概率。理想情况下，我们希望 $P_i = 1/K$ 对所有 $i$ 成立，即每个专家获得相等的平均路由概率。
路由概率的方差定义为：
$$\text{Var}(P) = \frac{1}{K} \sum_{i=1}^{K} \left(P_i - \bar{P}\right)^2$$
其中 $\bar{P} = \frac{1}{K} \sum_{i=1}^{K} P_i = \frac{K'}{K}$ 是平均路由概率（对于Top-K路由，$K'$ 是激活专家数，对于软路由，$K' = 1$）。方差 $\text{Var}(P)$ 越大，说明专家之间的负载越不均衡；方差越小，说明负载越均衡。
Importance Weight损失直接以方差为目标：
$$\mathcal{L}_{\text{importance}} = \text{Var}(P) = \frac{1}{K} \sum_{i=1}^{K} \left(P_i - \frac{K'}{K}\right)^2$$
这个损失在 $P_i = K'/K$ 对所有 $i$ 成立时最小（值为0），此时所有专家具有相等的平均路由概率，负载完全均衡。
从优化的角度看，Importance Weight损失关于门控网络参数 $\mathbf{W}$ 和 $\mathbf{b}$ 的梯度可以表示为：
$$\frac{\partial \mathcal{L}_{\text{importance}}}{\partial \mathbf{W}} = \frac{2}{K} \sum_{i=1}^{K} \left(P_i - \frac{K'}{K}\right) \cdot \frac{\partial P_i}{\partial \mathbf{W}}$$
其中 $\frac{\partial P_i}{\partial \mathbf{W}}$ 是平均路由概率关于门控参数的梯度。这个梯度表达式表明：只有当专家 $i$ 的路由概率 $P_i$偏离目标值 $K'/K$ 时，才会产生非零的更新梯度；偏离越远，梯度越大，这引导门控网络调整参数以使 $P_i$ 向目标值靠拢。
Importance Weight机制的一个优势是其目标函数是光滑的（相比Top-K的离散选择），便于梯度优化。然而，它也存在潜在的问题：过度最小化方差可能导致专家的专业化程度降低，所有专家变得相似，失去了MoE架构通过分工提升表达能力的初衷。因此，在实践中需要仔细权衡负载均衡与专家专业化之间的关系。
一种改进的Importance Weight变体引入了一个margin参数 $m$ 来保护专家专业化：
$$\mathcal{L}_{\text{importance}}^{\text{margin}} = \frac{1}{K} \sum_{i=1}^{K} \max\left(0, P_i - \frac{K'}{K} - m\right)^2$$
这个损失只在 $P_i$ 显著高于目标值（超过margin）时才施加惩罚，允许专家的路由概率在一定范围内波动，从而保留专家专业化的空间。margin参数 $m$ 控制了允许的专业化程度：$m=0$ 等同于标准Importance Weight，$m$ 越大允许的偏离越大。
在实际应用中，Importance Weight损失通常与其他负载均衡策略结合使用。例如，可以同时使用 Importance Weight 损失（促进路由概率均衡）和负载均衡辅助损失（促进实际分配频率均衡），两种损失的优势互补：路由概率的均衡确保了每个专家都有机会被选中，实际分配频率的均衡确保了计算资源的均匀利用。
负载均衡策略的效果可以通过多个指标来监控：专家分配频率的标准差 $\sqrt{\text{Var}(f)}$、路由概率的标准差 $\sqrt{\text{Var}(P)}$、最大分配频率与最小分配频率的比值 $\max(f)/\min(f)$，以及专家激活的熵 $H(f) = -\sum_{i=1}^{K} f_i \log f_i$。这些指标提供了对负载均衡状态的全面定量评估，帮助研究者诊断和调整MoE训练策略。