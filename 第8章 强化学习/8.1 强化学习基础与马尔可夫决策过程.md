# 第8章 强化学习
## 8.1 强化学习基础与马尔可夫决策过程
强化学习是机器学习的一个重要分支，它研究智能体（Agent）如何在与环境（Environment）的交互中学习，以最大化累积奖励（Cumulative Reward）。与监督学习不同，强化学习没有现成的"正确答案"标签；相反，智能体必须通过试错（Trial and Error）来探索环境，从延迟的、稀疏的奖励信号中学习最优的行为策略。
### 8.1.1 强化学习基本框架
强化学习的核心框架可以抽象为智能体与环境之间的持续交互循环。这个框架包含以下几个核心要素：
-   **智能体（Agent）**：学习和决策的主体。在大语言模型的语境下，智能体就是语言模型本身。
-   **环境（Environment）**：智能体交互的外部世界。对于语言模型，环境可以是用户、对话系统或任何需要模型生成响应的场景。
-   **状态（State, $s_t$）**：在时间步 $t$ 时，环境的完整描述。在对话中，状态可以包括当前的对话历史。
-   **动作（Action, $a_t$）**：智能体在状态 $s_t$ 下可以执行的操作。对于语言模型，动作就是生成下一个词（token）或完整的句子。
-   **奖励（Reward, $r_{t+1}$）**：智能体在状态 $s_t$ 下执行动作 $a_t$ 后，环境在下一个时间步 $t+1$ 返回的标量反馈信号。奖励是评估动作好坏的直接标准，正奖励表示鼓励，负奖励表示惩罚。
这个交互过程可以描述为：在每个时间步 $t$，智能体观察到当前状态 $s_t$，根据其内部策略选择一个动作 $a_t$。环境接收到动作 $a_t$ 后，会转移到一个新的状态 $s_{t+1}$，并给智能体一个即时奖励 $r_{t+1}$。智能体的目标就是学习一个策略，使得从长远来看，它能获得的累积奖励最大化。
### 8.1.2 马尔可夫决策过程（MDP）的数学定义
马尔可夫决策过程（Markov Decision Process，MDP）是描述强化学习问题的标准数学框架。一个MDP由一个五元组 $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$ 定义：
-   $\mathcal{S}$：**状态空间（State Space）**，是所有可能状态的集合。
-   $\mathcal{A}$：**动作空间（Action Space）**，是所有可能动作的集合。
-   $P$：**状态转移概率函数（State Transition Probability Function）**，$P(s' | s, a) = \mathbb{P}[S_{t+1}=s' | S_t=s, A_t=a]$。它定义了在状态 $s$ 下执行动作 $a$ 后，转移到下一个状态 $s'$ 的概率。
-   $R$：**奖励函数（Reward Function）**，$R(s, a, s') = \mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$。它定义了在状态 $s$ 执行动作 $a$ 并转移到 $s'$ 后，智能体能获得的期望即时奖励。
-   $\gamma$：**折扣因子（Discount Factor）**，$\gamma \in [0, 1]$，用于平衡即时奖励和未来奖励的重要性。
MDP的核心是**马尔可夫性质（Markov Property）**，即"未来只与现在有关，与过去无关"。在数学上，这意味着状态转移概率只依赖于当前的状态和动作，而与之前的状态和动作历史无关：
$$ \mathbb{P}[S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, \ldots] = \mathbb{P}[S_{t+1} | S_t, A_t] $$
马尔可夫性质极大地简化了强化学习问题的建模和求解。
### 8.1.3 状态价值函数 $V^\pi(s)$ 与动作价值函数 $Q^\pi(s, a)$
为了评估一个策略的好坏，我们需要定义价值函数。
**策略（Policy）** $\pi(a|s)$ 是一个从状态到动作的映射，它定义了智能体在状态 $s$ 下选择动作 $a$ 的概率。
**回报（Return）** $G_t$ 是从时间步 $t$ 开始的未来折扣奖励之和：
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$
-   **状态价值函数（State-Value Function）** $V^\pi(s)$：它表示从状态 $s$ 开始，遵循策略 $\pi$ 能获得的期望回报。数学定义为：
    $$ V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] $$

    $V^\pi(s)$ 回答了"在当前状态 $s$ 下，遵循策略 $\pi$ 有多好？"
-   **动作价值函数（Action-Value Function）** $Q^\pi(s, a)$：它表示在状态 $s$ 下执行动作 $a$，然后继续遵循策略 $\pi$ 能获得的期望回报。数学定义为：
    $$ Q^\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a] $$
    $Q^\pi(s, a)$ 回答了"在当前状态 $s$ 下，选择动作 $a$ 有多好？"
两者之间的关系是：状态的价值等于在该状态下，遵循策略 $\pi$ 执行所有可能动作的价值的期望。
$$ V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s, a) $$
### 8.1.4 贝尔曼方程与最优策略
贝尔曼方程（Bellman Equation）是强化学习的基石，它给出了价值函数之间的递归关系。
**贝尔曼期望方程（Bellman Expectation Equation）**将当前状态的价值与其后继状态的价值联系起来：
$$ V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^\pi(s') \right] $$
$$ Q^\pi(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s', a') \right] $$

这个方程表明，当前价值等于即时奖励的期望加上下一状态的折扣价值的期望。
强化学习的目标是找到一个**最优策略（Optimal Policy）** $\pi^*$，使得它在所有状态下的价值都最大化，即 $V^{\pi^*}(s) \ge V^\pi(s)$ 对所有 $s \in \mathcal{S}$ 和所有策略 $\pi$ 成立。对应的最优价值函数记为 $V^*(s)$ 和 $Q^*(s)$。
**贝尔曼最优方程（Bellman Optimality Equation）**描述了最优价值函数满足的性质。它与期望方程的区别在于，它不再求期望，而是直接选择能带来最大价值的动作：
$$ V^*(s) = \max_{a \in \mathcal{A}} \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma V^*(s') \right] $$
$$ Q^*(s, a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \left[ R(s,a,s') + \gamma \max_{a' \in \mathcal{A}} Q^*(s', a') \right] $$

贝尔曼最优方程是非线性的，通常没有闭式解，但它为许多动态规划和强化学习算法（如价值迭代和Q-Learning）提供了理论基础。
### 8.1.5 折扣因子 $\gamma$ 的数学意义
折扣因子 $\gamma$ 在强化学习中扮演着至关重要的角色，其数学意义体现在以下几个方面：
1.  **保证回报收敛**：在持续不断的任务中（没有终止状态），回报 $G_t$ 是一个无限项级数。如果奖励是有界的（例如 $|R_t| \le R_{\max}$），且 $\gamma < 1$，那么回报 $G_t$ 的上界为 $\sum_{k=0}^{\infty} \gamma^k R_{\max} = \frac{R_{\max}}{1-\gamma}$，这是一个有限值。$\gamma$ 确保了价值函数的计算在数学上是收敛和有意义的。
2.  **权衡即时与未来奖励**：$\gamma$ 体现了智能体对未来的重视程度。
    -   当 $\gamma \to 0$ 时，智能体变得"短视"（myopic），几乎只关心即时奖励 $R_{t+1}$。
    -   当 $\gamma \to 1$ 时，智能体变得"有远见"（far-sighted），对未来的奖励给予几乎与即时奖励同等的重要性，目标是最大化所有未来奖励的无折扣总和。

3.  **模型不确定性的体现**：在某些解释中，$\gamma$ 也可以看作是在每个时间步，任务有 $1-\gamma$ 的概率会终止。因此，未来的奖励因为任务可能终止而自然地被打了折扣。
在RLHF中，折扣因子的选择会影响模型学习到的行为模式。例如，一个较低的 $\gamma$ 可能会让模型更倾向于生成立即能获得高分的、华丽但不一定连贯的句子；而一个较高的 $\gamma$ 则会鼓励模型进行更长远的规划，生成整体质量更高的段落，即使某些部分的即时奖励不是最高的。