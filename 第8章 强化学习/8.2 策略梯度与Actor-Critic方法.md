## 8.2 策略梯度与Actor-Critic方法

在理解了马尔可夫决策过程的基本框架后，我们需要解决一个核心问题：如何找到一个最优策略 $\pi^*$？上一节介绍的贝尔曼最优方程主要适用于状态空间和动作空间较小、环境模型已知（已知状态转移概率 $P$ 和奖励函数 $R$）的情况，这种情况被称为"基于模型的强化学习"（Model-Based RL）。然而，在大语言模型的应用场景中，环境是极其复杂的对话系统或用户交互环境，其状态转移概率和奖励函数都是未知的。更重要的是，策略本身通常由神经网络参数化表示，这意味着我们需要一套可以直接优化参数化策略的方法论。策略梯度方法正是为解决这一问题而生的，它允许我们直接在参数空间中通过梯度上升来优化策略，无需显式建模环境。
### 8.2.1 策略梯度定理
策略梯度方法的核心是直接优化策略参数 $\theta$，使得累积期望奖励最大化。设策略由参数化函数 $\pi_\theta(a|s)$ 表示，我们的优化目标是最大化期望累积奖励，也称为策略的价值：

$$ J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1} \right] $$
其中 $\tau = (s_0, a_0, s_1, a_1, \ldots)$ 表示智能体与环境交互产生的一条轨迹（Trajectory），$\tau \sim \pi_\theta$ 表示轨迹是根据策略 $\pi_\theta$ 采样得到的。
**策略梯度定理**（Policy Gradient Theorem）给出了策略价值函数 $J(\theta)$ 对策略参数 $\theta$ 的梯度表达式，这是整个策略梯度方法的理论基础：

$$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot Q^{\pi_\theta}(s_t, a_t) \right] $$
这个定理的深刻之处在于，它将复杂的期望累积奖励的梯度问题，转化为了一条轨迹上的期望梯度求和问题。具体来说，对于轨迹上的每一个时间步 $t$，我们计算在该状态下采取该动作的对数概率梯度 $\nabla_\theta \log \pi_\theta(a_t | s_t)$，并将其乘以该状态-动作对的动作价值 $Q^{\pi_\theta}(s_t, a_t)$。直观地理解，这个梯度的方向告诉我们：对于轨迹中出现的每一个 $(s_t, a_t)$ 对，如果它最终带来了正向的回报（即 $Q^{\pi_\theta}(s_t, a_t) > 0$），那么我们应该增加在该状态下选择该动作的概率；反之，如果它带来了负向的回报，则应该降低选择该动作的概率。
在实际应用中，为了简化计算，我们通常使用有限时长的轨迹或者使用蒙特卡洛采样来估计轨迹的总回报 $G_t$，从而得到梯度的无偏估计：
$$ \nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T_i} \nabla_\theta \log \pi_\theta(a_{i,t} | s_{i,t}) \cdot G_{i,t} $$
其中 $N$ 是采样的轨迹数量，$T_i$ 是第 $i$ 条轨迹的长度，$G_{i,t}$ 是第 $i$ 条轨迹中时刻 $t$ 的累积回报。这个估计量是无偏的，但方差可能较大。
### 8.2.2 REINFORCE算法
REINFORCE算法是Williams于1992年提出的，是最早也是最经典的策略梯度算法之一。它直接实现了策略梯度定理，使用蒙特卡洛采样来估计轨迹的总回报，并据此更新策略参数。REINFORCE算法的核心思想可以概括为"沿着回报高的轨迹增加动作概率，沿着回报低的轨迹减少动作概率"。
REINFORCE算法的更新公式为：

$$ \theta_{t+1} = \theta_t + \alpha \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot G_t $$
其中 $\alpha$ 是学习率，$G_t$ 是从时刻 $t$ 开始到轨迹结束的折扣累积回报：
$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots $$
REINFORCE算法的完整流程如下：
**步骤1：采样**。使用当前策略 $\pi_\theta$ 与环境交互，收集 $N$ 条完整的轨迹 $\{\tau_1, \tau_2, \ldots, \tau_N\}$。每条轨迹由状态、动作、奖励序列组成：$\tau_i = (s_{i,0}, a_{i,0}, r_{i,1}, s_{i,1}, a_{i,1}, r_{i,2}, \ldots, s_{i,T_i})$。
**步骤2：计算回报**。对于每条轨迹中的每个时间步 $t$，计算折扣累积回报 $G_{i,t} = \sum_{k=0}^{T_i-t-1} \gamma^k r_{i,t+k+1}$。
**步骤3：更新策略**。对所有采样轨迹中的每个时间步，使用以下公式更新策略参数：
$$ \theta \leftarrow \theta + \alpha \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T_i} G_{i,t} \nabla_\theta \log \pi_\theta(a_{i,t} | s_{i,t}) $$
REINFORCE算法的一个显著优点是它的简洁性和理论基础。作为一个基于蒙特卡洛的方法，它使用完整的轨迹回报来更新策略，避免了对环境动态模型的依赖。然而，这个算法也存在一些明显的局限性：
**高方差问题**。由于 $G_t$ 是整个轨迹累积回报的随机采样，不同轨迹之间可能有很大的差异，导致策略梯度估计的方差很大。高方差意味着算法收敛速度慢，需要更多的采样才能获得稳定的更新方向。
**样本效率低**。每次参数更新后，策略都会发生变化，这意味着之前采样的轨迹可能不再服从新的策略分布（即"分布漂移"问题）。在实践中，我们需要大量的采样来覆盖不同的状态-动作空间，这在大规模应用中可能非常昂贵。
**更新步长难以确定**。由于回报值的大小取决于具体的任务和奖励函数设置，学习率 $\alpha$ 的选择需要仔细调整。如果学习率过大，策略可能会在接收到高回报的随机波动后过度更新；如果学习率过小，收敛速度又会太慢。
### 8.2.3 Baseline与方差缩减
为了解决REINFORCE算法的高方差问题，研究者们引入了基线（Baseline）技术。基线的核心思想是在策略梯度中添加一个与动作无关的基线函数 $b(s_t)$，使得估计量的期望保持不变，但方差显著降低。
考虑在策略梯度中添加一个基线 $b$：
$$ \nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot (Q^{\pi_\theta}(s_t, a_t) - b) \right] $$
为了证明基线不改变梯度的无偏性，我们需要验证：

$$ \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot b \right] = 0 $$
这个等式成立的原因是：$b$ 是在状态 $s_t$ 下确定的常数（不依赖于动作 $a_t$），而 $\sum_a \pi_\theta(a|s_t) \nabla_\theta \log \pi_\theta(a|s_t) = \nabla_\theta \sum_a \pi_\theta(a|s_t) = \nabla_\theta (1) = 0$。因此，添加基线不会改变策略梯度估计量的期望值，即梯度仍然是无偏的。
**最优基线**。不同的基线选择会带来不同程度的方差缩减效果。理论上，可以证明使方差最小化的最优基线是**状态价值函数** $V^{\pi_\theta}(s_t)$，或者说是在某些条件下的加权平均。最优基线 $b^*$ 满足：
$$ b^* = \frac{\mathbb{E} \left[ (\nabla_\theta \log \pi_\theta(a_t|s_t))^2 Q^{\pi_\theta}(s_t, a_t) \right]}{\mathbb{E} \left[ (\nabla_\theta \log \pi_\theta(a_t|s_t))^2 \right]} $$
然而，这个最优基线的计算通常也很复杂。在实践中，一个简单但有效的选择是使用**常数基线**（如回报的平均值）或者使用一个学习到的**价值函数近似器**来作为基线。
### 8.2.4 Actor-Critic框架：V(s)作为Critic
为了更有效地利用基线技术，现代强化学习算法通常采用**Actor-Critic架构**。在这个框架中，"Actor"指的是策略网络（即我们正在学习的策略 $\pi_\theta(a|s)$），它负责选择动作；"Critic"指的是价值函数网络 $V_\phi(s)$，它负责评估当前策略的价值，为Actor的决策提供反馈。Actor-Critic架构结合了策略梯度方法和价值函数逼近方法的优点，既可以直接优化策略，又可以利用学习到的价值函数来减少梯度的方差。
在Actor-Critic框架中，我们使用**优势函数**（Advantage Function）来替代原始的回报 $G_t$ 或动作价值 $Q^{\pi_\theta}(s,a)$。优势函数定义为：

$$ A^{\pi}(s, a) = Q^{\pi}(s, a) - V^{\pi}(s) $$
优势函数衡量的是"在状态 $s$ 下选择特定动作 $a$ 比遵循当前策略的平均表现好多少"。如果优势函数为正，说明这个动作比平均水平更好，应该增加其被选中的概率；如果优势函数为负，则应该减少其概率。
在实际应用中，由于真实的 $Q^{\pi}(s, a)$ 和 $V^{\pi}(s)$ 都是未知的，我们需要使用**时序差分学习**（Temporal Difference Learning，TD）来估计它们。一种常用的优势函数估计方法是使用TD误差：
$$ \delta_t = r_{t+1} + \gamma V_\phi(s_{t+1}) - V_\phi(s_t) $$
可以证明，在单步TD学习中，$\delta_t$ 是优势函数 $A^{\pi}(s_t, a_t)$ 的一个无偏估计，即 $\mathbb{E}[\delta_t | s_t, a_t] = A^{\pi}(s_t, a_t)$。因此，我们可以使用TD误差 $\delta_t$ 作为优势函数的估计值来计算策略梯度：
$$ \nabla_\theta J(\theta) \approx \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot \delta_t \right] $$
Actor-Critic算法的完整流程如下：
**步骤1：初始化**。初始化Actor网络参数 $\theta$ 和Critic网络参数 $\phi$。
**步骤2：采样**。使用当前策略 $\pi_\theta$ 与环境交互，收集一批状态-动作-奖励-下一状态序列 $(s_t, a_t, r_{t+1}, s_{t+1})$。
**步骤3：更新Critic**。使用时序差分目标更新价值函数网络。对于每个采样得到的转移，计算TD目标 $y_t = r_{t+1} + \gamma V_\phi(s_{t+1})$，然后最小化均方误差损失 $\mathcal{L}_{critic} = \frac{1}{2} (y_t - V_\phi(s_t))^2$，通过梯度下降更新参数 $\phi$。
**步骤4：更新Actor**。使用优势函数估计（通常使用GAE或简单TD误差）计算策略梯度，然后通过梯度上升更新策略参数 $\theta$。策略梯度为 $\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=0}^{T_i} \nabla_\theta \log \pi_\theta(a_{i,t} | s_{i,t}) \cdot \hat{A}_{i,t}$，其中 $\hat{A}_{i,t}$ 是优势函数的估计值。
**步骤5：重复**。返回步骤2，直到策略收敛或达到预设的训练步数。
Actor-Critic架构相对于纯REINFORCE算法的优势在于：
**方差显著降低**。通过使用学习到的价值函数 $V_\phi(s)$ 作为基线，我们不再依赖于完整的轨迹回报 $G_t$，而是使用单步TD误差 $\delta_t$。TD误差只依赖于局部的状态转移信息，其方差远小于蒙特卡洛回报的方差。
**在线学习能力**。REINFORCE算法必须等到轨迹结束后才能计算回报并进行更新，而Actor-Critic算法可以在每一步交互后就进行参数更新。这使得算法可以更高效地利用样本数据。
**更稳定的训练**。Critic网络持续评估策略的价值，为Actor的更新提供了稳定的参考，避免了纯策略梯度方法中常见的训练不稳定问题。
### 8.2.5 优势函数 $A^\pi(s,a)$ 的深入理解
优势函数是强化学习中一个核心而深刻的概念，它量化了在特定状态下采取特定动作相对于"平均水平"的优劣程度。深入理解优势函数对于掌握现代强化学习算法至关重要。
从数学定义出发，优势函数 $A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$。由于 $V^\pi(s) = \sum_{a'} \pi(a'|s) Q^\pi(s, a')$，我们可以将优势函数改写为：

$$ A^\pi(s, a) = Q^\pi(s, a) - \sum_{a'} \pi(a'|s) Q^\pi(s, a') $$
这个表达式清晰地表明，优势函数衡量的是：相比于遵循当前策略 $\pi$ 采取动作的期望收益，在状态 $s$ 下采取特定动作 $a$ 的收益差异。
**优势函数的性质**包含以下几个重要方面：
**零和性质**。优势函数在动作空间上的加权平均为零，即 $\sum_a \pi(a|s) A^\pi(s, a) = 0$。这是因为 $\sum_a \pi(a|s) Q^\pi(s, a) - V^\pi(s) \sum_a \pi(a|s) = V^\pi(s) - V^\pi(s) = 0$。这个性质保证了正负优势函数相互抵消，策略梯度的更新方向是平衡的。
**单调性**。如果对于所有状态 $s$，动作 $a_1$ 的优势函数 $A^\pi(s, a_1) \ge A^\pi(s, a_2)$，那么执行策略梯度更新后，选择 $a_1$ 的概率增加量大于 $a_2$ 的概率增加量。这保证了算法向正确的方向优化。
**优势函数与策略改进**。优势函数的符号直接指示了策略改进的方向。对于任何满足 $A^\pi(s, a) > 0$ 的动作 $a$，我们都应该增加其在状态 $s$ 下被选中的概率；反之，对于 $A^\pi(s, a) < 0$ 的动作，我们应该减少其概率。
**广义优势估计（GAE）**。在实际应用中，为了平衡偏差（Bias）和方差（Variance），我们通常使用广义优势估计（Generalized Advantage Estimation）来计算优势函数。GAE通过引入两个参数 $\lambda$ 和 $\gamma$ 来控制估计的偏差-方差权衡：
$$ \hat{A}^{GAE}_{t} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l} $$
其中 $\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$ 是TD误差。当 $\lambda = 0$ 时，GAE退化为单步TD误差（高偏差、低方差）；当 $\lambda = 1$ 时，GAE退化为蒙特卡洛回报（无偏差、高方差）。通过调节 $\lambda$，我们可以在偏差和方差之间找到最优的平衡点。
在大语言模型的RLHF训练中，优势函数的概念同样适用。我们可以训练一个奖励模型（Reward Model）来预测人类对模型输出的偏好，这个奖励模型的输出可以看作是 $Q^\pi(s, a)$ 的估计，从而计算出优势函数，指导策略的优化方向。这种思想正是PPO（Proximal Policy Optimization）等现代RLHF算法的核心。