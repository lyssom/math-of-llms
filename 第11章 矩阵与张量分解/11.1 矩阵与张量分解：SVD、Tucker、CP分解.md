在深度学习的数学基础中，矩阵与张量分解占据着极为重要的地位。高维数据往往具有冗余性，例如图像中相邻像素的相关性、自然语言中词语共现的模式、以及深度神经网络权重矩阵中的低秩结构。分解技术的核心思想正是利用这种冗余性，用更紧凑的参数化形式来表示原始数据或模型。从经典的奇异值分解（SVD）到高阶张量分解（CP分解、Tucker分解），这些数学工具不仅为降维和压缩提供了理论基础，更深刻地影响了现代深度学习中的模型设计、正则化与高效推理。本节将系统地介绍这些分解技术的数学原理、计算方法及其在深度学习中的关键应用。
现实世界中的高维数据通常不是均匀分布在整个高维空间中的，而是集中在某个低维流形附近或在某些方向上具有高度相关性。这种现象被称为"数据的内在低维性"或"冗余性"。例如，一张$256 \times 256$的灰度图像可以表示为一个$65536$维的向量，但实际上图像的信息量远低于这个维度，相邻像素之间存在强相关性，整幅图像可以用远少于$65536$个参数来有效描述。
**定义 11.1（数据冗余度）** 设$\mathbf{X} \in \mathbb{R}^{m \times n}$为数据矩阵，其秩为$r = \text{rank}(\mathbf{X})$。如果$r \ll \min(m, n)$，则称数据具有高冗余度。在这种情况下，矩阵的行（或列）张成的空间维度远小于矩阵的维度，存在大量的线性相关性。
深度神经网络中的权重矩阵同样表现出类似的低秩特性。研究表明，经过训练的网络权重矩阵通常具有较低的"有效秩"，虽然完整矩阵可能是满秩的，但其奇异值快速衰减，前几个奇异值就能解释大部分方差。这一观察为模型压缩提供了数学基础：通过分解权重矩阵并保留主要奇异值，可以在几乎不损失模型性能的前提下大幅减少参数量。

## 11.1.1 从主成分分析到张量分解

主成分分析（PCA）是最经典的降维方法，其本质就是针对协方差矩阵的SVD分解。PCA通过寻找数据方差最大的方向（主成分），将数据投影到低维子空间，实现降维目的。然而，PCA只能处理二维矩阵形式的数据。当数据天然具有更高维的结构时，如彩色图像的三个通道、视频的时空序列、或者深度神经网络的多维权重张量，我们需要将PCA的思想推广到高阶张量，这就是张量分解的范畴。
**张量分解的统一视角** 张量分解可以看作是PCA在张量形式上的推广。对于矩阵，PCA将数据矩阵分解为 $\mathbf{X} = \mathbf{U}\mathbf{S}\mathbf{V}^{\top}$，其中$\mathbf{U}$和$\mathbf{V}$是正交矩阵，$\mathbf{S}$是对角矩阵（包含主成分）。对于张量，分解的目标是找到一组"基张量"的线性组合来表示原始张量。不同的分解方法对应不同的基张量假设：CP分解假设基张量是秩一张量的和，Tucker分解假设基张量是一个紧凑的"核张量"与各mode因子矩阵的乘积。
**表 11.1.1 分解方法的层次结构**

| 数据形式                                                    | 分解方法      | 核心目标     | 分解形式                                                                                                                                                                                 |
| ------------------------------------------------------- | --------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 向量$\mathbf{x} \in \mathbb{R}^n$                         | 基底展开      | 用基向量线性表示 | $\mathbf{x} = \sum_i \alpha_i \mathbf{b}_i$                                                                                                                                          |
| 矩阵 $\mathbf{X} \in \mathbb{R}^{m \times n}$             | SVD/PCA   | 低秩近似     | $\mathbf{X} \approx \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$                                                                                                                      |
| 三阶张量 $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$ | CP/Tucker | 高阶分解     | $\mathcal{X} \approx \sum_r \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_rX≈∑r​ar​∘br​∘cr$​ 或$G×1A×2B×3C\mathcal{G} \times_1 \mathbf{A} \times_2 \mathbf{B} \times_3 \mathbf{C}$ |
理解这些分解技术的数学本质，不仅有助于设计更高效的模型压缩算法，更能深化我们对深度学习表示学习机制的理论认识。

## 11.1.2 奇异值分解：矩阵分解的基石

### SVD的数学定义

奇异值分解（Singular Value Decomposition，简称SVD）是矩阵分解最基本、最重要的工具。对于任意实矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$，其SVD分解为：
$$
\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}
$$
其中各矩阵的定义和性质如下所述。设$\mathbf{A}$的秩为$r = \text{rank}(\mathbf{A})$，则存在正交矩阵 $\mathbf{U} \in \mathbb{R}^{m \times m}$和 $\mathbf{V} \in \mathbb{R}^{n \times n}$，以及对角矩阵$\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$，使得上述分解成立。
**定义 11.2（奇异值）** 对角矩阵$\mathbf{\Sigma}$的对角元素$\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$称为矩阵$\mathbf{A}$的奇异值。非零奇异值的个数等于矩阵的秩$r$。当$i > r$时，$\sigma_i = 0$。
**定理 11.1（矩阵的SVD存在性）** 任意实矩阵$\mathbf{A}$都可以分解为$\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$，其中$\mathbf{U}$和 $\mathbf{V}$是正交矩阵，$\mathbf{\Sigma}$是非负对角矩阵。
**证明思路**：考虑矩阵$\mathbf{A}^{\top}\mathbf{A}$，它是半正定对称矩阵，因此可以进行特征分解 $\mathbf{A}^{\top}\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{\top}$，其中 $\mathbf{V}$是正交矩阵，$\mathbf{\Lambda}$是对角矩阵。设$\mathbf{\Lambda} = \text{diag}(\lambda_1, \lambda_2, \dots, \lambda_n)$，则$\lambda_i \geq 0$是 $\mathbf{A}^{\top}\mathbf{A}$的特征值。令 $\sigma_i = \sqrt{\lambda_i}$​​，则$\mathbf{\Sigma} = \text{diag}(\sigma_1, \sigma_2, \dots)$。取前$r$个非零奇异值对应的特征向量构成$\mathbf{V}_r$​，定义$\mathbf{U}_r = \mathbf{A}\mathbf{V}_r\mathbf{\Sigma}_r^{-1}$，可以证明$\mathbf{U}_r$的列是正交的。最后补充正交基完成$\mathbf{U}$和$\mathbf{V}$的构造。
### SVD的几何解释
SVD具有优美的几何解释。从线性变换的角度看，矩阵$\mathbf{A}$代表的线性变换可以分解为三个基本变换的复合：
$$
\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}
$$
其中$\mathbf{V}^{\top}$是旋转（或反射），$\mathbf{\Sigma}$是沿坐标轴的缩放，$\mathbf{U}$是另一个旋转（或反射）。因此，SVD将线性变换分解为"旋转-缩放-旋转"的形式。
**引理 11.1（几何变换分解）** 设$\mathbf{x} \in \mathbb{R}^n$ 为输入向量，则：
$$
\mathbf{A}\mathbf{x} = \mathbf{U}(\mathbf{\Sigma}(\mathbf{V}^{\top}\mathbf{x}))
$$
变换过程为：首先，$\mathbf{V}^{\top}\mathbf{x}$将$\mathbf{x}$旋转到某个标准方向；然后，$\mathbf{\Sigma}$沿坐标轴进行缩放，各坐标分别缩放 $\sigma_i$倍；最后，$\mathbf{U}$将结果旋转到最终方向。
![[svd_graph.png]]
**几何解释图示**：考虑$\mathbb{R}^2$中的单位圆$\mathcal{S}^1$。经过$\mathbf{V}^{\top}$旋转后，单位圆变为椭圆（形状不变）。经过 $\mathbf{\Sigma}$缩放后，椭圆的主轴分别缩放$\sigma_1$和$\sigma_2$倍，变为长轴为$\sigma_1$、短轴为$\sigma_2$的椭圆。经过$\mathbf{U}$旋转后，椭圆的方向发生改变。SVD正是这种几何变换的参数化表示。

### 截断SVD与低秩近似

SVD的核心应用之一是低秩近似。在许多实际应用中，数据矩阵的秩远小于其维度，但精确的低秩分解可能包含大量接近零的奇异值。通过保留最大的$k$个奇异值及其对应的奇异向量，可以得到原矩阵的最佳$k$秩近似。

**定义 11.3（截断SVD）** 设$\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$，取前$k$个最大的奇异值，得到截断SVD：
$$
\mathbf{A}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^{\top} = \mathbf{U}_k \mathbf{\Sigma}_k \mathbf{V}_k^{\top}
$$
其中$\mathbf{U}_k \in \mathbb{R}^{m \times k}$包含前$k$个左奇异向量，$\mathbf{V}_k \in \mathbb{R}^{n \times k}$包含前$k$个右奇异向量，$\mathbf{\Sigma}_k = \text{diag}(\sigma_1, \dots, \sigma_k)$。

**定理 11.2（Eckart-Young-Mirsky定理）** 截断SVD $\mathbf{A}_k$​是矩阵$\mathbf{A}$在Frobenius范数或谱范数意义下的最佳$k$秩近似。即对于任意秩不超过$k$的矩阵$\mathbf{B}$，有：
$$
\|\mathbf{A} - \mathbf{A}_k\|_F \leq \|\mathbf{A} - \mathbf{B}\|_F
$$
$$
\|\mathbf{A} - \mathbf{A}_k\|_2 \leq \|\mathbf{A} - \mathbf{B}\|_2
$$
其中$\|\cdot\|_F$是Frobenius范数，$\|\cdot\|_2$​是谱范数。

**推论 11.1（近似误差界）** 截断SVD的近似误差可以用被丢弃的奇异值来界：
$$
\|\mathbf{A} - \mathbf{A}_k\|_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}
$$$$
\|\mathbf{A} - \mathbf{A}_k\|_2 = \sigma_{k+1}
$$
这些公式表明，近似误差完全由被丢弃的奇异值决定，与具体的奇异向量无关。奇异值衰减越快，低秩近似的效果越好。

**注记 11.1（奇异值分布与可压缩性）** 如果矩阵的奇异值按某种规律快速衰减（如指数衰减或多项式衰减），则称该矩阵是"可压缩的"。深度神经网络中的权重矩阵通常具有这种可压缩性，这为模型压缩提供了数学依据。

### SVD在深度学习中的应用

SVD在深度学习中有多种重要应用。在模型压缩方面，全连接层的权重矩阵 $\mathbf{W} \in \mathbb{R}^{m \times n}$可以进行SVD分解$\mathbf{W} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$，然后用$\mathbf{U}_k \mathbf{\Sigma}_k$和$\mathbf{V}_k^{\top}$分别替代原来的权重矩阵。这将一个$m \times n$的全连接层分解为两个较小的全连接层，参数量从$mn$减少到$k(m+n)$。

**引理 11.2（SVD压缩的参数效率）** 设原始全连接层参数量为$mn$，SVD压缩后参数量为$k(m+n)$。当$k≪min(m,n)$时，压缩比约为$\frac{mn}{k(m+n)} \approx \frac{\min(m,n)}{k}k(m+n)$​。例如，当$m = n = 1000，k = 100$时，压缩比约为$10$。

在初始化分析方面，SVD揭示了权重矩阵的奇异值分布对网络训练稳定性的影响。如果权重矩阵的奇异值接近1，则信息可以在正向传播和反向传播中稳定传递；如果某些奇异值过大或过小，则可能导致梯度消失或爆炸。
**表 11.1.2 SVD在深度学习中的应用场景**

| 应用场景   | 具体方法                                                                  | 压缩比计算                | 性能影响   |
| ------ | --------------------------------------------------------------------- | -------------------- | ------ |
| 全连接层压缩 | $\mathbf{W} \approx \mathbf{U}_k\mathbf{\Sigma}_k\mathbf{V}_k^{\top}$ | $\frac{mn}{k(m+n)}$​ | 近似误差可控 |
| 卷积核分解  | 空间维度分解                                                                | 取决于核大小               | 精度损失小  |
| 权重分析   | 奇异值分布统计                                                               | —                    | 诊断工具   |

## 11.1.3 张量代数基础

### 张量的基本表示

在介绍高阶张量分解之前，我们回顾张量代数的基本概念和运算规则。张量是多维数组的自然推广，阶数（Order）表示张量的维度数。
**定义 11.4（张量的阶与维度）** 设 $\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times \dots \times I_N}$为$N$阶张量，其阶数为$N$，第$n$模（mode）的维度为$I_n$​。标量是0阶张量（0维），向量是1阶张量（1维），矩阵是2阶张量（2维）。
**符号约定**：本节使用以下符号约定，标量用小写斜体$x$或$a$，向量用小写粗体$\mathbf{x}$，矩阵用大写粗体$\mathbf{X}$，张量用欧拉脚本体$\mathcal{X}$。张量元素表示为$x_{i_1 i_2 \dots i_N}$​​，其中$i_n \in \{1, 2, \dots, I_n\}$。
### 模n乘积
模n乘积（mode-n product）是张量代数的核心运算，它定义了张量与矩阵在特定维度上的线性变换。
**定义 11.5（模n乘积）** 张量$\mathcal{X} \in \mathbb{R}^{I_1 \times \dots \times I_n \times \dots \times I_N}$与矩阵$\mathbf{A} \in \mathbb{R}^{J_n \times I_n}$的模n乘积是一个$(I_1 \times \dots \times J_n \times \dots \times I_N)$阶的张量，记为$\mathcal{Y} = \mathcal{X} \times_n \mathbf{A}$，其元素定义为：
$$
y_{i_1 \dots j_n \dots i_N} = \sum_{i_n=1}^{I_n} x_{i_1 \dots i_n \dots i_N} \cdot a_{j_n i_n}
$$​​**引理 11.3（模n乘积的性质）** 模n乘积满足以下性质：
1.**结合性**：$(\mathcal{X} \times_n \mathbf{A}) \times_m \mathbf{B} = \mathcal{X} \times_m \mathbf{B} \times_n \mathbf{A}$（当$m \neq n$ 时）
2.**分配律**：$\mathcal{X} \times_n (\mathbf{A} + \mathbf{B}) = \mathcal{X} \times_n \mathbf{A} + \mathcal{X} \times_n \mathbf{B}$
3.**缩放性质**：$(\alpha \mathcal{X}) \times_n \mathbf{A} = \alpha (\mathcal{X} \times_n \mathbf{A})$
**几何解释**：模n乘积可以理解为沿第$n$维度的线性变换。它将原张量沿第$n$维度的每个"切片"与矩阵$\mathbf{A}$相乘，然后将结果重新组合成新的张量。

### 张量展开（Matricization）
张量展开是将高阶张量重塑为二维矩阵的过程，也称为"摊平"或" matricization"。这一操作对于将张量分解问题转化为标准矩阵分解问题至关重要。
**定义 11.6（模n展开）** 张量$\mathcal{X} \in \mathbb{R}^{I_1 \times I_2 \times \dots \times I_N}$的模n展开记为$\mathbf{X}_{(n)} \in \mathbb{R}^{I_n \times (I_1 \cdots I_{n-1} I_{n+1} \cdots I_N)}$，其中行对应第$n$模的索引，列对应其余所有模的索引的字典序排列。
**引理 11.4（展开与模n乘积的关系）** 张量$\mathcal{X}$与矩阵$\mathbf{A}$的模n乘积的展开等于张量展开与Kronecker积的乘积：
$$
(\mathcal{X} \times_n \mathbf{A})_{(n)} = \mathbf{A} \cdot \mathbf{X}_{(n)}
$$​这一关系表明，模n乘积在展开后的矩阵空间中对应标准的矩阵乘法。
### Khatri-Rao积与Kronecker积
Khatri-Rao积和Kronecker积是张量分解中常用的矩阵运算，它们描述了多个小矩阵如何组合成大矩阵。
**定义 11.7（Kronecker积）** 两个矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$和$\mathbf{B} \in \mathbb{R}^{p \times q}$的Kronecker积$\mathbf{A} \otimes \mathbf{B} \in \mathbb{R}^{(mp) \times (nq)}$定义为：
$$
\mathbf{A} \otimes \mathbf{B} = \begin{pmatrix} a_{11}\mathbf{B} & a_{12}\mathbf{B} & \cdots & a_{1n}\mathbf{B} \\ a_{21}\mathbf{B} & a_{22}\mathbf{B} & \cdots & a_{2n}\mathbf{B} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1}\mathbf{B} & a_{m2}\mathbf{B} & \cdots & a_{mn}\mathbf{B} \end{pmatrix}
$$
**定义 11.8（Khatri-Rao积）** 两个具有相同列数的矩阵$\mathbf{A} \in \mathbb{R}^{m \times r}$和$\mathbf{B} \in \mathbb{R}^{n \times r}$的Khatri-Rao积 $\mathbf{A} \odot \mathbf{B} \in \mathbb{R}^{(mn) \times r}$定义为列向量的逐元素外积：
$$
[\mathbf{A} \odot \mathbf{B}]_{(i-1)n+j, k} = a_{i k} b_{j k}
$$**引理 11.5（Khatri-Rao积的性质）** Khatri-Rao积是列对齐的，即：
$$
\mathbf{A} \odot \mathbf{B} = \begin{pmatrix} a_1 \otimes b_1 & a_2 \otimes b_2 & \cdots & a_r \otimes b_r \end{pmatrix}
$$其中$a_k$是$\mathbf{A}$的第$k$列，$b_k$是$\mathbf{B}$的第$k$列。
**表 11.1.3 矩阵积的对比**

| 运算          | 符号                              | 结果维度                                               | 特点       |
| ----------- | ------------------------------- | -------------------------------------------------- | -------- |
| 矩阵乘法        | $\mathbf{A}\mathbf{B}$          | $(m \times n)(n \times p) \to (m \times p)$        | 内部维度必须匹配 |
| Kronecker积  | $\mathbf{A} \otimes \mathbf{B}$ | $(m \times n)(p \times q) \to (mp \times nq)(m×n)$ | 任意维度可运算  |
| Khatri-Rao积 | $\mathbf{A} \odot \mathbf{B}$   | $(m \times r)(n \times r) \to (mn \times r)$       | 列维度必须相同  |

## 11.1.4 CP分解：秩一张量分解

### CP分解的数学形式
CP分解（CANDECOMP/PARAFAC分解）将张量表示为有限个秩一张量（rank-one tensor）的和。秩一张量是外积形式定义的向量三元组的乘积。

**定义 11.9（秩一张量）** 给定三个向量$\mathbf{a} \in \mathbb{R}^I$，$\mathbf{b} \in \mathbb{R}^J$，$\mathbf{c} \in \mathbb{R}^K$，它们的外积定义了一个三阶张量 $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$：
$$
\mathcal{X} = \mathbf{a} \circ \mathbf{b} \circ \mathbf{c}
$$
其元素为$x_{ijk} = a_i b_j c_k$​。
**定义 11.10（CP分解）** 三阶张量$\mathcal{X} \in \mathbb{R}^{I \times J \times K}$ 的CP分解形式为：
$$
\mathcal{X} \approx \sum_{r=1}^{R} \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r
$$其中$R$称为CP秩（CP-rank），$\mathbf{a}_r \in \mathbb{R}^I$，$\mathbf{b}_r \in \mathbb{R}^J$，$\mathbf{c}_r \in \mathbb{R}^K$是分解的因子向量。
**引理 11.6（CP分解的元素形式）** CP分解的元素形式为：
$$
x_{ijk} \approx \sum_{r=1}^{R} a_{ir} b_{jr} c_{kr}
$$​利用Khatri-Rao积和矩阵乘法，可以紧凑地表示为：
$$
\mathbf{X}_{(1)} \approx \mathbf{A}(\mathbf{C} \odot \mathbf{B})^{\top}
$$
$$
\mathbf{X}_{(2)} \approx \mathbf{B}(\mathbf{C} \odot \mathbf{A})^{\top}
$$
$$
\mathbf{X}_{(3)} \approx \mathbf{C}(\mathbf{B} \odot \mathbf{A})^{\top}
$$
其中$\mathbf{A} = [\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_R] \in \mathbb{R}^{I \times R}$，$\mathbf{B} = [\mathbf{b}_1, \mathbf{b}_2, \dots, \mathbf{b}_R] \in \mathbb{R}^{J \times R}$，$\mathbf{C} = [\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_R] \in \mathbb{R}^{K \times R}$是因子矩阵。

### 张量秩与矩阵秩的区别
张量秩（Tensor Rank）的概念与矩阵秩有本质区别，这是理解CP分解的关键。
**定义 11.11（张量秩）** 张量$\mathcal{X}$的秩定义为能够精确表示该张量的最小秩一张量个数。记为 $\text{rank}(\mathcal{X})$。
**定理 11.3（张量秩的性质）** 张量秩具有以下与矩阵秩不同的性质：
1.**非可加性**：对于两个张量$\mathcal{X}$和$\mathcal{Y}$，$\text{rank}(\mathcal{X} + \mathcal{Y}) \leq \text{rank}(\mathcal{X}) + \text{rank}(\mathcal{Y})$，但等号不一定成立
2.**NP困难性**：确定任意张量的秩是NP困难问题
3.**病态性**：存在任意接近某个低秩张量的高秩张量
**引理 11.7（矩阵秩的可加性 vs 张量秩的非可加性）**
对于矩阵，$\text{rank}(\mathbf{A} + \mathbf{B}) \leq \text{rank}(\mathbf{A}) + \text{rank}(\mathbf{B})$通常取等号。但对于张量，即使两个秩一张量相加，也可能得到秩远大于2的张量。
**注记 11.2（CP秩的定义挑战）** 由于张量秩的病态性，实际应用中CP秩$R$通常是需要预先指定的超参数。确定最优的$R$需要权衡近似精度和分解复杂度。
### 交替最小二乘法（ALS）
给定张量$\mathcal{X}$和目标秩$R$，CP分解的参数可以通过交替最小二乘法（Alternating Least Squares，ALS）来求解。ALS的核心思想是固定其他因子矩阵，优化一个因子矩阵，然后交替进行。
**算法 11.1（ALS算法框架）**
**输入**：张量$\mathcal{X} \in \mathbb{R}^{I \times J \times K}$，目标秩$R$
**输出**：因子矩阵$\mathbf{A} \in \mathbb{R}^{I \times R}$，$\mathbf{B} \in \mathbb{R}^{J \times R}$，$\mathbf{C} \in \mathbb{R}^{K \times R}$
1.初始化$\mathbf{A}$，$\mathbf{B}$，$\mathbf{C}$（随机或SVD初始化）
2.重复以下步骤直到收敛：
- 固定$\mathbf{B}$和$\mathbf{C}$，求解$\mathbf{A}$：$\min_{\mathbf{A}} \|\mathbf{X}_{(1)} - \mathbf{A}(\mathbf{C} \odot \mathbf{B})^{\top}\|_F^2$
- 固定$\mathbf{A}$和$\mathbf{C}$，求解$\mathbf{B}$：$\min_{\mathbf{B}} \|\mathbf{X}_{(2)} - \mathbf{B}(\mathbf{C} \odot \mathbf{A})^{\top}\|_F^2$
- 固定$\mathbf{A}$和$\mathbf{B}$，求解$\mathbf{C}$：$\min_{\mathbf{C}} \|\mathbf{X}_{(3)} - \mathbf{C}(\mathbf{B} \odot \mathbf{A})^{\top}\|_F^2$
3.返回$\mathbf{A}，\mathbf{B}，\mathbf{C}$
**引理 11.8（ALS的子问题闭式解）** 以求解$\mathbf{A}$为例，最小二乘问题的闭式解为：
$$
\mathbf{A} = \mathbf{X}_{(1)} (\mathbf{C} \odot \mathbf{B})^{\top} \left[(\mathbf{C} \odot \mathbf{B})^{\top} (\mathbf{C} \odot \mathbf{B})\right]^{-1}
$$
如果矩阵可逆。这个解利用了正规方程的求解方法。
**定理 11.4（ALS的收敛性）** ALS的目标函数是平方误差 $\|\mathcal{X} - \sum_r \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r\|^2$。每次固定两个因子矩阵优化第三个因子矩阵时，目标函数单调不增。ALS收敛到某个（可能是局部）极小值，但不保证收敛到全局极小值或张量的精确CP表示。
**表 11.1.4 CP分解的ALS优化特点**

| 特性    | 描述             |
| ----- | -------------- |
| 收敛性   | 单调收敛，可能陷入局部极小  |
| 计算复杂度 | 每次迭代 $O(IJKR)$ |
| 初始化敏感 | SVD初始化优于随机初始化  |
| 秩选择   | 需要预先指定$R$      |

### CP分解的唯一性
CP分解的一个重要特性是在一定条件下的唯一性，即分解是本质唯一的（essentially unique）。
**定理 11.5（CP分解的唯一性条件）** 如果张量$\mathcal{X}$的CP分解满足以下条件，则分解在尺度变换和置换意义下是唯一的：因子矩阵$\mathbf{A}，\mathbf{B}，\mathbf{C}$的列张成的空间在一般情况下位置，且分解的秩$R$不超过某个与张量维度相关的阈值。

**引理 11.9（尺度变换自由度）** CP分解存在尺度变换自由度：若$(\mathbf{a}_r, \mathbf{b}_r, \mathbf{c}_r)$是一个有效的分解，则$(\alpha_r \mathbf{a}_r, \beta_r \mathbf{b}_r, \gamma_r \mathbf{c}_r)$也是有效分解，只要$\alpha_r \beta_r \gamma_r = 1$。通常通过归一化来消除这种自由度，例如要求 $\|\mathbf{a}_r\| = 1$。
## 11.1.5 Tucker分解：高阶主成分分析
### Tucker分解的数学形式
Tucker分解将张量分解为一个核张量（Core Tensor）与各模因子矩阵的乘积形式。它可以看作是SVD在高阶张量上的推广，也被称为"高阶PCA"。
**定义 11.12（Tucker分解）** 三阶张量$\mathcal{X} \in \mathbb{R}^{I \times J \times K}$的Tucker分解形式为：
$$\mathcal{X} \approx \mathcal{G} \times_1 \mathbf{A} \times_2 \mathbf{B} \times_3 \mathbf{C}
$$其中$\mathcal{G} \in \mathbb{R}^{P \times Q \times R}$是核张量，$\mathbf{A} \in \mathbb{R}^{I \times P}$，$\mathbf{B} \in \mathbb{R}^{J \times Q}$，$\mathbf{C} \in \mathbb{R}^{K \times R}$是因子矩阵。通常$P \leq I$，$Q \leq J$，$R \leq K$，因此Tucker分解是一种降维分解。
**引理 11.10（Tucker分解的元素形式）** Tucker分解的元素形式为：
$$
x_{ijk} \approx \sum_{p=1}^{P} \sum_{q=1}^{Q} \sum_{r=1}^{R} g_{pqr} a_{ip} b_{jq} c_{kr}
$$​利用矩阵运算，可以紧凑地表示为模n展开形式：
$$
\mathbf{X}_{(1)} \approx \mathbf{A}\mathbf{G}_{(1)}(\mathbf{C} \odot \mathbf{B})^{\top}
$$
$$\mathbf{X}_{(2)} \approx \mathbf{B}\mathbf{G}_{(2)}(\mathbf{C} \odot \mathbf{A})^{\top}
$$
$$\mathbf{X}_{(3)} \approx \mathbf{C}\mathbf{G}_{(3)}(\mathbf{B} \odot \mathbf{A})^{\top}
$$
其中$\mathbf{G}_{(n)}$是核张量$\mathcal{G}$的模n展开。
### Tucker分解与SVD的关系
当核张量是对角超张量（即仅在对角线上有非零元素）时，Tucker分解退化为CP分解的特定形式。更一般地，Tucker分解与SVD密切相关，因子矩阵可以通过对张量各模的切片矩阵进行SVD得到。

**定义 11.13（高阶SVD，HOSVD）** 张量$\mathcal{X}$的高阶SVD定义为：

$$\mathcal{X} = \mathcal{S} \times_1 \mathbf{U} \times_2 \mathbf{V} \times_3 \mathbf{W}
$$

其中$\mathbf{U}，\mathbf{V}，\mathbf{W}$分别是模1、模2、模3切片矩阵的左奇异向量矩阵，$\mathcal{S}$是"管心张量"（Tube Tensor），其元素为$s_{ijk} = \langle \mathbf{u}_i, \mathbf{x}_{ijk} \cdot \mathbf{v}_j \cdot \mathbf{w}_k\rangle$。

**定理 11.6（HOSVD的性质）** HOSVD分解中的因子矩阵$\mathbf{U}，\mathbf{V}，\mathbf{W}$是正交的，核张量$\mathcal{S}$满足所谓的"正交性"条件：$\mathcal{S}$的水平、侧向和正面切片是正交的。

**引理 11.11（HOSVD与最佳低秩Tucker分解）** HOSVD提供了一种计算Tucker分解的启发式方法。首先对每个模的切片矩阵进行SVD，得到因子矩阵；然后计算核张量 $\mathcal{G} = \mathcal{X} \times_1 \mathbf{U}^{\top} \times_2 \mathbf{V}^{\top} \times_3 \mathbf{W}^{\top}$。这个分解不一定是最佳的（最小二乘意义下），但提供了良好的初始化。

### Tucker分解的计算方法
Tucker分解的计算通常采用交替最小二乘法（类似于CP分解）或基于高阶正交迭代（HOOI）的方法。
**算法 11.2（HOOI算法框架）**
**输入**：张量$\mathcal{X} \in \mathbb{R}^{I \times J \times K}$，目标秩$(P, Q, R)$
**输出**：核张量$\mathcal{G}$，因子矩阵$\mathbf{A}，\mathbf{B}，\mathbf{C}$
1.初始化$\mathbf{A}，\mathbf{B}，\mathbf{C}$（例如通过HOSVD）
2.重复以下步骤直到收敛：
- 计算模1展开：$\mathbf{Y}_{(1)} = \mathbf{X}_{(1)}(\mathbf{C} \odot \mathbf{B})$
- 对 $\mathbf{Y}_{(1)}$​ 进行SVD，取前$P$个左奇异向量更新$\mathbf{A}$
- 计算模2展开：$\mathbf{Y}_{(2)} = \mathbf{X}_{(2)}(\mathbf{C} \odot \mathbf{A})$
- 对$\mathbf{Y}_{(2)}$进行SVD，取前$Q$个左奇异向量更新$\mathbf{B}$
- 计算模3展开：$\mathbf{Y}_{(3)} = \mathbf{X}_{(3)}(\mathbf{B} \odot \mathbf{A})$
- 对$\mathbf{Y}_{(3)}$ 进行SVD，取前$R$个左奇异向量更新$\mathbf{C}$
- 更新核张量：$\mathcal{G} = \mathcal{X} \times_1 \mathbf{A}^{\top} \times_2 \mathbf{B}^{\top} \times_3 \mathbf{C}^{\top}$
3.返回 $\mathcal{G}，\mathbf{A}，\mathbf{B}，C\mathbf{C}$
**引理 11.12（HOOI的收敛性）** HOOI算法通过交替更新各模的因子矩阵来优化Tucker分解的最小二乘目标。每次更新都使得目标函数单调不增，因此算法收敛到某个（可能是局部）极小值。
### CP分解与Tucker分解的比较
CP分解和Tucker分解是两种主要的张量分解方法，它们各有优缺点，适用于不同的应用场景。
**表 11.1.5 CP分解与Tucker分解的对比**

| 特性    | CP分解                                                                                  | Tucker分解                                                                                              |
| ----- | ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- |
| 分解形式  | 秩一张量和 $\mathcal{X} \approx \sum_r \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r$ | 核张量与因子矩阵$\mathcal{X} \approx \mathcal{G} \times_1 \mathbf{A} \times_2 \mathbf{B} \times_3 \mathbf{C}$ |
| 参数数量  | $R(I+J+K)$                                                                            | $PQR + IP + JQ + KR$                                                                                  |
| 唯一性   | 某些条件下唯一                                                                               | 通常不唯一                                                                                                 |
| 计算复杂度 | ALS迭代 $O(IJKR)$                                                                       | HOOI迭代 $O(IJK(P+Q+R))$                                                                                |
| 解释性   | 明确的秩一成分                                                                               | 核张量表示成分间交互                                                                                            |
| 适用场景  | 稀疏因子、成分可解释                                                                            | 维度缩减、全局结构                                                                                             |
**定理 11.7（分解的选择原则）** 选择CP分解还是Tucker分解取决于应用需求。当目标是发现稀疏的潜在因子时，CP分解更合适；当目标是降维或保留全局结构时，Tucker分解更合适。对于相同的信息保留量，Tucker分解通常需要更少的参数。
**引理 11.13（TT分解作为Tucker的推广）** 张量链（Tensor Train，TT）分解是Tucker分解的另一种推广形式，将高阶张量分解为一阶张量（向量）的序列。TT分解在量子多体物理和深度学习中有重要应用。

## 11.1.6 深度学习中的张量分解应用
### 卷积神经网络的张量分解

现代深度学习模型，特别是卷积神经网络（CNN），可以被自然地表示为张量操作。张量分解技术为模型压缩和加速提供了有效手段。

**定义 11.14（卷积核的张量表示）** 四维卷积核$\mathcal{W} \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}} \times K \times K}$可以看作一个四阶张量。将张量分解应用于卷积核，可以将卷积运算分解为更简单的运算的组合。

**引理 11.14（CP分解用于CNN压缩）** 设卷积核$\mathcal{W}$进行CP分解：
$\mathcal{W} \approx \sum_{r=1}^{R} \mathbf{a}_r \circ \mathbf{b}_r \circ \mathbf{c}_r \circ \mathbf{d}_r$，其中 $\mathbf{a}_r \in \mathbb{R}^{C_{\text{out}}}$，$\mathbf{b}_r \in \mathbb{R}^{C_{\text{in}}}$​，$\mathbf{c}_r, \mathbf{d}_r \in \mathbb{R}^K$。则卷积运算$y = \mathcal{W} * x$可以分解为$R$个小卷积的和，每个小卷积使用秩一核$\mathbf{b}_r \circ \mathbf{c}_r \circ \mathbf{d}_r$，然后通过$\mathbf{a}_r$进行线性组合。

**定理 11.8（压缩比与精度权衡）** 设原始卷积层参数量为$C_{\text{out}} \times C_{\text{in}} \times K^2$，CP分解后参数量为 $R(C_{\text{out}} + C_{\text{in}} + 2K)$。压缩比约为$(Cout+Cin+2K)\frac{C_{\text{out}} C_{\text{in}} K^2}{R(C_{\text{out}} + C_{\text{in}} + 2K)}$。通过选择适当的秩$R$，可以在压缩比和近似精度之间取得平衡。

### 注意力机制的张量视角

Transformer中的自注意力机制也可以从张量角度理解。注意力矩阵的计算涉及查询、键、值的矩阵乘法，可以看作张量收缩操作。

**引理 11.15（多头注意力的分解）** 多头注意力机制中的多个头可以看作是张量分解的一种形式。设$h$个注意力头的输出为$\mathbf{O}_1, \mathbf{O}_2, \dots, \mathbf{O}_h$，则最终输出$\mathbf{O} = \text{Concat}(\mathbf{O}_1, \dots, \mathbf{O}_h)\mathbf{W}_O$。这可以看作是对注意力输出张量$\mathcal{O} \in \mathbb{R}^{h \times N \times d}$沿第一个模的"折叠"操作。

### 张量回归层

传统的全连接层将输入展平后进行线性变换，丢失了输入的空间结构信息。张量回归层（Tensor Regression Layer）通过张量收缩保持输入的结构。

**定义 11.15（张量回归层）** 设输入张量$\mathcal{X} \in \mathbb{R}^{I_1 \times \dots \times I_N}$，输出$\mathbf{y} \in \mathbb{R}^M$，张量回归层定义为：
$$
\mathbf{y} = \mathcal{W} \times_1 \mathbf{X}^{(1)} \times_2 \mathbf{X}^{(2)} \times \dots \times_N \mathbf{X}^{(N)} + \mathbf{b}
$$
其中$\mathcal{W} \in \mathbb{R}^{M \times I_1 \times \dots \times I_N}$是权重张量，$\mathbf{X}^{(n)} \in \mathbb{R}^{I_n \times I_n}X(n)$是沿第$n$模的投影矩阵。

**引理 11.16（张量回归的参数效率）** 张量回归层通过因子分解$\mathcal{W} = \mathcal{G} \times_1 \mathbf{A}_1 \times_2 \dots \times_N \mathbf{A}_N$来减少参数量，其中$\mathcal{G} \in \mathbb{R}^{R_1 \times \dots \times R_N}$​，$\mathbf{A}_n \in \mathbb{R}^{I_n \times R_n}$​。参数量从$\prod_n I_n$减少到$\prod_n R_n + \sum_n I_n R_n$​。

## 11.1.7 本节小结

本节系统地介绍了矩阵与张量分解的数学基础及其在深度学习中的应用。我们从SVD的严格数学定义出发，展示了其"旋转-缩放-旋转"的几何解释，以及Eckart-Young定理所保证的最佳低秩近似性质。SVD不仅是理解高阶张量分解的基础，更是深度学习模型压缩的核心工具。

在张量代数部分，我们建立了模n乘积、展开运算、Khatri-Rao积等基本工具，为后续的高阶分解奠定数学基础。这些运算虽然初看复杂，但它们正是将矩阵分解的直觉推广到高维空间的桥梁。

CP分解将张量表示为秩一张量的和，其元素形式简洁优雅，但张量秩的NP困难性给实际计算带来挑战。ALS算法通过交替优化提供了一种实用的求解方法，尽管只能保证收敛到局部极小值。Tucker分解则通过引入核张量的概念，实现了更高灵活性的分解，HOSVD和HOOI算法为其计算提供了有效手段。

从深度学习应用的角度，张量分解为卷积核压缩、注意力机制分析、张量回归层设计等问题提供了统一的数学框架。通过选择适当的分解方法和秩参数，可以在模型效率和性能之间取得有意义的平衡。

理解这些分解技术的数学本质，不仅有助于设计更高效的深度学习模型，更能深化我们对高维数据表示学习的理论认识。随着深度学习模型规模的持续增长，张量分解作为处理高维数据的核心数学工具，其重要性将更加凸显。