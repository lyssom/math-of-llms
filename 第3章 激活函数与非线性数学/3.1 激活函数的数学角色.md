激活函数是神经网络能够拟合复杂非线性函数的核心数学组件。在线性模型中，无论堆叠多少层神经元，网络的表达能力始终局限于输入的线性变换——这正是"万有逼近定理"所揭示的深刻数学原理。激活函数的引入打破了这种线性局限，使得神经网络能够表示任意复杂的非线性映射关系。本节将从数学的角度系统性地分析激活函数的理论基础、概率解释以及信息论意义，为理解大语言模型中非线性变换的数学本质奠定坚实基础。

## 3.1.1 非线性的数学必要性

神经网络之所以能够拟合任意复杂的函数模式，其根本原因在于激活函数引入的非线性变换。为了深刻理解这一数学本质，我们需要从线性映射的局限性出发，逐步揭示非线性变换的必要性和数学价值。这种分析不仅具有理论意义，也为理解Transformer架构中非线性组件的设计提供了数学基础。

### 线性映射的数学结构

考虑一个没有激活函数的浅层神经网络，其数学表示可以清晰地揭示线性变换的本质局限性。设输入向量为$x \in \mathbb{R}^d$，网络包含一个隐藏层，该隐藏层有$m$个神经元，输出层有$k$个神经元。则网络的前向传播可以表示为两个连续的线性变换：
$$
z = W_2(W_1 x + b_1) + b_2​
$$
其中$W_1 \in \mathbb{R}^{m \times d}$是输入层到隐藏层的权重矩阵，$b_1 \in \mathbb{R}^m$是相应的偏置向量，$W_2 \in \mathbb{R}^{k \times m}$是隐藏层到输出层的权重矩阵，$b_2 \in \mathbb{R}^k$是输出层的偏置向量。通过矩阵乘法的结合律和分配律，上述表达式可以简化为：
$$
z = (W_2 W_1)x + (W_2 b_1 + b_2) = W'x + b'
$$
其中$W' = W_2 W_1$是两个权重矩阵的乘积，$b' = W_2 b_1 + b_2$是组合偏置向量。这个结果表明，即使网络包含多个隐藏层，只要不使用激活函数，整个网络仍然等价于一个简单的线性变换$W'x + b'$。这个数学事实揭示了一个深刻的原理：**线性变换的复合仍然是线性变换**。

**定义3.1.1（线性映射）** 一个映射$f: \mathbb{R}^n \to \mathbb{R}^m$是线性的，当且仅当对于任意向量$x, y \in \mathbb{R}^n$和任意标量$\alpha, \beta \in \mathbb{R}$，满足以下两个条件：
$$
f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)
$$
这个定义包含了线性映射的两个核心性质：
- **齐次性**（Homogeneity）$f(\alpha x) = \alpha f(x)$
- **可加性**（Additivity）$f(x + y) = f(x) + f(y)$。
线性映射可以用矩阵乘法完全描述，即$f(x) = Wx + b$，其中$W \in \mathbb{R}^{m \times n}$是变换矩阵，$b \in \mathbb{R}^m$是平移向量。

从几何角度理解，线性映射描述了输入空间通过旋转、缩放和反射（由矩阵$W$描述）变换到输出空间的过程，然后通过平移（由向量$b$描述）调整位置。这种变换保持了几何结构的许多性质：直线映射为直线，平行线保持平行，原点保持不动（除非有平移分量）。然而，正是这些保持的性质限制了线性映射的表达能力——它无法表示复杂的曲线、曲面或更复杂的几何结构。

### 线性模型表达能力的根本局限

线性模型虽然具有优美的数学性质，如凸优化问题和解析解的存在性，但其表达能力受到根本性的限制。设真实的数据生成过程涉及两个变量的交互效应，例如$y = x_1 x_2$​，这是一个简单的乘法交互。线性模型只能学习形如$y = w_1 x_1 + w_2 x_2 + b$的函数，完全无法捕获$x_1$和$x_2$之间的乘积交互。这种局限性在处理现实世界的复杂数据时尤为突出，因为语言、图像等数据中充满了非线性模式。

考虑一个二维空间中的分类问题，数据分布为两个交织的螺旋形状。要将这两个类别分开，需要一条非线性的决策边界——可能是曲线、折线或更复杂的形状。线性分类器（如逻辑回归、支持向量机）只能学习直线或超平面作为决策边界，对于这种复杂的分布，无论训练多长时间、调整多少超参数，都无法达到良好的分类效果。这个例子直观地说明了线性模型的表达能力边界。

从数学上分析，线性模型的学习能力可以用函数空间的维度来刻画。设输入空间为$\mathbb{R}^d$，线性模型的函数空间是所有仿射函数的集合，其维度为$d + 1$（$d$个权重加 1 个偏置）。无论训练数据有多少，只要模型结构固定（线性），其能表示的函数就被限制在这个有限维的函数空间中。当真实的数据生成过程超出这个空间时，线性模型必然产生系统性误差。

### 非线性变换打破表达瓶颈

激活函数的引入彻底改变了这一局面。设隐藏层的输出为$h = \sigma(W_1 x + b_1)$，其中$\sigma$是非线性激活函数。此时网络的输出为：
$$
y = W_2 h + b_2 = W_2 \sigma(W_1 x + b_1) + b_2​
$$
由于$\sigma$是非线性函数，$W_2 \sigma(W_1 x + b_1)$不再能够简化为$W'x + b'$的形式。网络现在是**分段线性**或**非线性**的，其表达能力不再受限于线性变换的复合。这种非线性的引入使得神经网络能够学习任意复杂的输入-输出映射，从简单的曲线到高维空间中的复杂流形。

**与第五章注意力的联系**：在Transformer的自注意力机制中，Query、Key、Value的投影计算是线性的，但Softmax激活函数引入了关键的非线性。注意力权重的计算公式为$A = \text{Softmax}(QK^T/\sqrt{d_k})$，Softmax的非线性确保了注意力权重具有归一化的概率解释，同时使得注意力机制能够学习非线性的Token交互模式。没有Softmax的线性归一化将无法实现注意力的"软选择"功能，模型将退化为简单的加权平均，无法捕获Token之间的复杂依赖关系。

从数学角度看，激活函数将线性变换的输出通过一个非线性函数进行变换，然后在下一层再次进行线性变换。这种"线性-非线性-线性-非线性"的交替结构是深度学习成功的关键数学基础。每一次非线性变换都扩展了网络能够表示的函数空间，多层堆叠使得网络能够表示极其复杂的函数。

**定义3.1.2（逐元素非线性变换）** 设激活函数$\sigma: \mathbb{R} \to \mathbb{R}$逐元素应用于向量$h \in \mathbb{R}^C$，则输出向量为：
$$
\sigma(h) = (\sigma(h_1), \sigma(h_2), \ldots, \sigma(h_C))^T
$$
这种逐元素应用的方式保持了向量结构，同时对每个维度施加了非线性变换。在反向传播中，这种结构使得梯度计算可以高效地逐元素进行。

## 3.1.2 激活函数作为函数逼近的基石

神经网络之所以被称为"通用逼近器"，是因为在适当条件下，单层神经网络可以逼近任意连续函数到任意精度。激活函数在这一逼近能力中扮演着核心角色——正是非线性激活函数赋予了网络这种强大的表达能力。本节将从函数逼近论的角度，深入分析激活函数的数学角色，揭示其作为"函数逼近基石"的理论基础。

### 通用逼近定理的数学表述

神经网络通用逼近能力的核心定理表明，只要激活函数满足一定的数学条件，单隐藏层神经网络就可以以任意精度逼近任意连续函数。这是神经网络理论中最深刻的结果之一，它从数学上证明了深度学习的表达能力基础。

**定理3.1.1（通用逼近定理）** 设$\sigma: \mathbb{R} \to \mathbb{R}$是任意非常数的有界连续函数，$K \subset \mathbb{R}^d$是紧集（封闭有界集合）。对于任意连续函数$f: K \to \mathbb{R}$和任意$\epsilon > 0$，存在一个单隐藏层神经网络：
$$
g_N(x) = \sum_{i=1}^N w_i \sigma(a_i^T x + b_i)
$$
其中$a_i \in \mathbb{R}^d$是输入权重向量，$b_i \in \mathbb{R}$是偏置，$w_i \in \mathbb{R}$是输出权重，使得：
$$
\sup_{x \in K} |g_N(x) - f(x)| < \epsilon
$$
这个定理的数学意义极其深远。它指出：**只要激活函数是"非平凡"的（非常数），就具有通用逼近能力**。这意味着Sigmoid、Tanh、ReLU、GELU等激活函数在理论上都具有同等的表达能力——它们都可以作为通用逼近器的核心组件。

定理的证明思路基于Stone-Weierstrass逼近定理。Stone-Weierstrass定理指出，如果一组函数满足以下条件：
(1) 包含所有常数函数，
(2) 对加法和乘法封闭，
(3) 能区分任意两点，则它们的线性组合可以逼近任意连续函数。
激活函数$\sigma(a^T x + b)$构成的函数族在适当条件下可以满足这些要求，从而通过Stone-Weierstrass定理推导出通用逼近能力。

### 局部基函数与函数重构

通用逼近定理的核心数学洞见在于：**非线性激活函数的"非单调性"或"有界振荡性"使得函数族$\{\sigma(a^T x + b)\}$能够在输入空间中形成"局部基函数"**。每个神经元$\sigma(a_i^T x + b_i)$ 可以被视为一个"局部探测器"——它在输入空间的某个区域激活，在其他区域抑制。大量这样的局部探测器通过线性组合，可以精确重构任意复杂的连续函数模式。

从几何角度理解，每个激活函数$\sigma(a^T x + b)$在输入空间中定义了一个"超平面" $a^T x + b = 0$。这个超平面将输入空间分为两个半空间：在一侧，激活函数的输出值较高；在另一侧，输出值较低。因此，每个神经元本质上是在输入空间中划分一个"决策边界"，将空间分为"激活区"和"抑制区"。多个这样的超平面组合，可以形成复杂的决策边界，从而逼近任意形状的函数曲面。

考虑一个简单的例子：使用Sigmoid函数作为激活函数。每个Sigmoid神经元$\sigma(a^T x + b)$在输入空间中形成一个"S形"曲面。当多个这样的曲面进行线性组合时，可以通过调整每个曲面的位置、方向和幅度来构造任意复杂的曲面。这种"局部曲面叠加"的能力正是通用逼近能力的几何解释。

**激活函数类型与逼近效率**：通用逼近定理指出"任意非常数有界连续函数"都可以作为激活函数，但不同激活函数的逼近效率是不同的。对于相同的逼近精度，使用"表达能力更强"的激活函数可能需要更少的神经元。历史上，常用的激活函数如Sigmoid和Tanh被选择是因为它们具有良好的数学性质（可微、有界、光滑），但近年来ReLU的流行表明，简单性有时比数学上的"优雅"更有价值。ReLU虽然数学形式简单（分段线性），但其分段线性的性质使得它在大规模网络中表现优异，因为它的稀疏激活特性和线性区域的恒定导数有利于深层网络的训练。

### 深度与宽度的数学权衡

通用逼近定理表明，足够宽的单层网络可以逼近任意函数。然而，在实践中，我们通常使用较窄但较深的网络来达到相同的表达能力。这种"深度优先"的设计选择背后有深刻的数学原因，理解这些原因对于理解现代大语言模型的架构设计至关重要。

在高维输入空间中，若目标函数具有**组合（compositional）或层次化结构**，则深度神经网络在表示效率上可能显著优于浅层网络。已有理论结果表明，对于某些函数类，浅层网络需要指数级数量的神经元才能逼近，而深度网络仅需多项式规模。

从直观上看，深度网络通过逐层的非线性变换，可以学习从低级局部模式到高级抽象语义的层次化表示。这种结构与许多自然数据（如图像、语音、语言）中的生成机制相契合，因此在实践中表现出更强的表示能力和泛化性能。

从数学上分析，深度网络能够表示的函数空间远大于同等参数量的浅层网络。设一个深度为$L$、宽度为$m$的全连接网络，其参数数量为$O(L m^2)$（考虑权重和偏置）。研究表明，深度为$L$的网络可以等效于宽度为指数级别的浅层网络，即$m_{\text{shallow}} \approx \exp(m_{\text{deep}})$。这就是著名的"深度指数优势"——深度网络用指数级更少的参数可以达到与浅层网络相同的表达能力。

**与Transformer架构的联系**：Transformer采用了深度网络的架构设计，其注意力层和前馈层堆叠形成深层结构。每个注意力层学习Token之间的交互模式，不同层的注意力可能关注不同类型的关系（如词汇关系、句法关系、语义关系）。这种层次化的注意力学习是大语言模型能力的重要来源。激活函数（如GELU）在每一层提供必要的非线性变换，使得注意力模式可以逐层组合，形成更复杂的Token交互模式。

**定义3.1.3（层次化特征学习）** 设$h^{(l)}$是第$l$层的表示向量，则层次化特征学习可以表示为：
$$
h^{(l)} = \sigma^{(l)}(W^{(l)} h^{(l-1)} + b^{(l)})
$$
其中每一层的变换$W^{(l)} h^{(l-1)} + b^{(l)}$和激活$\sigma^{(l)}$将前一层的表示转换为新的表示。理想的层次化学习应该满足：$h^{(l)}$捕获比$h^{(l-1)}$更抽象、更语义化的特征。这种抽象化过程正是深度学习成功的关键数学机制。

## 3.1.3 概率视角下的激活函数

从概率论的角度，许多激活函数可以解释为某种概率分布的参数变换。这种概率解释不仅加深了我们对激活函数数学本质的理解，也为设计新的激活函数和理解训练动态提供了理论指导。本节将系统性地分析主要激活函数的概率解释，揭示它们与统计学和信息论之间的深层联系。

### Sigmoid函数与伯努利分布

Sigmoid函数$\sigma(x) = \frac{1}{1+e^{-x}}$将实数域映射到$(0,1)$区间，这正好是概率值的取值范围。这种映射关系使得Sigmoid函数在概率建模中具有直接的应用价值。在伯努利分布中，成功概率$p$的对数几率（Log-Odds）定义为$\log \frac{p}{1-p}$​。Sigmoid函数的逆函数正是对数几率的变换：
$$
x = \log \frac{\sigma(x)}{1-\sigma(x)}
$$
这个恒等式可以从代数上验证。将$p = \sigma(x)$代入对数几率的表达式：
$$
\log \frac{p}{1-p} = \log \frac{\sigma(x)}{1-\sigma(x)} = \log \frac{\frac{1}{1+e^{-x}}}{1-\frac{1}{1+e^{-x}}} = \log \frac{\frac{1}{1+e^{-x}}}{\frac{e^{-x}}{1+e^{-x}}} = \log e^x = x
$$
因此，Sigmoid函数可以将任意实数$x$解释为伯努利分布的成功概率$p = \sigma(x)$。在神经网络中，Sigmoid输出常用于二分类任务的概率预测——网络的原始输出（logits）经过Sigmoid变换后，得到样本属于正类的概率估计。

**定义3.1.4（Logits与概率的变换）** 设网络的原始输出为$z \in \mathbb{R}$（称为logit），则对应的概率为$p = \sigma(z)$。这个变换的逆变换为$z = \log \frac{p}{1-p}$​。Logits可以理解为"对数几率"，它是一个无界的实数，可以取任意值；概率是归一化后的值，限制在$(0,1)$区间内。

从贝叶斯推断的角度，Sigmoid变换可以理解为将先验信息（用logits表示）转换为后验概率的过程。在逻辑回归中，参数$w$的后验分布与似然函数和先验的乘积成正比。Sigmoid函数将线性组合$w^T x$转换为概率，完成了从线性预测到概率预测的关键一步。

### Softmax函数与多项式分布

Softmax函数是Sigmoid在多类别情况下的推广，它将$C$维向量映射到概率单纯形$\Delta^{C-1} = \{p \in \mathbb{R}^C \mid p_i \geq 0, \sum_i p_i = 1\}$。这与多项式分布的参数空间完全一致。在多项式分布中，概率参数$p = (p_1, \ldots, p_C)$的对数几率为$\log p_i - \log p_C$（以类别$C$为基准）。Softmax变换正是这种对数几率的指数化和归一化。

**定义3.1.5（Softmax函数与概率单纯形）** 对于输入向量$z = (z_1, z_2, \ldots, z_C)^T \in \mathbb{R}^C$，Softmax函数定义为：
$$
\text{Softmax}(z)_i = \frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)} \triangleq \hat{p}_i​
$$
Softmax的输出$\hat{p}$满足概率分布的所有公理：$\hat{p}_i > 0$对所有$i$成立，且$\sum_{i=1}^C \hat{p}_i = 1$。因此$\hat{p}$ 是概率单纯形$\Delta^{C-1}$中的点。

从多项式分布的角度，Softmax输出的概率$\hat{p}_i$表示样本属于第$i$类的概率估计。多项式分布的似然函数为：
$$
P(y = i \mid x; \theta) = \hat{p}_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}
$$
其中$z = Wx + b$是网络的原始输出（logits）。对数似然为：
$$
\log P(y = i \mid x; \theta) = z_i - \log \sum_j \exp(z_j)
$$
这个表达式在分类任务的训练中至关重要，它正是交叉熵损失的核心组成部分。

**与第四章损失函数的联系**：概率解释为损失函数的设计提供了坚实的理论基础。第四章详细讨论的交叉熵损失$-\sum_i p_i \log \hat{p}_i$正是衡量两个概率分布（真实分布$p$和预测分布$\hat{p}$）差异的信息论度量。当预测分布 $\hat{p}$由Softmax给出时，交叉熵损失具有清晰的概率解释——它是负对数似然的期望。激活函数的概率解释与损失函数的信息论基础共同构成了分类任务训练的数学框架。

### Gumbel-Softmax与离散分布采样

在需要从离散分布采样的场景中（如变分自编码器的离散潜在变量、策略网络的离散动作选择），Gumbel-Softmax提供了一种可微的采样近似。这种技术使得离散随机变量的重参数化成为可能，极大地促进了离散分布表示学习的发展。

**定义3.1.6（Gumbel-Softmax分布）** 设$z \in \mathbb{R}^C$是 logits，Gumbel-Softmax样本定义为：
$$
y_i = \frac{\exp((z_i + g_i)/\tau)}{\sum_{j=1}^C \exp((z_j + g_j)/\tau)}​
$$
其中$g_i = -\log(-\log u_i)$是Gumbel噪声，$u_i \sim \text{Uniform}(0,1)$是独立同分布的均匀随机变量，$\tau > 0$是温度参数。

Gumbel-Softmax的数学性质由温度参数$\tau$控制。当$\tau \to 0$时，$\exp((z_i + g_i)/\tau)$中最大值的指数增长占主导，Gumbel-Softmax趋向于one-hot采样（对应Hardmax）；当$\tau \to \infty$时，所有指数项趋向于1，Gumbel-Softmax趋向于均匀分布。温度参数$\tau$提供了从"软"（接近均匀分布）到"硬"（接近one-hot采样）的连续调节能力。

从概率论角度，Gumbel-Softmax是Gumbel分布和Softmax分布的结合。Gumbel分布是极值分布的一种，用于模拟独立随机变量序列的最大值分布。通过向logits添加Gumbel噪声并进行Softmax变换，我们得到了一个可以近似离散采样的连续分布。这个分布在重参数化梯度计算中扮演关键角色：样本$y$可以表示为确定函数$y(z,u)$，其中$u$是随机噪声源，因此$\nabla_z \mathbb{E}[f(y)] = \mathbb{E}[\nabla_y f(y) \cdot \nabla_z y]$ 可以通过采样$u$来估计。

## 3.1.4 信息论视角与激活函数

从信息论的角度，激活函数可以被理解为对输入信息的某种非线性编码和变换。这种视角揭示了激活函数在信息处理中的深层作用，也为分析神经网络的信息流和表示学习提供了数学工具。信息论提供了量化信息、熵和互信息的数学框架，将这些概念应用于激活函数分析，可以揭示其信息处理能力的本质。

### 激活函数的信息压缩作用

许多激活函数（如Sigmoid、Tanh）将实数轴压缩到有限的区间$[0,1]$或$[-1,1]$。这种压缩本质上是信息的**有损编码**——输入的无限精度被截断为有限范围的输出。然而，这种有损压缩在机器学习中是有益的：它提供了数值稳定性，限制了输出值的范围，使得不同层之间的信息传递更加可控。

**定义3.1.7（互信息）** 两个随机变量$X$和$Y$之间的互信息定义为：
$$
I(X; Y) = \mathbb{E}_{P_{X,Y}}\left[\log \frac{P(x,y)}{P(x)P(y)}\right] = H(X) - H(X \mid Y)
$$
互信息衡量的是知道$Y$后关于$X$的信息量（反之亦然），它度量了$X$和$Y$之间的依赖程度。互信息是非负的，当且仅当$X$和$Y$独立时互信息为零。

从信息论角度，激活函数可以视为一种信息瓶颈（Information Bottleneck）。设输入为$X$，经过激活函数的输出为$Y = \sigma(WX + b)$。信息瓶颈理论认为，良好的表示$Y$应该最大化$I(Y;T)$（保留关于目标$T$的信息），同时最小化$I(Y;X)$（去除关于输入的冗余信息）。激活函数的非线性变换正是实现这种信息筛选的机制。

**ReLU的信息选择性**：ReLU函数$\sigma(x) = \max(0, x)$ 的行为类似于信息选择器——它保留正信息（$x > 0$），完全抑制负信息（$x < 0$）。从信息论角度，ReLU可以被解释为一种"阈值化"操作：只有超过阈值的信号才能通过。这种选择性与生物神经元的行为类似——生物神经元在膜电位超过阈值时发放动作电位。

ReLU的稀疏激活特性（在任何输入下，约有一半的神经元输出为零）使得网络具有稀疏表示的能力。稀疏表示在信息论上具有几个优势：首先，稀疏编码通常更高效地利用参数——少量的活跃神经元可以编码大量的信息；其次，稀疏表示对噪声更加鲁棒——单个噪声样本不太可能同时激活多个稀疏神经元；第三，稀疏表示提供了更好的可解释性——活跃的神经元可以被解释为检测到特定模式的"探测器"。

### 激活函数与互信息最大化

在自监督学习中，神经网络的目标通常是最大化输入不同部分之间的互信息。激活函数在这个过程中扮演关键角色，它们决定了信息如何被编码和传递。

考虑一个简单的互信息估计问题：给定输入$x$和其上下文$c$（如同一序列中相邻的Token），我们希望最大化$I(x;c)$。神经网络通过编码器$f$和$g$分别编码$x$和$c$，然后使用激活函数（如ReLU或GELU）处理编码向量，最后通过对比损失（如InfoNCE，参见第四章）估计和最大化互信息。激活函数的非线性变换增加了编码的丰富性，使得$f(x)$和$g(c)$能够捕获输入之间的复杂依赖关系。

**定理3.1.2（互信息的下界估计）** InfoNCE损失提供了互信息的下界估计：
$$
I(x; c) \geq \log N - L_{\text{InfoNCE}}​
$$
其中$N$是负样本的数量。这个不等式表明，最小化InfoNCE损失等价于最大化互信息的下界。激活函数的选择影响这个下界的紧致程度——表达能力更强的激活函数可能提供更紧的下界，从而更准确地估计和最大化互信息。

**与位置编码的信息论联系**：在Transformer中，位置编码（参见第六章）为序列中的每个位置添加位置信息。位置编码的数学设计（如正弦余弦编码的频率分解）使得不同位置具有可区分的编码，同时相邻位置之间具有平滑的插值性质。从信息论角度，位置编码可以被理解为对"位置信息"的显式注入——它将位置信息从隐式（通过序列顺序隐含）转换为显式（通过编码向量表示）。

位置编码与激活函数的结合决定了位置信息如何在网络中传递。正弦余弦编码的频率分解特性意味着不同频率的成分携带不同尺度的位置信息——低频成分编码粗粒度的位置关系，高频成分编码细粒度的位置关系。激活函数（如GELU）对位置编码的处理会保留或抑制不同频率的成分，从而影响模型对位置模式的捕获能力。GELU的平滑特性有助于保持相邻位置信息的连续性，这对于学习位置敏感的Token交互模式至关重要。

### 激活函数的信息几何视角

信息几何是研究概率分布空间几何性质的理论框架。在这个框架下，概率分布被视为一个流形，而各种信息量（如Fisher信息、KL散度）定义了流形上的几何结构。激活函数在这个几何框架中扮演着重要角色，它们定义了从输入空间到表示空间的映射，而这个映射的几何性质决定了神经网络的学习动态和表达能力。

**定义3.1.8（Fisher信息度量）** 在参数化的概率分布族$\{P_\theta\}$上，Fisher信息矩阵定义为：
$$
I(\theta)_{ij} = \mathbb{E}_{x \sim P_\theta} \left[ \frac{\partial \log P_\theta(x)}{\partial \theta_i} \frac{\partial \log P_\theta(x)}{\partial \theta_j} \right]
$$
Fisher信息矩阵定义了概率流形上的黎曼度量。在这个度量下，两个概率分布之间的"距离"不是欧几里得距离，而是由Fisher信息加权的距离。激活函数通过影响概率分布的形状，间接影响Fisher信息度量的几何结构。

对于Softmax激活函数，输出分布$\hat{p} = \text{Softmax}(z)$关于输入$z$的Fisher信息矩阵正好是Softmax雅可比矩阵与自身转置的乘积：$J_{\text{Softmax}}(z) J_{\text{Softmax}}(z)^T$。这个矩阵的谱结构决定了梯度下降在概率空间中的几何行为——较大的奇异值对应于"陡峭"的方向，较小的奇异值对应于"平坦"的方向。理解这个几何结构对于设计有效的优化算法至关重要。

**与损失函数的联系**：在第四章中，我们将详细分析损失函数的优化性质。交叉熵损失 $L_{CE}(z) = -\sum_i p_i \log \text{Softmax}(z)_i$的Hessian矩阵与Fisher信息矩阵密切相关。具体而言，$H_{CE}(z) = I_{\text{Softmax}}(z)$，即交叉熵损失的曲率正好由Softmax分布的Fisher信息度量给出。这种联系揭示了为什么Softmax与交叉熵的组合在优化上具有特殊的性质——它们的联合结构使得损失景观具有特定的几何形状，有利于梯度下降的收敛。

## 3.1.5 激活函数与Transformer架构的数学协同

激活函数在Transformer架构中的应用体现了深度学习组件之间数学协同的设计哲学。Transformer中的激活函数选择不是随意的，而是经过深思熟虑的数学设计。本节将系统分析激活函数与Transformer其他组件（注意力机制、位置编码、前馈网络）之间的数学协同关系。

### 前馈网络中的激活函数选择

Transformer中的前馈网络（Feed-Forward Network，FFN）定义为：
$$
\text{FFN}(x) = W_2 \sigma(W_1 x + b_1) + b_2​
$$
其中$\sigma$是激活函数。现代Transformer（如GPT、BERT、LLaMA）普遍选择GELU作为$\sigma$。这个选择背后有深刻的数学考量。

GELU的定义$\text{GELU}(x) = x \cdot \Phi(x)$表明它是输入$x$与其通过标准正态累积分布函数$\Phi(x)$权重的乘积。这种定义使得GELU具有概率解释：它可以被理解为"以概率$\Phi(x)$ 通过$x$"。这种概率解释与注意力机制中的Softmax概率分布形成了数学上的呼应——两者都涉及概率加权的信号传递。

从梯度角度分析，GELU的导数$\Phi(x) + x \phi(x)$ 在整个实数轴上都是正的，这意味着GELU不会像ReLU那样产生"死神经元"问题。同时，GELU的导数不会过于接近零（除了在极端负值区域），这避免了Sigmoid和Tanh的梯度饱和问题。这种"非饱和但有界"的梯度特性使得GELU特别适合训练非常深的网络。

### 注意力机制与激活函数的协同

Transformer的自注意力机制本身不包含显式的激活函数，但Softmax操作本质上是另一种形式的非线性激活。注意力权重的计算$A = \text{Softmax}(QK^T/\sqrt{d_k})$将Query-Key相似度分数转换为概率分布，这个过程与Sigmoid将logits转换为概率的过程在数学上是相似的。

**定义3.1.9（缩放点积注意力）** Scaled Dot-Product Attention定义为：
$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$
这里，$\frac{QK^T}{\sqrt{d_k}}$计算的是Query和Key之间的相似度矩阵，Softmax将每一行归一化为概率分布，最后用这个分布对Value进行加权平均。

注意力机制中的Softmax和FFN中的GELU形成了"双重非线性"结构。Softmax提供Token间的交互非线性（学习Token之间的注意力模式），GELU提供Token内部的特征非线性（对注意力输出进行非线性变换）。这种双重非线性的组合使得Transformer能够学习极其复杂的函数映射。

### 激活函数与位置编码的配合

位置编码（参见第六章）为序列中的每个位置分配数学表示。绝对位置编码（如正弦余弦编码）定义为：
$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$
这种编码将位置信息嵌入到固定范围的正弦和余弦函数中。正弦和余弦函数的取值范围是$[-1, 1]$，这与GELU输入的范围需求相匹配。

从信息论角度，位置编码的频率分解意味着不同维度携带不同尺度的位置信息——偶数维度（低频）和奇数维度（高频）分别编码粗粒度和细粒度的位置关系。激活函数对这种频率分解信息的处理决定了位置信息如何在网络中传递。GELU的平滑特性有助于保持相邻位置信息的连续性，这对于学习位置敏感的Token交互模式至关重要。

**定义3.1.10（位置感知的特征表示）** 设$PE$是位置编码矩阵，$E$是Token嵌入矩阵，则位置感知的嵌入表示为：
$$
h = \text{GELU}(W_{pe} PE + W_e E)
$$
这个公式表明，位置信息和Token信息通过不同的权重矩阵投影后相加，然后经过GELU激活。GELU的非线性变换将这种位置感知的嵌入转换为新的表示，该表示保留了位置信息，同时通过非线性组合创造了位置-Token的交互特征。


## 3.1.6 本节小结

本节从多个数学视角系统性地分析了激活函数的角色。从线性映射的局限性出发，我们揭示了非线性激活函数打破表达瓶颈的数学必然性。从函数逼近论的角度，我们论证了激活函数作为通用逼近器核心组件的数学基础。从概率论的角度，我们分析了Sigmoid、Softmax等激活函数与概率分布的内在联系。从信息论的角度，我们探讨了激活函数的信息压缩、选择和互信息最大化作用。最后，我们分析了激活函数与Transformer架构（注意力机制、位置编码、前馈网络）的数学协同关系。这些分析为后续章节（特别是第五章注意力机制、第六章位置编码、第四章损失函数）奠定了理论基础，揭示了大语言模型中非线性变换的数学本质。
