# 第3章 激活函数与非线性数学
## 3.1 激活函数的数学角色

激活函数是神经网络能够拟合复杂非线性函数的核心数学组件。在线性模型中，无论堆叠多少层神经元，网络的表达能力始终局限于输入的线性变换。激活函数的引入打破了这种线性局限，使得神经网络能够表示任意复杂的非线性映射关系。本节将从数学角度系统分析激活函数的理论基础、概率解释以及信息论意义。

### 3.1.1 非线性的数学必要性

神经网络之所以能够拟合任意复杂的函数模式，其根本原因在于激活函数引入的非线性变换。理解这一数学本质，需要从线性映射的局限性出发。

考虑一个没有激活函数的浅层神经网络。设输入向量为$x \in \mathbb{R}^d$，网络包含一个隐藏层（有$m$个神经元）和输出层（有$k$个神经元）。前向传播可表示为两个连续的线性变换：
$$
z = W_2(W_1 x + b_1) + b_2\tag{3.1.1}
$$
其中$W_1 \in \mathbb{R}^{m \times d}$和$W_2 \in \mathbb{R}^{k \times m}$是权重矩阵，$b_1 \in \mathbb{R}^m$和$b_2 \in \mathbb{R}^k$是偏置向量。通过矩阵乘法的结合律，上述表达式可简化为：
$$
z = (W_2 W_1)x + (W_2 b_1 + b_2) = W'x + b'\tag{3.1.2}
$$
其中$W' = W_2 W_1$是两个权重矩阵的乘积。这个结果表明，即使网络包含多个隐藏层，只要不使用激活函数，整个网络仍然等价于一个简单的线性变换。这个数学事实揭示了一个深刻原理：**线性变换的复合仍然是线性变换**。

定义3.1.1（线性映射）：映射$f: \mathbb{R}^n \to \mathbb{R}^m$是线性的，当且仅当对任意向量$x, y \in \mathbb{R}^n$和标量$\alpha, \beta \in \mathbb{R}$，满足：
$$
f(\alpha x + \beta y) = \alpha f(x) + \beta f(y)\tag{3.1.3}
$$
这个定义包含线性映射的两个核心性质：齐次性$f(\alpha x) = \alpha f(x)$和可加性$f(x + y) = f(x) + f(y)$。

线性模型虽然具有优美的数学性质，但其表达能力受到根本性限制。设真实数据生成过程涉及两个变量的交互效应，例如$y = x_1 x_2$，这是一个简单的乘法交互。线性模型只能学习形如$y = w_1 x_1 + w_2 x_2 + b$的函数，完全无法捕获$x_1$和$x_2$之间的乘积交互。这种局限性在处理现实世界的复杂数据时尤为突出，因为语言、图像等数据中充满了非线性模式。

从数学上分析，线性模型的学习能力可用函数空间的维度来刻画。设输入空间为$\mathbb{R}^d$，线性模型的函数空间是所有仿射函数的集合，其维度为$d + 1$（$d$个权重加1个偏置）。无论训练数据有多少，只要模型结构固定（线性），其能表示的函数就被限制在这个有限维的函数空间中。

激活函数的引入彻底改变了这一局面。设隐藏层的输出为$h = \sigma(W_1 x + b_1)$，其中$\sigma$是非线性激活函数。此时网络的输出为：
$$
y = W_2 h + b_2 = W_2 \sigma(W_1 x + b_1) + b_2\tag{3.1.4}
$$
由于$\sigma$是非线性函数，$W_2 \sigma(W_1 x + b_1)$不再能够简化为$W'x + b'$的形式。网络现在是分段线性或非线性的，其表达能力不再受限于线性变换的复合。

定义3.1.2（逐元素非线性变换）：设激活函数$\sigma: \mathbb{R} \to \mathbb{R}$逐元素应用于向量$h \in \mathbb{R}^C$，则输出向量为：
$$
\sigma(h) = (\sigma(h_1), \sigma(h_2), \ldots, \sigma(h_C))^T\tag{3.1.5}
$$
这种逐元素应用的方式保持了向量结构，同时对每个维度施加了非线性变换。

### 3.1.2 激活函数作为函数逼近的基石

神经网络之所以被称为"通用逼近器"，是因为在适当条件下，单层神经网络可以逼近任意连续函数。激活函数在这一逼近能力中扮演着核心角色。

神经网络通用逼近能力的核心定理表明，只要激活函数满足一定的数学条件，单隐藏层神经网络就可以以任意精度逼近任意连续函数。

定理3.1.1（通用逼近定理）：设$\sigma: \mathbb{R} \to \mathbb{R}$是任意非常数的有界连续函数，$K \subset \mathbb{R}^d$是紧集。对于任意连续函数$f: K \to \mathbb{R}$和任意$\epsilon > 0$，存在一个单隐藏层神经网络：
$$
g_N(x) = \sum_{i=1}^N w_i \sigma(a_i^T x + b_i)\tag{3.1.6}
$$
使得：
$$
\sup_{x \in K} |g_N(x) - f(x)| < \epsilon\tag{3.1.7}
$$
这个定理的数学意义极其深远：**只要激活函数是"非平凡"的（非常数），就具有通用逼近能力**。这意味着Sigmoid、Tanh、ReLU、GELU等激活函数在理论上都具有同等的表达能力。

通用逼近定理的核心数学洞见在于：**非线性激活函数的"非单调性"或"有界振荡性"使得函数族$\{\sigma(a^T x + b)\}$能够在输入空间中形成"局部基函数"**。每个神经元$\sigma(a_i^T x + b_i)$可被视为一个"局部探测器"——它在输入空间的某个区域激活，在其他区域抑制。

从几何角度理解，每个激活函数$\sigma(a^T x + b)$在输入空间中定义了一个"超平面" $a^T x + b = 0$。这个超平面将输入空间分为两个半空间：在一侧，激活函数的输出值较高；在另一侧，输出值较低。因此，每个神经元本质上是在输入空间中划分一个"决策边界"。

通用逼近定理表明，足够宽的单层网络可以逼近任意函数。然而，在实践中，我们通常使用较窄但较深的网络来达到相同的表达能力。

在高维输入空间中，若目标函数具有**组合（compositional）或层次化结构**，则深度神经网络在表示效率上可能显著优于浅层网络。已有理论结果表明，对于某些函数类，浅层网络需要指数级数量的神经元才能逼近，而深度网络仅需多项式规模。

从数学上分析，深度为$L$、宽度为$m$的全连接网络，参数数量为$O(L m^2)$。研究表明，深度为$L$的网络可以等效于宽度为指数级别的浅层网络，即$m_{\text{shallow}} \approx \exp(m_{\text{deep}})$。这就是著名的"深度指数优势"——深度网络用指数级更少的参数可以达到与浅层网络相同的表达能力。

定义3.1.3（层次化特征学习）：设$h^{(l)}$是第$l$层的表示向量，则：
$$
h^{(l)} = \sigma^{(l)}(W^{(l)} h^{(l-1)} + b^{(l)})
$$
每一层的变换和激活将前一层的表示转换为新的表示。理想的层次化学习应该满足：$h^{(l)}$捕获比$h^{(l-1)}$更抽象、更语义化的特征。

### 3.1.3 概率视角下的激活函数

从概率论的角度，许多激活函数可以解释为某种概率分布的参数变换。这种概率解释加深了对激活函数数学本质的理解。

Sigmoid函数$\sigma(x) = \frac{1}{1+e^{-x}}$将实数域映射到$(0,1)$区间，这正好是概率值的取值范围。在伯努利分布中，成功概率$p$的对数几率（Log-Odds）定义为$\log \frac{p}{1-p}$。Sigmoid函数的逆函数正是对数几率的变换：
$$
x = \log \frac{\sigma(x)}{1-\sigma(x)}\tag{3.1.8}
$$
将$p = \sigma(x)$代入对数几率的表达式可直接验证：
$$
\log \frac{p}{1-p} = \log \frac{\sigma(x)}{1-\sigma(x)} = \log e^x = x\tag{3.1.9}
$$
因此，Sigmoid函数可以将任意实数$x$解释为伯努利分布的成功概率$p = \sigma(x)$。

定义3.1.4（Logits与概率的变换）：设网络的原始输出为$z \in \mathbb{R}$（称为logit），则对应的概率为$p = \sigma(z)$。逆变换为$z = \log \frac{p}{1-p}$。Logits是"对数几率"，是无界的实数；概率是归一化后的值，限制在$(0,1)$区间内。

Softmax函数是Sigmoid在多类别情况下的推广，它将$C$维向量映射到概率单纯形$\Delta^{C-1} = \{p \in \mathbb{R}^C \mid p_i \geq 0, \sum_i p_i = 1\}$。

定义3.1.5（Softmax函数与概率单纯形）：对于输入向量$z = (z_1, z_2, \ldots, z_C)^T \in \mathbb{R}^C$，Softmax函数定义为：
$$
\text{Softmax}(z)_i = \frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)}\tag{3.1.10}
$$
Softmax的输出满足概率分布的所有公理：$\hat{p}_i > 0$对所有$i$成立，且$\sum_{i=1}^C \hat{p}_i = 1$。

从多项式分布的角度，Softmax输出的概率$\hat{p}_i$表示样本属于第$i$类的概率估计。多项式分布的似然函数为：
$$
P(y = i \mid x; \theta) = \hat{p}_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}\tag{3.1.11}
$$
对数似然为：
$$
\log P(y = i \mid x; \theta) = z_i - \log \sum_j \exp(z_j)\tag{3.1.12}
$$
这个表达式正是交叉熵损失的核心组成部分。

在需要从离散分布采样的场景中，Gumbel-Softmax提供了一种可微的采样近似。

定义3.1.6（Gumbel-Softmax分布）：设$z \in \mathbb{R}^C$是logits，Gumbel-Softmax样本定义为：
$$
y_i = \frac{\exp((z_i + g_i)/\tau)}{\sum_{j=1}^C \exp((z_j + g_j)/\tau)}\tag{3.1.13}
$$
其中$g_i = -\log(-\log u_i)$是Gumbel噪声，$u_i \sim \text{Uniform}(0,1)$是独立同分布的均匀随机变量，$\tau > 0$是温度参数。

Gumbel-Softmax的数学性质由温度参数$\tau$控制。当$\tau \to 0$时，Gumbel-Softmax趋向于one-hot采样（对应Hardmax）；当$\tau \to \infty$时，Gumbel-Softmax趋向于均匀分布。温度参数$\tau$提供了从"软"到"硬"的连续调节能力。

### 3.1.4 信息论视角与激活函数

从信息论的角度，激活函数可以被理解为对输入信息的某种非线性编码和变换。

许多激活函数（如Sigmoid、Tanh）将实数轴压缩到有限的区间$[0,1]$或$[-1,1]$。这种压缩本质上是信息的**有损编码**——输入的无限精度被截断为有限范围的输出。然而，这种有损压缩在机器学习中是有益的：它提供了数值稳定性，限制了输出值的范围。

定义3.1.7（互信息）：两个随机变量$X$和$Y$之间的互信息定义为：
$$
I(X; Y) = \mathbb{E}_{P_{X,Y}}\left[\log \frac{P(x,y)}{P(x)P(y)}\right] = H(X) - H(X \mid Y)\tag{3.1.14}
$$
互信息衡量知道$Y$后关于$X$的信息量，度量$X$和$Y$之间的依赖程度。

**ReLU的信息选择性**：ReLU函数$\sigma(x) = \max(0, x)$的行为类似于信息选择器——它保留正信息（$x > 0$），完全抑制负信息（$x < 0$）。从信息论角度，ReLU可被解释为一种"阈值化"操作：只有超过阈值的信号才能通过。

在自监督学习中，神经网络的目标通常是最大化输入不同部分之间的互信息。激活函数在这个过程中扮演关键角色，它们决定了信息如何被编码和传递。

定理3.1.2（互信息的下界估计）：InfoNCE损失提供了互信息的下界估计：
$$
I(x; c) \geq \log N - L_{\text{InfoNCE}}\tag{3.1.15}
$$
其中$N$是负样本的数量。这个不等式表明，最小化InfoNCE损失等价于最大化互信息的下界。


信息几何是研究概率分布空间几何性质的理论框架。在这个框架下，概率分布被视为一个流形，而各种信息量定义了流形上的几何结构。

定义3.1.8（Fisher信息度量）：在参数化的概率分布族$\{P_\theta\}$上，Fisher信息矩阵定义为：
$$
I(\theta)_{ij} = \mathbb{E}_{x \sim P_\theta} \left[ \frac{\partial \log P_\theta(x)}{\partial \theta_i} \frac{\partial \log P_\theta(x)}{\partial \theta_j} \right]\tag{3.1.16}
$$
Fisher信息矩阵定义了概率流形上的黎曼度量。对于Softmax激活函数，输出分布$\hat{p} = \text{Softmax}(z)$关于输入$z$的Fisher信息矩阵正好是Softmax雅可比矩阵与自身转置的乘积。

### 3.1.5 激活函数与Transformer架构的数学协同

激活函数在Transformer架构中的应用体现了深度学习组件之间数学协同的设计哲学。


Transformer中的前馈网络（Feed-Forward Network，FFN）定义为：
$$
\text{FFN}(x) = W_2 \sigma(W_1 x + b_1) + b_2\tag{3.1.17}
$$
其中$\sigma$是激活函数。现代Transformer普遍选择GELU作为$\sigma$。GELU的定义$\text{GELU}(x) = x \cdot \Phi(x)$表明它是输入$x$与其通过标准正态累积分布函数$\Phi(x)$权重的乘积。这种定义使得GELU具有概率解释：它可被理解为"以概率$\Phi(x)$通过$x$"。

从梯度角度分析，GELU的导数$\Phi(x) + x \phi(x)$在整个实数轴上都是正的，这意味着GELU不会像ReLU那样产生"死神经元"问题。

Transformer的自注意力机制本身不包含显式的激活函数，但Softmax操作本质上是另一种形式的非线性激活。注意力权重的计算$A = \text{Softmax}(QK^T/\sqrt{d_k})$将Query-Key相似度分数转换为概率分布。

定义3.1.9（缩放点积注意力）：Scaled Dot-Product Attention定义为：
$$
\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\tag{3.1.18}
$$
这里，$\frac{QK^T}{\sqrt{d_k}}$计算Query和Key之间的相似度矩阵，Softmax将每一行归一化为概率分布，最后用这个分布对Value进行加权平均。注意力机制中的Softmax和FFN中的GELU形成了"双重非线性"结构。

位置编码为序列中的每个位置分配数学表示。绝对位置编码（如正弦余弦编码）定义为：
$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)\tag{3.1.19}
$$
这种编码将位置信息嵌入到固定范围的正弦和余弦函数中。从信息论角度，位置编码的频率分解意味着不同维度携带不同尺度的位置信息。

定义3.1.10（位置感知的特征表示）：设$PE$是位置编码矩阵，$E$是Token嵌入矩阵，则位置感知的嵌入表示为：
$$
h = \text{GELU}(W_{pe} PE + W_e E)\tag{3.1.20}
$$
GELU的非线性变换将这种位置感知的嵌入转换为新的表示，保留了位置信息，同时通过非线性组合创造了位置-Token的交互特征。

### 3.1.6 本节小结

本节从多个数学视角系统分析了激活函数的角色。从线性映射的局限性出发，揭示了非线性激活函数打破表达瓶颈的数学必然性。从函数逼近论的角度，论证了激活函数作为通用逼近器核心组件的数学基础。从概率论的角度，分析了Sigmoid、Softmax等激活函数与概率分布的内在联系。从信息论的角度，探讨了激活函数的信息压缩、选择和互信息最大化作用。最后，分析了激活函数与Transformer架构（注意力机制、位置编码、前馈网络）的数学协同关系。