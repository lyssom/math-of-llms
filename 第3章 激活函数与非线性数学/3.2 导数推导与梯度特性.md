激活函数的导数性质是理解神经网络训练动力学的核心数学基础。在反向传播算法中，损失函数关于网络参数的梯度需要通过激活函数的导数逐层传递。激活函数的导数结构直接决定了梯度如何在不同层之间传播，进而影响模型的收敛速度、稳定性和最终性能。本节将从数学的角度系统性地推导各类激活函数的导数公式，分析其几何意义和梯度特性，并深入探讨激活函数导数对神经网络训练的影响。这些数学分析为理解大语言模型的训练过程提供了必要的理论基础，也为分析和改进现有优化算法提供了理论工具。

## 3.2.1 Sigmoid函数的导数与性质

Sigmoid函数是神经网络中最经典的激活函数之一，它将实数映射到 $(0,1)$ 区间，天然具有概率解释。虽然在大规模语言模型中Sigmoid已被ReLU及其变体取代，但它在理解激活函数导数性质方面具有重要的教学价值，其导数的"自化"形式在数学上非常优美，也使得Sigmoid在某些特定场景下仍有应用价值。

### Sigmoid函数的数学定义

**定义3.2.1（Sigmoid函数）** Sigmoid函数（也称为逻辑函数）定义为：
$$
\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}\tag{3.2.1}​
$$
其定义域为$\mathbb{R}$，值域为$(0,1)$。Sigmoid函数是单调递增函数，其图像呈标准的S形曲线，在$x = 0$处有拐点，$\sigma(0) = 0.5$。从几何上看，Sigmoid函数将整个实数轴压缩到$(0,1)$区间，将输入的尺度归一化，同时保留了输入的相对顺序信息。

Sigmoid函数的反函数为对数几率函数（Logit函数），定义为：
$$
\sigma^{-1}(p) = \log \frac{p}{1-p}, \quad p \in (0,1)\tag{3.2.2}
$$
这个反函数在逻辑回归中具有重要意义，它将概率值转换回实数域（logits空间），使得参数估计可以在无界的实数空间中进行。

从极限的角度分析Sigmoid函数的边界行为：当$x \to +\infty$时，$e^{-x} \to 0$，因此$\sigma(x) \to 1$；当 $x \to -\infty$ 时，$e^{-x} \to +\infty$，因此$\sigma(x) \to 0$。这种渐近行为表明Sigmoid函数在正无穷处趋向于饱和值1，在负无穷处趋向于饱和值0。

### 导数推导与自化性质

Sigmoid函数的导数具有一个极为优美的性质：**导数可以用函数自身来表示**，这一性质被称为"自化"（Self-Derivative Property）。这种数学上的简洁性不仅在理论上令人赞叹，在实际计算中也具有重要的应用价值——在反向传播中，我们不需要存储或重新计算导数，只需要知道当前的$\sigma(x)$值即可。

**定理3.2.1（Sigmoid导数的自化形式）** Sigmoid函数$\sigma(x) = \frac{1}{1+e^{-x}}$的导数为：
$$
\sigma'(x) = \sigma(x)(1 - \sigma(x))\tag{3.2.3}
$$
**证明**：将Sigmoid函数重写为$\sigma(x) = (1 + e^{-x})^{-1}$，使用链式法则求导：
$$
\begin{align*}
\frac{d\sigma}{dx} &= -1 \cdot (1 + e^{-x})^{-2} \cdot (-e^{-x}) \\ &= \frac{e^{-x}}{(1 + e^{-x})^2} \end{align*}\tag{3.2.4}
$$
利用$\sigma(x) = \frac{1}{1+e^{-x}}$​，我们可以将导数表示为：
$$
\frac{e^{-x}}{(1 + e^{-x})^2} = \frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1+e^{-x}} = \sigma(x) \cdot \left(1 - \frac{1}{1+e^{-x}}\right) = \sigma(x)(1 - \sigma(x))\tag{3.2.5}
$$
证毕。

这个证明揭示了Sigmoid导数自化性质的数学根源：Sigmoid函数的分母是分子的积分（从某种意义上），这种积分-微分关系导致了导数的简洁表示。从几何角度看，$\sigma(x)(1-\sigma(x))$ 表示在Sigmoid曲线上各点的斜率，斜率的最大值出现在$x = 0$处，此时$\sigma(0) = 0.5$，$\sigma'(0) = 0.25$。

### 高阶导数与泰勒展开

Sigmoid函数的高阶导数可以递归地通过低阶导数表示，这种递推关系揭示了Sigmoid非线性变换的深层数学结构。

**定理3.2.2（Sigmoid的二阶导数）** Sigmoid函数的二阶导数为：
$$
\sigma''(x) = \sigma(x)(1 - \sigma(x))(1 - 2\sigma(x))\tag{3.2.6}
$$
**证明**：使用乘积法则对$\sigma'(x) = \sigma(x)(1-\sigma(x))$求导：
$$
\begin{align*}
\frac{d^2\sigma}{dx^2} &= \frac{d}{dx}[\sigma(x)(1-\sigma(x))] \\ &= \sigma'(x)(1-\sigma(x)) + \sigma(x)(-\sigma'(x)) \\ &= \sigma'(x) - 2\sigma(x)\sigma'(x) \\ &= \sigma(x)(1-\sigma(x)) - 2\sigma(x)^2(1-\sigma(x)) \\ &= \sigma(x)(1-\sigma(x))(1 - 2\sigma(x)) \end{align*}\tag{3.2.7}
$$
证毕。

二阶导数的几何意义表示Sigmoid曲线的曲率变化。在$x=0$处，$\sigma''(0) = 0.5 \times 0.5 \times (1 - 1) = 0$，这表明拐点处的曲率为零，曲线在该点由上凸转为下凸。
当$x < 0$时，$1 - 2\sigma(x) > 0$，二阶导数为正，曲线下凸；
当$x>0$时，$1 - 2\sigma(x) < 0$，二阶导数为负，曲线上凸。

**三阶导数**：类似地，可以推导出Sigmoid的三阶导数：
$$
\sigma'''(x) = \sigma(x)(1-\sigma(x))(1 - 6\sigma(x)(1-\sigma(x)))\tag{3.2.8}
$$
高阶导数的复杂结构反映了Sigmoid非线性变换的深层特性。随着阶数增加，导数表达式变得越来越复杂，但它们都保持着用$\sigma(x)$ 和$(1-\sigma(x))$表示的形式，这体现了Sigmoid函数的代数闭合性。

### 几何解释与梯度特性

从几何角度分析，$\sigma'(x) = \sigma(x)(1-\sigma(x))$表示Sigmoid曲线在各点的斜率。在 $x \to -\infty$时，$\sigma(x) \to 0$，$\sigma'(x) \to 0$，曲线趋于水平；在$x \to +\infty$ 时，$\sigma(x) \to 1$，$\sigma'(x) \to 0$，曲线也趋于水平。在$x = 0$处，$\sigma(0) = 0.5$，$\sigma'(0) = 0.25$，这是曲线的"最陡点"。Sigmoid导数的这种"钟形"结构意味着激活值在0.5附近时变化最快，而在极端值（接近0或1）时变化最慢。

**定义3.2.2（梯度饱和）** 对于激活函数$\sigma(x)$，如果存在输入区域使得$\sigma'(x) \approx 0$，则在该区域输入的微小变化不会引起输出的显著变化，这种现象称为梯度饱和。

Sigmoid函数的导数取值范围是$(0, 0.25]$，最大值在$x = 0$处取得。当输入$x$的绝对值很大时，$\sigma'(x)$非常接近 0 。具体而言，当$|x| \geq 4$ 时，$\sigma'(x) \leq 0.018$；当$|x| \geq 6$时，$\sigma'(x) \leq 0.002$。这意味着，如果神经元的输入（加权求和结果）落在饱和区域，其激活值对输入的微小变化不敏感，梯度几乎为零。

**与梯度消失问题的联系**：在深层网络中，这种饱和效应会逐层累积放大。考虑一个$L$层的前馈网络，每层的激活输出为$h^{(l)} = \sigma(W^{(l)} h^{(l-1)} + b^{(l)})$。在反向传播中，梯度从输出层向输入层传播时，如果每一层的激活都处于饱和状态（即$\sigma'(W^{(l)} h^{(l-1)} + b^{(l)})$ 接近 0），则梯度会按指数级衰减。设平均每层的梯度衰减因子为$\lambda < 1$，经过$L$层后，梯度衰减为原来的$\lambda^L$ 。当$L=100$且$\lambda = 0.5$时，梯度衰减为$2^{-100} \approx 10^{-30}$，在数值上完全不可检测，这就  是所谓的"梯度消失"问题。

## 3.2.2 Tanh函数的导数与性质

Tanh函数（双曲正切函数）是另一个经典的激活函数，它将实数映射到$(−1,1)$区间。Tanh与Sigmoid有密切的数学关系——Tanh可以视为Sigmoid的缩放和平移版本。Tanh的零中心性质（输出均值为0）使得它在实践中通常比Sigmoid表现更好，减少了梯度偏移问题，加速了训练收敛。

### Tanh函数的数学定义与性质

**定义3.2.3（Tanh函数）** Tanh函数（双曲正切函数）定义为：
$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\tag{3.2.9}​
$$
其定义域为$\mathbb{R}$，值域为$(-1, 1)$。Tanh函数是奇函数，满足$\tanh(-x) = -\tanh(x)$，在$x=0$处有$\tanh(0) = 0$。与Sigmoid相比，Tanh的输出以0为中心，这带来了两个重要的数学优势。

首先，零中心输出减少了梯度偏移问题。在Sigmoid中，输出总是正的，这意味着梯度总是非负的（在反向传播中，$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial h} \cdot \frac{\partial h}{\partial w}$，如果$\frac{\partial L}{\partial h}$为正，则$\frac{\partial L}{\partial w}$只能为正或零）。这种单向梯度可能导致优化过程的"zig-zag"行为，浪费计算资源。Tanh的零中心输出消除了这个问题，梯度可以正可以负，优化更加灵活高效。

其次，零中心输出使得各层的输入分布更加稳定，减少了协变量偏移问题。当上一层输出的均值不为零时，下一层的输入分布会发生偏移，可能导致训练不稳定。Tanh的零中心性质缓解了这个问题。

### 与Sigmoid的数学关系

Tanh和Sigmoid存在直接的数学关系，通过这个关系可以清晰地看到Tanh如何从Sigmoid演化而来。

**定理3.2.3（Tanh与Sigmoid的关系）** Tanh函数可以表示为Sigmoid函数的缩放和平移：
$$
\tanh(x) = 2\sigma(2x) - 1\tag{3.2.10}
$$
**证明**：设$y = \tanh(x)$，则：
$$
\begin{align*}
2\sigma(2x) - 1 &= 2 \cdot \frac{1}{1+e^{-2x}} - 1 \\ &= \frac{2}{1+e^{-2x}} - \frac{1+e^{-2x}}{1+e^{-2x}} \\ &= \frac{2 - 1 - e^{-2x}}{1+e^{-2x}} \\ &= \frac{1 - e^{-2x}}{1 + e^{-2x}} \\ &= \frac{e^x - e^{-x}}{e^x + e^{-x}} = \tanh(x) \end{align*}\tag{3.2.11}
$$
证毕。

这个关系表明，Tanh本质上是Sigmoid的"零中心版本"。Sigmoid输出$(0,1)$区间，Tanh输出$(−1,1)$区间；Sigmoid以0.5为中心，Tanh以0为中心。从变换的角度，Tanh先对输入进行2倍缩放，应用Sigmoid，然后将输出平移和缩移到$(−1,1)$区间。

### 导数推导与性质分析

**定理3.2.4（Tanh的导数）** Tanh函数$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$的导数为：
$$
\frac{d}{dx}\tanh(x) = 1 - \tanh^2(x)\tag{3.2.12}
$$
**证明**：使用商的求导法则对$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$求导：
$$
\begin{align*}
\frac{d}{dx}\tanh(x) &= \frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2} \\ &= \frac{(e^x + e^{-x})^2 - (e^x - e^{-x})^2}{(e^x + e^{-x})^2} \end{align*}\tag{3.2.13}
$$
利用平方差公式$a^2 - b^2 = (a-b)(a+b)$：
$$
\begin{align*}
(e^x + e^{-x})^2 - (e^x - e^{-x})^2 &= [(e^x + e^{-x}) - (e^x - e^{-x})][(e^x + e^{-x}) + (e^x - e^{-x})] \\ &= (2e^{-x})(2e^x) \\ &= 4\end{align*}\tag{3.2.14}
$$
因此：
$$
\frac{d}{dx}\tanh(x) = \frac{4}{(e^x + e^{-x})^2} = 4 \cdot \frac{e^x e^{-x}}{(e^x + e^{-x})^2} = 4 \cdot \frac{1}{(e^{x/2} + e^{-x/2})^2} = \text{sech}^2(x)\tag{3.2.15}
$$
其中$\text{sech}(x) = \frac{2}{e^x + e^{-x}}$是双曲正割函数。使用$\tanh(x)$自身表示导数：
$$
\frac{d}{dx}\tanh(x) = 1 - \tanh^2(x)\tag{3.2.16}
$$
证毕。

**与Sigmoid导数的比较**：Tanh导数 $\tanh'(x) = 1 - \tanh^2(x)$与Sigmoid导数$\sigma'(x) = \sigma(x)(1-\sigma(x))$在形式上非常相似。两者都是函数值与其"补"的乘积：对于Sigmoid，"补"是$1-\sigma(x)$；对于Tanh，"补"是$1-\tanh^2(x)$。这种相似性反映了两个函数在数学结构上的内在联系。

**Tanh导数的取值范围**：Tanh导数的取值范围是$(0, 1]$，最大值在$x=0$处取得，此时$\tanh'(0) = 1$ 。与Sigmoid相比，Tanh的导数峰值更大（1 vs 0.25），这意味着在$x = 0$附近，Tanh的梯度"更强"。然而，当$|x|$增大时，Tanh导数同样会迅速衰减至 0——当$|x| \geq 3$时，$\tanh'(x) \leq 0.01$ 。这种快速饱和特性仍然是Tanh的主要局限性。

**与Sigmoid的梯度比较**：设输入的绝对值较大时，Sigmoid导数$\sigma'(x) \approx 0$，Tanh导数$\tanh'(x) \approx 0$，两者都存在梯度饱和问题。但由于Tanh的导数峰值是1而Sigmoid只有0.25，Tanh在激活值接近零的区域（未饱和区域）有更强的梯度信号，这使得Tanh在训练初期通常比Sigmoid收敛更快。然而，当网络较深时，Tanh的饱和问题依然严重，导致深层网络的训练困难。

## 3.2.3 ReLU函数族的导数与性质

ReLU（Rectified Linear Unit，修正线性单元）是目前深度学习中最广泛使用的激活函数。ReLU的数学形式极其简单：$\text{ReLU}(x) = \max(0, x)$，正是这种简单性带来了卓越的计算效率和优异的性能。ReLU及其变体（包括Leaky ReLU、ELU、GELU等）构成了现代神经网络激活函数的主流选择，在Transformer架构中扮演着关键角色。

### ReLU函数的定义与导数

**定义3.2.4（ReLU函数）** ReLU函数定义为：
$$
\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}\tag{3.2.17}
$$​其定义域为$\mathbb{R}$，值域为$[0, +\infty)$。ReLU是分段线性函数，在$x = 0$处不可导。从几何上看，ReLU将负半轴的输入映射为 0，将正半轴的输入保持不变，形成了一个"折线"形状的激活函数。

**导数推导**：ReLU的导数是一个简单的阶梯函数：
$$
\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \end{cases}\tag{3.2.18}​
$$
在$x=0$处，ReLU不可导，因为左导数为 0，右导数为 1。但在实践中，通常使用次梯度（Subgradient）来处理$x = 0$的情况。常用的约定是取次梯度集合$[0, 1]$中的任意值，常见选择是 0 或 0.5 或 1。不同的选择可能导致不同的优化行为，但在大规模训练中，这种差异通常可以忽略。

**定义3.2.5（次梯度）** 对于凸函数$f$，在点$x$处的次梯度$g$满足：
$$
f(y) \geq f(x) + g^T (y - x), \quad \forall y\tag{3.2.19}
$$
对于ReLU函数，在$x=0$处的次梯度集合为$[0, 1]$中的任意值。选择$g = 0$意味着将$x = 0$视为"负区域"，选择$g = 1$意味着将其视为"正区域"，选择$g = 0.5$则是一个折中。在实际实现中，不同的深度学习框架可能有不同的默认选择。

### 稀疏激活特性与信息选择

ReLU最重要的特性之一是稀疏激活。对于任意输入，约有一半的神经元输出为 0（因为输入的分布通常关于 0 对称）。这种稀疏性带来了几个重要的数学优势。

**定义3.2.6（稀疏表示）** 稀疏表示是指在给定的表示中，大部分元素为零或接近零的性质。ReLU的输出向量$h = \text{ReLU}(z)$中，如果$z$的各元素独立同分布于均值为0的分布，则约有一半的元素为负，经过ReLU后变为0。

稀疏激活的第一个优势是**自动特征选择**。只有"重要"的特征（激活值大于0）被保留，不重要的特征被抑制。这种选择机制使得网络能够自动学习哪些特征对任务有用，而不需要额外的正则化。第二个优势是**计算效率提升**。零值神经元不需要参与后续计算，可以被跳过。在大规模神经网络中，这种稀疏计算可以显著减少计算量和内存使用。第三个优势是**正则化效应**。稀疏性提供了某种形式的隐式正则化，有助于防止过拟合。直觉上，如果一个神经元对任务没有贡献，它会逐渐"死亡"（输出恒为0），而网络会使用其他神经元来补偿。

然而，稀疏激活也带来了"死神经元"问题。当神经元的输入（加权求和结果）持续为负时，ReLU的输出恒为0，梯度也恒为0，该神经元将永远不会被激活，无法学习任何东西。这就是所谓的"dying ReLU"问题。

### Leaky ReLU与负区域处理

Leaky ReLU是ReLU的重要变体，旨在解决ReLU的"神经元死亡"问题。通过在负输入区域保留小的斜率，Leaky ReLU确保即使对于负输入，神经元也能学习。

**定义3.2.7（Leaky ReLU）** Leaky ReLU定义为：
$$
\text{LeakyReLU}(x) = \max(\alpha x, x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}\tag{3.2.20}​
$$
其中$\alpha \in (0, 1)$是一个小的正常数（通常取 0.01 或 0.02）。Leaky ReLU的导数为：
$$
\text{LeakyReLU}(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha & \text{if } x < 0 \end{cases}\tag{3.2.21}​
$$
在$x=0$处的次梯度集合为$[\alpha, 1]$。

**参数化ReLU（PReLU）**：将$\alpha$作为可学习参数而非固定值，得到参数化ReLU：
$$
\text{PReLU}(x) = \max(0, x) + \alpha \min(0, x)\tag{3.2.22}
$$
其中$\alpha$通过反向传播自动学习。PReLU的灵活性使其能够适应不同的数据分布，在某些任务上可能获得更好的性能。

**Leaky ReLU的数学优势**：Leaky ReLU在负输入区域保留了小的梯度$\alpha$，确保即使对于持续为负的输入，神经元也能通过小的梯度更新逐渐改变其权重，最终可能使输入变为正。这种"复活"机制有效解决了死神经元问题。同时，较小的斜率$\alpha$保持了ReLU的大部分稀疏特性——只有输入非常接近零或为负时，神经元才会被轻微激活。

### ELU与指数线性单元

ELU（Exponential Linear Unit）是另一种处理负输入的策略，它使用指数函数来平滑负区域，提供了比Leaky ReLU更平滑的过渡。

**定义3.2.8（ELU函数）** ELU函数定义为：
$$
\text{ELU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \leq 0 \end{cases}\tag{3.2.23}​
$$
其中$\alpha > 0$是缩放参数（通常取 1）。ELU的导数为：
$$
\text{ELU}(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha e^x & \text{if } x \leq 0 \end{cases}\tag{3.2.24}
$$
**ELU的数学性质**：ELU在正区域与ReLU完全相同，保持了ReLU的稀疏激活和计算效率。在负区域，ELU使用指数函数$e^x - 1$进行平滑过渡，这带来了几个优势。首先，当$x \to -\infty$时，$\alpha(e^x - 1) \to -\alpha$，导数$\alpha e^x \to 0$，这意味着负区域的输出有界，不会趋向于$-\infty$。其次，ELU在负区域是光滑的（处处可导），这可能有利于基于二阶导数的优化方法。第三，ELU的负输出可能有助于将下一层的输入分布推向零均值，可能改善训练动态。

然而，ELU的计算成本略高于ReLU，因为需要计算指数函数。在大规模应用中，这种计算开销可能变得显著。

### GELU与Transformer的标准选择

GELU（Gaussian Error Linear Unit）是近年来在自然语言处理中广泛使用的激活函数，尤其在BERT和GPT等模型中。GELU的设计融合了概率论的直觉，成为Transformer架构的标准激活函数选择。

**定义3.2.9（GELU函数）** GELU定义为：
$$
\text{GELU}(x) = x \cdot \Phi(x) = \frac{x}{2} \left[ 1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right) \right]\tag{3.2.25}
$$
其中$\Phi(x)$是标准正态分布的累积分布函数，$\text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt$是误差函数。GELU的设计理念是：**以概率$\Phi(x)$通过输入$x$**，这种概率解释与Dropout的随机性有内在联系。

**GELU的导数**：使用乘积法则求导：
$$
\frac{d}{dx}\text{GELU}(x) = \Phi(x) + x \cdot \phi(x)\tag{3.2.26}
$$
其中$\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$ 是标准正态分布的概率密度函数。

**GELU导数的性质**：GELU的导数在整个实数轴上都是正的，这意味着GELU永远不会产生"死神经元"问题。同时，当$x \to +\infty$时，$\Phi(x) \to 1$，$x \cdot \phi(x) \to 0$，导数趋向于 1；当$x \to -\infty$时，$\Phi(x) \to 0$，$x \cdot \phi(x) \to 0$，导数也趋向于 0。因此，GELU的导数始终在$(0,1)$ 区间内，避免了梯度爆炸问题。

**与Softmax的概率一致性**：GELU的$\Phi(x)$形式使其与Softmax的概率输出具有数学一致性。在Transformer中，注意力权重是概率分布（Softmax输出），GELU可以平滑地处理这些概率信息，同时引入非线性变换。这种一致性可能有助于梯度在注意力层和前馈层之间的稳定传播。

**GELU的近似计算**：由于误差函数$\text{erf}(x)$没有简单的闭式表达式，实际实现中通常使用近似公式。一个常用近似是：
$$
\text{GELU}(x) \approx 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)\tag{3.2.27}
$$
另一个更简单的近似是$\text{GELU}(x) \approx x \cdot \sigma(1.702x)$，其中$\sigma$是Sigmoid函数。这些近似在精度和计算效率之间提供了良好的权衡。

## 3.2.4 Softmax的导数矩阵分析

Softmax函数是分类任务和多标签预测的核心组件，它将任意实数向量映射到概率单纯形。与单变量激活函数不同，Softmax操作的是一个向量，其输出也是一个向量。这种多输入多输出的特性使得Softmax的导数具有矩阵结构，理解这个导数矩阵对于实现高效的梯度计算至关重要。Softmax的导数分析也是理解注意力机制梯度的数学基础。

### Softmax函数的数学定义

**定义3.2.10（Softmax函数）** 对于输入向量$z = (z_1, z_2, \ldots, z_C)^T \in \mathbb{R}^C$，Softmax函数定义为：
$$
\hat{p}_i = \text{Softmax}(z)_i = \frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)}, \quad i = 1, 2, \ldots, C\tag{3.2.28}
$$
Softmax的输出$\hat{p} = (\hat{p}_1, \ldots, \hat{p}_C)^T$是概率单纯形$\Delta^{C-1}$中的点，满足$\sum_i \hat{p}_i = 1$且$\hat{p}_i > 0$。概率单纯形是$\mathbb{R}^C$中由坐标轴截距为 1 的超平面与第一卦限相交形成的$(C−1)$维流形。

**Softmax的概率解释**：Softmax的输出可以解释为多项式分布的概率参数。在分类任务中，$\hat{p}_i$表示样本属于第$i$类的概率估计。Softmax将网络的原始输出（logits）转换为有效的概率分布，使得分类结果的解释更加直观。

**与Sigmoid的关系**：当$C = 2$时，Softmax退化为：
$$
\hat{p}_1 = \frac{\exp(z_1)}{\exp(z_1) + \exp(z_2)}, \quad \hat{p}_2 = \frac{\exp(z_2)}{\exp(z_1) + \exp(z_2)}\tag{3.2.29}​
$$
设$z_2 = 0$并用$z$表示$z_1$​，则$\hat{p}_1 = \frac{e^z}{e^z + 1} = \sigma(z)$，这正是Sigmoid函数。因此，Softmax是Sigmoid在多类别情况下的推广。

### 雅可比矩阵的推导

Softmax的导数可以表示为一个$C \times C$的雅可比矩阵$J$，其中$J_{ij} = \frac{\partial \hat{p}_i}{\partial z_j}$​​。由于Softmax的输出是概率分布，各元素之和为1，雅可比矩阵具有特殊的结构。

**定理3.2.5（Softmax雅可比矩阵）** Softmax函数关于输入$z$的雅可比矩阵为：
$$
J_{ij} = \frac{\partial \hat{p}_i}{\partial z_j} = \begin{cases} \hat{p}_i (1 - \hat{p}_i) & \text{if } i = j \\ -\hat{p}_i \hat{p}_j & \text{if } i \neq j \end{cases}\tag{3.2.30}​
$$
**证明**：分别考虑对角元素和非对角元素。

当$i = j$时：
$$
\begin{align*}
\frac{\partial \hat{p}_i}{\partial z_i} &= \frac{\partial}{\partial z_i} \left( \frac{\exp(z_i)}{\sum_k \exp(z_k)} \right) \\ &= \frac{\exp(z_i) \sum_k \exp(z_k) - \exp(z_i) \exp(z_i)}{(\sum_k \exp(z_k))^2} \\ &= \frac{\exp(z_i)}{\sum_k \exp(z_k)} \left( 1 - \frac{\exp(z_i)}{\sum_k \exp(z_k)} \right) \\ &= \hat{p}_i (1 - \hat{p}_i) \end{align*}\tag{3.2.31}
$$
当$i \neq j$ 时：
$$
\begin{align*}
\frac{\partial \hat{p}_i}{\partial z_j} &= \frac{\partial}{\partial z_j} \left( \frac{\exp(z_i)}{\sum_k \exp(z_k)} \right) \\ &= \exp(z_i) \cdot \left( -\frac{\exp(z_j)}{(\sum_k \exp(z_k))^2} \right) \\ &= -\frac{\exp(z_i)}{\sum_k \exp(z_k)} \cdot \frac{\exp(z_j)}{\sum_k \exp(z_k)} \\ &= -\hat{p}_i \hat{p}_j \end{align*}\tag{3.2.32}
$$
证毕。

**定义3.2.11（Softmax雅可比矩阵的矩阵形式）** Softmax的雅可比矩阵可以简洁地表示为：
$$
J_{\text{Softmax}}(z) = \text{diag}(\hat{p}) - \hat{p} \hat{p}^T\tag{3.2.33}
$$

其中$\text{diag}(\hat{p})$是以$\hat{p}$ 的元素为对角元素的对角矩阵，$\hat{p} \hat{p}^T$是秩为 1 的外积矩阵。

### 雅可比矩阵的性质分析

Softmax雅可比矩阵具有几个重要的数学性质，这些性质对于理解梯度传播和优化动态至关重要。

**性质3.2.1（行和为零）** 雅可比矩阵的每行和为零：
$$
\sum_{j=1}^C J_{ij} = \hat{p}_i (1 - \hat{p}_i) + \sum_{j \neq i} (-\hat{p}_i \hat{p}_j) = \hat{p}_i - \hat{p}_i \sum_{j=1}^C \hat{p}_j = \hat{p}_i - \hat{p}_i \cdot 1 = 0\tag{3.2.34}
$$
这个性质反映了概率分布的归一化约束$\sum_i \hat{p}_i = 1$。由于输出元素的和为常数1，输出相对于输入的变化必然满足某种守恒关系。

**性质3.2.2（半负定性）** 雅可比矩阵是半负定的：对于任意向量$v \in \mathbb{R}^C$，有$v^T J v \leq 0$。

**证明**：
$$
\begin{align*}
v^T J v &= v^T (\text{diag}(\hat{p}) - \hat{p} \hat{p}^T) v \\ &= \sum_{i=1}^C \hat{p}_i v_i^2 - \left(\sum_{i=1}^C \hat{p}_i v_i\right)^2 \\ &= \mathbb{E}_{\hat{p}}[v^2] - (\mathbb{E}_{\hat{p}}[v])^2 \\ &= \text{Var}_{\hat{p}}(v) \geq 0 \end{align*}\tag{3.2.35}
$$
实际上，$v^T (-J) v = \text{Var}_{\hat{p}}(v) \geq 0$，因此$J$是半负定的。证毕。

半负定性意味着在Softmax输出的概率分布下，$v$的方差非负。这种方差结构反映了概率分布的分散程度——当$\hat{p}$接近均匀分布时，方差较大；当$\hat{p}$接近one-hot分布时，方差较小。

**性质3.2.3（谱结构）** 雅可比矩阵的特征值包括：$\lambda_1 = -1$（对应特征向量$\mathbf{1} = (1, 1, \ldots, 1)^T$的方向），以及$\lambda_i = \hat{p}_i$（对应$C−1$个正特征值）。

**与第五章注意力的联系**：Softmax雅可比矩阵的结构直接决定了注意力机制的梯度计算。在Scaled Dot-Product Attention中，注意力权重矩阵$A = \text{Softmax}(QK^T/\sqrt{d_k})$的计算涉及Softmax操作，其关于$Q$和$K$的梯度结构与上述分析高度相似。特别地，$\frac{\partial A}{\partial Q}$的形式包含类似$(\text{diag}(A) - AA^T)$的项，这正是Fisher信息矩阵的结构。这种数学上的相似性意味着，在实现注意力机制的梯度计算时，可以借鉴Softmax梯度计算的优化策略。

### 与交叉熵梯度的联合分析

在分类任务中，Softmax通常与交叉熵损失结合使用。Softmax的雅可比矩阵与交叉熵梯度的联合作用产生了一个极为简洁的梯度表达式。

**定义3.2.12（交叉熵损失）** 设损失函数为$L = -\sum_i y_i \log \hat{p}_i$​，其中$y$是one-hot真实标签（$y_i = 1$ 当且仅当$i = c^*$，真实类别），$\hat{p} = \text{Softmax}(z)$。

**定理3.2.6（Softmax-CrossEntropy的梯度）** 交叉熵损失关于$z_k$的梯度为：
$$
\frac{\partial L}{\partial z_k} = \hat{p}_k - y_k\tag{3.2.36}​
$$
**证明**：损失关于$\hat{p}_i$的梯度为$\frac{\partial L}{\partial \hat{p}_i} = -\frac{y_i}{\hat{p}_i}$​​。应用链式法则：
$$
\begin{align*}
\frac{\partial L}{\partial z_k} &= \sum_i \frac{\partial L}{\partial \hat{p}_i} \frac{\partial \hat{p}_i}{\partial z_k} \\ &= \sum_i \left( -\frac{y_i}{\hat{p}_i} \right) J_{ik} \\ &= -\frac{y_k}{\hat{p}_k} \hat{p}_k (1 - \hat{p}_k) - \sum_{i \neq k} \frac{y_i}{\hat{p}_i} (-\hat{p}_i \hat{p}_k) \\ &= -y_k (1 - \hat{p}_k) + \hat{p}_k \sum_{i \neq k} y_i \\ &= -y_k + y_k \hat{p}_k + \hat{p}_k (1 - y_k) \\ &= \hat{p}_k - y_k \end{align*}\tag{3.2.37}
$$
证毕。

这个简洁的结果$\frac{\partial L}{\partial z} = \hat{p} - y$是深度学习中最重要的梯度公式之一。它表明，在Softmax和交叉熵的组合中，梯度就是预测概率与真实标签的差值。这种简洁性源于Softmax雅可比矩阵的特殊结构——非对角元素的负号与交叉熵的负对数形式完美抵消。

**与第四章损失函数的联系**：这个梯度公式是第四章详细讨论的Softmax-CrossEntropy梯度计算的核心。交叉熵损失$-\sum_i p_i \log \hat{p}_i$衡量的是两个概率分布（真实分布$p$和预测分布$\hat{p}$）之间的差异，Softmax的雅可比矩阵与交叉熵梯度的联合作用产生了一个简洁而优雅的结果。这种数学上的简洁性使得分类任务的训练非常高效，也为理解注意力机制中的类似结构提供了理论基础。

## 3.2.5 激活函数导数的矩阵形式

在深度神经网络中，激活函数通常作用于向量的每个元素，即逐元素应用。然而，在理解梯度传播时，考虑激活函数的矩阵形式更有助于分析其对信息流的影响。激活函数的导数矩阵决定了梯度如何在不同维度之间传递，进而影响网络的训练动态和表示学习。

### 逐元素激活的雅可比矩阵

**定义3.2.13（逐元素激活的雅可比矩阵）** 设激活函数$\sigma: \mathbb{R} \to \mathbb{R}$逐元素应用于向量$h \in \mathbb{R}^C$，即$\sigma(h) = (\sigma(h_1), \sigma(h_2), \ldots, \sigma(h_C))^T$。则激活函数关于$h$的雅可比矩阵为：
$$
J_\sigma(h) = \frac{\partial \sigma(h)}{\partial h} = \text{diag}(\sigma'(h))\tag{3.2.38}
$$
其中$\sigma'(h) = (\sigma'(h_1), \sigma'(h_2), \ldots, \sigma'(h_C))^T$是逐元素导数向量。

这个雅可比矩阵是对角矩阵，意味着每个输出元素$\sigma(h_i)$只依赖于对应的输入元素$h_i$，各元素之间是解耦的。这种解耦性是逐元素激活函数的典型特征，也是它们在实践中易于实现和计算的原因。

**与线性变换的联合雅可比矩阵**：在神经网络中，激活函数通常与线性变换组合使用。设$z = Wh + b$，然后$\hat{h} = \sigma(z)$。则从$h$到$\hat{h}$的联合变换的雅可比矩阵为：
$$
J_{h \to \hat{h}} = \frac{\partial \hat{h}}{\partial h} = \frac{\partial \hat{h}}{\partial z} \frac{\partial z}{\partial h} = \text{diag}(\sigma'(z)) W\tag{3.2.39}
$$
这个复合雅可比矩阵包含了对角激活导数矩阵和权重矩阵的乘积，描述了输入变化如何通过线性变换和非线性激活传播到输出。

### 对角雅可比矩阵的梯度传播影响

在反向传播中，梯度通过激活函数时的变换为：
$$
\frac{\partial L}{\partial h} = \frac{\partial L}{\partial \hat{h}} \odot \sigma'(h)\tag{3.2.40}
$$
其中$\odot$表示逐元素乘法。这个公式表明，激活函数的导数对反向传播的梯度起到**逐元素的门控作用**：如果$\sigma'(h_i) \approx 0$，则即使$\frac{\partial L}{\partial \hat{h}_i}$很大，$\frac{\partial L}{\partial h_i}$也会被压缩到接近0；如果$\sigma'(h_i)$较大，则梯度可以无衰减地传播。

**Sigmoid/Tanh的梯度抑制效应**：对于Sigmoid和Tanh激活函数，激活函数的导数在大部分区域都小于1。这意味着在反向传播过程中，梯度会逐层被"压缩"。考虑一个深度网络，如果每一层的激活导数平均为$\lambda < 1$，则经过$L$层后，梯度会被压缩为原来的$\lambda^L$ 。当$L$很大时（如Transformer的数十层），这种累积效应会导致"深层梯度消失"问题。

**ReLU的梯度恒等传递**：对于ReLU激活函数，$\sigma'(h_i) = \mathbb{1}\{h_i > 0\}$ 。这意味着当神经元激活时（$h_i > 0$），梯度可以无衰减地传递；当神经元不激活时（$h_i \leq 0$），梯度被完全阻断（置零）。这种"开关"行为使得ReLU在训练深层网络时比Sigmoid/Tanh表现更好，因为激活区域（$h_i > 0$）的梯度可以完整传递。

**GELU的梯度特性**：对于GELU激活函数，导数$\Phi(x) + x\phi(x)$在整个实数轴上都是正的，且接近 1（在$x = 0$附近，$\Phi(0) = 0.5$，$0 \cdot \phi(0) = 0$，导数约为 0.5；在$x > 0$区域，导数趋向于 1；在$x < 0$区域，导数趋向于 0）。这种特性使得GELU在保持梯度流动的同时，避免了极端的梯度放大或抑制。

### 激活函数导数的谱分析

激活函数导数的谱性质（特征值和特征向量结构）决定了梯度在不同方向上的传播行为。对于对角雅可比矩阵$J_\sigma(h) = \text{diag}(\sigma'(h))$，其谱结构直接由导数值$\sigma'(h_i)$决定。

**定义3.2.14（有效奇异值）** 线性变换的奇异值决定了梯度变换的伸缩程度。对于激活函数的对角雅可比矩阵，其奇异值就是$|\sigma'(h_i)|$的绝对值（因为对角矩阵的奇异值是其对角元素的绝对值）。

在反向传播中，梯度范数的变换满足：
$$
\left\|\frac{\partial L}{\partial h}\right\|_2 \leq \max_i |\sigma'(h_i)| \cdot \left\|\frac{\partial L}{\partial \hat{h}}\right\|_2 \cdot \|W\|_2\tag{3.2.41}
$$
这个不等式表明，激活函数导数的最大值决定了梯度范数的上界。如果$\max_i |\sigma'(h_i)| < 1$，则梯度范数可能收缩；如果存在$\sigma'(h_i)| > 1$的区域，则梯度范数可能放大。

**与权重初始化的关系**：权重初始化的一个核心目标是确保各层的前向激活和反向梯度都具有稳定的方差。Xavier初始化将权重初始化为$W \sim \mathcal{N}(0, \frac{2}{n_{in} + n_{out}})$，确保对于Sigmoid和Tanh激活函数，前向激活和反向梯度的方差在传播过程中保持稳定。He初始化将权重初始化为$W \sim \mathcal{N}(0, \frac{2}{n_{in}})$，针对ReLU激活函数进行了优化，考虑了ReLU激活一半神经元输出为零的特性。

**定义3.2.15（方差守恒条件）** 对于线性层$y = Wx + b$，如果$x$的各元素独立同分布且方差为$\text{Var}(x)$，则$y$的方差为$\text{Var}(y) = n_{in} \cdot \text{Var}(W) \cdot \text{Var}(x)$。为了保持方差稳定$\text{Var}(y) = \text{Var}(x)$，需要$\text{Var}(W) = \frac{1}{n_{in}}$（前向）或$\text{Var}(W) = \frac{1}{n_{out}}$（反向）。Xavier初始化正是这个原理的应用，它取两者的平均 $\frac{2}{n_{in} + n_{out}}$。

### 与位置编码的联合作用

在Transformer中，位置编码与激活函数共同决定了信息在网络中的流动方式。位置编码为每个Token添加了位置信息，这些信息通过线性变换后输入到激活函数。

**定义3.2.16（位置感知的激活计算）** 设$PE$是位置编码矩阵，$E$是Token嵌入矩阵，则位置感知的嵌入表示经过激活函数处理为：
$$
h = \sigma(W_{pe} PE + W_e E)\tag{3.2.42}
$$
这个公式表明，位置信息和Token信息首先进行线性组合，然后经过非线性激活。激活函数的导数结构决定了位置信息和Token信息如何影响梯度传播。

正弦余弦位置编码的频率分解特性意味着不同维度携带不同尺度的位置信息——低频成分编码粗粒度的位置关系，高频成分编码细粒度的位置关系。激活函数对这种频率分解信息的处理决定了位置信息如何在网络中传递。GELU的平滑特性有助于保持相邻位置信息的连续性，这对于学习位置敏感的Token交互模式至关重要。

**定义3.2.17（频率感知的梯度流）** 设位置编码的第$k$个维度编码频率$\omega_k$，则激活函数对该维度的导数为$\sigma'(z_k)$。高频维度（$k$ 较大）和低频维度（$k$较小）可能具有不同的激活值$z_k$​，从而具有不同的导数值$\sigma'(z_k)$。这种频率依赖的梯度流可能影响模型对不同尺度位置模式的学习。


## 3.2.6 本节小结
本节从数学的角度系统性地分析了各类激活函数的导数性质。我们详细推导了Sigmoid函数的导数及其自化形式，揭示了梯度饱和问题的数学根源；分析了Tanh函数与Sigmoid的数学关系及其零中心性质的优势；深入探讨了ReLU函数族的导数结构，包括Leaky ReLU、ELU和GELU等变体的数学特性；系统推导了Softmax的雅可比矩阵及其与交叉熵梯度的联合作用，得到了 $\frac{\partial L}{\partial z} = \hat{p} - y$这一重要结果；最后讨论了激活函数导数的矩阵形式及其对梯度传播的影响。这些数学分析为理解神经网络训练的动力学过程提供了坚实的理论基础，也为后续章节（特别是第五章注意力机制和第四章损失函数）奠定了必要的数学工具。
