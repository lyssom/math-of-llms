## 3.2 导数推导与梯度特性

激活函数的导数性质是理解神经网络训练动力学的核心数学基础。在反向传播算法中，损失函数关于网络参数的梯度需要通过激活函数的导数逐层传递。激活函数的导数结构直接决定了梯度如何在不同层之间传播，进而影响模型的收敛速度、稳定性和最终性能。本节将系统推导各类激活函数的导数公式，分析其几何意义和梯度特性。

### 3.2.1 Sigmoid函数的导数与性质

Sigmoid函数将实数映射到$(0,1)$区间，天然具有概率解释。虽然在大规模语言模型中已被ReLU及其变体取代，但Sigmoid在理解激活函数导数性质方面具有重要的教学价值。

**定义3.2.1（Sigmoid函数）** Sigmoid函数定义为：
$$
\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}\tag{3.2.1}
$$
其定义域为$\mathbb{R}$，值域为$(0,1)$。Sigmoid函数是单调递增函数，其图像呈标准的S形曲线，在$x = 0$处有拐点，$\sigma(0) = 0.5$。

Sigmoid函数的反函数为对数几率函数：
$$
\sigma^{-1}(p) = \log \frac{p}{1-p}, \quad p \in (0,1)\tag{3.2.2}
$$
这个反函数在逻辑回归中具有重要意义，它将概率值转换回实数域。

Sigmoid函数的导数具有一个极为优美的性质：**导数可以用函数自身来表示**，这一性质被称为"自化"。

**定理3.2.1（Sigmoid导数的自化形式）** Sigmoid函数$\sigma(x) = \frac{1}{1+e^{-x}}$的导数为：
$$
\sigma'(x) = \sigma(x)(1 - \sigma(x))\tag{3.2.3}
$$
**证明**：将Sigmoid函数重写为$\sigma(x) = (1 + e^{-x})^{-1}$，使用链式法则求导：
$$
\frac{d\sigma}{dx} = -1 \cdot (1 + e^{-x})^{-2} \cdot (-e^{-x}) = \frac{e^{-x}}{(1 + e^{-x})^2}\tag{3.2.4}
$$
利用$\sigma(x) = \frac{1}{1+e^{-x}}$，可表示为：
$$
\frac{e^{-x}}{(1 + e^{-x})^2} = \sigma(x) \cdot \left(1 - \frac{1}{1+e^{-x}}\right) = \sigma(x)(1 - \sigma(x))\tag{3.2.5}
$$
证毕。

这个证明揭示了Sigmoid导数自化性质的数学根源：Sigmoid函数的分母是分子的积分。从几何角度看，$\sigma(x)(1-\sigma(x))$表示Sigmoid曲线上各点的斜率，斜率的最大值出现在$x = 0$处，此时$\sigma(0) = 0.5$，$\sigma'(0) = 0.25$。

Sigmoid函数的高阶导数可以递归地通过低阶导数表示。

**定理3.2.2（Sigmoid的二阶导数）** Sigmoid函数的二阶导数为：
$$
\sigma''(x) = \sigma(x)(1 - \sigma(x))(1 - 2\sigma(x))\tag{3.2.6}
$$
**证明**：使用乘积法则对$\sigma'(x) = \sigma(x)(1-\sigma(x))$求导：
$$
\frac{d^2\sigma}{dx^2} = \sigma'(x)(1-\sigma(x)) + \sigma(x)(-\sigma'(x)) = \sigma(x)(1-\sigma(x))(1 - 2\sigma(x))\tag{3.2.7}
$$
证毕。

二阶导数的几何意义表示Sigmoid曲线的曲率变化。在$x=0$处，$\sigma''(0) = 0$，这表明拐点处的曲率为零。当$x < 0$时，二阶导数为正，曲线下凸；当$x > 0$时，二阶导数为负，曲线上凸。

从几何角度分析，$\sigma'(x) = \sigma(x)(1-\sigma(x))$表示Sigmoid曲线在各点的斜率。在$x \to -\infty$时，$\sigma(x) \to 0$，$\sigma'(x) \to 0$；在$x \to +\infty$时，$\sigma(x) \to 1$，$\sigma'(x) \to 0$。在$x = 0$处，$\sigma'(0) = 0.25$，这是曲线的"最陡点"。

**定义3.2.2（梯度饱和）** 对于激活函数$\sigma(x)$，如果存在输入区域使得$\sigma'(x) \approx 0$，则在该区域输入的微小变化不会引起输出的显著变化，这种现象称为梯度饱和。

Sigmoid函数的导数取值范围是$(0, 0.25]$，最大值在$x = 0$处取得。当输入$x$的绝对值很大时，$\sigma'(x)$非常接近0。当$|x| \geq 6$时，$\sigma'(x) \leq 0.002$。这意味着，如果神经元的输入落在饱和区域，其激活值对输入的微小变化不敏感，梯度几乎为零。

**与梯度消失问题的联系**：在深层网络中，这种饱和效应会逐层累积放大。考虑一个$L$层的前馈网络，每层的激活输出为$h^{(l)} = \sigma(W^{(l)} h^{(l-1)} + b^{(l)})$。在反向传播中，梯度从输出层向输入层传播时，如果每一层的激活都处于饱和状态，则梯度会按指数级衰减。设平均每层的梯度衰减因子为$\lambda < 1$，经过$L$层后，梯度衰减为原来的$\lambda^L$。当$L=100$且$\lambda = 0.5$时，梯度衰减为$2^{-100} \approx 10^{-30}$，这就是所谓的"梯度消失"问题。

### 3.2.2 Tanh函数的导数与性质

Tanh函数（双曲正切函数）将实数映射到$(-1,1)$区间。Tanh与Sigmoid有密切的数学关系——Tanh可以视为Sigmoid的缩放和平移版本。Tanh的零中心性质使得它在实践中通常比Sigmoid表现更好。


**定义3.2.3（Tanh函数）** Tanh函数定义为：
$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\tag{3.2.8}
$$
其定义域为$\mathbb{R}$，值域为$(-1, 1)$。Tanh函数是奇函数，满足$\tanh(-x) = -\tanh(x)$，在$x=0$处有$\tanh(0) = 0$。

零中心输出带来了两个重要优势。首先，减少了梯度偏移问题。在Sigmoid中，输出总是正的，这意味着梯度总是非负的，可能导致优化过程的"zig-zag"行为。其次，零中心输出使得各层的输入分布更加稳定，减少了协变量偏移问题。


**定理3.2.3（Tanh与Sigmoid的关系）** Tanh函数可以表示为Sigmoid函数的缩放和平移：
$$
\tanh(x) = 2\sigma(2x) - 1\tag{3.2.9}
$$
这个关系表明，Tanh本质上是Sigmoid的"零中心版本"。Sigmoid输出$(0,1)$区间，Tanh输出$(-1,1)$区间。


**定理3.2.4（Tanh的导数）** Tanh函数的导数为：
$$
\frac{d}{dx}\tanh(x) = 1 - \tanh^2(x)\tag{3.2.10}
$$
**证明**：使用商的求导法则对$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$求导，利用平方差公式可得：
$$
\frac{d}{dx}\tanh(x) = \frac{4}{(e^x + e^{-x})^2} = \text{sech}^2(x)\tag{3.2.11}
$$
其中$\text{sech}(x) = \frac{2}{e^x + e^{-x}}$是双曲正割函数。使用$\tanh(x)$自身表示导数：
$$
\frac{d}{dx}\tanh(x) = 1 - \tanh^2(x)\tag{3.2.12}
$$
证毕。

Tanh导数$\tanh'(x) = 1 - \tanh^2(x)$与Sigmoid导数$\sigma'(x) = \sigma(x)(1-\sigma(x))$在形式上非常相似。两者都是函数值与其"补"的乘积。

**Tanh导数的取值范围**：Tanh导数的取值范围是$(0, 1]$，最大值在$x=0$处取得，此时$\tanh'(0) = 1$。与Sigmoid相比，Tanh的导数峰值更大（1 vs 0.25），这意味着在$x = 0$附近，Tanh的梯度"更强"。然而，当$|x|$增大时，Tanh导数同样会迅速衰减至0——当$|x| \geq 3$时，$\tanh'(x) \leq 0.01$。

### 3.2.3 ReLU函数族的导数与性质

ReLU（Rectified Linear Unit）是目前深度学习中最广泛使用的激活函数。ReLU的数学形式极其简单：$\text{ReLU}(x) = \max(0, x)$，正是这种简单性带来了卓越的计算效率和优异的性能。


**定义3.2.4（ReLU函数）** ReLU函数定义为：
$$
\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}\tag{3.2.13}
$$
其定义域为$\mathbb{R}$，值域为$[0, +\infty)$。ReLU是分段线性函数，在$x = 0$处不可导。

**导数推导**：ReLU的导数是一个简单的阶梯函数：
$$
\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \end{cases}\tag{3.2.14}
$$
在$x=0$处，ReLU不可导，但通常使用次梯度来处理。常用的约定是取次梯度集合$[0, 1]$中的任意值。

**定义3.2.5（次梯度）** 对于凸函数$f$，在点$x$处的次梯度$g$满足：
$$
f(y) \geq f(x) + g^T (y - x), \quad \forall y\tag{3.2.15}
$$
对于ReLU函数，在$x=0$处的次梯度集合为$[0, 1]$中的任意值。


ReLU最重要的特性之一是稀疏激活。对于任意输入，约有一半的神经元输出为0。这种稀疏性带来了几个重要的数学优势：**自动特征选择**（只有"重要"的特征被保留）、**计算效率提升**（零值神经元不需要参与后续计算）、**正则化效应**（有助于防止过拟合）。

然而，稀疏激活也带来了"死神经元"问题。当神经元的输入持续为负时，ReLU的输出恒为0，梯度也恒为0，该神经元将永远不会被激活。


Leaky ReLU是ReLU的重要变体，通过在负输入区域保留小的斜率来解决"神经元死亡"问题。

**定义3.2.7（Leaky ReLU）** Leaky ReLU定义为：
$$
\text{LeakyReLU}(x) = \max(\alpha x, x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}\tag{3.2.16}
$$
其中$\alpha \in (0, 1)$是一个小的正常数（通常取0.01或0.02）。Leaky ReLU的导数为：
$$
\text{LeakyReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha & \text{if } x < 0 \end{cases}\tag{3.2.17}
$$
Leaky ReLU在负输入区域保留了小的梯度$\alpha$，确保即使对于持续为负的输入，神经元也能通过小的梯度更新逐渐改变其权重。


GELU（Gaussian Error Linear Unit）是近年来在自然语言处理中广泛使用的激活函数，尤其在BERT和GPT等模型中。

**定义3.2.9（GELU函数）** GELU定义为：
$$
\text{GELU}(x) = x \cdot \Phi(x) = \frac{x}{2} \left[ 1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right) \right]\tag{3.2.18}
$$
其中$\Phi(x)$是标准正态分布的累积分布函数，$\text{erf}(x)$是误差函数。

**GELU的导数**：使用乘积法则求导：
$$
\frac{d}{dx}\text{GELU}(x) = \Phi(x) + x \cdot \phi(x)\tag{3.2.19}
$$
其中$\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$是标准正态分布的概率密度函数。

**GELU导数的性质**：GELU的导数在整个实数轴上都是正的，这意味着GELU永远不会产生"死神经元"问题。同时，当$x \to +\infty$时，导数趋向于1；当$x \to -\infty$时，导数趋向于0。因此，GELU的导数始终在$(0,1)$区间内，避免了梯度爆炸问题。

![[fig_activation_functions.png]]

### 3.2.4 Softmax的导数矩阵分析

Softmax函数是分类任务的核心组件，它将任意实数向量映射到概率单纯形。Softmax的导数具有矩阵结构，理解这个导数矩阵对于实现高效的梯度计算至关重要。


**定义3.2.10（Softmax函数）** 对于输入向量$z = (z_1, z_2, \ldots, z_C)^T \in \mathbb{R}^C$，Softmax函数定义为：
$$
\hat{p}_i = \text{Softmax}(z)_i = \frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)}, \quad i = 1, 2, \ldots, C\tag{3.2.20}
$$
Softmax的输出$\hat{p}$是概率单纯形$\Delta^{C-1}$中的点，满足$\sum_i \hat{p}_i = 1$且$\hat{p}_i > 0$。

**与Sigmoid的关系**：当$C = 2$时，Softmax退化为Sigmoid函数。因此，Softmax是Sigmoid在多类别情况下的推广。


Softmax的导数可以表示为一个$C \times C$的雅可比矩阵$J$，其中$J_{ij} = \frac{\partial \hat{p}_i}{\partial z_j}$。

**定理3.2.5（Softmax雅可比矩阵）** Softmax函数关于输入$z$的雅可比矩阵为：
$$
J_{ij} = \frac{\partial \hat{p}_i}{\partial z_j} = \begin{cases} \hat{p}_i (1 - \hat{p}_i) & \text{if } i = j \\ -\hat{p}_i \hat{p}_j & \text{if } i \neq j \end{cases}\tag{3.2.21}
$$
**证明**：当$i = j$时，使用商的求导法则可得$\hat{p}_i (1 - \hat{p}_i)$；当$i \neq j$时，利用分母的导数项可得$-\hat{p}_i \hat{p}_j$。

**定义3.2.11（Softmax雅可比矩阵的矩阵形式）** Softmax的雅可比矩阵可以简洁地表示为：
$$
J_{\text{Softmax}}(z) = \text{diag}(\hat{p}) - \hat{p} \hat{p}^T\tag{3.2.22}
$$
其中$\text{diag}(\hat{p})$是以$\hat{p}$的元素为对角元素的对角矩阵，$\hat{p} \hat{p}^T$是秩为1的外积矩阵。


**性质3.2.1（行和为零）** 雅可比矩阵的每行和为零：
$$
\sum_{j=1}^C J_{ij} = \hat{p}_i (1 - \hat{p}_i) + \sum_{j \neq i} (-\hat{p}_i \hat{p}_j) = 0\tag{3.2.23}
$$
这个性质反映了概率分布的归一化约束$\sum_i \hat{p}_i = 1$。

**性质3.2.2（半负定性）** 雅可比矩阵是半负定的：对于任意向量$v \in \mathbb{R}^C$，有$v^T J v \leq 0$。这意味着在Softmax输出的概率分布下，$v$的方差非负。

**性质3.2.3（谱结构）** 雅可比矩阵的特征值包括：$\lambda_1 = -1$（对应特征向量$\mathbf{1}$的方向），以及$\lambda_i = \hat{p}_i$（对应$C-1$个正特征值）。

**定义3.2.12（交叉熵损失）** 设损失函数为$L = -\sum_i y_i \log \hat{p}_i$，其中$y$是one-hot真实标签，$\hat{p} = \text{Softmax}(z)$。

**定理3.2.6（Softmax-CrossEntropy的梯度）** 交叉熵损失关于$z_k$的梯度为：
$$
\frac{\partial L}{\partial z_k} = \hat{p}_k - y_k\tag{3.2.24}
$$
**证明**：应用链式法则，利用Softmax雅可比矩阵的特殊结构可得。

这个简洁的结果$\frac{\partial L}{\partial z} = \hat{p} - y$是深度学习中最重要的梯度公式之一。它表明，在Softmax和交叉熵的组合中，梯度就是预测概率与真实标签的差值。

### 3.2.5 激活函数导数的矩阵形式

在深度神经网络中，激活函数通常作用于向量的每个元素。理解激活函数的导数矩阵有助于分析其对信息流的影响。


**定义3.2.13（逐元素激活的雅可比矩阵）** 设激活函数$\sigma: \mathbb{R} \to \mathbb{R}$逐元素应用于向量$h \in \mathbb{R}^C$，则激活函数关于$h$的雅可比矩阵为：
$$
J_\sigma(h) = \frac{\partial \sigma(h)}{\partial h} = \text{diag}(\sigma'(h))\tag{3.2.25}
$$
这个雅可比矩阵是对角矩阵，意味着每个输出元素$\sigma(h_i)$只依赖于对应的输入元素$h_i$，各元素之间是解耦的。

在反向传播中，梯度通过激活函数时的变换为：
$$
\frac{\partial L}{\partial h} = \frac{\partial L}{\partial \hat{h}} \odot \sigma'(h)\tag{3.2.26}
$$
其中$\odot$表示逐元素乘法。这个公式表明，激活函数的导数对反向传播的梯度起到**逐元素的门控作用**。

**Sigmoid/Tanh的梯度抑制效应**：对于Sigmoid和Tanh激活函数，激活函数的导数在大部分区域都小于1。这意味着在反向传播过程中，梯度会逐层被"压缩"。

**ReLU的梯度恒等传递**：对于ReLU激活函数，$\sigma'(h_i) = \mathbb{1}\{h_i > 0\}$。这意味着当神经元激活时，梯度可以无衰减地传递；当神经元不激活时，梯度被完全阻断。


**定义3.2.14（有效奇异值）** 对于激活函数的对角雅可比矩阵，其奇异值就是$|\sigma'(h_i)|$的绝对值。

在反向传播中，梯度范数的变换满足：
$$
\left\|\frac{\partial L}{\partial h}\right\|_2 \leq \max_i |\sigma'(h_i)| \cdot \left\|\frac{\partial L}{\partial \hat{h}}\right\|_2 \cdot \|W\|_2\tag{3.2.27}
$$
这个不等式表明，激活函数导数的最大值决定了梯度范数的上界。

**与权重初始化的关系**：Xavier初始化将权重初始化为$W \sim \mathcal{N}(0, \frac{2}{n_{in} + n_{out}})$，确保对于Sigmoid和Tanh激活函数，前向激活和反向梯度的方差在传播过程中保持稳定。He初始化将权重初始化为$W \sim \mathcal{N}(0, \frac{2}{n_{in}})$，针对ReLU激活函数进行了优化。

### 3.2.6 本节小结

本节系统分析了各类激活函数的导数性质。我们详细推导了Sigmoid函数的导数及其自化形式，揭示了梯度饱和问题的数学根源；分析了Tanh函数与Sigmoid的数学关系及其零中心性质的优势；深入探讨了ReLU函数族的导数结构，包括Leaky ReLU和GELU等变体的数学特性；系统推导了Softmax的雅可比矩阵及其与交叉熵梯度的联合作用，得到了$\frac{\partial L}{\partial z} = \hat{p} - y$这一重要结果；最后讨论了激活函数导数的矩阵形式及其对梯度传播的影响。这些数学分析为理解神经网络训练的动力学过程提供了坚实的理论基础。