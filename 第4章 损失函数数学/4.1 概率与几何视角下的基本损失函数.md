# 第4章 损失函数数学
## 4.1 概率与几何视角下的基本损失函数
损失函数是机器学习和深度学习的核心组成部分，它定义了模型预测与真实目标之间的差距度量，是优化过程的优化目标。从数学角度看，损失函数的选择深刻反映了机器学习问题的本质假设：回归任务假设数据服从高斯分布，分类任务假设数据服从离散分布。从几何角度看，损失函数定义了参数空间中的度量方式，不同的损失函数对应不同的几何结构。本节将从概率论和信息论的基本原理出发，系统推导均方误差和交叉熵损失函数的数学本质，揭示其背后的几何解释，并深入分析Softmax函数与交叉熵损失函数为何构成"天然组合"。

### 4.1.1 信息论基础与概率分布
理解损失函数的数学本质需要首先了解信息论的基本概念和概率分布的核心性质。信息论提供了量化概率分布差异的严格数学框架，而概率分布的假设则是损失函数设计的理论基础。
自信息是信息论的基础概念，它衡量的是一个随机事件发生所带来的"惊喜"程度。直觉上，概率越小的事件发生时，其信息量应当越大；反之，必然事件的发生几乎不带来任何新信息。基于这一直觉，自信息的数学定义为：对于概率空间中发生概率为$p(x)$的事件$x$，其自信息定义为$I(x) = -\log_b p(x) = \log_b \frac{1}{p(x)}$。对数底数$b$决定了信息的计量单位，当$b=2$时单位为比特，当$b=e$时单位为纳特。在机器学习领域，自然对数因其在微分运算中的便利性而被广泛采用。
香农熵从整体上刻画了一个概率分布的信息特征，它衡量的是按照该概率分布采样时期望获得的信息量。熵代表了分布的"不确定性程度"或"随机性程度"：分布越均匀（越不确定），熵值越大；分布越集中（越确定），熵值越小。设离散随机变量$X$的概率分布为$P = \{p_1, p_2, \ldots, p_n\}$，其中$p_i = P(X = x_i)$且$\sum_{i=1}^n p_i = 1$，则其香农熵定义为$H(X) = -\sum_{i=1}^n p_i \log p_i = \sum_{i=1}^n p_i \log \frac{1}{p_i}$。从期望的形式来看，熵实际上是自信息的数学期望。熵具有非负性，对于任意概率分布$P$有$H(P) \geq 0$，当且仅当分布为退化分布时取等号；同时，熵在均匀分布时取得最大值。
Kullback-Leibler散度（KL散度）是度量两个概率分布之间差异的重要工具。设$P$和$Q$是定义在同一概率空间上的两个离散概率分布，其中$P$为真实分布（目标分布），$Q$为模型分布（近似分布），则KL散度定义为$D_{KL}(P \parallel Q) = \sum_{i=1}^n p_i \log \frac{p_i}{q_i} = \sum_{i=1}^n p_i (\log p_i - \log q_i)$。KL散度的非负性（Gibbs不等式）表明，对于任意两个分布有$D_{KL}(P \parallel Q) \geq 0$，当且仅当$P = Q$时取等号。然而，KL散度不满足对称性，即$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$，这反映了它不是真正意义上的距离度量，而是有向的"相对熵"度量。
从几何视角理解，概率分布可以视为定义在概率单纯形（Probability Simplex）上的点。概率单纯形是定义在$\mathbb{R}^n$中满足$p_i \geq 0$且$\sum_{i=1}^n p_i = 1$的点集，其维度为$n-1$。例如，二分类问题的概率分布位于二维平面中的一条线段上，三分类问题的概率分布位于三维空间中的一个三角形区域内。熵函数在这个流形上是一个严格凹函数，其等高线描述了不同概率分布的"信息丰富程度"，为后续讨论KL散度和交叉熵的几何结构提供了基础框架。

### 4.1.2 均方误差的概率论推导：高斯似然的负对数
均方误差（Mean Squared Error，MSE）是回归任务中最广泛使用的损失函数。从概率论的视角来看，MSE的数学形式可以从高斯分布的最大似然估计自然导出，这揭示了MSE作为回归任务标准损失的统计学理论基础。
考虑一个标准的监督学习场景：给定训练数据集$\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$，其中$x_i \in \mathbb{R}^d$是输入特征，$y_i \in \mathbb{R}$是连续型目标变量，模型通过参数$\theta$定义了一个预测函数$\hat{y} = f(x; \theta)$。均方误差定义为预测值与真实值之差的平方的平均值：$\text{MSE} = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2 = \frac{1}{N}\sum_{i=1}^N (y_i - f(x_i; \theta))^2$。这个定义的数学意义非常明确：对于每个样本，计算预测误差$(y_i - \hat{y}_i)$，取平方得到$(y_i - \hat{y}_i)^2$以确保误差为非负值，最后对所有样本的平方误差求平均。
MSE与高斯分布之间存在深刻的联系。假设在给定输入$x$的条件下，目标变量$y$服从高斯分布：$y \sim \mathcal{N}(f(x; \theta), \sigma^2)$。在这个假设下，条件概率密度函数为$p(y \mid x; \theta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y - f(x; \theta))^2}{2\sigma^2}\right)$。给定独立同分布的样本集，数据的似然函数为
$$
\begin{align*}
&\mathcal{L}(\theta) \\& = \prod_{i=1}^N p(y_i \mid x_i; \theta) \\&= \prod_{i=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f(x_i; \theta))^2}{2\sigma^2}\right)\tag{4.1.1}
\end{align*}
$$
取对数后得到对数似然函数：$\log \mathcal{L}(\theta) = -\frac{N}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - f(x_i; \theta))^2$。最大化对数似然等价于最小化$\sum_{i=1}^N (y_i - f(x_i; \theta))^2$，也就是最小化MSE。这个推导表明：在高斯噪声假设下，最大似然估计（MLE）给出的最优损失函数就是MSE。在这个推导中，常数项$-\frac{N}{2}\log(2\pi\sigma^2)$不影响优化结果，而$\frac{1}{2\sigma^2}$是缩放因子，对于固定的$\sigma^2$也不影响最优参数的位置。因此，高斯假设下的MLE问题简化为最小化平方误差之和，即最小化MSE。
**定理 4.1.1** 在条件高斯分布假设$p(y \mid x; \theta) = \mathcal{N}(f(x; \theta), \sigma^2)$下，最小化均方误差等价于最大化数据的对数似然。这个定理具有深刻的统计学意义：当我们使用MSE作为回归任务的损失函数时，我们实际上是在假设数据中的噪声服从高斯分布，并寻找在该假设下最可能的模型参数。这种假设在许多实际场景中是合理的，因为中心极限定理表明，大量独立随机因素的和趋向于服从高斯分布。

### 4.1.3 交叉熵与离散分布的最大似然估计
交叉熵损失函数是分类任务中最广泛采用的优化目标。从概率论的角度，交叉熵损失函数可以从离散分布的最大似然估计自然导出，这建立了分类任务损失函数与经典统计推断之间的深刻联系。
交叉熵的定义与KL散度密切相关。给定两个概率分布$P$（真实分布）和$Q$（模型分布），交叉熵定义为$H(P, Q) = -\sum_{i=1}^n p_i \log q_i = H(P) + D_{KL}(P \parallel Q)$。交叉熵的定义可以分解为两部分：第一部分是真实分布$P$的熵$H(P)$，它仅取决于真实分布本身，与模型无关；第二部分是KL散度$D_{KL}(P \parallel Q)$，它度量了模型分布$Q$与真实分布$P$之间的差异。在大多数学习任务中，$P$是固定的（由训练数据决定），因此最小化交叉熵等价于最小化KL散度。
考虑一个多分类问题。设第$n$个样本的真实类别标签为$y^{(n)} \in \{1, 2, \ldots, C\}$，模型预测的概率分布为$\hat{p}^{(n)} = [\hat{p}_1^{(n)}, \hat{p}_2^{(n)}, \ldots, \hat{p}_C^{(n)}]^T$，其中$\hat{p}_i^{(n)} = P(y = i \mid x^{(n)}; \theta)$。在分类任务中，真实标签通常被视为确定性的，我们可以将数据生成过程建模为以概率$\hat{p}_i^{(n)}$预测第$i$类。样本$x^{(n)}$被正确分类的概率为$\hat{p}_{y^{(n)}}^{(n)}$。假设样本之间相互独立，整个数据集的似然函数为$\mathcal{L}(\theta; \mathcal{D}) = \prod_{n=1}^N P(y^{(n)} \mid x^{(n)}; \theta) = \prod_{n=1}^N \hat{p}_{y^{(n)}}^{(n)}$。
对数似然为$\ell(\theta; \mathcal{D}) = \sum_{n=1}^N \log \hat{p}_{y^{(n)}}^{(n)}$。最小化负对数似然等价于最大化对数似然：$\min_\theta \left( -\sum_{n=1}^N \log \hat{p}_{y^{(n)}}^{(n)} \right) = \min_\theta \sum_{n=1}^N \left( -\log \hat{p}_{y^{(n)}}^{(n)} \right)$。这正是多分类交叉熵损失函数！因此，在分类任务中最小化交叉熵损失等价于最大化数据的似然函数。
**定理 4.1.2** 在离散分类问题中，交叉熵损失函数等价于负对数似然，最小化交叉熵等价于最大化数据的似然函数。
这个定理揭示了交叉熵损失函数的概率论本质：它不是任意选择的损失度量，而是从数据的概率生成模型自然导出的优化目标。当我们假设给定输入$x$后，输出类别$y$服从由神经网络参数化的分类分布时，最大化该分布下数据的似然就自然得到了交叉熵损失函数。
![[fig_CH04_loss_surfaces_gradients.png]]

### 4.1.4 均方误差的几何解释：欧氏距离
MSE不仅具有清晰的概率论解释，还具有深刻的几何意义。在向量空间中，MSE对应于预测向量与目标向量之间的欧几里得距离。这种几何视角不仅揭示了MSE的最优性条件，也为理解线性回归的闭式解提供了几何直觉。
在$N$维欧几里得空间$\mathbb{R}^N$中，我们可以将目标向量$y$和预测向量$\hat{y}$视为空间中的两个点。MSE正是这两点之间欧几里得距离的平方除以样本数：$\text{MSE} = \frac{1}{N}\|y - \hat{y}\|_2^2 = \frac{1}{N}(y - \hat{y})^T(y - \hat{y})$。这种几何视角揭示了MSE的核心性质：MSE度量的是预测向量与目标向量之间的"距离"，优化的目标是最小化这个距离。
考虑预测空间的概念。设模型族$\mathcal{F} = \{f(\cdot; \theta) \mid \theta \in \Theta\}$定义了所有可能的预测向量集合。对于给定的参数$\theta$，预测向量$\hat{y}(\theta) = [f(x_1; \theta), f(x_2; \theta), \ldots, f(x_N; \theta)]^T$。所有可能的预测向量形成$\mathbb{R}^N$空间中的一个子流形，这个子流形就是"预测空间"。模型学习的过程，就是在这个预测空间中寻找最接近目标向量$y$的点。
MSE最优解的几何解释是其最深刻的结果之一：在MSE最小化的意义下，最优预测向量$\hat{y}^*$是目标向量$y$在预测空间上的正交投影。设$\mathcal{S} \subset \mathbb{R}^N$是预测空间，目标是在$\mathcal{S}$中寻找与目标向量$y$距离最小的点。这个优化问题可以形式化为$\hat{y}^* = \arg\min_{\hat{y} \in \mathcal{S}} \|y - \hat{y}\|_2$。对于线性模型（或其他能够生成凸预测空间的模型），最优解$\hat{y}^*$满足正交性条件：误差向量$e = y - \hat{y}^*$与预测空间$\mathcal{S}$正交，即对于任意$\hat{y} \in \mathcal{S}$，有$(y - \hat{y}^*)^T (\hat{y} - \hat{y}^*) = 0$。
**定理 4.1.3（MSE的最优解为正交投影）** 在MSE最小化的意义下，最优预测向量是目标向量在预测空间上的正交投影。
对于线性回归模型$y = X\theta + \epsilon$，其中$X \in \mathbb{R}^{N \times d}$是设计矩阵，$\theta \in \mathbb{R}^d$是参数向量，预测向量为$\hat{y} = X\theta$。最小二乘估计$\hat{\theta} = (X^TX)^{-1}X^Ty$对应的预测为$\hat{y} = X\hat{\theta} = X(X^TX)^{-1}X^Ty = Py$，其中$P = X(X^TX)^{-1}X^T$是投影矩阵。可以验证，投影矩阵$P$满足幂等性（$P^2 = P$）和对称性（$P^T = P$），这是正交投影矩阵的两个关键性质。

### 4.1.5 交叉熵的几何解释：单纯形上的Bregman散度
交叉熵损失函数的几何解释与MSE有本质的不同。在概率单纯形上，交叉熵对应于Bregman散度的一种特殊形式，它定义了概率分布空间中的"信息几何"结构。与欧几里得距离不同，KL散度诱导的几何结构是黎曼几何结构，其度量张量由Fisher信息矩阵给出。
Bregman散度是定义在凸函数基础上的一类散度度量。设$\phi$是一个严格凸的可微函数，定义域为凸集$\mathcal{C}$，则Bregman散度为：
$$D_\phi(p, q) = \phi(p) - \phi(q) - \langle \nabla \phi(q), p - q\rangle\tag{4.1.2}$$Bregman散度度量了点$p$与点$q$之间相对于函数$\phi$的"距离"。当$\phi(p) = \frac{1}{2}\|p\|_2^2$时，Bregman散度退化为欧几里得距离的一半：$D_\phi(p, q) = \frac{1}{2}\|p - q\|_2^2$。
在概率分布空间中，负熵函数$\phi(p) = \sum_{i=1}^n p_i \log p_i$是一个重要的Bregman散度生成函数。设$p$和$q$是两个概率分布，则负熵函数诱导的Bregman散度为$D_\phi(p, q) = \sum_{i=1}^n p_i \log \frac{p_i}{q_i}$，这正是KL散度！
**定理 4.1.4** KL散度是概率单纯形上由负熵函数诱导的Bregman散度：
$$
\begin{align*}
& D_{KL}(p \parallel q) \\& = -\sum_{i=1}^n p_i \log q_i - (-\sum_{i=1}^n p_i \log p_i) + \sum_{i=1}^n p_i \log \frac{p_i}{q_i} \\& = D_\phi(p, q)\tag{4.1.3}
\end{align*}
$$
交叉熵$H(P, Q) = -\sum_i p_i \log q_i$可以分解为熵与KL散度之和：$H(P, Q) = H(P) + D_{KL}(P \parallel Q)$。当真实分布$P$固定时，最小化交叉熵等价于最小化KL散度，即在Bregman散度意义下将模型分布$Q$逼近真实分布$P$。
在概率单纯形上，KL散度定义了非对称的黎曼度量。对于两个相邻的概率分布$p$和$q$，它们之间的"信息距离"由Fisher信息矩阵$I(p)$给出：$ds^2 = dq^T I(p) dq$，其中$I(p) = \mathbb{E}_p\left[(\nabla \log p(x))(\nabla \log p(x))^T\right]$是Fisher信息矩阵。这种信息几何结构反映了概率分布空间的内在曲率，使得优化过程在这个几何框架下具有更清晰的理论性质。
**表 4.1.1 MSE与交叉熵的几何结构对比**

| 损失函数 | 几何空间 | 散度类型 | 度量性质 | 投影类型 |
| ------- | -------- | -------- | -------- | -------- |
| MSE | 欧几里得空间$\mathbb{R}^N$ | 欧几里得距离 | 黎曼度量（常曲率） | 正交投影 |
| 交叉熵 | 概率单纯形 | KL散度（Bregman散度） | Fisher信息度量（变曲率） | 信息投影 |

### 4.1.6 Softmax与交叉熵的天然组合
在深度学习的分类任务中，Softmax函数与交叉熵损失函数的组合构成了事实上的标准配置。这种组合并非偶然，而是由数学上的优美性质决定的。本节将从数学上严格证明这一"天然组合"的合理性。
多分类任务要求模型为每个样本预测其在所有$C$个类别上的概率分布。设样本的真实类别为$c^*$，模型输出的未归一化分数（logits）为$z = [z_1, z_2, \ldots, z_C]^T$，其中$z_i$表示模型对第$i$类的"偏好程度"。将logits转换为有效概率分布的函数就是Softmax函数，其定义为$\text{Softmax}(z)_i = \frac{\exp(z_i)}{\sum_{j=1}^C \exp(z_j)} \triangleq \hat{p}_i$。
**定理 4.1.5** 多分类交叉熵损失关于第$k$个logit的梯度等于模型预测的第$k$类概率与真实第$k$类概率的差值：$\frac{\partial L}{\partial z_k} = \hat{p}_k - p_k$。
这个推导表明，梯度$\frac{\partial L}{\partial z} = \hat{p} - p$具有极其简洁的形式，其中$\hat{p}$是Softmax输出，$p$是one-hot编码的真实标签。这种简洁性源于Softmax与交叉熵的联合设计：当预测概率接近真实概率时，梯度趋近于零；当预测概率偏离真实概率时，梯度按差值大小进行调节。
这种组合的"天然性"体现在以下几个方面。首先，数值稳定性：Softmax函数将任意实数向量映射到概率单纯形上，保证了输出的有效性；交叉熵损失在概率空间上定义，具有良好的数值性质。其次，梯度特性：交叉熵损失的梯度不受$\hat{p}(1-\hat{p})$的调制，这使得它在线性饱和区域仍能保持较大的梯度值，有效缓解了梯度消失问题。第三，计算效率：Softmax与交叉熵的组合可以融合计算（fused computation），避免在中间步骤中存储Softmax输出，不仅减少了冗余计算，更从根本上解决了数值溢出问题。
考虑融合计算：多分类交叉熵损失
$$
L = -\log \hat{p}_{c^*} = -\log \frac{\exp(z_{c^*})}{\sum_j \exp(z_j)} = -(z_{c^*} - \log \sum_j \exp(z_j))\tag{4.1.4}
$$应用LogSumExp技巧：
$$\log \sum_{j=1}^C \exp(z_j) = M + \log \sum_{j=1}^C \exp(z_j - M)\tag{4.1.5}$$其中$M = \max_k z_k$。这样，计算损失$L$只需要计算$M$、$z_j - M$、LogSumExp求和，最后计算$L = z_{c^*} - M - \log \sum_j \exp(z_j - M)$。这种计算方式避免了显式计算任何$\exp(z_i)$，从根本上解决了数值溢出问题。
**定理 4.1.6** Softmax函数与交叉熵损失在数学上构成同构结构：Softmax将logits空间映射到概率单纯形，交叉熵在该单纯形上定义KL散度度量。两者的组合在数学上等价于将logits空间通过信息几何结构连接到真实分布空间。
从信息论的角度，这种组合的合理性更加清晰。Softmax函数在概率单纯形上定义了一个双射映射，保证了信息的无损转换；交叉熵损失在该单纯形上使用KL散度度量分布差异，这与信息论中"相对熵"的基本定义完全一致。从最大似然估计的角度，当假设数据服从参数化的分类分布时，Softmax提供了参数化分布的规范形式，交叉熵损失等价于负对数似然，两者的组合自然地实现了MLE目标。

### 4.1.7 本节小结
本节从概率论和信息论的基本原理出发，系统分析了均方误差和交叉熵损失函数的数学本质及其几何解释。在高斯噪声假设下，条件概率$p(y \mid x; \theta) = \mathcal{N}(f(x; \theta), \sigma^2)$的对数似然函数与均方误差成正比。
最小化MSE等价于最大化高斯分布下的对数似然，即进行最大似然估计。这一等价性表明，MSE作为回归任务的标准损失函数，是从数据的概率生成模型自然导出的优化目标。在离散分类问题中，交叉熵损失函数等价于负对数似然。当假设给定输入后输出类别服从参数化的分类分布时，最大化该分布下数据的似然自然得到交叉熵损失函数。
交叉熵与KL散度的关系$H(P, Q) = H(P) + D_{KL}(P \parallel Q)$表明，在真实分布固定的情况下，最小化交叉熵等价于最小化模型分布与真实分布之间的KL散度。在向量空间中，MSE对应于预测向量与目标向量之间的欧几里得距离。最优解对应于目标向量在预测空间上的正交投影，这一几何性质不仅解释了MSE的最优性条件，也为理解线性回归的闭式解提供了几何直觉。正交投影的误差分解性质是回归分析中$R^2$统计量的几何基础。在概率单纯形上，交叉熵对应于KL散度，而KL散度是负熵函数诱导的Bregman散度。与欧几里得距离不同，KL散度诱导的信息几何结构是黎曼几何结构，其度量张量由Fisher信息矩阵给出。这种几何结构反映了概率分布空间的内在曲率，使得优化过程具有更清晰的理论性质。
Softmax函数将任意实数向量映射到概率单纯形上，交叉熵损失在该单纯形上使用KL散度度量分布差异。两者的组合具有简洁的梯度形式$\frac{\partial L}{\partial z} = \hat{p} - p$，可以融合计算以提高数值稳定性，并且从信息论和最大似然估计的角度都是自然的选择。这种"天然组合"的设计体现了深度学习中损失函数设计的数学优美性。