在前四节中，我们建立了损失函数的数学基础，从均方误差的信息论起源开始，经过交叉熵的概率论推导，到损失函数结构的系统性对比，最后揭示了InfoNCE与注意力机制中Softmax操作的数学统一性。本节将把这些理论基础应用于大语言模型（Large Language Models，简称LLMs）的具体场景，分析现代LLM训练中使用的各种损失函数的数学本质及其设计原理。

大语言模型的训练涉及多个层面的损失函数设计：在预训练阶段，主要使用语言建模损失（自回归或掩码方式）；在监督微调阶段，使用标准的分类损失或序列生成损失；在对齐阶段，使用基于人类反馈的强化学习损失（RLHF）或直接偏好优化损失（DPO）。理解这些损失函数的数学结构，不仅有助于深入理解LLM的训练机制，也为改进模型训练策略提供了理论基础。

## 4.5.1 语言建模损失：下一个Token预测的数学框架

### 自回归语言建模的形式化定义

自回归语言建模是大语言模型预训练的核心任务。设输入序列为$x = (x_1, x_2, \ldots, x_T)$，其中每个$x_t$是来自词汇表$\mathcal{V}$的Token。自回归语言建模的目标是建模序列的联合概率分布$P(x_1, x_2, \ldots, x_T)$。

**定义 4.5.1（自回归分解）** 根据概率论的链式法则，序列的联合分布可以分解为条件分布的乘积：
$$
P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_1, x_2, \ldots, x_{t-1})\tag{4.5.1}
$$
这个分解利用了序列数据的有序性质：第$t$个Token的分布仅依赖于其前面的Token（上下文）。这种条件依赖假设是自回归语言建模的理论基础。

**语言建模损失**：给定一个包含$N$个序列的训练集$\mathcal{D} = \{x^{(1)}, x^{(2)}, \ldots, x^{(N)}\}$，语言建模的负对数似然损失为：
$$
$\mathcal{L}_{LM} = -\sum_{n=1}^N \sum_{t=1}^{T^{(n)}} \log P_\theta\left(x_t^{(n)} \mid x_1^{(n)}, \ldots, x_{t-1}^{(n)}\right)\tag{4.5.2}
$$
其中$\theta$是模型参数，$P_\theta(\cdot \mid \cdot)$是模型预测的条件概率分布。

对于每个位置$t$，模型首先计算一个未归一化的logit向量$z_t = W h_t + b$，其中$h_t$是位置$t$的隐藏状态。然后通过Softmax将logits转换为概率分布：
$$
P_\theta(x_t \mid x_{<t}) = \text{Softmax}(z_t)_{x_t} = \frac{\exp(z_t[x_t])}{\sum_{v \in \mathcal{V}} \exp(z_t[v])}\tag{4.5.3}​
$$
根据4.2节的推导，负对数似然$-\log P_\theta(x_t \mid x_{<t})$正是交叉熵损失。因此，**语言建模损失本质上是交叉熵损失在序列预测任务中的应用**。

### 位置无关性与注意力机制的数学关系

在Transformer架构中，位置信息通过位置编码注入模型（详见第六章）。设$h_t$是位置$t$的Token嵌入经过多层自注意力计算后的隐藏状态。位置编码的核心作用是打破自注意力机制的"排列不变性"，使得模型能够区分不同位置的Token。

**定理 4.5.1（位置编码对语言建模损失的必要性）** 设$h_t$仅依赖于Token嵌入而不包含位置信息，则对于任意位置置换$\pi$，有$P_\theta(x_{\pi(t)} \mid x_{\pi(<t)}) = P_\theta(x_t \mid x_{<t})$，即模型无法区分序列的不同排列顺序。此时语言建模损失无法学习到序列中的位置模式，模型将退化为"词袋"模型。

**证明**：如果不包含位置信息，则$h_t$仅是位置$t$的Token集合的函数，而非具体序列顺序的函数。对于任意置换$\pi$，Token嵌入的集合$\{e_{x_1}, \ldots, e_{x_T}\}$在置换下保持不变（只是重排），因此$h_{\pi(t)}$​ 与$h_t$具有相同的函数形式。由自注意力的对称性（Softmax对所有位置加权求和），模型的输出分布满足$P_\theta(x_{\pi(t)} \mid x_{\pi(<t)}) = P_\theta(x_t \mid x_{<t})$。

位置编码（如正弦余弦编码或RoPE）为每个位置分配唯一的"位置签名"，使得$h_t$真正依赖于位置$t$的具体身份，从而打破了上述对称性。位置编码的选择直接影响模型学习位置模式的能力，进而影响语言建模损失的收敛速度和最终性能。

### 前缀语言建模与填充Token的数学处理

在实际的预训练数据处理中，序列通常被组织为"前缀-目标"对的形式。设输入序列为$(x_1, x_2, \ldots, x_L)$，其中$x_1$到$x_K$是前缀，$x_{K+1}$到$x_L$是生成目标。模型需要基于前缀预测后续Token。

**定义 4.5.2（前缀语言建模损失）** 前缀语言建模损失仅对目标Token计算损失：
$$
\mathcal{L}_{PrefixLM} = -\sum_{t=K+1}^L \log P_\theta(x_t \mid x_1, \ldots, x_{t-1})\tag{4.5.4}
$$
前缀Token $x_1$到 $x_K$参与前向传播计算，但不参与损失计算。这种设计反映了实际应用场景：模型被训练来根据给定的提示生成内容，但提示本身不需要被"预测"。

**填充Token的数学处理**：在实际实现中，为了处理变长序列，通常使用特殊的填充Token（Padding Token）将所有序列填充到统一长度。设填充后的序列长度为$T_{max}$，定义一个有效位置掩码$M \in \{0,1\}^{T_{max}}$​，其中$M_t = 1$表示位置$t$是有效Token，$M_t = 0$表示位置$t$是填充Token。

**定义 4.5.3（掩码语言建模损失）** 带掩码的语言建模损失为：
$$
\mathcal{L}_{MaskedLM} = -\frac{1}{\sum_t M_t} \sum_{t=1}^{T_{max}} M_t \log P_\theta(x_t \mid x_{\text{all}})\tag{4.5.5}
$$
这个定义与标准交叉熵的区别在于：仅对有效位置（$M_t = 1$）计算损失，且损失值被有效位置的数量归一化。

在Prefix Language Model（PrefixLM）中，解码器可以看到前缀的所有Token，但只对目标Token计算损失。因果注意力掩码确保位置$t$只能关注位置$\leq t$的Token。在PrefixLM的实际实现中，前缀部分通常使用双向注意力（允许关注前缀内的所有位置），而目标部分使用因果注意力。这种混合注意力模式与前缀语言建模损失的设计是一致的：前缀需要"全局上下文"来理解任务，目标则需要基于前缀进行自回归生成。

## 4.5.2 掩码语言建模：完形填空任务的数学分析

### 掩码语言建模的形式化定义

掩码语言建模（Masked Language Modeling，简称MLM）是BERT等模型采用的预训练任务。与自回归语言建模不同，MLM同时利用左右上下文来预测被掩码的Token，这种"双向"建模能力使其在理解任务上具有优势。

**定义 4.5.4（掩码语言建模）** 给定输入序列$x = (x_1, x_2, \ldots, x_T)$，随机选择一部分位置（通常为15%）进行掩码。设掩码位置集合为$\mathcal{M}$，则MLM的目标是预测被掩码的Token：
$$
P_\theta(x_t \mid x_{\setminus t}) = \text{Softmax}(W h_t + b)_{x_t}, \quad t \in \mathcal{M}\tag{4.5.6}
$$
其中$x_{\setminus t}$表示除位置$t$外所有Token的序列，$h_t$是位置$t$的双向上下文表示。

**MLM损失函数**：
$$
\mathcal{L}_{MLM} = -\sum_{t \in \mathcal{M}} \log P_\theta(x_t \mid x_{\setminus t})\tag{4.5.7}
$$
### 掩码策略的数学分析

不同的掩码策略会导致不同的损失函数形式和模型学习行为。标准的BERT使用随机均匀掩码：每个位置以固定概率$p$被独立地选为掩码位置。更复杂的掩码策略（如Whole Word Masking、Dynamic Masking）有各自的特点。

**定义 4.5.5（Span掩码）** Span掩码策略连续掩码一个Token片段（Span），而不是独立地掩码单个Token。设掩码Span的长度服从某种分布（如几何分布），则MLM损失变为：
$$
\mathcal{L}_{SpanMLM} = -\frac{1}{\sum_{s \in \mathcal{S}} |\text{span}_s|} \sum_{s \in \mathcal{S}} \sum_{t \in \text{span}_s} \log P_\theta(x_t \mid x_{\setminus \text{span}_s})\tag{4.5.8}
$$
其中$\mathcal{S}$是被掩码的Span集合，$|\text{span}_s|$是Span的长度。

**掩码策略的信息论分析**：从信息论角度，不同的掩码策略对应于不同的"掩码分布"。设$Q(t)$是位置$t$被掩码的概率分布，则期望的掩码比例为$\mathbb{E}[|\mathcal{M}|/T] = \sum_t Q(t)$。

**掩码率与学习效率**：掩码率$\gamma = \mathbb{E}[|\mathcal{M}|/T]$的选择是一个关键的超参数：

- 较高的掩码率（接近100%）会导致上下文信息过于稀疏，模型难以学习有效的表示
- 较低的掩码率（接近0%）会导致监督信号不足，学习效率低下
- BERT的15%掩码率是一个经验性的折中，这个比例保证了足够的上下文信息同时提供了足够的预测挑战

在MLM中，被掩码位置的双向表示$h_t$需要聚合来自左右两侧的Token信息。位置编码在这个聚合过程中扮演关键角色：它为每个位置提供唯一的身份标识，使得模型能够区分"来自左边第$k$个Token的信息"和"来自右边第$k$个Token的信息"。RoPE等位置编码的相对位置敏感性（详见6.4节）使得$h_t$能够更好地捕获位置$t$与其上下文之间的相对关系，从而提高MLM的预测准确性。

### 分词器对损失函数的影响

大语言模型通常使用子词分词器（如BPE、WordPiece）将文本切分为Token序列。分词器的选择直接影响损失函数的结构和模型的学习行为。

**定义 4.5.6（分词后的MLM损失）** 设分词器将原始文本映射为Token序列$x = (x_1, x_2, \ldots, x_T)$，其中每个$x_t$是子词单元。MLM损失为：
$$
\mathcal{L}_{MLM} = -\sum_{t \in \mathcal{M}} \log P_\theta(x_t \mid x_{\setminus t})\tag{4.5.9}
$$
**Whole Word Masking（WWM）的数学改进**：WWM策略确保如果一个词的一部分被掩码，则该词的所有子词都被掩码。设词 www 对应的子词索引集合为$\mathcal{T}_w$，则WWM的掩码策略为：如果$\mathcal{T}_w \cap \mathcal{M} \neq \emptyset$，则$\mathcal{T}_w \subseteq \mathcal{M}$。

**定义 4.5.7（WWMLM损失）** Whole Word Masking后的MLM损失：
$$
$\mathcal{L}_{WWMLM} = -\sum_{w: \mathcal{T}_w \subseteq \mathcal{M}} \sum_{t \in \mathcal{T}_w} \log P_\theta(x_t \mid x_{\setminus \mathcal{M}})\tag{4.5.10}
$$
WWM的优势在于：它迫使模型基于完整的词级上下文进行预测，而不是依赖词内的子词相关性。这通常能够提高模型对词级语义的理解能力。

## 4.5.3 多任务学习中的损失函数组合

### 多任务学习的数学框架

大语言模型通常在多种任务上进行联合训练，以获得更好的泛化能力。多任务学习（Multi-Task Learning）的数学框架涉及如何组合不同任务的损失函数。

**定义 4.5.8（多任务损失）** 设有$K$个任务，任务$k$的损失函数为$\mathcal{L}_k(\theta)$，其中$\theta$是共享参数。多任务学习的总体损失为：
$$
\mathcal{L}_{MTL}(\theta) = \sum_{k=1}^K w_k \mathcal{L}_k(\theta)\tag{4.5.11}
$$
其中$w_k$是任务$k$的权重，决定了各任务在优化过程中的相对重要性。

**任务权重的影响**：任务权重$w_k$的选择直接影响优化轨迹和最终性能：

- 权重较高的任务主导学习过程，模型倾向于在该任务上表现良好
- 权重较低的任务可能被"忽略"，模型在该任务上表现不佳
- 过大的权重差异可能导致某些任务的梯度主导，阻碍其他任务的学习

### 梯度归一化与帕累托优化

当不同任务的损失函数具有不同的尺度时，直接加权可能导致训练不稳定。梯度归一化方法通过调整梯度范数来实现任务间的平衡。

**定义 4.5.9（梯度归一化）** 设$g_k = \nabla_\theta \mathcal{L}_k(\theta)$是任务$k$的梯度向量，梯度归一化的更新方向为：
$$
\hat{g}_k = \frac{g_k}{\|g_k\|_2}, \quad g_{total} = \sum_{k=1}^K w_k \hat{g}_k\tag{4.5.12}​
$$
这种归一化确保每个任务对更新方向的贡献在范数上是一致的，避免了某些任务因梯度范数过大而主导训练。

在多任务学习中，一个解$\theta^*$ 是帕累托最优的，如果不存在另一个解$\theta$使得在所有任务上都不差且至少一个任务上更好。梯度归一化方法的一个理论优势是：它能够确保优化过程收敛到帕累托前沿上的某个点。

### 大语言模型中的典型多任务设置

现代大语言模型的预训练和微调通常涉及多种任务的组合。

**预训练阶段**：自回归语言建模（Next Token Prediction）通常是唯一的预训练任务，但也可能结合其他自监督任务（如去噪自编码、对比学习）。

**监督微调阶段**：常见的任务包括：

- 问答任务（如SQuAD的跨度预测）
- 文本分类任务（如情感分析）
- 摘要生成任务
- 翻译任务
- 指令遵循任务

**数学形式统一**：这些任务在数学上都可以统一为某种形式的预测任务：

- 分类任务：$\mathcal{L}_{cls} = -\log P_\theta(y \mid x)$
- 序列标注任务：$\mathcal{L}_{seq} = -\sum_t \log P_\theta(y_t \mid x)$
- 序列生成任务：$\mathcal{L}_{gen} = -\sum_t \log P_\theta(x_t \mid x_{<t})$

所有这些损失函数都共享Softmax-交叉熵的核心结构，只是输入和输出的形式有所不同。

在多任务学习中，不同的任务可能需要模型关注输入的不同部分。例如，问答任务需要关注问题中的关键实体，摘要任务需要关注文档的核心内容。注意力机制提供了动态调整关注焦点的机制，使得模型能够为不同任务学习不同的"注意力模式"。多头注意力中的不同头可以专门化于处理不同类型的任务，这种专门化是多任务学习成功的关键因素之一。

## 4.5.4 人类反馈强化学习损失（RLHF）

### RLHF的数学框架

人类反馈强化学习（Reinforcement Learning from Human Feedback，简称RLHF）是将人类偏好融入模型训练的关键技术。RLHF的核心思想是：使用人类对模型输出的偏好判断来训练一个奖励模型，然后使用这个奖励模型来指导策略优化。

**定义 4.5.10（RLHF的三阶段流程）**

1.**监督微调阶段**：使用人类编写的示范数据$\mathcal{D}_{SFT} = \{(x, y)\}$训练策略模型$\pi_\theta$，最大化专家行为的似然：  
$$
\mathcal{L}_{SFT} = -\mathbb{E}_{(x,y) \sim \mathcal{D}_{SFT}}[\log \pi_\theta(y \mid x)]\tag{4.5.13}
$$
2.**奖励模型训练阶段**：收集人类对不同模型输出的偏好比较数据$\mathcal{D}_{RM} = \{(x, y^+, y^-)\}$，其中$y^+$被人类偏好于$y^-$。训练奖励模型$r_\phi(x, y)$来预测偏好概率：  
$$
\mathcal{L}_{RM} = -\mathbb{E}_{(x,y^+,y^-) \sim \mathcal{D}_{RM}}[\log \sigma(r_\phi(x, y^+) - r_\phi(x, y^-))]\tag{4.5.14}
$$
其中$\sigma$是Sigmoid函数。

3.**策略优化阶段**：使用奖励模型指导策略优化，最大化期望奖励同时保持与初始策略的接近：  
$$\mathcal{L}_{RLHF} = -\mathbb{E}_{x \sim \mathcal{D}_{policy}, y \sim \pi_\theta(\cdot \mid x)}[r_\phi(x, y)] + \beta \cdot D_{KL}(\pi_\theta(\cdot \mid x) \parallel \pi_{ref}(\cdot \mid x))\tag{4.5.15}$$
### KL散度约束的数学分析

RLHF损失函数中的KL散度项$D_{KL}(\pi_\theta \parallel \pi_{ref})$是一个关键的数学组件，它实现了"策略正则化"的功能。

**KL散度项的作用**：
- 防止策略模型偏离初始策略$\pi_{ref}$太远
- 避免奖励模型的过拟合导致的策略崩溃
- 保持模型生成的多样性
**定义 4.5.11（带KL约束的策略优化）** RLHF的目标可以写为：  
$$\max_\theta \mathbb{E}_{x,y \sim \pi_\theta}[r_\phi(x, y)] - \beta D_{KL}(\pi_\theta \parallel \pi_{ref})\tag{4.5.16}$$

**解析解**：对于离散动作空间，这个问题有一个解析解。考虑一个输入$x$，策略优化的目标为：  
$$
\max_\theta \sum_y \pi_\theta(y \mid x) r_\phi(x, y) - \beta \sum_y \pi_\theta(y \mid x) \log \frac{\pi_\theta(y \mid x)}{\pi_{ref}(y \mid x)}\tag{4.5.17}
$$
使用变分推断，可以证明最优策略为：  
$$\pi^*(y \mid x) \propto \pi_{ref}(y \mid x) \exp\left(\frac{1}{\beta} r_\phi(x, y)\right)\tag{4.5.18}$$
这正是著名的"软最优策略"（Softmax策略）的形式。KL约束项使得优化后的策略不是简单地选择奖励最高的动作，而是在奖励和策略多样性之间进行权衡。

**与交叉熵的联系**：对上述最优策略取对数：  
$$\log \pi^*(y \mid x) = \log \pi_{ref}(y \mid x) + \frac{1}{\beta} r_\phi(x, y) - \log Z(x)l\tag{4.5.19}$$
其中$Z(x)$是归一化常数。这个表达式可以解释为：策略的logit是参考策略的logit加上奖励项的缩放。这与标准的分类任务中logit = 嵌入 + 变换的结构高度相似。

### PPO在RLHF中的应用

近端策略优化（Proximal Policy Optimization，简称PPO）是RLHF中最常用的策略优化算法。PPO的核心思想是：限制策略更新的幅度，确保新策略不会与旧策略相差太远。

**定义 4.5.12（PPO损失）** PPO的目标函数为：  
$$
\mathcal{L}_{PPO} = -\mathbb{E}_{x \sim \mathcal{D}_{policy}, y \sim \pi_\theta(\cdot \mid x)}\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right]\tag{4.5.20}
$$
其中：
- $r_t(\theta) = \frac{\pi_\theta(y_t \mid x_t)}{\pi_{\theta_{old}}(y_t \mid x_t)}$是重要性采样比率
- $\hat{A}_t$是优势函数（Advantage Function）的估计
- $\epsilon$是裁剪参数（通常为0.1到0.2）
裁剪操作确保当$r_t(\theta)$偏离1太远时，目标函数不会被进一步优化。这有效地限制了策略更新的步长，防止策略发生剧烈变化。

PPO中的"裁剪"机制与均方误差中的梯度裁剪有类似的功能——都用于防止优化过程中的过大更新。然而，PPO的裁剪是基于策略比率的几何约束，而均方误差的裁剪是基于梯度的范数约束。这种差异反映了强化学习与监督学习在优化目标上的根本区别。

## 4.5.5 直接偏好优化（DPO）

### DPO的数学推导

直接偏好优化（Direct Preference Optimization，简称DPO）是近年来提出的一种简化的对齐方法，它绕过了显式的奖励模型，直接使用偏好数据优化策略。

**核心思想**：DPO的核心洞察是：RLHF中的策略优化问题可以通过重新参数化直接求解，而不需要显式的奖励模型。

**定义 4.5.13（DPO目标）** 给定偏好对$(y^+, y^-)$，DPO的损失函数为：  
$$
\\mathcal{L}_{DPO} = -\mathbb{E}_{(x,y^+,y^-)} \left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} - \beta \log \frac{\pi_\theta(y^- \mid x)}{\pi_{ref}(y^- \mid x)} \right) \right]\tag{4.5.21}
$$

其中$\sigma$是Sigmoid函数，$\beta$是温度参数。

**简化形式**：设$\Delta \log \pi(y) = \log \pi_\theta(y \mid x) - \log \pi_{ref}(y \mid x)$，则：  $$\mathcal{L}_{DPO} = -\mathbb{E}\left[ \log \sigma\left( \beta (\Delta \log \pi(y^+) - \Delta \log \pi(y^-)) \right) \right]\tag{4.5.22}$$DPO损失在形式上是二元交叉熵损失的一种变体。考虑将偏好对$(y^+, y^-)$视为一个二分类问题：标签为1（偏好$y^+$），预测为$\sigma(\beta \Delta \log \pi(y^+) - \beta \Delta \log \pi(y^-))$。这与4.2节中二元交叉熵的形式完全一致。可以证明，在一定的正则性条件下，DPO的最优解与RLHF的最优解是等价的。具体而言，DPO目标关于$\theta$的梯度与PPO目标的梯度在期望意义下是相近的。

### DPO的梯度分析

**定理 4.5.2（DPO的梯度）** DPO损失关于$\theta$的梯度为：  
$$
\nabla_\theta \mathcal{L}_{DPO} = -\beta \mathbb{E}\left[ \sigma(-\Delta) \cdot \left( \nabla_\theta \log \pi_\theta(y^+ \mid x) - \nabla_\theta \log \pi_\theta(y^- \mid x) \right) \right]\tag{4.5.23}
$$
其中$\Delta = \beta (\Delta \log \pi(y^+) - \Delta \log \pi(y^-))$。

**梯度解释**：

- $\nabla_\theta \log \pi_\theta(y^+ \mid x)$是增加$y^+$似然的梯度
- $\nabla_\theta \log \pi_\theta(y^- \mid x)$是减少$y^-$似然的梯度
- $\sigma(-\Delta)$是权重因子：当偏好差异$\Delta$较大时，权重较小（不需要大的更新）；当偏好差异较小时，权重较大（需要更大的更新来纠正）
**与InfoNCE的联系**：DPO的梯度结构与InfoNCE（4.4节）的梯度高度相似。InfoNCE的梯度为：  
$$\nabla_\theta L_{InfoNCE} \propto \mathbb{E}\left[ (\hat{p} - p) \cdot \nabla_\theta s \right]\tag{4.5.24}$$
两者都是"正确选项梯度减去错误选项梯度"的形式，只是权重因子的计算方式不同。这种相似性表明，DPO可以被视为一种"带参考策略的对比学习"。

### 隐式奖励与对齐机制

DPO的一个重要理论贡献是揭示了策略比率与隐式奖励之间的联系。

**定义 4.5.14（隐式奖励）** 从DPO的目标函数可以反推出一个隐式的奖励函数：  
$$r_{DPO}(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{ref}(y \mid x)}\tag{4.5.25}$$
这个奖励函数直接由策略和参考策略的比率定义，不需要显式的奖励模型。

**对齐机制**：DPO的优化过程实际上是在调整策略$\pi_\theta$，使得：  
$$\log \frac{\pi_\theta(y^+ \mid x)}{\pi_\theta(y^- \mid x)} > \log \frac{\pi_{ref}(y^+ \mid x)}{\pi_{ref}(y^- \mid x)}\tag{4.5.26}$$
即相对于参考策略，模型更偏好$y^+$而非$y^-$。这种偏好调整正是对齐的目标。

在生成过程中，模型的自注意力机制决定了每个位置关注哪些前面的Token。DPO训练的策略会改变这些注意力模式——增加对"好"Token的注意，减少对"坏"Token的注意。虽然这种改变不是直接作用于注意力权重，而是作用于整个生成策略，但它最终会反映在模型的注意力模式中。

## 4.5.6 损失函数前沿与未来方向

### 损失函数的理论极限

从理论上分析，现有损失函数存在一些固有的局限性。

**交叉熵的理论极限**：交叉熵损失假设预测分布与真实分布的差异可以通过KL散度度量。然而，当模型容量有限时，交叉熵损失可能无法捕获数据分布的所有方面。此外，交叉熵损失对于"分布外"（Out-of-Distribution）样本的行为是未定义的——模型可能对这些样本产生过度自信的预测。

**对比学习的理论局限**：InfoNCE的目标是最大化互信息的下界，但这个下界可能非常松散。当负样本数量有限时，InfoNCE可能无法有效区分真正相似的样本和表面相似的样本。

**RLHF的理论挑战**：RLHF依赖于人类偏好数据的质量和一致性。如果偏好数据存在噪声或不一致性，奖励模型可能学习到错误的偏好模式，从而导致策略优化走向错误的方向。

### 自适应损失函数

未来的研究方向之一是设计自适应的损失函数，根据训练动态自动调整各损失项的权重。

**定义 4.5.15（自适应多任务损失）** 自适应损失函数可以写为：  
$$\mathcal{L}_{adaptive} = \sum_k w_k(t) \mathcal{L}_k(\theta)\tag{4.5.27}$$
其中权重$w_k(t)$是时间$t$的函数，可以根据梯度范数、损失值或验证性能动态调整。

**梯度归一化的推广**：一种简单但有效的自适应策略是基于各任务梯度的范数调整权重：  
$$w_k(t) = \frac{1}{\|g_k(t)\|_2 + \epsilon}\tag{4.5.28}$$
这确保了各任务对参数更新的贡献在尺度上是一致的。

### 超越Softmax的归一化

尽管Softmax是当前损失函数设计的主流选择，但它存在一些已知的局限性，如对异常值敏感、计算复杂度与类别数线性相关等。

**稀疏Softmax**：为了提高计算效率，可以采用稀疏Softmax，仅保留top-k个最大的分数：
$$
\begin{cases}
\frac{\exp(z_i)}{\sum_{j \in \text{top-k}} \exp(z_j)} & \text{if } i \in \text{top-k} \\ 0 & \text{otherwise} \end{cases}\tag{4.5.29}
$$
**门控Softmax**：引入可学习的门控机制来调整Softmax的行为： $$\text{GatedSoftmax}(z) = \text{Softmax}(g \odot z)\tag{4.5.30}$$ 其中 $g$ 是门控向量，可以根据输入动态调整。 
### 损失函数与模型架构的协同设计 
未来的研究方向还包括损失函数与模型架构的协同设计，使得特定的损失函数能够更好地发挥模型的能力。 
**与Transformer架构的协同**：当前的Transformer架构（自注意力 + 前馈网络）是通用的，但可能不是所有任务的最优选择。针对特定的损失函数（如对比学习损失）设计专门的模型架构，可能会带来性能提升。
**与位置编码的协同**：不同的位置编码（绝对位置、相对位置、RoPE）会影响模型学习位置模式的能力，从而影响各种损失函数的收敛速度和最终性能。设计"损失感知"的位置编码是一个有前景的研究方向。 

## 4.5.7 本节小结
本节系统性地分析了大语言模型中使用的各种损失函数，从预训练阶段的语言建模损失和掩码语言建模损失，到监督微调阶段的多任务损失，再到对齐阶段的RLHF和DPO损失。我们展示了这些损失函数的数学本质：它们都建立在Softmax-交叉熵的框架之上，通过不同的方式定义"正样本"和"负样本"来引导模型学习。语言建模损失使用下一个Token作为正样本，其他所有Token作为隐式负样本；掩码语言建模损失使用被掩码位置的真实Token作为正样本；RLHF使用人类偏好的正样本和负样本；DPO直接优化正样本相对于负样本的概率优势。这些损失函数通过注意力机制实现信息聚合，通过位置编码引入序列结构，共同构成了大语言模型训练的数学基础。本章的内容与第七章（注意力机制）和第八章（位置编码）形成了紧密的呼应：损失函数定义了优化的目标，注意力机制和位置编码则提供了实现这些目标的计算框架，两者共同决定了大语言模型的性能和能力边界。 