## 4.4 大语言模型中的损失函数设计
大语言模型的训练涉及多个层面的损失函数设计：在预训练阶段，主要使用语言建模损失；在监督微调阶段，使用标准分类损失或序列生成损失；在对齐阶段，使用基于人类反馈的强化学习损失（RLHF）或直接偏好优化损失（DPO）。理解这些损失函数的数学结构，有助于深入理解大语言模型的训练机制，并为改进模型训练策略提供理论基础。

### 4.4.1 语言建模损失
自回归语言建模是大语言模型预训练的核心任务。设输入序列为$x = (x_1, x_2, \ldots, x_T)$，其中每个$x_t$是来自词汇表$\mathcal{V}$的Token。根据概率论的链式法则，序列的联合分布可以分解为条件分布的乘积：
$$
P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_1, x_2, \ldots, x_{t-1}) \tag{4.4.1}
$$
语言建模的负对数似然损失为：
$$
\mathcal{L}_{LM} = -\sum_{t=1}^T \log P_\theta(x_t \mid x_1, \ldots, x_{t-1}) \tag{4.4.2}
$$
对于每个位置$t$，模型首先计算未归一化的logit向量$z_t = W h_t + b$，然后通过Softmax将logits转换为概率分布。负对数似然$-\log P_\theta(x_t \mid x_{<t})$正是交叉熵损失，因此语言建模损失本质上是交叉熵损失在序列预测任务中的应用。
在Transformer架构中，位置信息通过位置编码注入模型。因果注意力掩码确保位置$t$只能关注位置$\leq t$的Token。前缀语言建模损失仅对目标Token计算损失：$\mathcal{L}_{PrefixLM} = -\sum_{t=K+1}^L \log P_\theta(x_t \mid x_1, \ldots, x_{t-1})$，前缀Token参与前向传播但不参与损失计算。

![[fig_CH4_token_level_loss_autoregressive.drawio.png]]
掩码语言建模（MLM）是BERT等模型采用的预训练任务。与自回归语言建模不同，MLM同时利用左右上下文来预测被掩码的Token。给定输入序列，随机选择一部分位置（通常为15%）进行掩码。设掩码位置集合为$\mathcal{M}$，则MLM的目标是预测被掩码的Token：
$$
\mathcal{L}_{MLM} = -\sum_{t \in \mathcal{M}} \log P_\theta(x_t \mid x_{\setminus t}) \tag{4.4.3}
$$
其中$x_{\setminus t}$表示除位置$t$外所有Token的序列，$h_t$是位置$t$的双向上下文表示。不同的掩码策略会导致不同的学习行为。Whole Word Masking（WWM）策略确保如果一个词的一部分被掩码，则该词的所有子词都被掩码，这通常能够提高模型对词级语义的理解能力。

大语言模型通常在多种任务上进行联合训练。多任务学习的总体损失为：
$$
\mathcal{L}_{MTL}(\theta) = \sum_{k=1}^K w_k \mathcal{L}_k(\theta) \tag{4.4.4}
$$
其中$w_k$是任务$k$的权重，决定了各任务在优化过程中的相对重要性。当不同任务的损失函数具有不同的尺度时，直接加权可能导致训练不稳定。梯度归一化方法通过调整梯度范数来实现任务间的平衡：
$$
\hat{g}_k = \frac{g_k}{\|g_k\|_2}, \quad g_{total} = \sum_{k=1}^K w_k \hat{g}_k \tag{4.4.5}
$$
这种归一化确保每个任务对更新方向的贡献在范数上是一致的。分类任务、序列标注任务和序列生成任务在数学上都可以统一为某种形式的预测任务，它们共享Softmax-交叉熵的核心结构。

### 4.4.2 人类反馈强化学习损失
人类反馈强化学习（RLHF）是将人类偏好融入模型训练的关键技术。RLHF包含三个阶段：监督微调阶段使用人类编写的示范数据训练策略模型；奖励模型训练阶段收集人类对不同模型输出的偏好比较数据；策略优化阶段使用奖励模型指导策略优化。
奖励模型训练的目标函数为：
$$
\mathcal{L}_{RM} = -\mathbb{E}_{(x,y^+,y^-)}[\log \sigma(r_\phi(x, y^+) - r_\phi(x, y^-))] \tag{4.4.6}
$$
策略优化阶段的目标为：
$$
\mathcal{L}_{RLHF} = -\mathbb{E}_{x,y \sim \pi_\theta}[r_\phi(x, y)] + \beta \cdot D_{KL}(\pi_\theta \parallel \pi_{ref}) \tag{4.4.7}
$$
KL散度项防止策略模型偏离初始策略太远，避免奖励模型过拟合导致的策略崩溃。最优策略为软最优策略形式：$\pi^*(y \mid x) \propto \pi_{ref}(y \mid x) \exp(\frac{1}{\beta} r_\phi(x, y))$，这与Softmax策略的形式高度相似。

直接偏好优化（DPO）绕过了显式的奖励模型，直接使用偏好数据优化策略。给定偏好对$(y^+, y^-)$，DPO的损失函数为：
$$
\begin{align*}
&\mathcal{L}_{DPO} \\& = -\mathbb{E}_{(x,y^+,y^-)}\left[ \log \sigma\left( \beta \log \frac{\pi_\theta(y^+ \mid x)}{\pi_{ref}(y^+ \mid x)} - \beta \log \frac{\pi_\theta(y^- \mid x)}{\pi_{ref}(y^- \mid x)} \right) \right] \tag{4.4.8}
\end{align*}
$$
DPO损失在形式上是二元交叉熵损失的一种变体。DPO目标关于$\theta$的梯度与PPO目标的梯度在期望意义下是相近的。DPO的梯度结构与InfoNCE高度相似：都是"正确选项梯度减去错误选项梯度"的形式，这表明DPO可以被视为一种"带参考策略的对比学习"。

### 4.4.3 损失函数前沿与未来方向
现有损失函数存在一些固有的局限性。交叉熵损失假设预测分布与真实分布的差异可以通过KL散度度量，但对于分布外样本，模型可能产生过度自信的预测。InfoNCE的目标是最大化互信息的下界，但这个下界可能非常松散。RLHF依赖于人类偏好数据的质量和一致性。
未来的研究方向包括设计自适应的损失函数，根据训练动态自动调整各损失项的权重。一种简单但有效的策略是基于各任务梯度的范数调整权重：$w_k(t) = \frac{1}{\|g_k(t)\|_2 + \epsilon}$，这确保了各任务对参数更新的贡献在尺度上是一致的。此外，损失函数与模型架构的协同设计也是重要的研究方向。

### 4.4.4 本节小结
本节系统性地分析了大语言模型中使用的各种损失函数。语言建模损失和掩码语言建模损失都建立在Softmax-交叉熵框架之上，通过不同的方式定义"正样本"和"负样本"来引导模型学习。多任务学习通过梯度归一化实现任务间的平衡。RLHF通过KL散度约束实现策略正则化，DPO则通过直接优化偏好对数比率实现对齐。理解这些损失函数的数学本质，对于设计和改进大语言模型的训练策略具有重要的理论和实践意义。