---
layout: default
title: 大模型中的数学
---
# Wir müssen wissen, wir werden wissen --David Hilbert
#####                               我们必须知道，我们终将知道 --大卫·希尔伯特

## 第一章 线性代数基础
- [[1.1 线性代数与张量运算]]
- [[1.2 概率论与统计]]
- [[1.3 微积分与优化基础]]
## 第二章 前馈网络数学
- [[2.1 神经元的数学模型]]
- [[2.2 神经网络的矩阵形式]]
- [[2.3 前向传播的数学本质]]
- [[2.4 反向传播梯度推导]]
## 第三章 激活函数与非线性数学
- [[3.1 激活函数的数学角色]]
- [[3.2 导数推导与梯度特性]]
- [[3.3 梯度饱和与梯度爆炸的数学根源]]
## 第四章 损失函数数学
- [[4.1 概率与几何视角下的基本损失函数]]
- [[4.2 损失函数的数学结构与优化性质]]
- [[4.3 Softmax统一框架下的损失函数与注意力机制]]
- [[4.4 大语言模型中的损失函数设计]]
## 第五章 注意力机制数学
- [[5.1 注意力算子的数学定义与概率解释]]
- [[5.2 多头注意力的表示能力与子空间分解]]
- [[5.3 注意力中的稳定性机制：恒等映射与重参数化]]
- [[5.4 注意力如何建模长程依赖]]
- [[5.5 注意力矩阵的谱性质与低秩结构]]
- [[5.6 注意力机制的结构变体]]
## 第六章 位置编码数学
- [[6.1 绝对位置编码：傅里叶基与相位表示]]
- [[6.2 可学习位置编码：参数化偏移与子空间]]
- [[6.3 相对位置编码的统一视角]]
## 第七章 条件计算与稀疏模型：混合专家方法
- [[7.1 条件计算与门控函数的数学模型]]
- [[7.2 专家网络与负载均衡]]
- [[7.3 条件计算下的函数组合与训练优化]]
## 第八章 强化学习
- [[8.1 强化学习基础与马尔可夫决策过程]]
- [[8.2 策略梯度与Actor-Critic方法]]
- [[8.3 PPO算法与大模型训练]]
## 第9章 梯度流、正则化与训练稳定性
- [[9.1 梯度流的随机动力学视角]]
- [[9.2 Jacobian与Hessian与局部稳定性]]
- [[9.3 正则化与归一化]]
- [[9.4 Dropout与噪声注入]]
- [[9.5 稳定性控制的统一数学视角]]
## 第10章 概率与信息论统一视角
- [[10.1 自回归与链式法则]]
- [[10.2 最大似然与交叉熵的详细推导]]
- [[10.3 标度律]]
- [[10.4 信息瓶颈与注意力]]