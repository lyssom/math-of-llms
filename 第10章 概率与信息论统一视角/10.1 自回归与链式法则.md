# 第10章 概率与信息论统一视角
## 10.1 自回归与链式法则
语言建模本质上是对自然语言序列的概率分布进行建模。这种概率视角为大语言模型提供了严格的数学框架，也揭示了自注意力机制和Transformer架构的理论基础。本节将从概率论基本原理出发，推导自回归模型的数学表达，阐释链式法则的核心作用，并展示从N-gram模型到GPT系列的演进过程。

### 10.1.1 语言建模的数学基础
语言建模的第一步是建立数学框架来描述自然语言的结构。设$\mathcal{V}$为词汇表，包含模型能够处理的所有符号。现代大语言模型的词汇表通常包含数万个到数十万个词元（tokens）。
**定义 10.1** 给定词汇表$\mathcal{V}$，长度为$T$的文本序列定义为：
$$
\mathbf{x} = (x_1, x_2, \dots, x_T) \tag{10.1.1}
$$
其中每个$x_t \in \mathcal{V}$表示第$t$个词元。序列空间记为$\mathcal{V}^T$。
序列空间$\mathcal{V}^T$的基数为$|\mathcal{V}|^T$。当$|\mathcal{V}| = 50,000$且$T = 100$时，状态空间大小为$50,000^{100}$，这是一个天文数字。
这种指数级的状态空间爆炸揭示了直接建模联合概率分布$P(\mathbf{x}) = P(x_1, x_2, \dots, x_T)$的不可行性，我们需要链式法则来分解这个高维问题。
为了生成新的文本序列，我们需要在给定历史上下文的情况下预测下一个词元。这种条件概率框架构成了自回归模型的基础。
**定义 10.2** 在生成序列时，我们按照以下条件概率逐步生成：
$$
P(x_1) \quad \text{（生成第一个词元）} 
$$
$$
P(x_2 | x_1) \quad \text{（在给定 } x_1 \text{ 的条件下生成 } x_2\text{）} 
$$
$$
P(x_3 | x_1, x_2) \quad \text{（在给定前两个词元的条件下生成 } x_3\text{）} 
$$
$$
\vdots 
$$
$$
P(x_T | x_1, x_2, \dots, x_{T-1}) \quad \text{（在给定前 } T-1 \text{ 个词元的条件下生成 } x_T\text{）} 
$$
条件概率$P(x_t | x_{<t})$中的历史上下文$x_{<t} = (x_1, x_2, \dots, x_{t-1})$包含了生成第$t$个词元所需的所有相关信息。

### 10.1.2 概率链式法则与自回归分解
解决高维联合分布建模问题的关键在于概率论中的链式法则（Chain Rule of Probability），它允许我们将任何联合概率分布无损地分解为一系列条件概率的乘积。
**定理 10.1（概率链式法则）** 对于任意随机变量序列$(X_1, X_2, \dots, X_T)$，其联合概率分布可以分解为：
$$
\begin{align*}
&P(X_1, X_2, \dots, X_T) \\ &= P(X_1) \cdot P(X_2 | X_1) \cdot P(X_3 | X_1, X_2) \\ &\cdots P(X_T | X_1, X_2, \dots, X_{T-1}) \tag{10.1.2}
\end{align*}
$$
**证明**：使用数学归纳法进行严格证明。
**基础步骤**：当$T=2$时，根据条件概率的定义：
$$
P(X_1, X_2) = P(X_1) \cdot P(X_2 | X_1) \tag{10.1.3}
$$
**归纳步骤**：假设对于长度$T-1$的序列有：
$$
P(X_1, \dots, X_{T-1}) = \prod_{i=1}^{T-1} P(X_i | X_1, \dots, X_{i-1}) \tag{10.1.4}
$$
对于长度为$T$的序列：
$$
P(X_1, \dots, X_T) = P(X_1, \dots, X_{T-1}) \cdot P(X_T | X_1, \dots, X_{T-1}) \tag{10.1.5}
$$
代入归纳假设即可得证。
使用连乘符号，链式法则可以简洁地表示为：
$$
P(\mathbf{x}) = \prod_{t=1}^{T} P(x_t | \mathbf{x}_{<t}) \tag{10.1.6}
$$
其中$\mathbf{x}_{<t} = (x_1, x_2, \dots, x_{t-1})$表示在$x_t$之前的所有词元。
链式法则的分解不仅在数学上优雅，在计算上也是高效的。考虑直接计算$P(\mathbf{x})$与通过链式法则计算的时间复杂度对比。

![[fig_CH10_autoregressive_chain.drawio.png]]
- **直接方法**：计算$P(\mathbf{x})$需要建模$|\mathcal{V}|^T$个可能的序列，空间复杂度为$O(|\mathcal{V}|^T)$，完全不可行
- **链式法则方法**：通过$T$个条件概率的乘积计算，每个条件概率$P(x_t | \mathbf{x}_{<t})$的计算复杂度取决于模型架构
给定训练好的自回归模型，生成一个长度为$T$的序列需要$O(T)$次前向传播，每次传播的复杂度由模型架构决定。
在实际应用中，我们经常需要控制生成文本的随机性和多样性，通过引入温度参数$T > 0$来实现。
**定义 10.3** 给定原始概率分布$P_\theta(x_t | \mathbf{x}_{<t})$，温度缩放后的分布定义为：
$$
P_{\theta,T}(x_t | \mathbf{x}_{<t}) = \frac{\exp(\log P_\theta(x_t | \mathbf{x}_{<t}) / T)}{\sum_{x \in \mathcal{V}} \exp(\log P_\theta(x | \mathbf{x}_{<t}) / T)} \tag{10.1.7}
$$
**温度效应**
- 当$T \to 0$时，分布趋向于确定性的one-hot分布（贪心解码）
- 当$T \to \infty$时，分布趋向于均匀分布
- 当$T = 1$时，保持原始概率分布

### 10.1.3 神经网络参数化与似然函数
将自回归模型的概念转化为实际可计算的算法需要通过神经网络来参数化条件概率分布。神经网络作为函数逼近器，能够学习复杂的条件概率函数$P_\theta(x_t | \mathbf{x}_{<t})$。
**定义 10.4** 对于给定的上下文$\mathbf{x}_{<t}$，神经网络输出一个概率向量：
$$
\mathbf{p}_t = \text{Softmax}(f_\theta(\mathbf{x}_{<t})) \tag{10.1.8}
$$
其中$f_\theta: \mathcal{V}^{t-1} \to \mathbb{R}^{|\mathcal{V}|}$是由参数$\theta$定义的神经网络函数，Softmax函数确保输出向量满足概率分布的约束（元素非负且和为1）。
Softmax函数$\text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^d e^{z_j}}$具有以下重要性质：
1. 输出向量$\text{softmax}(\mathbf{z})$是概率分布
2. 对于任意输入$\mathbf{z}$和标量$\alpha$，有$\text{softmax}(\alpha \mathbf{z})_i = \frac{e^{\alpha z_i}}{\sum_j e^{\alpha z_j}}$，这与温度缩放相关
3. 对数Softmax的计算稳定性：$\log \text{softmax}(\mathbf{z})_i = z_i - \log \sum_{j=1}^d e^{z_j}$
在统计学习理论中，我们通过最大化数据在模型下的似然来学习参数。对于自回归模型，似然函数具有特殊的乘积结构。
**定义 10.5** 给定训练数据集$\mathcal{D} = \{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\}$，参数$\theta$下的似然函数为：
$$
\mathcal{L}(\theta) = \prod_{n=1}^{N} \prod_{t=1}^{T_n} P_\theta(x_t^{(n)} | \mathbf{x}_{<t}^{(n)}) \tag{10.1.9}
$$
**定义 10.6** 对数似然函数定义为：
$$
\ell(\theta) = \log \mathcal{L}(\theta) = \sum_{n=1}^{N} \sum_{t=1}^{T_n} \log P_\theta(x_t^{(n)} | \mathbf{x}_{<t}^{(n)}) \tag{10.1.10}
$$
对于自回归模型，最大化对数似然等价于最小化交叉熵损失：
$$
\max_\theta \ell(\theta) \equiv \min_\theta \left( -\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} \log P_\theta(x_t^{(n)} | \mathbf{x}_{<t}^{(n)}) \right) \tag{10.1.11}
$$
对数似然函数的梯度可以分解为各位置梯度的和：
$$
\nabla_\theta \ell(\theta) = \sum_{n=1}^{N} \sum_{t=1}^{T_n} \nabla_\theta \log P_\theta(x_t^{(n)} | \mathbf{x}_{<t}^{(n)}) \tag{10.1.12}
$$
这种分解性使得梯度可以通过反向传播算法高效计算。

### 10.1.4 从N-gram到GPT的演进
经典的N-gram模型是最早将链式法则应用于语言建模的尝试，但它引入了马尔可夫假设的限制性条件。
**定义 10.7** N-gram模型假设当前词元只依赖于前$N-1$个词元：
$$
P(x_t | \mathbf{x}_{<t}) \approx P(x_t | x_{t-N+1}, \dots, x_{t-1}) \tag{10.1.13}
$$
当$N=2$时，得到二元语法（Bigram）模型：
$$
P(x_t | \mathbf{x}_{<t}) \approx P(x_t | x_{t-1}) \tag{10.1.14}
$$
*马尔可夫假设在以下方面存在不足：
1. **长距离依赖丢失**：无法捕获相距较远的词元之间的语法和语义关系
2. **上下文长度固定**：无法根据任务需要调整依赖范围
3. **数据稀疏性**：随着N增加，N-gram组合的数量呈指数增长
现代大语言模型（如GPT系列）通过Transformer架构克服了N-gram模型的局限性，实现了真正的自回归建模而不依赖马尔可夫假设。
**定义 10.8** 给定输入序列$\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T]$，自注意力机制计算：
$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}}\right) \mathbf{V} \tag{10.1.15}
$$
其中查询矩阵$\mathbf{Q} = \mathbf{X}\mathbf{W}_Q$，键矩阵$\mathbf{K} = \mathbf{X}\mathbf{W}_K$，值矩阵$\mathbf{V} = \mathbf{X}\mathbf{W}_V$。
在训练阶段，Transformer通过掩码注意力确保位置$t$只能看到位置$1$到$t-1$，从而严格遵循自回归的数学框架：
$$
\text{Masked-Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}} + \mathbf{M}\right) \mathbf{V} \tag{10.1.16}
$$
其中$\mathbf{M}$是上三角掩码矩阵，确保位置$t$的查询不会注意到位置$t$及之后的键。

**表 10.1.1 语言建模方法的对比**

| 方法 | 概率形式 | 上下文长度 | 主要优势 | 主要局限 |
| --- | --- | --- | --- | --- |
| N-gram | $P(x_t \mid x_{t-1}, \dots, x_{t-N+1})$ | 固定 $N-1$ | 计算简单 | 丢失长依赖 |
| RNN | $P(x_t \mid \mathbf{h}_{t-1})$ | 理论无限 | 顺序处理 | 梯度消失 |
| Transformer | $P(x_t \mid \mathrm{Attention}(x_{<t}))$ | 可变长度 | 并行计算 | 复杂度 $O(T^2)$ |
| GPT | 同 Transformer | 可变长度 | 大规模预训练 | 训练成本高 |

### 10.1.5 本节小结
本节从概率论基本原理出发，系统地建立了自回归语言模型的数学框架。我们利用概率链式法则，将高维联合概率分布转化为一系列可计算的条件概率建模问题。通过神经网络参数化条件概率函数，并使用最大似然估计学习模型参数。对数似然与交叉熵损失的等价性为实际训练提供了理论基础。
通过对比经典的N-gram模型和现代的Transformer架构，我们看到了自回归建模思想如何在不同历史阶段以不同形式得到体现。N-gram模型受限于马尔可夫假设，而Transformer通过自注意力机制实现了真正的全上下文依赖，完美契合了自回归的数学框架。
这一概率视角不仅为大语言模型提供了理论框架，更为后续探讨注意力机制、位置编码、预训练技术等高级主题奠定了基础。