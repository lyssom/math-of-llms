## 5.2 多头注意力的表示能力与子空间分解
多头注意力（Multi-Head Attention）是Transformer架构的核心创新之一，它通过并行运行多个独立的注意力头，极大地增强了模型的表示能力。在单头注意力的框架下，模型只能在同一组语义空间中计算输入序列各位置之间的关联强度，这种单一的"视角"限制了模型捕获复杂语义关系的能力。类比于人类认知过程中会从多个角度、多个层面理解信息，多头注意力机制允许每个头专注于捕获输入数据的不同特征或不同类型的关联模式。本节将从子空间分解的角度深入分析多头注意力的表示能力，系统推导其数学结构，并探讨头数与头维度之间的权衡关系。通过本节的学习，读者将建立起对多头注意力如何增强模型表达能力的深刻理解。

### 5.2.1 从单头到多头的表示能力扩展动机
在深入数学推导之前，我们首先需要理解引入多头注意力的动机。考虑一个简单的例子：假设模型需要处理一个包含"猫坐在垫子上"这句话的序列。在单一的注意力机制下，模型只能学习到一种类型的关联模式，例如"猫"和"坐在"之间存在某种关联。但实际上，这句话中存在着多种语义关系：语法关系（"猫"是主语，"坐在"是谓语）、语义角色关系（"猫"是"坐"的主题）、空间关系（"垫子"是"坐"的处所）等。单一的注意力头难以同时捕获所有这些不同层面的关联信息。
多头注意力通过设置多个独立的注意力头来解决这个问题。每个注意力头都有自己独立的Query、Key、Value投影矩阵，因此可以学习到不同的特征表示和关联模式。某些头可能专注于学习语法结构，某些头可能专注于学习语义角色，还有某些头可能专注于学习位置相近词语之间的局部关联。这种"分而治之"的策略使得模型能够从多个角度同时建模输入数据，显著提升了表示的丰富性和任务的性能。
从数学角度来看，单头注意力可以被视为一个特定的核函数，它将输入映射到一个特征空间后计算相似度。多头注意力则可以被视为多个核函数的组合，每个核函数关注输入数据的不同方面。这种组合策略在核方法文献中被称为"多核学习"，已被证明在许多任务上优于单一核函数的使用。不同的头对应于不同的核函数，它们并行计算，最后通过线性组合（输出投影）融合结果。这种结构使得多头注意力能够逼近更广泛的函数类，具有更强的表示能力。

### 5.2.2 多头注意力的数学定义与矩阵结构
多头注意力的数学定义如公式（5.2.1）所示。设多头注意力的头数为$h$，每个头的维度为$d_k = d_v = d_{\text{model}}/h$，则多头注意力的计算过程为：
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h)W_O \tag{5.2.1}
$$
其中，每个注意力头$\text{head}_i$的计算与单头注意力相同，但使用各自独立的投影矩阵：
$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \tag{5.2.2}
$$
这里，$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$、$W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$、$W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$是第$i$个头的Query、Key、Value投影矩阵。$W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$是输出投影矩阵，将拼接后的多头输出映射回原始模型维度$d_{\text{model}}$。
公式（5.2.1）和（5.2.2）完整地描述了多头注意力的计算过程。首先，每个头独立地对自己的Query、Key、Value进行注意力计算，得到各自的输出$\text{head}_i$；然后，这些头输出按维度拼接起来，形成一个维度为$n \times (h \cdot d_v)$的矩阵；最后，通过输出投影矩阵$W_O$将拼接结果映射回$d_{\text{model}}$维空间，得到多头注意力的最终输出。
在实际实现中，多头注意力的投影计算可以通过矩阵运算高效地并行完成。首先，将所有头的Query投影矩阵堆叠成一个大的投影矩阵：
$$
W_Q^{\text{all}} = \begin{bmatrix} W_1^Q \\ W_2^Q \\ \vdots \\ W_h^Q \end{bmatrix} \in \mathbb{R}^{(h \cdot d_k) \times d_{\text{model}}} \tag{5.2.3}
$$
注意这里矩阵堆叠的方向——将$h$个头的投影矩阵按行堆叠，因此输出维度为$h \cdot d_k$。输入矩阵$X \in \mathbb{R}^{n \times d_{\text{model}}}$与堆叠后的投影矩阵相乘：$Q^{\text{all}} = XW_Q^{\text{all}} \in \mathbb{R}^{n \times (h \cdot d_k)}$，结果包含了所有头的Query向量。类似地，可以得到$K^{\text{all}}$和$V^{\text{all}}$。接下来，需要将结果按头进行拆分，通过张量重塑和转置操作将维度排列为$h \times n \times d_k$，这样每个头$i$的Query向量就位于张量的第$i$层。

### 5.2.3 参数数量与子空间分解结构
多头注意力引入了额外的参数，但这些参数的设计遵循着精心的配置。假设模型隐藏维度为$d_{\text{model}}$，头数为$h$，每个头的维度为$d_k = d_v = d_{\text{model}}/h$，则各部分的参数数量分析如下。
对于Query投影：$h$个头的投影矩阵总参数量为$h \times (d_{\text{model}} \times d_k) = h \times d_{\text{model}} \times (d_{\text{model}}/h) = d_{\text{model}}^2$。类似地，Key投影和Value投影的参数数量也各为$d_{\text{model}}^2$。因此，QKV投影的总参数量为$3d_{\text{model}}^2$。对于输出投影：$W_O$的维度为$(h \cdot d_v) \times d_{\text{model}} = d_{\text{model}}^2$。
这意味着，尽管多头注意力有$h$个头，但总参数量与单头注意力（使用$d_{\text{model}} \times d_{\text{model}}$的QKV投影）完全相同，都是$4d_{\text{model}}^2$。这是一个非常重要的特性：多头机制在保持参数总量不变的情况下，通过并行化计算增加了表示的丰富性。这种"免费"的能力增强是多头注意力设计的精妙之处。
从重参数化的角度审视多头注意力，我们可以发现一个有趣的结构。考虑多头注意力中$h$个头的Query投影矩阵$W_i^Q$的集合。从输入空间到查询空间的映射可以写为：
$$
Q^{\text{all}} = X \cdot \text{BlockDiag}(W_1^Q, W_2^Q, \ldots, W_h^Q) \tag{5.2.4}
$$
其中$\text{BlockDiag}(\cdot)$表示分块对角矩阵，每个块对应一个头的投影。分块对角结构意味着不同头的投影在输入空间中是不相交的——每个输入维度只贡献给特定的头。这种设计引入了一种归纳偏置：不同头的学习任务是相对独立的，每个头处理输入的不同"切片"。然而，由于输入嵌入$X$的各维度之间存在相关性，不同头之间仍然存在间接的交互。输出投影矩阵$W_O$会将所有头的信息混合起来，因此最终输出中各头的信息是相互交织的。

### 5.2.4 多头机制与子空间学习理论
多头注意力的表达能力提升可以从子空间学习的角度来理解。设每个头的Query、Key、Value投影定义了从输入空间到某个$d_k$维子空间的映射。由于$W_i^Q$、$W_i^K$、$W_i^V$是独立学习的，不同的头对应于不同的投影方向。考虑两个不同的头$i$和$j$，它们各自定义的子空间$\mathcal{S}_i$和$\mathcal{S}_j$可能相交，也可能正交，取决于投影矩阵的学习结果。
多头注意力的输出可以被视为在多个子空间中进行注意力计算的融合结果。对于输入序列中的每个位置，模型可以在不同的子空间中计算其与其他位置的关联强度，然后将这些关联信息进行融合。这种在多个子空间中的并行计算赋予了多头注意力更强的特征提取能力。
从线性代数的角度，我们可以分析多头注意力能够表示的函数类。设$\mathcal{F}_1$是单头注意力能够表示的函数类，$\mathcal{F}_h$是多头注意力能够表示的函数类。可以证明，$\mathcal{F}_h$严格包含$\mathcal{F}_1$，但存在一些函数可以用多头注意力表示而无法用单头注意力表示。一个简单的例子是"非对称的成对交互"：假设我们需要位置$i$和位置$j$之间的关联强度依赖于某种复杂的非线性函数，多个头可以通过各自的非线性变换组合出更丰富的交互模式。
从函数逼近理论的角度分析，多头注意力可以被视为一种"基函数展开"。考虑一个单层的多头注意力模块，其输出可以写为：
$$
f(X) = \text{Concat}\left( \phi_1(X), \ldots, \phi_h(X) \right) W_O \tag{5.2.5}
$$
其中，$\phi_i(X) = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$是第$i$个头定义的"基函数"。每个基函数$\phi_i$将输入矩阵$X$映射到一个$n \times d_k$的注意力输出，表示输入在特定子空间中的关联模式。基函数展开的理论保证告诉我们，如果基函数集合足够丰富，任意连续函数都可以用这些基函数的线性组合来逼近。多头注意力通过学习不同的投影矩阵，实际上是在自适应地构造基函数集合。

### 5.2.5 注意力头的分工与专业化机制
实证研究表明，训练良好的Transformer中的不同注意力头确实会发展出不同的"专业技能"。通过分析注意力权重矩阵的分布和头的输出表示，研究者发现某些头专注于学习局部上下文信息（如相邻词之间的关系），某些头专注于学习语法依存关系（如主语-谓语关系），还有一些头专注于学习语义相似性（如同义词或上下文中相关的词）。
从数学角度分析这种分工现象，我们可以考虑注意力头的输出表示之间的相关性。设$\text{head}_i$和$\text{head}_j$分别是两个头的输出，计算它们表示之间的余弦相似度：
$$
\rho_{ij} = \frac{\text{head}_i \cdot \text{head}_j}{\|\text{head}_i\| \|\text{head}_j\|} \tag{5.2.6}
$$
如果两个头学习到相似的特征，它们的输出表示会高度相关，$\rho_{ij}$接近1；如果两个头学习到正交的特理，$\rho_{ij}$接近0。实验观察表明，训练良好的Transformer中不同头的输出表示往往具有较低的相关性，这意味着各个头确实学习到了互补的信息。
这种专业化现象部分源于随机初始化和训练过程中的随机梯度下降。不同的头从不同的随机起点开始优化，逐渐收敛到不同的局部最优解。初始的微小差异通过训练过程的放大，形成了明显的分工模式。这与深度学习中常见的"对称性破缺"现象类似，相同的架构从对称的初始状态出发，最终学习到不对称的解。
在设计多头注意力架构时，头数$h$和头维度$d_k$的选择是一个重要的权衡问题。根据标准配置，我们通常有$d_k = d_v = d_{\text{model}}/h$，因此增加头数意味着减小每个头的维度，反之亦然。从表达能力的角度分析，较多的头数意味着更多的子空间并行计算，每个头可以专注于更细粒度的特征；但每个头的维度变小，可能限制了其捕获复杂模式的能力。实践中，Transformer的原始论文使用了$h = 8$个头的配置（隐藏维度$d_{\text{model}} = 512$，头维度$d_k = 64$），后续的模型如BERT使用了类似的配置，而GPT-3则增加了头数到96个。

### 5.2.6 本节小结
本节系统地探讨了多头注意力的表示能力与子空间分解。我们从多头注意力的设计动机出发，解释了为何单一注意力头难以捕获复杂的多层次语义关系，而多头机制通过并行多个独立的注意力头来解决这个问题。每个头有自己的Query、Key、Value投影矩阵，可以学习到不同的特征表示和关联模式。
我们深入分析了多头注意力的参数结构，揭示了一个重要的数学事实：尽管多头注意力有$h$个注意力头，但其总参数量与单头注意力完全相同。这意味着多头机制在"免费"地增加表示的丰富性，通过并行计算在多个子空间中提取特征，而不增加计算成本。我们还从子空间学习的角度论证了多头注意力能够表示比单头注意力更丰富的函数类，展示了分块对角投影结构如何引入归纳偏置，以及输出投影如何融合不同头的信息。
我们讨论了训练良好的模型中不同头会发展出"专业化"特征的现象，通过输出表示的相关性分析揭示了各头学习到的互补信息。最后，我们分析了头数与头维度之间的权衡关系，以及这种设计选择如何影响模型的表达能力和计算效率。多头注意力通过其精巧的数学结构，实现了表示能力的显著提升，成为Transformer架构成功的关键因素之一。