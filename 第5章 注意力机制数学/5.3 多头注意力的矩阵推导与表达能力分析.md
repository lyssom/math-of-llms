在前面两节中，我们详细探讨了Scaled Dot-Product Attention的数学定义与Query、Key、Value的矩阵变换。然而，单一的注意力头在表达能力上存在固有的局限性，它只能在同一组语义空间中计算输入序列各位置之间的关联强度。类比于人类认知过程中会从多个角度、多个层面理解信息，Multi-Head Attention（多头注意力）机制通过并行运行多个独立的注意力头，每个头可以专注于捕获输入数据的不同特征或不同类型的关联模式，从而极大地增强了模型的表达能力。

多头注意力的设计是Transformer架构的核心创新之一。它不仅在实践中取得了显著的性能提升，更在理论上展示了如何通过参数共享和并行计算来实现丰富的表示学习。本节将从矩阵推导的角度系统地阐述多头注意力的计算过程，深入分析其参数结构和计算复杂度，并从表示学习的理论视角探讨多头机制如何增强模型的表达能力。通过本节的学习，读者将建立起对多头注意力数学本质的完整理解。

## 5.3.1 从单头到多头的动机

在深入数学推导之前，我们首先需要理解引入多头注意力的动机。考虑一个简单的例子：假设模型需要处理一个包含"猫坐在垫子上"这句话的序列。在单一的注意力机制下，模型只能学习到一种类型的关联模式——例如，"猫"和"坐在"之间存在某种关联。但实际上，这句话中存在着多种语义关系：语法关系（"猫"是主语，"坐在"是谓语）、语义角色关系（"猫"是"坐"的主题）、空间关系（"垫子"是"坐"的处所）等。单一的注意力头难以同时捕获所有这些不同层面的关联信息。

多头注意力通过设置多个独立的注意力头来解决这个问题。每个注意力头都有自己独立的Query、Key、Value投影矩阵，因此可以学习到不同的特征表示和关联模式。某些头可能专注于学习语法结构，某些头可能专注于学习语义角色，还有某些头可能专注于学习位置相近词语之间的局部关联。这种"分而治之"的策略使得模型能够从多个角度同时建模输入数据，显著提升了表示的丰富性和任务的性能。

从数学角度来看，单头注意力可以被视为一个特定的核函数，它将输入映射到一个特征空间后计算相似度。多头注意力则可以被视为多个核函数的组合，每个核函数关注输入数据的不同方面。这种组合策略在核方法文献中被称为"多核学习"（Multiple Kernel Learning），已被证明在许多任务上优于单一核函数的使用。

## 5.3.2 多头注意力的数学定义

多头注意力的数学定义如公式（5.3.1）所示。设多头注意力的头数为$h$，每个头的维度为$d_k = d_v = d_{\text{model}}/h$，则多头注意力的计算过程为：
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h)W_O \tag{5.3.1}
$$
其中，每个注意力头$\text{head}_i$的计算与单头注意力相同，但使用各自独立的投影矩阵：
$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) \tag{5.3.2}
$$
这里，$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}​、W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}、W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$是第$i$个头的Query、Key、Value投影矩阵。$W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$是输出投影矩阵，将拼接后的多头输出映射回原始模型维度$d_{\text{model}}$。

公式（5.3.1）和（5.3.2）完整地描述了多头注意力的计算过程。首先，每个头独立地对自己的Query、Key、Value进行注意力计算，得到各自的输出$\text{head}_i$；然后，这些头输出按维度拼接起来，形成一个维度为$n \times (h \cdot d_v)$的矩阵；最后，通过输出投影矩阵$W_O$将拼接结果映射回$d_{\text{model}}$维空间，得到多头注意力的最终输出。

## 5.3.3 参数数量与维度配置

多头注意力引入了额外的参数，但这些参数的设计遵循着精心的配置。假设模型隐藏维度为$d_{\text{model}}$，头数为$h$，每个头的维度为$d_k = d_v = d_{\text{model}}/h$，则各部分的参数数量分析如下。

对于Query投影：$h$个头的投影矩阵总参数量为$h \times (d_{\text{model}} \times d_k) = h \times d_{\text{model}} \times (d_{\text{model}}/h) = d_{\text{model}}^2$。类似地，Key投影和Value投影的参数数量也各为$d_{\text{model}}^2$。因此，QKV投影的总参数量为$3d_{\text{model}}^2$​。

对于输出投影：$W_O$​的维度为$(h \cdot d_v) \times d_{\text{model}} = (h \cdot d_{\text{model}}/h) \times d_{\text{model}} = d_{\text{model}}^2$。

这意味着，尽管多头注意力有$h$个头，但总参数量与单头注意力（使用$d_{\text{model}} \times d_{\text{model}}$​的QKV投影）完全相同，都是$4d_{\text{model}}^2$。这是一个非常重要的特性：多头机制在保持参数总量不变的情况下，通过并行化计算增加了表示的丰富性。这种"免费"的能力增强是多头注意力设计的精妙之处。
## 5.3.4 投影矩阵的并行计算

在实际实现中，多头注意力的投影计算可以通过矩阵运算高效地并行完成。首先，我们将所有头的Query投影矩阵堆叠成一个大的投影矩阵：
$$
W_Q^{\text{all}} = \begin{bmatrix} W_1^Q \\ W_2^Q \\ \vdots \\ W_h^Q \end{bmatrix} \in \mathbb{R}^{(h \cdot d_k) \times d_{\text{model}}} \tag{5.3.3}
$$
注意这里矩阵堆叠的方向——我们将$h$个头的投影矩阵按行堆叠，因此输出维度为$h \cdot d_k$。类似地，可以构造$W_K^{\text{all}}$和$W_V^{\text{all}}$。

输入矩阵$X \in \mathbb{R}^{n \times d_{\text{model}}}$与堆叠后的投影矩阵相乘：
$$
Q^{\text{all}} = XW_Q^{\text{all}} \in \mathbb{R}^{n \times (h \cdot d_k)} \tag{5.3.4}
$$
结果$Q^{\text{all}}$包含了所有头的Query向量。类似地，可以得到$K^{\text{all}}$和$V^{\text{all}}$。

接下来，需要将$Q^{\text{all}}、K^{\text{all}}、V^{\text{all}}$按头进行拆分。常用的方法是通过张量重塑（reshape）和转置操作。设当前矩阵形状为$n \times (h \cdot d_k)$，首先重塑为$n \times h \times d_k$的三维张量，然后通过转置将维度排列为$h \times n \times d_k$​。这样，每个头$i$的Query向量$Q_i$​就位于张量的第$i$层。

## 5.3.5 多头注意力的张量计算形式

从张量代数的角度来看，多头注意力可以优雅地表示为一个紧凑的运算序列。设输入嵌入张量为$\mathcal{X} \in \mathbb{R}^{n \times d_{\text{model}}}$，投影权重张量$\mathcal{W}^Q \in \mathbb{R}^{h \times d_k \times d_{\text{model}}}$（其中$h \cdot d_k = d_{\text{model}}h⋅d$），则Query张量的计算为：
$$
\mathcal{Q}_{i,n,:} = \sum_{d=1}^{d_{\text{model}}} X_{n,d} \cdot W^Q_{i,k,d} \quad \forall i \in [h], n \in [n] \tag{5.3.5}
$$
这个运算可以视为张量收缩（Tensor Contraction）。类似的计算对Key和Value重复进行。

注意力的张量形式强调了多头的结构：每个头$i$的Query、Key、Value张量切片$(\mathcal{Q}_i, \mathcal{K}_i, \mathcal{V}_i)$独立地进行Scaled Dot-Product Attention计算：
$$
\mathcal{O}_i = \text{Attention}(\mathcal{Q}_i, \mathcal{K}_i, \mathcal{V}_i) \in \mathbb{R}^{n \times d_k} \tag{5.3.6}
$$
所有头的输出张量$\mathcal{O}_i$​沿头维度拼接：
$$
\mathcal{O}_{\text{concat}} = \text{Concat}_i(\mathcal{O}_i) \in \mathbb{R}^{n \times (h \cdot d_k)} \tag{5.3.7}
$$
最后，通过输出投影矩阵$W_O$进行最终的线性变换：
$$
\text{MultiHeadOutput} = \mathcal{O}_{\text{concat}} W_O \in \mathbb{R}^{n \times d_{\text{model}}} \tag{5.3.8}
$$
张量形式的表示虽然符号上更为复杂，但它清晰地展示了多头注意力的并行结构，便于进行理论分析和硬件优化。现代深度学习框架（如PyTorch的TensorFlow的张量计算）都对这种张量运算进行了高度优化。

## 5.3.6 注意力头的重参数化视角

从重参数化（Reparameterization）的角度审视多头注意力，我们可以发现一个有趣的结构。考虑两个相邻的多头注意力层：第一层的输出是第二层的输入。设第一层的输出为$X^{(1)}$，第二层的输入为$X^{(2)}$，则：
$$
X^{(2)} = \text{LayerNorm}(X^{(1)} + \text{MultiHead}(X^{(1)}W_Q^{\text{all}}, X^{(1)}W_K^{\text{all}}, X^{(1)}W_V^{\text{all}})W_O) \tag{5.3.9}
$$
整个计算可以视为对输入$X^{(1)}$进行一系列线性变换和非线性变换的组合。多头注意力中的拼接和输出投影操作，实际上定义了输入空间的一种特定的分块对角结构。

具体而言，考虑多头注意力中$h$个头的Query投影矩阵$W_i^Q$​的集合。从输入空间到查询空间的映射可以写为：
$$
Q^{\text{all}} = X \cdot \text{BlockDiag}(W_1^Q, W_2^Q, \ldots, W_h^Q) \tag{5.3.10}
$$
其中$\text{BlockDiag}(\cdot)$表示分块对角矩阵，每个块对应一个头的投影。分块对角结构意味着不同头的投影在输入空间中是不相交的——每个输入维度只贡献给特定的头。这种设计引入了一种归纳偏置：不同头的学习任务是相对独立的，每个头处理输入的不同"切片"。

然而，值得注意的是，虽然投影矩阵在结构上是分块对角的，但由于输入嵌入$X$的各维度之间存在相关性，不同头之间仍然存在间接的交互。此外，输出投影矩阵$W_O$会将所有头的信息混合起来，因此最终输出中各头的信息是相互交织的。
## 5.3.7 多头机制与子空间学习

多头注意力的表达能力提升可以从子空间学习的角度来理解。设每个头的Query、Key、Value投影定义了从输入空间到某个$d_k$​维子空间的映射。由于$W_i^Q、W_i^K​、W_i^V$​是独立学习的，不同的头对应于不同的投影方向。考虑两个不同的头$i$和$j$，它们各自定义的子空间$\mathcal{S}_i$和$\mathcal{S}_j$可能相交，也可能正交，取决于投影矩阵的学习结果。

多头注意力的输出可以被视为在多个子空间中进行注意力计算的融合结果。对于输入序列中的每个位置，模型可以在不同的子空间中计算其与其他位置的关联强度，然后将这些关联信息进行融合。这种在多个子空间中的并行计算赋予了多头注意力更强的特征提取能力。

从线性代数的角度，我们可以分析多头注意力能够表示的函数类。设$\mathcal{F}_1$​是单头注意力能够表示的函数类，$\mathcal{F}_h$​是多头注意力能够表示的函数类。可以证明，$\mathcal{F}_h$严格包含$\mathcal{F}_1$​，但存在一些函数可以用多头注意力表示，无法用单头注意力表示。一个简单的例子是"非对称的成对交互"：假设我们需要位置$i$和位置$j$之间的关联强度依赖于某种复杂的非线性函数，多个头可以通过各自的非线性变换组合出更丰富的交互模式。

## 5.3.8 与混合专家模型的类比

多头注意力与Mixture of Experts（MoE，混合专家）模型有着有趣的类比。在MoE模型中，不同的"专家"网络处理输入的不同部分，最终输出是各专家输出的加权组合。多头注意力中的"头"可以类比为"专家"：每个头有自己的参数（QKV投影），独立处理输入，最终通过拼接和投影进行融合。

然而，两者之间也存在重要的区别。在MoE中，通常使用门控机制（Gating Mechanism）来决定各专家的权重，权重依赖于输入数据；而在标准的多头注意力中，各头的输出被简单地拼接起来，不进行显式的加权混合。输出投影矩阵$W_O$​隐含地学习了一种加权方案，但它是在训练过程中从数据中学习的，而非显式地依赖于输入。

从计算复杂度的角度看，多头注意力与MoE也有所不同。MoE通常需要为每个输入样本选择活跃的专家子集（稀疏激活），以控制计算成本；而标准的多头注意力在所有头上都进行完整的计算，计算量与头数成正比。当然，后续的研究也提出了稀疏激活的多头注意力变体，在保持表达能力的同时降低计算成本。

## 5.3.9 注意力头的分工与专业化

实证研究表明，训练良好的Transformer中的不同注意力头确实会发展出不同的"专业技能"。通过分析注意力权重矩阵的分布和头的输出表示，研究者发现某些头专注于学习局部上下文信息（如相邻词之间的关系），某些头专注于学习语法依存关系（如主语-谓语关系），还有一些头专注于学习语义相似性（如同义词或上下文中相关的词）。

从数学角度分析这种分工现象，我们可以考虑注意力头的输出表示之间的相关性。设$\text{head}_i$​和$\text{head}_j$分别是两个头的输出，计算它们表示之间的余弦相似度：
$$
\rho_{ij} = \frac{\text{head}_i \cdot \text{head}_j}{\|\text{head}_i\| \|\text{head}_j\|} \tag{5.3.11}
$$
如果两个头学习到相似的特征，它们的输出表示会高度相关，$\rho_{ij}$​接近1；如果两个头学习到正交的特征，$\rho_{ij}$​接近0。实验观察表明，训练良好的Transformer中不同头的输出表示往往具有较低的相关性，这意味着各个头确实学习到了互补的信息。

这种专业化现象部分源于随机初始化和训练过程中的随机梯度下降。不同的头从不同的随机起点开始优化，逐渐收敛到不同的局部最优解。初始的微小差异通过训练过程的放大，形成了明显的分工模式。这与深度学习中常见的"对称性破缺"现象类似，相同的架构从对称的初始状态出发，最终学习到不对称的解。

## 5.3.10 头数与头维度的权衡

在设计多头注意力架构时，头数$h$和头维度$d_k$（以及$d_v$）的选择是一个重要的权衡问题。根据标准配置，我们通常有$d_k = d_v = d_{\text{model}}/h$，因此增加头数意味着减小每个头的维度，反之亦然。

从表达能力的角度分析，较多的头数意味着更多的子空间并行计算，每个头可以专注于更细粒度的特征；但每个头的维度变小，可能限制了其捕获复杂模式的能力。较少的头数意味着更大的头维度，每个头有更强的计算能力；但并行计算的"广度"降低，可能无法同时捕获足够多样的特征。

实践中，Transformer的原始论文使用了$h = 8$个头的配置（隐藏维度$d_{\text{model}} = 512$，头维度$d_k = 64$）。后续的模型如BERT使用了类似的配置，而GPT-3则增加了头数到96个（隐藏维度12288，头维度128）。这些配置的选择通常基于经验性的消融实验和计算资源约束。

从理论分析的角度，我们可以考虑注意力头的有效容量。设输入序列的长度为$n$，每个头的维度为$d_k$​，则该头的注意力权重矩阵有$n$个自由度（每行是一个概率分布），参数数量为$O(d_{\text{model}} \cdot d_k)$。当头数增加时，虽然总参数数量不变，但每个头的参数数量减少，过拟合的风险降低，模型的泛化能力可能提升；但同时，模型的表示能力可能受到限制。
## 5.3.11 多头注意力的复杂度

多头注意力的时间复杂度分析需要分别考虑各个计算阶段。设序列长度为$n$，隐藏维度为$d_{\text{model}}$​，头数为$h$，每个头的维度为$d_k = d_{\text{model}}/h$。

对于Query、Key、Value的投影阶段：输入矩阵$X \in \mathbb{R}^{n \times d_{\text{model}}}$​与堆叠投影矩阵$W_Q^{\text{all}} \in \mathbb{R}^{d_{\text{model}} \times (h \cdot d_k)}$相乘，时间复杂度为$O(n \cdot d_{\text{model}} \cdot h \cdot d_k) = O(n \cdot d_{\text{model}}^2)$。由于$h \cdot d_k = d_{\text{model}}$，这个复杂度与单头注意力相同。

对于注意力计算阶段：每个头独立进行$O(n^2 \cdot d_k)$的注意力计算，$h$个头的总复杂度为$h \cdot O(n^2 \cdot d_k) = O(n^2 \cdot h \cdot d_k) = O(n^2 \cdot d_{\text{model}})$。这同样与单头注意力相同。

对于输出投影阶段：拼接后的输出矩阵（维度$n \times d_{\text{model}}$​）与$W_O \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$相乘，复杂度为$O(n \cdot d_{\text{model}}^2)$。

综合以上三个阶段，多头注意力的总时间复杂度为$O(n^2 \cdot d_{\text{model}} + n \cdot d_{\text{model}}^2)$。对于常见的$n \ll d_{\text{model}}$的场景（如短序列处理），主导项是$O(n \cdot d_{\text{model}}^2)$；对于长序列场景（如$n$与$d_{\text{model}}$​可比），主导项是$O(n^2 \cdot d_{\text{model}})$。

重要的是，这个复杂度分析与头数$h$无关——无论设置多少个头，总计算量保持不变。这是多头注意力设计的另一个精妙之处：增加头数不增加计算成本，但能提升表示的丰富性。

多头注意力的空间复杂度分析与时间复杂度类似。考虑需要存储的中间结果：
Query、Key、Value的投影结果：$Q^{\text{all}}$、$K^{\text{all}}$、$V^{\text{all}}$的维度均为$n \times d_{\text{model}}$，总空间为$O(n \cdot d_{\text{model}})$。
注意力分数矩阵：每个头的注意力分数矩阵维度为$n \times n$，$h$个头的总空间为$O(h \cdot n^2) = O(n^2 \cdot d_{\text{model}}/d_k)$。由于$d_k = d_{\text{model}}/h$，这等于$O(h \cdot n^2)$。注意这个空间需求与头数成正比。输出投影后的结果：维度为$n \times d_{\text{model}}$，空间为$O(n \cdot d_{\text{model}})$。

综合来看，多头注意力的空间复杂度为$O(n^2 \cdot h + n \cdot d_{\text{model}})$。在标准配置（$h = d_{\text{model}}/d_k$​）下，空间复杂度与单头注意力相当。但在某些配置下（如固定$d_k$​，增加$h$），空间需求会增加。值得注意的是，在推理阶段，可以通过缓存机制减少空间需求。每个头的Key和Value可以缓存历史位置的结果，避免重复计算。缓存的空间需求为$O(h \cdot n \cdot d_k) = O(n \cdot d_{\text{model}})$，与输入序列长度和模型维度成正比。

## 5.3.12 并行计算与硬件加速

多头注意力的设计天然适合并行计算。现代GPU的大规模并行能力可以充分利用这一特性。在实践中，每个注意力头的计算可以视为一个独立的"线程块"（thread block）或"流式多处理器"（SM）上的任务。

考虑一个典型的GPU配置：假设有$S$个流式多处理器，每个可以并行执行$T$个线程束（warp）。如果头数$h$是$S$的约数，可以将每个头分配给一个流式多处理器，独立地进行注意力计算。每个头内部的$QK^T$计算可以利用Tensor Core进行矩阵乘法加速，Softmax操作可以在每个线程束内高效地归约求和。

现代深度学习框架（如PyTorch、TensorFlow）和专门的推理引擎（如TensorRT、vLLM）都对多头注意力的并行实现进行了高度优化。例如，Flash Attention算法通过分块计算和IO感知优化，显著提升了长序列注意力的计算效率。在Flash Attention的框架下，多头注意力可以自然地被分解为块级别的计算，每个块处理序列的一个子集。

此外，混合精度计算（FP16/BF16）在多头注意力中被广泛使用。Query、Key、Value的矩阵乘法可以在半精度下进行，Softmax和输出投影则通常使用全精度以保持数值稳定性。Tensor Core对半精度矩阵乘法的加速使得多头注意力的计算效率大幅提升。
## 5.3.13 注意力头的稀疏激活

标准的多头注意力在所有头上都进行完整的计算，计算量与头数成正比。对于头数较多的模型（如GPT-3的96个头），这可能成为计算瓶颈。稀疏激活（Sparse Activation）的多头注意力变体通过只激活部分头来降低计算成本。

Sparse Mixture-of-Attention（Sparse MoA）是一种典型的变体。类似于MoE模型，Sparse MoA使用一个门控网络为每个输入动态地选择"活跃"的头子集。设门控函数为$G(X) \in \mathbb{R}^h$，表示每个头被选中的概率或权重，则：
$$
\text{SparseMultiHead}(Q, K, V) = \sum_{i \in \text{Topk}(G(X))} \text{head}_i \cdot \text{Softmax}(G(X))_i \tag{5.3.12}
$$
其中，$\text{Topk}(\cdot)$选择权重最大的$k$个头参与计算，$k \ll h$。这种设计将计算量从$O(h)$降低到$O(k)$，同时保持了对所有头的访问能力（通过门控权重）。

从数学角度分析，稀疏激活引入了一种非线性，只有被选中的头参与输出计算。这使得模型可以根据输入动态地调整计算分配，对于简单的输入使用较少的头（节省计算），对于复杂的输入使用更多的头（保证质量）。

## 5.3.14 注意力头的分组与跨头交互

另一种多头注意力的变体是"分组多头注意力"（Grouped Multi-Head Attention）或"跨头交互"（Cross-Head Interaction）。在标准的多头注意力中，各头独立计算后简单拼接，输出投影负责混合信息。分组注意力将头分为若干组，组内进行全连接交互，组间进行独立的计算。

设头数$h$被分为$g$组，每组有$h/g$个头。分组多头注意力的计算可以表示为：
$$
\begin{align}
&\text{GroupedMultiHead}(Q, K, V) \\& = \text{MLP}\left( \text{Concat}\left( \text{Group}_1(Q, K, V), \ldots, \text{Group}_g(Q, K, V) \right) \right)
\end{align}
\tag{5.3.13}
$$
其中，$\text{Group}_i(\cdot)$计算第$i$组内所有头的注意力输出并拼接，$\text{MLP}(\cdot)$是一个小型多层感知机，负责组间的信息混合。这种设计在保持参数效率的同时，引入了一个额外的非线性变换层，增强了模型的表达能力。

跨头交互的另一种方式是使用"注意力头的注意力"，让一个注意力机制学习如何组合不同头的输出。这种设计的数学形式为：
$$
\text{HeadMix}(Q, K, V) = \text{Attention}\left( \text{head}_{\text{mix}} Q, \text{head}_{\text{mix}} K, \text{head}_{\text{mix}} V \right) \tag{5.3.14}
$$
其中，$\text{head}_{\text{mix}}$是一个可学习的线性变换，将各头的输出投影到一个新的空间。这种设计允许模型学习到复杂的头间交互模式。

## 5.3.15 多查询注意力与分组查询注意力

多查询注意力（Multi-Query Attention）和分组查询注意力（Grouped-Query Attention）是两种针对Key和Value的变体设计。在标准的多头注意力中，每个头有独立的Key和Value投影；而在多查询注意力中，所有头共享同一个Key和Value：
$$
\begin{align}
&\text{MultiQueryAttention}(Q, K_{\text{shared}}, V_{\text{shared}}) \\&= \text{Attention}(Q_1, K_{\text{shared}}, V_{\text{shared}}), \ldots, \text{Attention}(Q_h, K_{\text{shared}}, V_{\text{shared}}) \tag{5.3.15}
\end{align}
$$
多查询注意力的参数数量从$3d_{\text{model}}^2$减少到$d_{\text{model}}^2 + 2d_{\text{model}} \cdot d_k$​（Key和Value共享一个$d_k$​维投影），显著降低了参数量和内存需求。这种设计在推理时特别有利，因为Key和Value的缓存可以大幅减少。

分组查询注意力是多查询注意力的折中方案：$g$个Query头为一组，共享同一个Key和Value，其中$g$通常远小于$h$。这种设计在保持大部分推理效率提升的同时，保留了更多的表达能力。LLaMA-2等现代大语言模型就采用了分组查询注意力配置。
## 5.3.16 多头注意力的函数逼近视角

从函数逼近理论的角度分析，多头注意力可以被视为一种"基函数展开"（Basis Function Expansion）。考虑一个单层的多头注意力模块，其输出可以写为：
$$
f(X) = \text{Concat}\left( \phi_1(X), \ldots, \phi_h(X) \right) W_O \tag{5.3.16}
$$
其中，$\phi_i(X) = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$是第$i$个头定义的"基函数"。每个基函数$\phi_i$​将输入矩阵$X$映射到一个$n \times d_k$的注意力输出，表示输入在特定子空间中的关联模式。

基函数展开的理论保证告诉我们，如果基函数集合足够丰富，任意连续函数都可以用这些基函数的线性组合来逼近。多头注意力通过学习不同的投影矩阵，实际上是在自适应地构造基函数集合。与固定的基函数（如傅里叶基、小波基）不同，多头注意力的基函数是从数据中学习的，因此可以直接适应特定任务的需求。

## 5.3.17 头间的信息流动分析

多头注意力输出中的信息流动可以通过信息论的工具来分析。设$H(\cdot)$表示熵，$I(X; Y)$表示互信息。考虑第$i$个头的输出$\text{head}_i$携带了多少关于输入$X$的信息，以及不同头的输出之间有多少冗余或互补信息。

首先，分析单个头的信息容量。注意力输出$\text{head}_i$的信息量受到其维度$d_k$​和注意力权重的约束。设注意力权重矩阵$P_i = \text{Softmax}(Q_iK_i^T/\sqrt{d_k})$，则$\text{head}_i = P_i V_i$​。由于$P_i$​是随机矩阵（行和为1），$\text{head}_i$​的信息量最多为$d_k$​位（假设每个维度携带独立的信息）。

其次，分析不同头之间的信息互补性。如果两个头的输出$\text{head}_i$​和$\text{head}_j$是独立的，则它们的联合信息量是各自信息量之和；如果高度相关，则存在信息冗余。理想的情况是各个头学习到互补的信息，使得总的信息量最大化。

从优化的角度，训练过程会自然地推动各头学习互补的特征。这是因为，如果两个头学习到完全相同的特征，梯度信号会高度相似，参数更新方向几乎相同，这实际上是一种参数冗余。随机梯度的微小差异会逐渐放大，使得不同的头收敛到不同的解。

## 5.3.18 多头注意力与卷积运算的关系

多头注意力与卷积运算之间存在着深刻的数学联系，可以帮助我们从另一个角度理解多头注意力的本质。在卷积神经网络中，卷积核定义了一组可学习的滤波器，每个滤波器负责检测输入中的某种局部模式。在多头注意力中，"头"扮演着类似的角色，每个头定义了一种特定的关联模式检测器。

数学上，考虑一个简单的1D卷积操作：$y = W \ast x$，其中$W$是卷积核，$x$是输入序列。卷积可以写为矩阵乘法形式：$y = C(W) x$，其中$C(W)$是卷积矩阵。在多头注意力中，输出是各头注意力的拼接和投影：$y = W_O \cdot [\text{head}_1; \text{head}_2; \ldots; \text{head}_h]$。

关键的区别在于卷积的局部性和注意力的全局性。卷积核的感受野是固定的、局部的（由卷积核大小决定），而注意力的感受野是全局的——每个位置可以直接与所有其他位置交互。多头注意力的"局部性"体现在参数层面：每个头只使用自己的一组投影参数，不同的头学习不同的关联模式。

另一个联系是通过"低秩近似"的视角。理论上，卷积矩阵可以视为具有特殊结构（Toeplitz矩阵）的矩阵，而注意力矩阵是数据依赖的动态矩阵。多头注意力可以被视为对动态注意力矩阵的一种"因式分解"：通过将Q和K分别投影到$h$个子空间，在每个子空间内计算相对简单的注意力模式，然后拼接结果。

## 5.3.19 本节小结

本节系统地探讨了多头注意力的矩阵推导与表达能力分析。我们从多头注意力的设计动机出发，详细推导了其数学定义：$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W_O$​，其中每个头$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$独立地进行注意力计算。

我们深入分析了多头注意力的参数结构，揭示了一个重要的数学事实：尽管多头注意力有$h$个注意力头，但其总参数量与单头注意力完全相同（均为$4d_{\text{model}}^2$）。这意味着多头机制在"免费"地增加表示的丰富性，通过并行计算在多个子空间中提取特征，而不增加计算成本。

在表达能力分析部分，我们从子空间学习的角度论证了多头注意力能够表示比单头注意力更丰富的函数类；通过与混合专家模型的类比，揭示了两者在结构上的相似性和差异性；并通过实证研究的发现，讨论了训练良好的模型中不同头会发展出"专业化"特征的现象。

我们还详细分析了多头注意力的计算复杂度（时间复杂度$O(n^2 \cdot d_{\text{model}} + n \cdot d_{\text{model}}^2)$，与头数无关）、空间需求、并行计算特性，以及各种变体设计（稀疏激活、分组注意力、多查询注意力等）的数学原理。

通过本节的学习，读者应该能够从数学层面深入理解多头注意力的架构设计、表达能力来源和计算特性。下一节，我们将从谱性质的角度分析注意力矩阵的数学特性，探讨其低秩结构及其对模型行为的影响。