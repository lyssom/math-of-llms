## 5.6 注意力机制的结构变体
在前面的几节中，我们系统地探讨了标准Transformer中注意力机制的数学原理。然而，标准注意力机制存在一个根本性的计算瓶颈：对于序列长度为$n$、隐藏维度为$d$的输入，注意力计算的时间复杂度和空间复杂度均为$O(n^2)$。当处理长序列时，$O(n^2)$的复杂度成为效率和可扩展性的主要障碍。为了解决这些问题并适应不同的应用需求，研究者们提出了大量的注意力机制变体。本节将从数学角度系统分析主要的注意力变体，包括稀疏注意力、线性注意力、Flash注意力、多查询注意力等，探讨它们的数学原理、计算复杂度以及在不同场景下的适用性。

### 5.6.1 稀疏注意力的数学框架
稀疏注意力的核心思想是通过限制每个位置只能关注少数其他位置，而非所有位置，来降低计算复杂度。设序列长度为$n$，每个位置的注意力窗口大小为$k$（$k \ll n$），则稀疏注意力矩阵$A^{\text{sparse}} \in \mathbb{R}^{n \times n}$定义为：
$$
A^{\text{sparse}}_{ij} = \begin{cases} \text{Softmax}\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right) & \text{if } j \in \mathcal{N}_i \\ 0 & \text{otherwise} \end{cases} \tag{5.6.1}
$$
其中，$\mathcal{N}_i$是位置$i$的邻域集合，包含该位置被允许关注的位置索引。稀疏注意力矩阵的非零元素比例为$O(k/n)$，远小于标准注意力的100%。不同的稀疏模式对应不同的归纳偏置和计算特性。
滑动窗口注意力每个位置只关注其左右各$w$个位置，即$\mathcal{N}_i = \{j : |i - j| \leq w\}$。这种模式引入了局部性的归纳偏置，假设依赖关系主要存在于邻近位置之间。数学上，滑动窗口注意力类似于带状矩阵，其非零元素集中在主对角线附近的$2w+1$条对角线上。膨胀窗口注意力在滑动窗口的基础上引入膨胀因子$d$，即$\mathcal{N}_i = \{j : |i - j| \leq w \cdot d, (i - j) \equiv 0 \pmod{d}\}$，可以在保持计算量不变的情况下扩大感受野。随机注意力每个位置关注若干随机选择的位置，打破了局部性的限制，允许与远距离的随机位置交互，有助于捕获全局依赖。全局-局部混合注意力结合全局位置和局部位置，全局位置可以与所有位置交互，局部位置只与邻近位置交互，这种模式在Longformer、BigBird等模型中被广泛使用。

### 5.6.2 线性注意力的数学定义
线性注意力的核心思想是通过替换Softmax函数，将注意力计算的复杂度从$O(n^2)$降低到$O(n)$。标准注意力的计算瓶颈在于Softmax操作需要计算和存储完整的$n \times n$注意力矩阵。线性注意力用核函数近似Softmax，定义注意力输出为：
$$
\text{LinearAttention}(Q, K, V) = \phi(Q) \left( \phi(K)^T V \right) \tag{5.6.2}
$$
其中，$\phi : \mathbb{R}^{d_k} \rightarrow \mathbb{R}^m$是一个特征映射函数。通过选择合适的$\phi$，计算可以重新排列为：
$$
\text{output}_i = \frac{\sum_{j=1}^{n} \phi(q_i) \cdot \phi(k_j)^T \cdot v_j}{\sum_{j=1}^{n} \phi(q_i) \cdot \phi(k_j)^T} \tag{5.6.3}
$$
令$U = \phi(K)^T V \in \mathbb{R}^{m \times d_v}$，$Z = \sum_{j=1}^{n} \phi(k_j)^T \in \mathbb{R}^m$，则输出为$\text{output}_i = \frac{\phi(q_i) U}{\phi(q_i) Z}$。这种形式的计算复杂度为$O(n \cdot m \cdot d_k)$，如果$m$是常数，则总复杂度为$O(n)$，实现了线性复杂度。线性注意力的关键在于设计合适的特征映射函数$\phi$，不同的特征映射对应不同的核函数，从而对应不同的注意力行为，包括指数特征映射、ReLU特征映射、多项式特征映射、ELU特征映射等。

### 5.6.3 Flash注意力的数学原理
Flash注意力是一种IO感知的注意力计算算法，其核心思想是通过分块计算和重计算来减少内存带宽消耗，从而加速注意力计算。标准注意力计算需要将完整的注意力矩阵存储在SRAM中，对于$n=2048$、$d_k = 64$的序列，注意力矩阵需要约16MB的存储，这通常超过了GPU SRAM的容量。因此，标准实现需要将数据从HBM多次加载到SRAM，造成内存带宽瓶颈。
Flash注意力将Query、Key、Value矩阵划分为小块，逐块计算注意力输出。算法利用Softmax的数学性质：
$$
\text{Softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)} = \frac{\exp(x_i - m)}{\sum_j \exp(x_j - m)} \tag{5.6.4}
$$
其中，$m = \max_j x_j$是最大值。通过使用行最大值进行归一化，可以避免数值溢出，同时通过累积因子$\exp(m_{\text{prev}} - m_{\text{curr}})$确保数学上的等价性。Flash注意力的时间复杂度与标准注意力相同（$O(n^2 \cdot d_k)$），但实际性能显著优于标准实现，因为内存访问量从$O(n^2)$降低到$O(n \cdot d_k)$。

### 5.6.4 多查询注意力的数学定义
多查询注意力是标准多头注意力的参数共享变体，其核心思想是让所有注意力头共享同一个Key和Value投影，只保留多个独立的Query投影：
$$
Q_i = XW_Q^{(i)}, \quad i = 1, \ldots, h; \quad K = XW_K^{\text{shared}}; \quad V = XW_V^{\text{shared}} \tag{5.6.5}
$$
注意力输出为
$$\begin{equation}
\begin{aligned}
\mathrm{MultiQueryAttention}(Q, K, V)
\\= \mathrm{Concat}\Bigl(
    \mathrm{Attention}(Q_1, K, V), \\
\qquad\qquad
    \ldots,
    \mathrm{Attention}(Q_h, K, V)
\Bigr) W_O
\end{aligned}
\tag{5.6.6}
\end{equation}
$$
与标准多头注意力相比，多查询注意力的参数数量显著减少。设隐藏维度为$d$，头数为$h$，头维度为$d_k = d/h$，标准多头注意力的QKV投影参数量为$3d^2$，多查询注意力的总参数量为$d^2 + 2d \cdot d_k = d^2 + 2d^2/h$。当$h$较大时，多查询注意力可以将参数量减少约$2/3$。
分组查询注意力是标准多头注意力和多查询注意力的折中方案，它将Query头分为$g$组（$g < h$），同一组内的Query头共享同一个Key和Value，不同组的Query头使用不同的Key和Value。Query投影参数量为$d^2$，Key和Value投影参数量为$g \cdot d^2/h$，总参数量为$d^2(1 + 2g/h)$。当$g = h$时退化为标准多头注意力，当$g = 1$时退化为多查询注意力。多查询注意力的主要优势在于推理效率的提升，由于所有Query头共享同一个Key和Value，Key和Value的缓存可以大幅减少，从$h$个减少到$g$个或1个。

### 5.6.5 注意力变体的比较与选择
不同的注意力变体有不同的计算复杂度和表达能力，适用于不同的场景。标准注意力的时间复杂度为$O(n^2 \cdot d)$，适用于需要建模所有位置对依赖的场景，但不适合长序列。稀疏注意力的时间复杂度为$O(n \cdot k \cdot d)$，其中$k$是平均稀疏度，适用于局部依赖为主的场景。线性注意力的时间复杂度为$O(n \cdot m \cdot d)$，适用于需要线性复杂度的场景，如在线推理。Flash注意力的时间复杂度与标准注意力相同，但实际运行时间显著更短，因为内存访问量减少。多查询注意力的时间复杂度与标准注意力相同，但Key和Value的缓存量从$O(h \cdot n \cdot d)$减少到$O(n \cdot d)$，适用于推理场景。
从函数逼近的角度，不同的注意力变体对应于对原始注意力函数的不同逼近方式。标准注意力可以精确表示的函数类为$\mathcal{F}_{\text{std}}$，变体注意力只能逼近这个函数类的某个子集。稀疏注意力通过限制非零元素的位置来逼近标准注意力，线性注意力通过核函数近似来逼近标准注意力。选择注意力变体时，需要综合考虑任务需求、计算资源、数据特性等因素。对于通用场景，标准注意力或Flash注意力是安全的选择；对于长序列场景，稀疏注意力或线性注意力可能更合适；对于推理场景，多查询注意力可以显著提升效率。

### 5.6.6 本节小结
本节系统地介绍了注意力机制的主要变体，包括稀疏注意力、线性注意力、Flash注意力、多查询注意力等。我们从数学角度分析了每种变体的定义、计算流程和复杂度特性。稀疏注意力通过限制注意力窗口降低计算复杂度，线性注意力通过核函数近似实现线性复杂度，Flash注意力通过IO感知的分块计算减少内存访问开销，多查询注意力通过参数共享减少推理缓存需求。这些变体从不同角度改进了标准注意力机制，为处理长序列、提升推理效率提供了数学理论基础。