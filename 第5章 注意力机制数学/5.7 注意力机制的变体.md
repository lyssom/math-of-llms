## 5.7 注意力机制的变体
在前面的几节中，我们系统地探讨了标准Transformer中注意力机制的数学原理，包括Scaled Dot-Product Attention的定义、Query-Key-Value的矩阵变换、多头注意力的表达能力、长程依赖建模机制，以及注意力矩阵的谱性质与低秩结构。这些内容为深入理解注意力机制奠定了坚实的理论基础。

然而，标准注意力机制存在一个根本性的计算瓶颈：对于序列长度为$n$、隐藏维度为$d$的输入，注意力计算的时间复杂度和空间复杂度均为$O(n^2)$。当处理长序列（如长文档、长对话、基因组序列等）时，$O(n^2)$的复杂度成为效率和可扩展性的主要障碍。此外，标准注意力机制的一些设计选择（如Softmax归一化、全连接注意力模式等）可能在某些场景下并非最优。

为了解决这些问题并适应不同的应用需求，研究者们提出了大量的注意力机制变体。这些变体在保持注意力机制核心思想的同时，从不同角度对标准注意力进行了改进或扩展。本节将从数学角度系统分析主要的注意力变体，包括稀疏注意力、线性注意力、Flash注意力、多查询注意力、门控注意力等，探讨它们的数学原理、计算复杂度、以及在不同场景下的适用性。通过本节的学习，读者将建立起对注意力变体全景图的理解，为在实际应用中选择和设计合适的注意力机制提供理论指导。

### 5.7.1 稀疏注意力的数学框架

稀疏注意力（Sparse Attention）的核心思想是通过限制每个位置只能关注少数其他位置，而非所有位置，来降低计算复杂度。设序列长度为$n$，每个位置的"注意力窗口"大小为$k$（$k \ll n$），则稀疏注意力矩阵$A^{\text{sparse}} \in \mathbb{R}^{n \times n}$定义为：
$$
A^{\text{sparse}}_{ij} = \begin{cases} \text{Softmax}\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right) & \text{if } j \in \mathcal{N}_i \\ 0 & \text{otherwise} \end{cases} \tag{5.7.1}
$$
其中，$\mathcal{N}_i$是位置$i$的邻域集合，包含该位置被允许关注的位置索引。稀疏注意力矩阵的非零元素比例为$O(k/n)$，远小于标准注意力的100%。

稀疏注意力的关键是设计合适的稀疏模式$\mathcal{N}_i$​。不同的稀疏模式对应不同的归纳偏置和计算特性。从数学角度，稀疏模式可以分为以下几类。

滑动窗口注意力（Sliding Window Attention）：每个位置只关注其左右各$w$个位置，即$\mathcal{N}_i = \{j : |i - j| \leq w\}$。这种模式引入了局部性的归纳偏置，假设依赖关系主要存在于邻近位置之间。数学上，滑动窗口注意力类似于带状矩阵（Band Matrix），其非零元素集中在主对角线附近的$2w+1$条对角线上。

膨胀窗口注意力（Dilated Window Attention）：在滑动窗口的基础上引入膨胀因子$d$，即$\mathcal{N}_i = \{j : |i - j| \leq w \cdot d, (i - j) \equiv 0 \pmod{d}\}$。这种模式可以在保持计算量不变的情况下扩大感受野。

随机注意力（Random Attention）：每个位置关注若干随机选择的位置，即$\mathcal{N}_i$是随机采样的子集。随机注意力打破了局部性的限制，允许每个位置与远距离的随机位置交互，有助于捕获全局依赖。

全局-局部混合注意力（Global-Local Hybrid Attention）：结合全局位置和局部位置。全局位置（如CLS标记或特殊标记）可以与所有位置交互，局部位置只与邻近位置交互。这种模式在Longformer、BigBird等模型中被广泛使用。

### 5.7.2 稀疏模式的学习与优化

固定稀疏模式（如滑动窗口）虽然计算高效，但可能无法适应所有任务的需求。自适应稀疏模式（Adaptive Sparse Pattern）通过学习来确定每个位置应该关注哪些位置。基于重要性的稀疏化是最常见的自适应策略。设注意力分数矩阵为$S = QK^T / \sqrt{d_k}$，我们可以根据重要性分数选择top-k个位置：
$$
\mathcal{N}_i = \text{TopK}\left( \{j : s_{ij}\} \right) \tag{5.7.2}
$$
其中，$s_{ij}$​是位置$i$对位置$j$的原始注意力分数，$\text{TopK}(\cdot)$返回分数最高的$k$个索引。这种策略的优点是保留了最重要的连接，缺点是仍然需要计算完整的注意力分数矩阵（$O(n^2)$复杂度）。可学习的稀疏模式将稀疏模式参数化，通过反向传播学习最优的稀疏结构。设稀疏模式的参数为$\Theta$，则邻域定义为：
$$
\mathcal{N}_i(\Theta) = \left\{ j : f_\Theta(i, j) > \tau \right\} \tag{5.7.3}
$$
其中，$f_\Theta$是一个可学习的评分函数，$\tau$是阈值。评分函数可以是神经网络，参数$\Theta$通过梯度下降优化。
Sinkhorn稀疏化是一种基于最优传输的稀疏化方法。设原始注意力矩阵为$A$，Sinkhorn迭代定义为：
$$
A^{(t+1)} = T_{\text{row}} \left( T_{\text{col}} \left( \exp(A^{(t)} \cdot \lambda) \right) \right) \tag{5.7.4}
$$
其中，$T_{\text{row}}$和$T_{\text{col}}$​分别是行归一化和列归一化操作，$\lambda$是温度参数。通过调节$\lambda$，可以得到不同稀疏程度的注意力矩阵。

### 5.7.3 稀疏注意力的复杂度分析

稀疏注意力显著降低了计算复杂度。考虑一个具有平均稀疏度$s$（即每个位置平均关注$s$个位置）的稀疏注意力模式。计算稀疏注意力分数矩阵的时间复杂度为$O(n \cdot s \cdot d)$，远小于标准注意力的$O(n^2 \cdot d)$。
空间复杂度同样降低到$O(n \cdot s)$，只需要存储稀疏的注意力权重和值向量。在推理时，Key和Value的缓存也可以采用稀疏格式，进一步降低内存占用。然而，稀疏注意力也引入了新的计算挑战。首先，稀疏矩阵运算在现代硬件（尤其是GPU）上的效率不如密集矩阵运算。GPU对密集矩阵乘法有高度优化的实现（如Tensor Core），而稀疏矩阵运算可能无法充分利用硬件并行能力。其次，稀疏模式的不规则内存访问可能导致缓存命中率降低，影响实际性能。
为了在现代硬件上高效实现稀疏注意力，通常需要将不规则的稀疏模式转换为规则的块稀疏格式（Block Sparse）。块稀疏将序列划分为固定大小的块（Block），每个块要么完全参与注意力计算，要么完全跳过。这种格式可以利用密集矩阵乘法的高效实现，同时保持大部分计算节省。
### 5.7.4 线性注意力的数学定义

线性注意力（Linear Attention）的核心思想是通过替换Softmax函数，将注意力计算的复杂度从$O(n^2)$降低到$O(n)$。标准注意力的计算瓶颈在于Softmax操作：$A = \text{Softmax}(QK^T / \sqrt{d_k})$需要计算和存储完整的$n \times n$注意力矩阵。
线性注意力用核函数近似Softmax，定义注意力输出为：
$$
\text{LinearAttention}(Q, K, V) = \phi(Q) \left( \phi(K)^T V \right) \tag{5.7.5}
$$
其中，$\phi : \mathbb{R}^{d_k} \rightarrow \mathbb{R}^m$是一个特征映射函数。通过选择合适的$\phi$，计算可以重新排列为：
$$
\text{output}_i = \frac{\sum_{j=1}^{n} \phi(q_i) \cdot \phi(k_j)^T \cdot v_j}{\sum_{j=1}^{n} \phi(q_i) \cdot \phi(k_j)^T} \tag{5.7.6}
$$
令$U = \phi(K)^T V \in \mathbb{R}^{m \times d_v}$，$Z = \sum_{j=1}^{n} \phi(k_j)^T \in \mathbb{R}^m$，则输出为：
$$
\text{output}_i = \frac{\phi(q_i) U}{\phi(q_i) Z} \tag{5.7.7}
$$
这种形式的计算复杂度为$O(n \cdot m \cdot d_k)$，其中$m$是特征维度。如果$m$是常数（不随$n$增长），则总复杂度为$O(n)$，实现了线性复杂度。

### 5.7.5 特征映射函数的设计

线性注意力的关键在于设计合适的特征映射函数$\phi$。不同的特征映射对应不同的核函数，从而对应不同的注意力行为。
指数特征映射（Exponential Feature Map）是最直接的选择：$\phi(x) = \exp(x)$。对应的核函数为$\kappa(x, y) = \exp(x)^T \exp(y) = \exp(x^T y)$，即指数核（Exponential Kernel）。然而，指数核的数值稳定性较差：$x^T y$可能很大，导致$\exp(x^T y)$溢出。
ReLU特征映射：$\phi(x) = \text{ReLU}(xW)$。对应的核函数为$\kappa(x, y) = \text{ReLU}(xW)^T \text{ReLU}(yW)$。这种映射的优点是数值稳定且计算简单，但表达能力受限。
多项式特征映射：$\phi(x) = [x, x \odot x, \ldots, x^{\odot p}]$，其中$\odot$是逐元素乘方，$p$是多项式度数。这种映射对应于多项式核函数$\kappa(x, y) = (x^T y + c)^p$。多项式核可以捕获特征之间的高阶交互。
elu特征映射：$\phi(x) = \text{ELU}(x) + 1$。这是Transformer原始论文提出的设计，对应的核函数为$\kappa(x, y) = \text{ELU}(x)^T \text{ELU}(y)$。ELU（Exponential Linear Unit）确保了特征的 positivity（正值性），有助于数值稳定性。
傅里叶特征映射：$\phi(x) = [\cos(\omega_1 x), \sin(\omega_1 x), \ldots, \cos(\omega_m x), \sin(\omega_m x)]$，其中$\omega_k$是频率参数。这种映射对应于高斯核或RBF核，在某些任务上表现良好。

### 5.7.6 线性注意力的表达能力分析

线性注意力的表达能力受限于所选的核函数。理论上，如果核函数是正定的（Positive Definite），则存在某个隐空间使得注意力计算等价于该隐空间中的线性运算。然而，正定性约束限制了可表示的注意力模式。

设$\kappa$是特征映射对应的核函数。如果$\kappa$是正定的，则对于任意有限的点集$\{x_1, \ldots, x_n\}$，Gram矩阵$K_{ij} = \kappa(x_i, x_j)$是半正定的。然而，正定核只能表示"正定"的注意力模式，无法表示某些负定或非定模式。
实际上，Softmax注意力可以表示负定的交互模式。例如，考虑三个位置$A,B,C$，注意力权重可以满足$A$关注$B$、$B$关注$C$、$C$关注$A$的循环依赖，这种模式对应于非正定的Gram矩阵。线性注意力通过核函数近似Softmax，可能无法精确表示这类模式。
为了增强线性注意力的表达能力，提出了各种改进。双向线性注意力（Bidirectional Linear Attention）通过同时计算前向和后向的信息流来增强上下文建模能力。门控线性注意力（Gated Linear Attention）引入门控机制来调节信息流动，相当于在核空间中引入了可学习的权重。非线性特征映射通过堆叠多层神经网络来学习复杂的特征映射，增强表达能力。

### 5.7.7 递归形式的线性注意力

线性注意力的一个重要优势是支持递归（Recursive）计算，这使得它能够自然地处理变长序列和流式输入。考虑在线推理场景：序列逐步输入，需要在每个新标记到来时更新注意力输出。

标准注意力的递归更新需要维护完整的注意力矩阵$A$，空间复杂度为$O(n^2)$。线性注意力的递归更新只需要维护累积量$U$和$Z$：
$$
U^{(t)} = U^{(t-1)} + \phi(k_t)^T v_t, \quad Z^{(t)} = Z^{(t-1)} + \phi(k_t)^T \tag{5.7.8}
$$
当新输入$x_t$​到来时，更新后的输出为：
$$
\text{output}_t = \frac{\phi(q_t) U^{(t)}}{\phi(q_t) Z^{(t)}} \tag{5.7.9}
$$
这种递归形式的复杂度为$O(m \cdot d_k)$每步，空间复杂度为$O(m)$，与序列长度无关。这使得线性注意力特别适合在线推理和流式处理场景。

递归形式的另一个优势是支持"遗忘"旧信息的能力。通过引入遗忘因子$\gamma$（$0 < \gamma < 1$），累积量可以写为：
$$
U^{(t)} = \gamma U^{(t-1)} + \phi(k_t)^T v_t, \quad Z^{(t)} = \gamma Z^{(t-1)} + \phi(k_t)^T \tag{5.7.10}
$$
遗忘机制确保了远处的旧信息逐渐衰减，避免了无限增长的累积量。这与人类记忆的遗忘特性有某种相似性。
### 5.7.8 Flash注意力的数学原理

Flash注意力（Flash Attention）是一种IO感知的注意力计算算法。其核心思想是通过分块计算（Tiling）和重计算（Recomputation）来减少内存带宽消耗，从而加速注意力计算。

标准注意力计算需要将完整的注意力矩阵$S = QK^T / \sqrt{d_k}$存储在SRAM（高速缓存）中。对于$n=2048$、$d_k = 64$的序列，$S$需要$2048 \times 2048 \times 4$字节$\approx 16$MB的存储，这通常超过了GPU SRAM的容量（通常为$20−48$MB）。因此，标准实现需要将数据从HBM（高带宽内存）多次加载到SRAM，造成内存带宽瓶颈。

Flash注意力将Query、Key、Value矩阵划分为小块（Tiles），逐块计算注意力输出。设块大小为$B_r$（Query块）和$B_c$​（Key/Value块），算法流程如下：

对于每个Query块$Q_i \in \mathbb{R}^{B_r \times d_k}$​：
- 初始化输出块$O_i \in \mathbb{R}^{B_r \times d_v}$​为零
- 初始化行最大值$m_i \in \mathbb{R}^{B_r}$​为$-\infty$
- 初始化行和$l_i \in \mathbb{R}^{B_r}$​为零
对于每个Key块$K_j$​和Value块$V_j$​：

- 计算块级别的注意力分数：$S_{ij} = Q_i K_j^T / \sqrt{d_k} \in \mathbb{R}^{B_r \times B_c}$
- 计算行最大值：$m_{ij} = \max(m_i, \text{rowmax}(S_{ij}))$
- 计算指数差值：$\tilde{P}_{ij} = \exp(S_{ij} - m_{ij})$
- 更新行和：$l_i = l_i \cdot \exp(m_i - m_{ij}) + \text{rowsum}(\tilde{P}_{ij})l$
- 更新输出：$O_i = O_i \cdot \exp(m_i - m_{ij}) + \tilde{P}_{ij} V_j$

最后，对输出进行归一化：$O_i = O_i / l_i$​。

### 5.7.9 Flash注意力的复杂度与性能分析

Flash注意力的时间复杂度与标准注意力相同，均为$O(n^2 \cdot d_k)$。然而，其实际性能显著优于标准实现，原因在于IO效率的提升。
考虑内存访问量。标准注意力的内存访问包括：加载$Q, K, V$矩阵（$3 \cdot n \cdot d_k$）、存储和加载注意力矩阵$S$（$n^2$）、存储和加载输出矩阵（$n \cdot d_v$​）。总HBM访问量约为$O(n^2)$。
Flash注意力的内存访问包括：逐块加载$Q, K, V$（$3 \cdot n \cdot d_k$​）、加载和存储中间状态$m_i, l_i$（$O(n)$）、存储输出（$O(n)$）。总HBM访问量约为$O(n \cdot d_k)$，显著小于$O(n^2)$。
对于典型配置（$n=2048$，$d_k = 64$），标准注意力的HBM访问量约为$16MB$ $\times$若干次，Flash注意力约为$2048 \times 64 \times 4$字节 ≈$\approx 0.5MB$ $\times$若干次，节省了约30倍的内存带宽。
实际性能测试表明，Flash注意力相比标准实现可以获得2-4倍的加速，且加速效果随序列长度增加而增大。这是因为内存带宽瓶颈在长序列下更加严重，Flash注意力的IO优化收益更大。

### 5.7.10 Flash注意力-2与Flash注意力-3

Flash注意力的后续版本进一步优化了算法实现和硬件利用率。Flash-Attention-2的主要改进包括：更精细的块大小选择（根据硬件特性自适应调整）、异步执行（利用GPU的并行计算能力）、以及更好的线程分配策略。

Flash-Attention-3进一步引入了异步执行和Tensor Core的更充分利用。其核心思想是在计算当前块的同时预加载下一块的数据，利用GPU的异步执行能力隐藏内存访问延迟。此外，Flash-3针对Hopper架构GPU（如H100）进行了专门优化，使用了新的异步指令（WGMMA）。

从数学角度看，Flash注意力的各个版本都遵循相同的核心理论：将完整的注意力计算分解为块级别的计算，通过正确的归一化累积得到最终结果。算法的正确性依赖于softmax的数学性质：
$$
\text{Softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)} = \frac{\exp(x_i - m) \exp(m)}{\sum_j \exp(x_j - m) \exp(m)} = \frac{\exp(x_i - m)}{\sum_j \exp(x_j - m)} \tag{5.7.11}
$$
其中，$m = \max_j x_j$​是最大值。通过使用行最大值进行归一化，可以避免数值溢出，同时通过累积因子$\exp(m_{\text{prev}} - m_{\text{curr}})$确保数学上的等价性。
### 5.7.11 多查询注意力的数学定义

多查询注意力（Multi-Query Attention，MQA）是标准多头注意力的参数共享变体。其核心思想是让所有注意力头共享同一个Key和Value投影，只保留多个独立的Query投影：
$$
\begin{aligned}
Q_i &= XW_Q^{(i)}, \quad i = 1, \ldots, h \\ K &= XW_K^{\text{shared}} \\ V &= XW_V^{\text{shared}} \end{aligned} \tag{5.7.12}$$ 注意力输出为： 
$$
\begin{aligned}
&\text{MultiQueryAttention}(Q, K, V) \\& = \text{Concat}\left( \text{Attention}(Q_1, K, V), \ldots, \text{Attention}(Q_h, K, V) \right) W_O 
\end{aligned}
\tag{5.7.13}$$与标准多头注意力相比，多查询注意力的参数数量显著减少。设隐藏维度为$d$，头数为$h$，头维度为$d_k = d/h$。 标准多头注意力的QKV投影参数量为$3 \times d \times d = 3d^2$（因为每个头的投影矩阵维度为$d \times d_k$，$h$个头的总参数量为$h \times d \times d_k = d^2$）。多查询注意力的Query投影参数量为$d \times d = d^2$（$h$个头的Query投影），Key和Value投影各为$d \times d_k$，总参数量为$d^2 + 2d \times d_k = d^2 + 2d^2/h$。 当$h$较大时（如$h = 32$或$64$），多查询注意力可以将参数量减少约$2/3$。 
#### 5.7.12 分组查询注意力的平衡设计 
分组查询注意力（Grouped-Query Attention，GQA）是标准多头注意力和多查询注意力的折中方案。它将Query头分为$g$组（$g < h$），同一组内的Query头共享同一个Key和Value，不同组的Query头使用不同的Key和Value。 数学定义如下： $$\begin{aligned} Q_i &= XW_Q^{(i)}, \quad i = 1, \ldots, h \\ K^{(g)} &= XW_K^{(g)}, \quad g = 1, \ldots, g \\ V^{(g)} &= XW_V^{(g)}, \quad g = 1, \ldots, g \end{aligned} \tag{5.7.14}$$其中，Query头$i$属于组$g(i)$。注意力输出为： 
$$
\begin{aligned}
&\text{GroupedQueryAttention}(Q, K, V) \\&= \text{Concat}\left( \text{Attention}(Q_1, K^{(g(1))}, V^{(g(1))}), \ldots, \text{Attention}(Q_h, K^{(g(h))}, V^{(g(h))}) \right) W_O
\end{aligned}
\tag{5.7.15}$$分组查询注意力的参数数量介于标准多头注意力和多查询注意力之间。Query投影参数量为$h \times d \times d_k = d^2$，Key和Value投影参数量为$g \times d \times d_k = g \cdot d^2/h$，总参数量为$d^2 (1 + 2g/h)$。 当$g = h$时，退化为标准多头注意力；当$g = 1$时，退化为多查询注意力。通过选择合适的$g$，可以在参数效率和模型质量之间取得平衡。 
#### 5.7.13 多查询注意力的推理优化 
多查询注意力的主要优势在于推理效率的提升。由于所有Query头共享同一个Key和Value，Key和Value的缓存可以大幅减少。在推理时，需要缓存的Key和Value数量从$h$个减少到$1$个（多查询注意力）或$g$个（分组查询注意力）。 设序列长度为$n$，隐藏维度为$d$，Key/Value维度为$d_k$，缓存量为$C = n \cdot d_k \cdot \text{bits}$。标准多头注意力的缓存量为$h \cdot C$，多查询注意力的缓存量为$C$，分组查询注意力的缓存量为$g \cdot C$。 对于长序列推理，缓存量是主要的内存瓶颈。以GPT-3（$h = 96$，$d = 12288$）为例，标准注意力的缓存量约为$96 \times n \times 128 \times 2$字节（双向注意力需要缓存Key和Value）。如果使用多查询注意力（$h = 96$，但只缓存1组Key/Value），缓存量减少96倍。 缓存量的减少不仅降低了内存占用，还提高了缓存命中率，减少了内存访问开销。在推理生成过程中，Key和Value需要为每个新生成的标记重复使用，减少缓存量意味着可以在高速缓存中保留更多的历史信息，避免频繁的从HBM加载。 
### 5.7.14 门控注意力的数学框架 
门控注意力（Gated Attention）在标准注意力计算中引入门控机制，以增强模型的非线性表达能力和信息流控制。门控机制源自LSTM和GRU等门控循环网络，其数学形式为： $$\text{gate} = \sigma(Wx + b) \tag{5.7.16}$$ 其中，$\sigma$是Sigmoid函数，$W$和$b$是可学习参数。门控值的范围在$(0, 1)$之间，可以软性地"开启"或"关闭"信息通道。 门控注意力有多种变体。最简单的形式是在注意力输出上应用门控： $$\text{GatedAttention}(Q, K, V) = g \odot \text{Attention}(Q, K, V) \tag{5.7.17}$$其中，$g$是门控向量，通过对Query或额外的输入进行非线性变换得到。 更复杂的门控形式包括Query门控和Key/Value门控： 
$$
\begin{aligned} g_Q &= \sigma(XW_{gQ}), \quad g_K = \sigma(XW_{gK}), \quad g_V = \sigma(XW_{gV}) \\ Q' &= g_Q \odot Q, \quad K' = g_K \odot K, \quad V' = g_V \odot V \\& \text{output} = \text{Attention}(Q', K', V') \end{aligned} \tag{5.7.18}$$ 这种形式的门控允许对不同位置、不同特征维度进行独立的"过滤"。
### 5.7.15 门控与注意力的交互分析 
门控机制与注意力计算的交互可以增强模型对不同输入模式的适应性。考虑门控注意力的一种特殊形式——门控交叉注意力（Gated Cross-Attention），常用于多模态模型（如视觉-语言模型）： 
$$
\begin{aligned}
&\text{GatedCrossAttention}(Q_{\text{language}}, K_{\text{vision}}, V_{\text{vision}}) \\&= \text{Attention}(Q_{\text{language}}, g \odot K_{\text{vision}}, g \odot V_{\text{vision}}) 
\end{aligned}
\tag{5.7.19}$$其中，门控$g$通过对语言输入进行变换得到：$g = \sigma(Q_{\text{language}}W_g)$。这种设计允许语言Query动态地"选择"视觉特征的哪些部分值得关注。 从数学角度，门控机制可以视为一种"软性的特征选择"。设原始特征为$x$，门控特征为$g \odot x$，则门控引入了输入依赖的缩放变换。与固定的缩放（如层归一化）不同，门控的缩放是数据依赖的，可以根据具体输入调整特征的重要性。 门控注意力的梯度分析也很重要。考虑一个简化的门控注意力：$y = g \odot \text{Attention}(x)$。损失函数$\mathcal{L}$关于门控$g$的梯度为： $$\frac{\partial \mathcal{L}}{\partial g} = \frac{\partial \mathcal{L}}{\partial y} \odot \text{Attention}(x) \tag{5.7.20}$$这意味着，当注意力输出较大时，梯度对门控的影响也较大。这种"放大"效应可能导致训练不稳定，需要谨慎初始化门控参数。 
### 5.7.16 门控注意力网络的表达能力 
门控注意力引入了额外的非线性变换，增强了网络的表达能力。从函数逼近的角度，单层标准注意力可以表示的函数类为$\mathcal{F}_{\text{std}} = \{ x \mapsto \text{Softmax}(xW_Q W_K^T x^T / \sqrt{d}) xW_V \}$。引入门控后，函数类扩展为： $$\mathcal{F}_{\text{gated}} = \{ x \mapsto \sigma(xW_g) \odot \text{Softmax}(xW_Q W_K^T x^T / \sqrt{d}) xW_V \} \tag{5.7.21}$$门控引入了额外的非线性（$\sigma$函数）和乘法交互（逐元素乘法），使得函数类$\mathcal{F}_{\text{gated}}$严格包含$\mathcal{F}_{\text{std}}$。具体而言，存在可以用门控注意力表示但无法用标准注意力表示的函数。 门控机制还与残差连接有密切关系。考虑带门控的残差连接： $$y = x + g \odot \text{Attention}(x) \tag{5.7.22}$$当$g = 1$时，退化为标准残差连接；当$g = 0$时，输出为输入（跳过注意力层）。门控允许模型学习最优的"残差权重"，在保留原始信息和引入注意力信息之间取得平衡。 
### 5.7.17 计算复杂度比较 
不同的注意力变体有不同的计算复杂度，适用于不同的场景。
标准注意力的时间复杂度为$O(n^2 \cdot d)$，空间复杂度为$O(n^2 + n \cdot d)$。这是最通用的形式，适用于需要建模所有位置对依赖的场景，但不适合长序列。 稀疏注意力的时间复杂度为$O(n \cdot k \cdot d)$，其中$k$是平均稀疏度。空间复杂度为$O(n \cdot k)$。适用于局部依赖为主的场景，如长文本处理。 线性注意力的时间复杂度为$O(n \cdot m \cdot d)$，其中$m$是特征映射维度。空间复杂度为$O(m \cdot d)$。适用于需要线性复杂度的场景，如在线推理。 Flash注意力的时间复杂度与标准注意力相同（$O(n^2 \cdot d)$），但实际运行时间显著更短，因为内存访问量减少。适用于需要高效处理中等长度序列的场景。 多查询注意力的时间复杂度与标准注意力相同，但Key和Value的缓存量从$O(h \cdot n \cdot d)$减少到$O(n \cdot d)$。适用于推理场景，尤其是长序列生成。 
### 5.7.18 表达能力与任务适配性 
不同的注意力变体有不同的表达能力，适用于不同的任务。理解这种适配性对于实际应用中的选择至关重要。 标准注意力的表达能力最强，可以建模任意复杂的位置对依赖关系。适用于需要捕获复杂长程依赖的任务，如机器翻译、文本摘要、长文档问答等。 稀疏注意力引入局部性的归纳偏置，假设依赖关系主要存在于邻近位置之间。适用于具有局部结构的数据，如代码、DNA序列、时间序列等。滑动窗口稀疏模式特别适合这类任务。 线性注意力通过核函数近似Softmax，表达能力受限于核函数的选择。适用于对效率要求高、对建模复杂依赖要求较低的场景，如实时推理、流式处理等。 门控注意力增强了非线性表达能力，适用于需要细粒度信息控制的场景，如多模态融合、特征选择等。 选择注意力变体时，需要综合考虑任务需求、计算资源、数据特性等因素。对于通用场景，标准注意力或Flash注意力是安全的选择；对于长序列场景，稀疏注意力或线性注意力可能更合适；对于推理场景，多查询注意力可以显著提升效率。 
### 5.7.19 注意力变体的融合趋势 
随着研究的深入，不同注意力变体的融合成为一种趋势。研究者发现，组合多种注意力机制可以兼顾效率和表达能力。 局部-全局融合是最常见的融合策略。例如，先使用滑动窗口注意力捕获局部依赖，再使用全局注意力或稀疏注意力捕获长程依赖。这种两阶段策略在Longformer、BigBird等模型中被广泛使用。 门控融合是另一种融合策略。例如，使用门控机制动态选择使用标准注意力还是线性注意力。门控的输出可以表示为： $$\text{FusedAttention}(x) = g \odot \text{StandardAttention}(x) + (1 - g) \odot \text{LinearAttention}(x) \tag{5.7.23}$$其中，门控$g$通过对输入进行变换得到。模型可以根据输入自适应地选择最合适的注意力模式。 层次化融合是深度模型中常用的策略。在Transformer的多层结构中，不同层使用不同的注意力变体。底层使用局部注意力捕获词汇和短语级别的特征，顶层使用全局注意力捕获句子和文档级别的特征。这种层次化设计与人脑处理信息的层级结构有某种相似性。 
### 5.7.20 注意力变体的逼近理论 
从函数逼近的角度，不同的注意力变体对应于对原始注意力函数的不同的逼近方式。标准注意力可以精确表示的函数类为$\mathcal{F}_{\text{std}}$，变体注意力只能逼近这个函数类的某个子集。 稀疏注意力通过限制非零元素的位置来逼近标准注意力。设标准注意力矩阵为$A$，稀疏注意力矩阵为$A^{\text{sparse}}$，逼近误差为$\|A - A^{\text{sparse}}\|$。如果稀疏模式设计得当，且$A$的实际非零元素集中在某些区域，逼近误差可以很小。 线性注意力通过核函数近似来逼近标准注意力。设标准Softmax注意力为$\text{Softmax}(QK^T / \sqrt{d})$，线性注意力为$\phi(Q) \phi(K)^T / (\phi(Q) \phi(K)^T \mathbf{1})$。逼近误差取决于核函数与Softmax的相似程度。 从理论上分析注意力变体的表达能力，需要建立注意力变体函数类与标准注意力函数类之间的包含关系。研究表明，稀疏注意力和线性注意力都是标准注意力的"降级"版本，在一般情况下无法完全等价。然而，在特定的输入分布和任务需求下，这些变体可能具有足够的表达能力。 
### 5.7.21 注意力变体的优化景观 
注意力变体的优化景观是另一个重要的理论问题。优化景观的平滑性、凸性、局部最优的数量等都影响训练的难度和最终性能。 标准注意力的优化景观相对复杂，存在大量的局部最优解和非凸区域。Softmax的非线性变换和矩阵乘法的组合使得损失函数高度非凸。然而，实践中随机梯度下降能够找到较好的解，可能因为存在大量的"好"局部最优。 线性注意力的优化景观可能更加平滑，因为核函数通常是凸的（或接近凸的）。然而，线性注意力的表达能力受限，即使找到全局最优，也可能无法达到标准注意力的性能。 门控注意力的优化景观引入了额外的门控参数，增加了优化复杂度。门控的Sigmoid非线性可能导致梯度消失或爆炸，需要仔细的初始化和学习率调度。 
### 5.7.22 注意力变体的归纳偏置 
不同的注意力变体隐含了不同的归纳偏置，这些偏置影响模型对不同类型数据的适应能力。 标准注意力的归纳偏置最弱，几乎没有对输入结构做任何假设。这使得标准注意力具有很高的通用性，但也可能导致过拟合或难以泛化。 稀疏注意力的归纳偏置是局部性：假设依赖关系主要存在于邻近位置之间。这种偏置与许多自然数据（如文本、图像、音频）的局部结构一致，可以帮助模型更好地泛化。 线性注意力的归纳偏置是低秩性：假设注意力矩阵可以用低秩分解近似。这种偏置与深度学习表示学习的经验观察一致（表示通常具有低维结构）。 门控注意力的归纳偏置是选择性：假设不同特征或位置有不同的重要性，可以通过门控机制学习这种选择性。这种偏置与人类注意力的一致性。 选择注意力变体时，需要考虑数据的特性和任务的归纳偏置。选择与数据特性匹配的变体可以加速学习、提高泛化能力。 
### 5.7.23 本节小结 
本节系统地介绍了注意力机制的主要变体，包括稀疏注意力、线性注意力、Flash注意力、多查询注意力、分组查询注意力和门控注意力等。我们从数学角度分析了每种变体的定义、计算流程、复杂度分析、表达能力，以及它们的优缺点和适用场景。 稀疏注意力通过限制每个位置的注意力窗口大小来降低计算复杂度，典型模式包括滑动窗口、膨胀窗口、随机选择和全局-局部混合。线性注意力通过核函数近似Softmax，将计算复杂度从$O(n^2)$降低到$O(n)$，适合长序列和在线推理场景。Flash注意力通过IO感知的分块计算减少内存访问开销，实现显著的加速。多查询注意力和分组查询注意力通过Key/Value的参数共享减少推理时的缓存需求，是高效推理的关键技术。门控注意力通过引入门控机制增强非线性表达能力，适用于需要细粒度信息控制的场景。 通过本节的学习，读者应该建立起对注意力变体全景图的理解，能够根据实际需求选择和设计合适的注意力机制。注意力变体的研究仍在快速发展，新的变体和优化技术不断涌现。掌握这些变体的数学原理，为进一步研究和创新奠定了坚实的基础。 