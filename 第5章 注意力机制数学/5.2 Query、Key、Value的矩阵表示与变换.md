在上一节中，我们介绍了Scaled Dot-Product Attention的数学公式，其中Query（查询）、Key（键）和Value（值）是三个核心概念。这三个向量构成了注意力机制的"语言"——Query表达了信息需求的抽象表示，Key提供了信息片段的标识符，而Value则承载了实际的信息内容。本节将从矩阵变换的角度深入探讨Query、Key、Value是如何从输入嵌入表示中生成的，以及这三个投影矩阵在数学上的性质和作用。

理解Query、Key、Value的矩阵变换是掌握注意力机制的关键一步。这三个向量并非凭空产生，而是通过可学习的线性变换从输入数据中投射到不同的"语义空间"。这种设计使得模型能够学习到什么样的信息应该被"查询"，什么样的特征应该作为"键"来匹配，以及什么样的内容应该作为"值"来传递。通过本节的学习，读者将建立起对注意力机制内部运作机制的完整理解。


## 5.2.1 输入嵌入矩阵的数学表示

在自然语言处理的语境下，模型的输入通常是一个变长的词元序列。每个词元首先通过词嵌入（Word Embedding）层转换为一个高维向量，捕获该词元的语义信息。设序列长度为$n$，词嵌入维度为$d_{\text{model}}$​，则输入嵌入矩阵$X \in \mathbb{R}^{n \times d_{\text{model}}}$可以表示为：
$$
X = \begin{bmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_n^T \end{bmatrix} \tag{5.2.1}
$$
其中，$x_i \in \mathbb{R}^{d_{\text{model}}}$​是位置$i$处词元的嵌入向量，$X$的第$i$行对应序列中第$i$个位置的嵌入表示。这个嵌入矩阵是整个注意力计算的起点，所有的Query、Key、Value都将从这个基础表示中通过线性变换生成。

值得注意的是，在Transformer架构中，输入嵌入通常还需要加上位置编码（Positional Encoding）以注入序列顺序信息。设位置编码矩阵为$P \in \mathbb{R}^{n \times d_{\text{model}}}$，则最终的输入表示为$X + P$。位置编码的具体形式将在后续章节详细讨论，本节暂时忽略位置编码的影响，专注于Q、K、V变换的数学本质。

## 5.2.2 线性投影的矩阵形式

Query、Key、Value通过三个独立的线性变换从输入嵌入中生成。这三个线性变换可以统一表示为矩阵乘法的形式：
$$
\begin{aligned} Q &= XW_Q \in \mathbb{R}^{n \times d_k} \\ K &= XW_K \in \mathbb{R}^{n \times d_k} \\ V &= XW_V \in \mathbb{R}^{n \times d_v} \end{aligned} \tag{5.2.2}
$$
其中，$W_Q \in \mathbb{R}^{d_{\text{model}} \times d_k}​、W_K \in \mathbb{R}^{d_{\text{model}} \times d_k}、W_V \in \mathbb{R}{d_{\text{model}} \times d_v}$是三个可学习的投影矩阵，它们包含了注意力机制需要学习的全部参数。输入矩阵$X$分别左乘这三个投影矩阵，得到Query矩阵、Key矩阵和Value矩阵。

从线性代数的角度来看，每个投影矩阵定义了从输入空间到各自子空间的一个线性映射。$W_Q$将每个输入嵌入向量$x_i$投影到一个$d_k$​维的查询空间，得到查询向量$q_i = x_i W_Q$​；类似地，$W_K$​和$W_V$分别定义了键空间和值空间的投影。这种设计允许模型在不同的语义空间中表示同一个输入，从而实现灵活的注意力计算。

## 5.2.3 维度设计与参数数量

在实际应用中，Query、Key、Value的维度设计是一个重要的设计决策。最常见的配置是令$d_k = d_v = d_{\text{model}} = d$，即三个空间的维度与输入嵌入维度相同。这种配置下的投影矩阵都是$d \times d$的方阵，参数数量为$3d^2$。另一种常见配置是令$d_k = d_v = d_{\text{model}}/h$，其中$h$是多头注意力中的头数，这种配置在多头注意力的语境下使用。

设多头注意力的头数为$h$，每个头的维度为$d_k = d_v = d_{\text{model}}/h$，则每个头的投影矩阵维度为$d_{\text{model}} \times (d_{\text{model}}/h)$。由于有$h$个头，总的投影参数数量为$h \times 3 \times (d_{\text{model}} \times d_{\text{model}}/h) = 3d_{\text{model}}^2$，与单头配置的参数数量相同。这意味着多头注意力在保持参数总量不变的情况下，增加了表示的丰富性——每个头可以学习不同的投影方向，捕捉输入的不同方面特征。

## 5.2.4 投影矩阵的行列式分析

投影矩阵$W_Q​、W_K、W_V$是方阵还是长矩阵取决于维度的选择。当$d_k < d_{\text{model}}$​时，这些矩阵是"压扁"的变换，将高维输入压缩到低维空间；当$d_k = d_{\text{model}}$时，它们是方阵变换，保持维度不变。理解这两种情况的数学差异对于理解注意力的表达能力至关重要。

对于方阵投影（$d_k = d_{\text{model}}$），投影矩阵$W$是可逆的当且仅当其行列式$\det(W) \neq 0$。可逆性意味着变换是对应的，不会丢失信息；不可逆的变换（奇异矩阵）会压缩输入空间到低维流形，导致信息损失。在实践中，投影矩阵通常通过适当的初始化（如Xavier初始化或Kaiming初始化）来确保在训练初期具有较好的条件数，避免极端的奇异性。

对于非方阵投影（$d_k < d_{\text{model}}$），矩阵$W$的秩最大为$d_k$​。这意味着变换后的表示必然位于一个$d_k$​维的子空间中，原始输入中的一部分信息必然被丢弃。这种维度压缩并非缺点，反而可能是有益的正则化，通过将输入投影到低维空间，模型被迫学习最相关的特征表示，过滤掉噪声和不重要信息。

## 5.2.5 投影矩阵的奇异值分解

奇异值分解（Singular Value Decomposition，SVD）为理解投影矩阵的几何性质提供了强大的工具。任意矩阵$W \in \mathbb{R}^{m \times n}$都可以分解为：
$$
W = U\Sigma V^T \tag{5.2.3}
$$
其中，$U \in \mathbb{R}^{m \times m}$和$V \in \mathbb{R}^{n \times n}$是正交矩阵，$\Sigma \in \mathbb{R}^{m \times n}$是对角矩阵，其对角线元素为奇异值$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0，r = \text{rank}(W)$。

将这一分解应用于投影矩阵$W_Q$​，我们可以将Query生成过程理解为：首先在输入空间中进行旋转和反射（$V^T$），然后沿奇异值方向进行缩放（$\Sigma$），最后在输出空间中进行另一次旋转和反射（$U$）。奇异值的大小决定了各个方向上的"拉伸"程度：较大的奇异值对应的方向会被放大，较小的奇异值对应的方向会被压缩。

从表示学习的角度来看，训练过程实际上是在调整这些投影矩阵的奇异值结构。模型学习到的重要特征会对应于较大的奇异值，而不重要的特征或噪声会对应于较小的奇异值。这种通过学习得到的特征选择机制是注意力机制强大表达能力的重要来源。

## 5.2.6 正交投影与投影矩阵的初始化

投影矩阵的初始化策略对模型的训练动态有重要影响。一个常见的初始化方案是Xavier初始化（或Glorot初始化），其设计目标是保持每层激活值和梯度的方差在各层之间稳定。对于维度为$d_{\text{in}} \times d_{\text{out}}$的矩阵，Xavier初始化的方差为：
$$
\text{Var}(W) = \frac{2}{d_{\text{in}} + d_{\text{out}}} \tag{5.2.4}
$$
这种初始化确保了信号和梯度在正向传播和反向传播过程中都不会过度放大或衰减。对于Query、Key、Value的投影矩阵，假设$d_{\text{in}} = d_{\text{model}}、d_{\text{out}} = d_k$，则每个元素的方差初始化为$\frac{2}{d_{\text{model}} + d_k}$。

另一种常见的初始化是Kaiming初始化（或称He初始化），其方差为$\frac{2}{d_{\text{in}}}$​，专门为ReLU激活函数设计。虽然Transformer中没有使用ReLU（使用的是GELU），但Kaiming初始化在实践中也被广泛使用。初始化的选择会影响训练初期的梯度流动和收敛速度，值得在实际应用中仔细考量。

## 5.2.7 查询空间、键空间与值空间的语义分离

Query、Key、Value通过三个独立的投影矩阵被映射到不同的空间，这种设计蕴含着深刻的语义洞见。考虑一个具体的例子：假设我们正在处理一个阅读理解任务，模型需要根据问题"大熊猫的主要食物是什么？"从文章中提取相关信息。在这个场景中，问题"大熊猫的主要食物是什么？"将被编码为Query向量，表示信息需求的抽象表示；文章中的各个词元（如"竹子"、"肉"、"苹果"等）将被编码为Key向量，表示各个信息片段的标识符；文章中各位置的上下文信息将被编码为Value向量，表示各片段携带的实际信息内容。

Query空间和Key空间的设计体现了"需求"与"供给"的分离。相同的输入嵌入在不同空间中被赋予了不同的角色：作为Query时，它表达的是"我需要什么信息"；作为Key时，它表达的是"我这里有什么信息"。这种分离允许模型学习到高度非线性的匹配函数，Query和Key不必在原始嵌入空间中相似，而是可以通过投影变换到隐空间中形成有意义的匹配。

值空间的设计同样具有深意。Value向量承载的是信息的内容本身，它需要保留足够的细节以供后续层使用。与Query和Key不同，Value不需要参与匹配计算，因此可以保留更多的原始信息。在某些变体设计中，Value空间可以与Query/Key空间有不同的维度，这为架构设计提供了额外的灵活性。

## 5.2.8 投影的几何视角：空间变换与特征提取

从几何角度来看，每个投影矩阵定义了一个从输入空间到输出空间的线性变换。这种变换可以分解为旋转、缩放和反射等基本操作的组合。设输入向量为$x$，经过投影矩阵$W_Q$​变换后得到Query向量$q = xW_Q$​。这个变换将$x$从原始的嵌入空间"旋转"到查询空间的新坐标系中。

在训练过程中，投影矩阵的列向量逐渐学习到输入数据的重要特征方向。考虑$W_Q$​的第一列，它对应于$q$的第一个维度，其方向决定了模型在评估"信息需求"时首先考虑的特征。类似地，$W_K$的列向量决定了"信息标识"的特征表示。模型通过学习这些投影矩阵，使得语义相关的Query和Key在变换后的空间中具有较高的相似度（点积）。

这种几何解释与主成分分析（PCA）有着有趣的类比。在PCA中，数据被投影到方差最大的正交方向上；而在注意力机制中，投影矩阵的列向量是通过数据驱动的学习得到的，不一定是正交的，但同样捕获了输入数据的重要结构。区别在于：PCA是一个无监督的降维方法，而注意力机制的投影是有监督的——它们的学习目标是最小化预测损失，这使得学习到的特征直接与下游任务相关。

## 5.2.9 注意力分数的矩阵分解表示

将Query和Key的生成过程代入注意力分数的计算中，我们可以得到一个统一的矩阵表达式。注意力分数矩阵$S$可以表示为：
$$
S = \frac{1}{\sqrt{d_k}} XW_Q W_K^T X^T \tag{5.2.5}
$$
这个表达式揭示了注意力分数的矩阵分解结构。令$M = W_Q W_K^T \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$，则：
$$
S = \frac{1}{\sqrt{d_k}} XMX^T \tag{5.2.6}
$$
矩阵$M$是Query投影和Key投影的某种"交叉协方差"矩阵，它捕获了Query空间和Key空间之间的关系。注意力分数$QK^T$实际上是在$M$定义的二次型下，$X$各行之间的"相似度"。这个视角将注意力的计算置于更一般的二次型框架中，便于进行理论分析。

从优化的角度来看，训练过程实际上是在调整$W_Q$和$W_K$，从而调整$M$的结构。模型学习到的$M$决定了什么样的输入模式会产生较高的注意力分数。这种学习到的相似度度量不同于简单的余弦相似度或欧氏距离，而是数据驱动的、任务相关的相似度函数。
## 5.2.10 反向传播中的梯度计算

理解Query、Key、Value投影矩阵的梯度对于分析模型的训练动态至关重要。考虑注意力输出的损失函数$\mathcal{L}$，我们希望计算$\frac{\partial \mathcal{L}}{\partial W_Q}​、\frac{\partial \mathcal{L}}{\partial W_K}$​和$\frac{\partial \mathcal{L}}{\partial W_V}$​。根据链式法则，这些梯度可以通过注意力输出对各矩阵的雅可比矩阵来计算。

以$W_Q$​的梯度为例，设$O = \text{Attention}(Q, K, V) = \text{Softmax}(QK^T/\sqrt{d_k})V$为注意力输出，则：
$$
\frac{\partial \mathcal{L}}{\partial W_Q} = \frac{\partial \mathcal{L}}{\partial O} \cdot \frac{\partial O}{\partial Q} \cdot \frac{\partial Q}{\partial W_Q} \tag{5.2.7}
$$
其中$\frac{\partial Q}{\partial W_Q} = X^T$，这是一个简单的矩阵乘法梯度。关键在于$\frac{\partial O}{\partial Q}$，它涉及到Softmax和矩阵乘法的复合梯度。Softmax的梯度形式已在7.1节中给出，而矩阵乘法的梯度为$\frac{\partial (AV)}{\partial A} = V^T$。

综合以上分析，$W_Q$的梯度可以写为：
$$
\frac{\partial \mathcal{L}}{\partial W_Q} = X^T \cdot \left( \frac{\partial \mathcal{L}}{\partial O} \cdot V^T \cdot \text{SoftmaxGrad}(S) \right) \tag{5.2.8}
$$
其中$\text{SoftmaxGrad}(S)$是缩放后注意力分数矩阵$S$的Softmax梯度矩阵。这个表达式表明，梯度从输出端通过Value矩阵反向传播回Query投影，再通过输入嵌入矩阵$X$累积到$W_Q$​。

## 5.2.11 缩放对梯度稳定性的影响

回顾7.1节中引入的缩放因子$\sqrt{d_k}$​​，它在梯度计算中同样发挥着重要作用。考虑未缩放的注意力分数$S_{\text{unscaled}} = QK^T$，其Softmax梯度与缩放后的版本有所不同。设$P = \text{Softmax}(S_{\text{unscaled}})$，则对于任意$i,j$：
$$
\frac{\partial P_{ij}}{\partial S_{\text{unscaled}, ab}} = \begin{cases} P_{ij}(1-P_{ij}) & \text{if } i=a, j=b \\ -P_{ij}P_{ab} & \text{if } i=a, j \neq b \\ 0 & \text{otherwise} \end{cases} \tag{5.2.9}
$$
注意这里的梯度不依赖于$S_{\text{unscaled}}$​的具体值，只依赖于$P$。然而，$P$本身的分布高度依赖于$S_{\text{unscaled}}$的量级。当$S_{\text{unscaled}}$的方差较大时，$P$趋向于尖锐分布（接近one-hot），此时对角线元素$P_{ii}(1-P_{ii})$趋向于0，梯度消失。

引入缩放因子后，$S_{\text{scaled}} = S_{\text{unscaled}} / \sqrt{d_k}$​​的方差被控制在一个稳定的范围内（如前节分析的结论），Softmax输出$P_{\text{scaled}}$保持在合理的分布范围内。这意味着无论模型深度如何或序列长度如何，梯度都能稳定地流动，避免了训练过程中的梯度消失或梯度爆炸问题。缩放因子的这一作用与批归一化（Batch Normalization）在卷积网络中的作用有异曲同工之妙，它们都通过控制激活值的分布来稳定训练动态。

## 5.2.12 多层堆叠中的表示变换

在Transformer架构中，注意力层通常被多次堆叠（通过残差连接和层归一化），每一层的输出作为下一层的输入。考虑两层堆叠的情况：设第一层的输入为$X^{(0)}$，经过第一层注意力后得到$X^{(1)}$，再经过第二层注意力得到$X^{(2)}$。每一层的Query、Key、Value投影都是独立的可学习参数：
$$
\begin{aligned}
Q^{(1)} &= X^{(0)}W_Q^{(1)}, \quad K^{(1)} = X^{(0)}W_K^{(1)}, \quad V^{(1)} = X^{(0)}W_V^{(1)} \\ Q^{(2)} &= X^{(1)}W_Q^{(2)}, \quad K^{(2)} = X^{(1)}W_K^{(2)}, \quad V^{(2)} = X^{(1)}W_V^{(2)} \end{aligned} \tag{5.2.10}$$ 这种多层堆叠使得模型能够学习到多层次的抽象表示。第一层可能学习到词汇级别的关联（如"猫"和"宠物"的关联），第二层可能学习到短语级别的关联（如"可爱的猫"和"毛茸茸的宠物"的关联），依此类推。每一层的投影矩阵$W_Q^{(l)}$、$W_K^{(l)}$、$W_V^{(l)}$都学习不同层次的特征提取器。 残差连接在多层堆叠中起着关键作用。设注意力层的输出为$\text{Attention}(Q, K, V)$，则残差连接后的输出为： $$X^{(l+1)} = X^{(l)} + \text{LayerNorm}(\text{Attention}(Q, K, V)) \tag{5.2.11}$$ 残差连接确保了梯度可以直接从输出端流向输入端，即使在非常深的网络中也能保持有效的梯度流动。这与缩放因子的作用相辅相成：缩放因子控制了注意力计算的数值稳定性，残差连接则控制了多层堆叠的梯度稳定性。 
## 5.2.13 投影矩阵的高效实现 
在实际实现中，Query、Key、Value的投影可以通过一次大的矩阵乘法高效完成。设输入矩阵为$X \in \mathbb{R}^{n \times d_{\text{model}}}$，三个投影矩阵拼接为一个大的投影矩阵$W_{QKV} \in \mathbb{R}^{d_{\text{model}} \times 3d_k}$，则： $$XW_{QKV} = [Q, K, V] \in \mathbb{R}^{n \times 3d_k} \tag{5.2.12}$$ 这个单一的矩阵乘法等价于三个独立的矩阵乘法$XW_Q$、$XW_K$、$XW_V$，但只需要一次矩阵乘法操作。现代深度学习框架（如PyTorch、TensorFlow）和GPU硬件都对这种批量的矩阵乘法进行了高度优化，一次大的矩阵乘法通常比三次小的矩阵乘法更快。 拼接后的矩阵可以进一步通过视图变换（view/reshape）拆分为三个独立的矩阵Q、K、V。这种实现方式不仅减少了计算开销，还提高了内存访问效率——一次连续的内存读取可以获取所有三个投影的结果，而三次独立的矩阵乘法需要三次内存读取。 
## 5.2.14 偏置向量的作用 
在Query、Key、Value的投影中，通常还会加入偏置向量（Bias Vector）。完整的投影公式为： $$\begin{aligned} Q &= XW_Q + b_Q \\ K &= XW_K + b_K \\ V &= XW_V + b_V \end{aligned} \tag{5.2.13}$$其中$b_Q \in \mathbb{R}^{d_k}$、$b_K \in \mathbb{R}^{d_k}$、$b_V \in \mathbb{R}{d_v}$是偏置向量。偏置向量允许投影后的表示不在过原点，提供了额外的灵活性。从几何角度来看，偏置向量定义了输出空间的一个平移变换；从优化角度来看，偏置向量为模型提供了独立的"基准"偏移调节能力。 在Transformer的原始实现中，偏置向量是被使用的，但在某些后续变体中（如某些高效Transformer变体），偏置向量被移除以减少参数量。偏置向量的影响相对较小，但其存在通常能提供更好的性能。在实践中，是否使用偏置向量可以根据具体的任务需求和计算资源进行权衡。 
## 5.2.15 训练与推理阶段的差异 
Query、Key、Value的生成在训练阶段和推理阶段有所不同。在训练阶段，由于需要计算完整的注意力分数矩阵以进行反向传播，所有位置的信息都可以被并行处理。Query、Key、Value可以一次性为整个序列生成，然后进行注意力计算。 在推理阶段（自回归生成时），序列是逐步构建的。在生成第$t$个词元时，Query向量只针对位置$t$生成，而Key和Value向量需要针对位置$1$到$t$生成。这意味着Key和Value矩阵会随着序列的增长而扩展，需要在内存中缓存历史位置的K和V向量，以便在生成新位置时复用。这种缓存机制是高效推理实现的关键优化点。 缓存K和V向量需要额外的内存开销。对于长度为$n$、维度为$d$的序列，缓存开销为$O(nd)$。对于长序列生成，这个开销可能成为内存瓶颈。各种高效的Transformer变体（如线性注意力、稀疏注意力等）正是针对这一问题的改进尝试。 
## 5.2.16 共享Q、K、V投影的变体
标准的Transformer使用三个独立的投影矩阵$W_Q$、$W_K$、$W_V$。然而，一些研究探索了这些矩阵之间的参数共享策略，以减少参数量或引入归纳偏置。一种极端的变体是使用相同的投影矩阵用于Q、K、V： $$Q = XW, \quad K = XW, \quad V = XW \tag{5.2.14}$$这种设计将参数量从$3d^2$减少到$d^2$，但通常会导致性能下降，因为Query和Key不再能够学习到独立的表示空间。 另一种折中方案是共享$W_Q$和$W_K$但保持$W_V$独立，这被称为"共享键值注意力"（Shared Key-Value Attention）。这种设计在某些任务上表现良好，因为它引入了Q和K的对称性假设，认为查询和键应该使用相同的表示空间。 
## 5.2.17 双向注意力与单向注意力的区别 
在编码器（Encoder）和解码器（Decoder）中，Q、K、V的使用有所不同。在编码器的自注意力层中，所有位置可以相互"attend to"，Query、Key、Value都来自同一个序列的嵌入，这种注意力是双向的。在解码器的自注意力层中（如前文讨论的因果注意力），需要使用掩码来防止位置$i$ attending to位置$j > i$，这种注意力是单向的。 此外，在解码器中还存在交叉注意力（Cross Attention）层，其中Query来自解码器的前一层表示，而Key和Value来自编码器的输出。这种设计允许解码器在生成每个位置时"查询"整个输入序列的信息，是序列到序列模型的核心机制。交叉注意力中的Query、Key、Value来自不同的源，因此需要分别处理。 交叉注意力的数学形式与自注意力相同，但输入源不同： $$\text{CrossAttention}(Q_{\text{dec}}, K_{\text{enc}}, V_{\text{enc}}) = \text{Softmax}\left(\frac{Q_{\text{dec}}K_{\text{enc}}^T}{\sqrt{d_k}}\right)V_{\text{enc}} \tag{5.2.15}$$其中$Q_{\text{dec}}$来自解码器，$K_{\text{enc}}$和$V_{\text{enc}}$来自编码器。这种设计使得模型能够灵活地建模输入和输出之间的对应关系，是Transformer在机器翻译等任务上取得突破的关键因素。
## 5.2.18 小结 
本节深入探讨了Query、Key、Value的矩阵表示与变换。我们从输入嵌入矩阵$X$出发，详细推导了Query、Key、Value的线性投影过程：$Q = XW_Q$、$K = XW_K$、$V = XW_V$。我们分析了投影矩阵的数学性质，包括奇异值分解、维度设计和初始化策略，以及这些因素如何影响模型的表达能力和训练稳定性。 Query、Key、Value的设计体现了"语义分离"的思想：Query表达信息需求，Key提供信息标识，Value承载信息内容。通过将输入嵌入投影到不同的语义空间，注意力机制能够学习到灵活、非线性的匹配函数，实现高效的信息检索和聚合。 我们还讨论了参数学习中的梯度流分析，揭示了缩放因子在梯度稳定性中的关键作用；分析了训练与推理阶段的差异，包括K、V向量缓存的实现考量；并简要介绍了Q、K、V的若干变体设计及其适用场景。 通过本节的学习，能够从矩阵变换的角度深入理解注意力机制的内部运作机制，为后续学习多头注意力、位置编码等高级主题奠定坚实的数学基础。下一节，我们将进一步探讨多头注意力的矩阵推导，理解如何通过并行多个注意力头来增强模型的表达能力。