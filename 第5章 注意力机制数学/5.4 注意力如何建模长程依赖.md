## 5.4 注意力如何建模长程依赖
在序列数据处理中，长程依赖建模是一个核心挑战。所谓长程依赖，指的是序列中相距较远的位置之间存在的语义关联或结构关系。例如，在句子"尽管昨天下了很大的雨，但我们仍然决定去爬山，最终我们在山顶欣赏到了美丽的日出"中，开头的"尽管"与后面的"但"形成转折关系，"爬山"与"日出"存在潜在的语义关联。这类依赖关系的跨度可能跨越数十个词元，甚至更长。传统的循环神经网络在建模长程依赖时面临根本性的困难，信息需要沿着序列逐步传递，路径长度与依赖关系的跨度成正比，这不仅导致计算效率低下，还容易引发梯度消失问题。注意力机制通过一种革命性的设计解决了这一问题：它允许序列中任意两个位置之间直接建立联系，无需经过中间节点的传递。本节将从数学角度深入分析注意力机制是如何实现这一突破的，探讨其建模长程依赖的数学原理，并分析这一能力的理论极限。

### 5.4.1 路径长度与依赖建模
从信息论的角度来看，序列中两个位置之间的依赖关系可以用路径长度来度量。路径长度指的是信息从一个位置传递到另一个位置所需经过的中间节点数，这一概念在图论框架下有着精确的数学定义。理解路径长度的概念及其对依赖建模的影响，是掌握注意力机制优势的关键所在。
考虑一个简单的循环神经网络单元，其隐藏状态的更新公式为：
$$
h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t) \tag{5.4.1}
$$
其中，$h_t$表示时刻$t$的隐藏状态，$x_t$表示时刻$t$的输入，$W_{hh}$和$W_{xh}$是权重矩阵。在这个结构中，从位置$i$到位置$j$（$i < j$）的路径长度为$j - i$，信息必须经过所有中间位置的隐藏状态才能传递。这意味着，如果要建模跨越$k$个位置的依赖关系，信息必须经过$k$步的顺序传递，每一步都可能引入信息的损失或变形。
从梯度分析的角度来看，这一问题变得更加严重。考虑损失函数$\mathcal{L}$关于$x_1$的梯度$\frac{\partial \mathcal{L}}{\partial x_1}$，根据链式法则，这个梯度可以展开为：
$$
\frac{\partial \mathcal{L}}{\partial x_1} = \frac{\partial \mathcal{L}}{\partial h_n} \cdot \frac{\partial h_n}{\partial h_{n-1}} \cdot \frac{\partial h_{n-1}}{\partial h_{n-2}} \cdots \frac{\partial h_2}{\partial h_1} \cdot \frac{\partial h_1}{\partial x_1} \tag{5.4.2}
$$
雅可比矩阵$\frac{\partial h_t}{\partial h_{t-1}}$的范数决定了梯度在传递过程中的缩放程度。设$\lambda_{\max}$是$W_{hh}$的最大奇异值，则$\left\|\frac{\partial h_t}{\partial h_{t-1}}\right\|_2 \approx \lambda_{\max}$（忽略非线性激活的影响）。经过$n-1$步传递后，梯度的范数缩放约为$\lambda_{\max}^{n-1}$。当$\lambda_{\max} < 1$时，梯度会指数级衰减，导致梯度消失问题；当$\lambda_{\max} > 1$时，梯度会指数级增长，导致梯度爆炸问题。这种与路径长度成正比的梯度缩放特性，是循环神经网络建模长程依赖的根本性障碍。
注意力机制从根本上改变了这一结构。在自注意力机制中，序列中任意两个位置之间可以直接建立联系，无需经过中间节点。设$A$为注意力权重矩阵，$A_{ij}$表示位置$i$对位置$j$的注意力权重，$V$为值矩阵，则注意力输出为$O = AV$。位置$i$的输出$o_i$是所有位置$j$的值$v_j$的加权平均：
$$
o_i = \sum_{j=1}^{n} A_{ij}v_j \tag{5.4.3}
$$
在这个表达式中，$o_i$直接依赖于所有$v_j$，无论$j$与$i$的距离有多远。这种直接依赖关系意味着从位置$i$到位置$j$的路径长度恒为1，与它们在序列中的距离无关。这种"扁平化"的信息聚合方式，是注意力机制建模长程依赖的数学基础。
为了量化注意力机制建模长程依赖的能力，我们需要定义一些数学度量。设序列长度为$n$，位置$i$和位置$j$之间的距离为$|i - j|$。注意力权重矩阵$A$定义了一种依赖关系图，其中$A_{ij} > \epsilon$（$\epsilon$为某个阈值）表示位置$i$依赖位置$j$。定义有效依赖跨度（Effective Dependency Span）为：
$$
\text{span}_{\text{eff}}(i) = \max\{ |i - j| : A_{ij} > \epsilon \} \tag{5.4.4}
$$
对于位置$i$，这个度量表示它实际关注的"最远"位置与自身的距离。如果注意力权重主要集中在邻近位置，$\text{span}_{\text{eff}}(i)$较小，说明该位置的注意力是"局部"的；如果注意力权重分散在很远的位置，$\text{span}_{\text{eff}}(i)$较大，说明该位置的注意力是"全局"的。在训练良好的Transformer中，不同层和不同头的有效依赖跨度差异很大，这种层级化的依赖建模是Transformer成功捕获复杂语言结构的关键因素。

### 5.4.2 注意力与卷积及循环结构的数学比较
为了更全面地理解注意力机制的数学特性，我们需要将其与卷积神经网络和循环神经网络进行系统的比较。这三种架构在信息传递方式、计算模式和归纳偏置方面存在本质差异，这些差异决定了它们在长程依赖建模方面的不同表现。
首先，从信息传递路径的角度进行比较。在循环神经网络中，信息传递路径必须沿着序列顺序：从$i$到$j > i$，路径为$i \rightarrow i+1 \rightarrow \cdots \rightarrow j$，路径长度为$j - i$。在卷积神经网络中，每个位置只能与一个固定大小窗口内的位置交互，感受野（Receptive Field）的大小受到卷积核尺寸的限制。以卷积核大小为$3 \times 3$的卷积为例，第$l$层的感受野大小约为$2l + 1$，要覆盖长度为$n$的序列，需要堆叠约$n/2$层。在自注意力中，位置$i$可以直接"看到"所有位置$j$，因此从$i$到$j$的路径长度恒为1，存在直接边$i \rightarrow j$。
这种路径长度的差异可以从数学上精确描述。定义从位置$i$到位置$j$的最小路径长度为：
$$
\text{dist}(i, j) = \begin{cases} 0 & \text{if } i = j \\ 1 & \text{if } A_{ij} > \epsilon \\ \min\{ \text{路径长度} \} & \text{otherwise} \end{cases} \tag{5.4.5}
$$
在注意力机制中，由于$A_{ij}$通常不会精确为零（Softmax输出的每个元素都是正的），我们通常假设$\text{dist}(i, j) = 1$对于所有$i \neq j$。这意味着任意两个位置之间都存在直接的信息传递路径，这是注意力机制的核心数学优势。
其次，从计算复杂度的角度进行比较。对于长度为$n$的序列，建模全局长程依赖（所有位置对之间的依赖）的时间复杂度如下。循环神经网络为$O(n \cdot d^2)$（每步进行矩阵乘法），且由于需要顺序计算，无法并行化。卷积神经网络的复杂度取决于卷积核的数量和大小，对于局部依赖可以通过卷积高效处理，但对于长程依赖需要多层堆叠，累积复杂度同样可观。注意力机制为$O(n^2 \cdot d)$（计算$QK^T$和$AV$），但可以完全并行化。
虽然注意力的渐进复杂度$O(n^2)$高于循环神经网络的$O(n)$，但这里的区别在于依赖关系的类型。循环神经网络的$O(n)$复杂度只能建模"局部"依赖——信息只能从相邻位置逐步传递。如果要建模跨越整个序列的长程依赖，循环神经网络实际上需要进行$O(n)$步的顺序计算，总复杂度为$O(n^2)$。注意力的$O(n^2)$是一次性建模所有位置对之间的依赖，包括长程依赖。换一种说法：循环神经网络建模长程依赖的"有效复杂度"是$O(n^2)$（需要$n$步顺序计算），而注意力的"有效复杂度"也是$O(n^2)$（可以并行）。但注意力的优势在于：无论依赖关系的跨度如何，建模成本都是固定的；而循环神经网络的建造成本与依赖跨度成正比。
第三，从内存带宽效率的角度进行比较。在循环神经网络中，每一步计算都需要读取前一步的隐藏状态，内存访问模式是"顺序的、依赖的"。这种模式不利于GPU的并行执行——当前一步的计算未完成时，下一步无法开始。注意力机制的内存访问模式更加规则。计算$QK^T$时，需要读取完整的$Q$和$K$矩阵，这些数据可以一次性加载到高速缓存中。计算$AV$时，需要读取完整的$A$和$V$矩阵，同样可以进行批量读取。这种"一次性加载、批量计算"的模式与现代GPU的架构特性高度匹配，能够充分利用内存带宽。
注意力机制的归纳偏置与循环结构和卷积结构也存在本质差异。循环神经网络的归纳偏置包括：序列顺序假设（假设信息沿着时间顺序流动）、局部性假设（每步只处理当前输入和前一步状态）、马尔可夫假设（未来状态只依赖于当前状态）。卷积网络的归纳偏置包括：局部性假设（只关注局部邻域）、平移不变性（对输入位置的平移具有鲁棒性）、层次化假设（通过堆叠层来扩大感受野）。注意力机制的归纳偏置包括：全连接假设（任意位置之间可以存在依赖关系）、内容驱动假设（依赖关系的强度由内容相似度决定）、位置编码假设（位置信息需要显式注入）。这种"弱归纳偏置"使得注意力机制具有很高的通用性，能够处理各种类型的依赖关系，但同时也意味着如果没有足够的数据或适当的正则化，模型可能过拟合或学习到不合理的依赖模式。

### 5.4.3 信息传播效率的量化解释
为了更深入地理解注意力机制在长程依赖建模方面的效率优势，我们需要建立一套量化指标来评估信息传播效率。这些指标能够帮助我们从数学上精确描述注意力机制相对于其他架构的优势所在，并指导我们进行更高效的架构设计。
首先，我们定义注意力权重的分布特性来量化信息聚合模式。设$A$为注意力权重矩阵，$A_{ij}$表示位置$i$对位置$j$的注意力权重。$A$的每一行是一个概率分布：$A_{ij} \geq 0$且$\sum_j A_{ij} = 1$。定义注意力权重的熵（Entropy）：
$$
H(A_i) = -\sum_{j=1}^{n} A_{ij} \log A_{ij} \tag{5.4.6}
$$
其中，$A_i$是$A$的第$i$行。熵度量了注意力分布的"均匀程度"：当注意力均匀分布在所有位置时，熵最大（$H_{\max} = \log n$）；当注意力完全集中于一个位置时，熵最小（$H_{\min} = 0$）。实证研究表明，训练良好的Transformer中不同头表现出不同的熵分布模式。某些头具有较高的熵，注意力分布较为均匀，表明它们在"探索"整个序列的信息；某些头具有较低的熵，注意力分布较为集中，表明它们在"聚焦"于少数关键位置。这种多样化的分布模式使得模型能够同时捕获全局信息和局部细节。
其次，我们分析局部注意力与全局注意力的数学特性。局部注意力指的是注意力权重集中在相邻或接近的位置。数学上，局部注意力满足：存在常数$C$，使得当$|i - j| > C$时，$A_{ij}$较小（接近0或显著小于邻居位置的权重）。局部注意力类似于卷积操作，关注输入的局部窗口，适合捕获短语结构、词序信息等局部依赖。全局注意力指的是注意力权重分布在整个序列上，没有明显的局部偏向。数学上，全局注意力满足：对于任意距离$d$，都存在一定比例的注意力权重分配给距离为$d$的位置对，适合捕获跨越长距离的语义关联、篇章级别的指代关系等。在Transformer中，不同层和头表现出不同的注意力模式：靠近输入的层（浅层）往往表现出更多的局部注意力，捕获词汇级别和短语级别的特征；靠近输出的层（深层）往往表现出更多的全局注意力，捕获句子级别和篇章级别的特征。
第三，我们建立多层堆叠的信息传递模型来分析层级化的依赖建模。设第$l$层的输入为$X^{(l)}$，输出为$X^{(l+1)}$。自注意力的输出为：
$$
\text{Attention}^{(l)} = \text{Softmax}\left(\frac{X^{(l)}W_Q^{(l)} (X^{(l)}W_K^{(l)})^T}{\sqrt{d_k}}\right) X^{(l)}W_V^{(l)} \tag{5.4.7}
$$
加上残差连接和层归一化后：
$$
X^{(l+1)} = \text{LayerNorm}\left(X^{(l)} + \text{Attention}^{(l)}\right) \tag{5.4.8}
$$
这个递推关系展示了信息如何在层间流动。位置$i$在第$l+1$层的表示$X^{(l+1)}_i$是$X^{(l)}_i$（残差连接保留的原始信息）与$\text{Attention}^{(l)}_i$（从整个序列聚合的信息）的融合。为了分析多层注意力如何建模长程依赖，考虑一个两层的情况。第一层学习到某种"桥接"注意力模式：位置$n$关注位置$m$（$1 < m < n$），位置$m$关注位置$1$。设第一层的注意力权重为$A^{(1)}$，则位置$n$的输出近似为$A^{(1)}_{nm} v_m$，而位置$m$的输出近似为$A^{(1)}_{m1} v_1$。因此，位置$1$的信息通过位置$m$作为"中继"，间接传递到了位置$n$。虽然这种传递不是直接的（需要两步），但它展示了多层注意力如何通过"信息中继"来建模超长距离的依赖关系。
第四，我们分析残差连接对信息传播效率的影响。残差连接提供了一条"捷径"，允许信息直接从较浅的层流向较深的层，无需经过中间层的变换。考虑一个$L$层的Transformer，位置$i$的原始嵌入$X_i^{(0)}$通过残差连接直接参与到最终层的表示：
$$
X_i^{(L)} = X_i^{(0)} + \Delta X_i^{(1)} + \Delta X_i^{(2)} + \cdots + \Delta X_i^{(L)} \tag{5.4.9}
$$
其中，$\Delta X_i^{(l)}$是第$l$层的残差贡献。位置$i$的最终表示保留了原始嵌入的直接成分，这确保了底层的信息不会被高层完全覆盖。从梯度的角度，残差连接提供了一条梯度传播的"高速公路"，使得梯度可以"无损"地回传到第一层，即使中间层的梯度很小，原始输入的梯度贡献仍然保持完整。这解决了深层网络的梯度消失问题，使得模型能够稳定地训练，从而间接提升了长程依赖建模的效率。
从函数空间的角度来看，注意力机制定义了一类特定的函数。设输入序列为$X \in \mathbb{R}^{n \times d}$，注意力输出为$\text{Attention}(X) = \text{Softmax}(XW_Q W_K^T X^T / \sqrt{d_k}) XW_V$，这个函数可以表示为：
$$
f(X) = A(X) \cdot XW_V \tag{5.4.10}
$$
其中，$A(X) = \text{Softmax}(XW_Q W_K^T X^T / \sqrt{d_k})$是输入依赖的注意力矩阵。注意力机制能够表示的函数类与传统的循环网络或卷积网络有显著差异：循环网络的函数类是"递归函数"，输出通过递归关系定义，信息必须逐步传递；卷积网络的函数类是"局部函数"，每个输出位置只依赖于输入的一个局部窗口；注意力机制的函数类是"全局函数"，每个输出位置可以依赖于整个输入序列。这种全局性使得注意力机制在理论上能够建模任意复杂的长程依赖模式，只要存在一种注意力权重配置使得对于每个位置$i$，注意力权重$A_{ij}$正确地反映了位置$j$对位置$i$的重要性，模型就能够捕获任意位置对之间的依赖关系。

### 5.4.4 本节小结
本节深入分析了注意力机制建模长程依赖的数学原理，从路径长度的量化定义出发，系统比较了注意力机制与传统循环神经网络和卷积神经网络在信息传播效率方面的差异。
在路径长度与依赖建模方面，我们揭示了传统循环结构的根本性困难：信息需要沿着序列逐步传递，路径长度与依赖关系的跨度成正比，导致梯度指数级衰减或爆炸。注意力机制通过全连接结构从根本上解决了这一问题，任意两个位置之间的路径长度恒为1，与位置距离无关。
在注意力与卷积及循环结构的数学比较方面，我们从信息传递路径、计算复杂度和内存带宽效率三个维度进行了系统分析。注意力的$O(n^2)$复杂度虽然高于循环神经网络的$O(n)$，但其并行化特性和固定的建模成本使其在长程依赖建模方面具有显著优势。
在信息传播效率的量化解释方面，我们建立了注意力权重熵、局部注意力与全局注意力、多层信息传递模型等量化指标，从理论和实证两个角度分析了注意力机制的高效性。残差连接作为信息传播的"高速公路"，进一步增强了深层注意力网络的长程依赖建模能力。
通过本节的学习，读者应该能够从数学层面深入理解注意力机制为何能够有效建模长程依赖，以及这种能力的来源和边界。