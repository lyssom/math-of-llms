在序列数据处理中，长程依赖（Long-Range Dependency）建模是一个核心挑战。所谓长程依赖，指的是序列中相距较远的位置之间存在的语义关联或结构关系。例如，在句子"尽管昨天下了很大的雨，但我们仍然决定去爬山，最终我们在山顶欣赏到了美丽的日出"中，开头的"尽管"与后面的"但"形成转折关系，"爬山"与"日出"存在潜在的语义关联。这类依赖关系的跨度可能跨越数十个词元，甚至更长。

传统的循环神经网络（Recurrent Neural Network，RNN）在建模长程依赖时面临根本性的困难。信息需要沿着序列逐步传递，路径长度与依赖关系的跨度成正比，这不仅导致计算效率低下，还容易引发梯度消失问题，当信息需要经过很多步传递时，梯度信号会指数级衰减，使得模型难以学习到远距离位置之间的关联。注意力机制通过一种革命性的设计解决了这一问题：它允许序列中任意两个位置之间直接建立联系，无需经过中间节点的传递。本节将从数学角度深入分析注意力机制是如何实现这一突破的，探讨其建模长程依赖的数学原理，并分析这一能力的理论极限。
## 5.4.1 循环神经网络的梯度流问题

为了理解注意力机制的突破，我们首先需要分析传统循环神经网络在长程依赖建模方面的局限性。考虑一个简单的RNN单元，其隐藏状态的更新公式为：
$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t) \tag{5.4.1}
$$
其中，$h_t$​是时刻$t$的隐藏状态，$x_t$​是时刻$t$的输入，$W_{hh}$​和$W_{xh}$​是权重矩阵。现在，考虑从时刻$1$到时刻$n$的信息传递。如果我们希望$h_n$​能够感知到$x_1$​的信息，那么$x_1$​的影响需要经过$n−1$步的传递才能到达$h_n$。

从梯度分析的角度来看，考虑损失函数$\mathcal{L}$关于$x_1$​的梯度$\frac{\partial \mathcal{L}}{\partial x_1}$​。根据链式法则，这个梯度可以展开为：
$$
\frac{\partial \mathcal{L}}{\partial x_1} = \frac{\partial \mathcal{L}}{\partial h_n} \cdot \frac{\partial h_n}{\partial h_{n-1}} \cdot \frac{\partial h_{n-1}}{\partial h_{n-2}} \cdots \frac{\partial h_2}{\partial h_1} \cdot \frac{\partial h_1}{\partial x_1} \tag{5.4.2}
$$
雅可比矩阵$\frac{\partial h_t}{\partial h_{t-1}}$​​的范数决定了梯度在传递过程中的缩放程度。设$⁡\lambda_{\max}$​是$W_{hh}$的最大奇异值，则$⁡\left\|\frac{\partial h_t}{\partial h_{t-1}}\right\|_2 \approx \lambda_{\max}​$（忽略非线性激活的影响）。经过$n−1$步传递后，梯度的范数缩放约为$\lambda_{\max}^{n-1}$。

当$\lambda_{\max} < 1$时，梯度会指数级衰减，导致梯度消失问题；当$\lambda_{\max} > 1$时，梯度会指数级增长，导致梯度爆炸问题。即使采用LSTM或GRU等门控机制缓解这一问题，梯度仍然需要经过$n−1$次矩阵乘法的累积，计算效率极低。

## 5.4.2 路径长度与依赖建模

从信息论的角度来看，序列中两个位置之间的依赖关系可以用路径长度来度量。路径长度指的是信息从一个位置传递到另一个位置所需经过的中间节点数。在RNN中，从位置$i$到位置$j$（$i<j$）的路径长度为$j−i$，信息必须经过所有中间位置的隐藏状态才能传递。

注意力机制从根本上改变了这一结构。在自注意力机制中，序列中任意两个位置之间可以直接建立联系，无需经过中间节点。这意味着从位置$i$到位置$j$的路径长度恒为1，与它们在序列中的距离无关。这一性质可以用数学语言精确描述。

设$A$为注意力权重矩阵，$A_{ij}$​表示位置$i$对位置$j$的注意力权重，$V$为值矩阵，则注意力输出为$O = AV$。位置$i$的输出$o_i$​是所有位置$j$的值$v_j$的加权平均：
$$
o_i = \sum_{j=1}^{n} A_{ij} v_j \tag{5.4.3}
$$
在这个表达式中，$o_i$​直接依赖于所有$v_j$​，无论$j$与$i$的距离有多远。这种直接依赖关系是注意力机制建模长程依赖的数学基础。与RNN的迭代依赖结构不同，注意力机制提供了一种"扁平化"的信息聚合方式。

## 5.4.3 依赖跨度的数学度量

为了量化注意力机制建模长程依赖的能力，我们需要定义一些数学度量。设序列长度为$n$，位置$i$和位置$j$之间的距离为$|i - j|$。注意力权重矩阵$A$定义了一种依赖关系图，其中$A_{ij} > \epsilon$（$\epsilon$为某个阈值）表示位置$i$"依赖"位置$j$。

定义有效依赖跨度（Effective Dependency Span）为：
$$
\text{span}_{\text{eff}}(i) = \max\{ |i - j| : A_{ij} > \epsilon \} \tag{5.4.4}
$$
对于位置$i$，这个度量表示它实际关注的"最远"位置与自身的距离。如果注意力权重主要集中在邻近位置，$\text{span}_{\text{eff}}(i)$较小，说明该位置的注意力是"局部"的；如果注意力权重分散在很远的位置，$\text{span}_{\text{eff}}(i)$较大，说明该位置的注意力是"全局"的。

在训练良好的Transformer中，不同层和不同头的有效依赖跨度差异很大。早期层（如第一层编码器）的头往往具有较小的有效依赖跨度，专注于学习局部上下文信息；深层（如最后一层编码器或解码器）的头往往具有较大的有效依赖跨度，能够建立跨整个序列的长程关联。这种层级化的依赖建模是Transformer成功捕获复杂语言结构的关键因素。
## 5.4.4 全连接结构与信息直达

注意力机制的核心特性是全连接（Fully Connected）结构。在自注意力中，每个位置可以与序列中的所有其他位置直接交互，形成一个完全连通图（Complete Graph）。从图论的角度来看，注意力权重矩阵$A$定义了一个有向加权图，其中每个节点（位置）都有指向所有其他节点的边，边的权重为注意力分数。

这种全连接结构与卷积结构形成鲜明对比。在卷积神经网络中，每个位置只能与一个固定大小窗口内的位置交互，感受野（Receptive Field）的大小受到卷积核尺寸的限制。虽然通过堆叠多层卷积可以扩大感受野，但感受野的扩展是渐进的、需要多层累积的。以卷积核大小为$3 \times 3$的卷积为例，第$l$层的感受野大小约为$2l+1$，要覆盖长度为$n$的序列，需要堆叠约$n/2$层。

在注意力机制中，单层注意力就能提供全局的感受野，每个位置可以直接访问序列中的所有其他位置。这是一种"一步到位"的信息整合方式，而非卷积那种"逐步扩展"的方式。这种差异在数学上体现为信息传递的路径长度：卷积的路径长度与层数成正比，而注意力的路径长度恒为1。

## 5.4.5 注意力权重的归一化与信息聚合

注意力机制通过Softmax归一化将原始的注意力分数转换为有效的权重分布，从而实现信息聚合。设原始注意力分数矩阵为$S = QK^T / \sqrt{d_k}$，则归一化后的注意力权重为：
$$
A_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^{n} \exp(S_{ik})} \tag{5.4.5}
$$
这个归一化过程有两个重要作用。第一，它确保注意力权重构成有效的概率分布：$A_{ij} \geq 0$且$\sum_j A_{ij} = 1$。这意味着每个位置的输出是值向量的凸组合（Convex Combination），权重由注意力分数隐式决定。第二，Softmax的非线性变换使得注意力权重具有一定的"尖锐性"——当某些位置的重要性显著高于其他位置时，权重会集中到这些位置上。

从信息聚合的角度来看，注意力输出$o_i$可以视为对值向量$\{v_j\}$的"软选择"（Soft Selection）。与硬选择（如取最大值）不同，软选择保留了所有位置的信息，只是根据重要性进行加权。这种设计有两个好处：一是梯度可以流回所有位置，优化过程可以调整所有位置的重要性；二是保留了信息的冗余性，即使某些位置的权重很小，其信息也可能被其他位置"接力"传递。

## 5.4.6 相对位置与内容无关的交互

注意力机制的一个重要特性是它可以同时建模位置相关的依赖和内容相关的依赖。在计算注意力分数时，查询向量$q_i$​和键向量$k_j$​既编码了位置信息（通过位置编码），也编码了内容信息（通过嵌入表示）。因此，注意力权重$A_{ij}$​同时依赖于位置$|i - j|$和内容$x_i, x_j$​。

这种双重依赖使得注意力能够建模多种类型的长程依赖。对于语法依赖（如主语-谓语关系），模型可以学习到当查询位置是主语词汇、键位置是谓语词汇时，注意力权重应该较大。对于语义依赖（如代词与指代对象的关联），模型可以学习到当查询位置是代词、键位置是其指代对象时，注意力权重应该较大。对于位置依赖（如长距离的修饰关系），位置编码提供的方向信息可以帮助模型捕获跨越长距离的关联。

值得注意的是，相对位置编码（Relative Positional Encoding）的发展进一步增强了注意力机制建模位置相关依赖的能力。在相对位置编码中，注意力分数不仅取决于查询和键的内容，还明确地包含查询和键之间的相对位置信息。这使得模型能够学习到与距离相关的注意力模式——某些头可能专注于学习近距离的依赖，某些头可能专注于学习远距离的依赖。
## 5.4.7 信息传递路径的数学分析

为了更精确地分析注意力机制在长程依赖建模方面的优势，我们定义信息传递路径长度的数学概念。设$G$是一个有向图，表示序列中各位置之间的信息流动关系。从位置$i$到位置$j$的信息传递路径是$G$中的一条有向路径$i \rightarrow \cdots \rightarrow j$。路径长度是路径中边的数量。

在RNN中，信息传递路径必须沿着序列顺序：从$i$到$j>i$，路径为$i \rightarrow i+1 \rightarrow \cdots \rightarrow j$，路径长度为$j−i$。在自注意力中，位置$i$可以直接"看到"所有位置$j$，因此从$i$到$j$的路径长度恒为1（存在直接边$i \rightarrow j$）。

定义从位置$i$到位置$j$的最小路径长度为：
$$
\text{dist}(i, j) = \begin{cases} 0 & \text{if } i = j \\ 1 & \text{if } A_{ij} > \epsilon \\ \min\{ \text{路径长度} \} & \text{otherwise} \end{cases} \tag{5.4.6}
$$
在注意力机制中，由于$A_{ij}$​通常不会精确为零（Softmax输出的每个元素都是正的），我们通常假设$\text{dist}(i, j) = 1$对于所有$i \neq j$。这意味着任意两个位置之间都存在直接的信息传递路径。

## 5.4.8 长序列中的效率优势

长序列是检验长程依赖建模能力的关键场景。设序列长度为$n$，依赖关系的最大跨度为$L$（即存在一对位置$(i,j)$使得$|i - j| \approx L$且它们之间存在依赖）。在RNN中，建模这种依赖需要$L$步的顺序计算；在注意力中，单层注意力即可完成。

考虑计算复杂度的对比。对于长度为$n$的序列，建模全局长程依赖（所有位置对之间的依赖）的时间复杂度如下。RNN为$O(n \cdot d^2)$（每步进行矩阵乘法），且由于需要顺序计算，无法并行化。注意力机制为$O(n^2 \cdot d)$（计算$QK^T$和$AV$），但可以完全并行化。

虽然注意力的渐进复杂度$O(n^2)$高于RNN的$O(n)$，但这里的区别在于依赖关系的类型。RNN的$O(n)$复杂度只能建模"局部"依赖——信息只能从相邻位置逐步传递。如果要建模跨越整个序列的长程依赖，RNN实际上需要进行$O(n)$步的顺序计算，总复杂度为$O(n^2)$。注意力的$O(n^2)$是一次性建模所有位置对之间的依赖，包括长程依赖。

换一种说法：RNN建模长程依赖的"有效复杂度"是$O(n^2)$（需要$n$步顺序计算），而注意力的"有效复杂度"也是$O(n^2)$（可以并行）。但注意力的优势在于：无论依赖关系的跨度如何，建模成本都是固定的；而RNN的建造成本与依赖跨度成正比。对于需要建模长程依赖的任务，注意力具有明显的效率优势。

## 5.4.9 内存带宽与计算效率

除了计算复杂度，注意力机制在内存带宽效率方面也具有优势。在RNN中，每一步计算都需要读取前一步的隐藏状态，内存访问模式是"顺序的、依赖的"。这种模式不利于GPU的并行执行——当前一步的计算未完成时，下一步无法开始。

注意力机制的内存访问模式更加规则。计算$QK^T$时，需要读取完整的$Q$和$K$矩阵，这些数据可以一次性加载到高速缓存中。计算$AV$时，需要读取完整的$A$和$V$矩阵，同样可以进行批量读取。这种"一次性加载、批量计算"的模式与现代GPU的架构特性高度匹配，能够充分利用内存带宽。

此外，注意力机制的中间结果（注意力分数矩阵$A$）可以保存在寄存器或共享内存中，避免重复从全局内存读取。这与RNN的"每步重新加载"形成对比。Flash Attention等算法通过精心设计的数据分块策略，进一步优化了注意力计算的内存访问模式，使得长序列注意力计算在实践中更加高效。
## 5.4.10 注意力权重的分布特性

理解注意力权重矩阵的分布特性是分析注意力如何建模长程依赖的关键。设$A$为注意力权重矩阵，$A_{ij}$​表示位置$i$对位置$j$的注意力权重。$A$的每一行是一个概率分布：$A_{ij} \geq 0$且$\sum_j A_{ij} = 1$。

从统计学的角度，我们可以分析注意力权重的分布特性。定义注意力权重的熵（Entropy）：
$$
H(A_i) = -\sum_{j=1}^{n} A_{ij} \log A_{ij} \tag{5.4.7}
$$
其中，$A_i$是$A$的第$i$行。熵度量了注意力分布的"均匀程度"：当注意力均匀分布在所有位置时，熵最大（$H_{\max} = \log n$）；当注意力完全集中于一个位置时，熵最小（$H_{\min} = 0$）。

实证研究表明，训练良好的Transformer中不同头表现出不同的熵分布模式。某些头具有较高的熵，注意力分布较为均匀，表明它们在"探索"整个序列的信息；某些头具有较低的熵，注意力分布较为集中，表明它们在"聚焦"于少数关键位置。这种多样化的分布模式使得模型能够同时捕获全局信息和局部细节。

## 5.4.11 局部注意力与全局注意力

注意力权重的空间分布，即位置$i$的注意力主要集中在哪些位置$j$，揭示了模型捕获的依赖类型。通过分析注意力权重的位置分布，我们可以区分"局部注意力"和"全局注意力"。

局部注意力指的是注意力权重集中在相邻或接近的位置。数学上，局部注意力满足：存在常数$C$，使得当$|i - j| > C$时，$A_{ij}$​较小（接近0或显著小于邻居位置的权重）。局部注意力类似于卷积操作，关注输入的局部窗口。局部注意力适合捕获短语结构、词序信息等局部依赖。

全局注意力指的是注意力权重分布在整个序列上，没有明显的局部偏向。数学上，全局注意力满足：对于任意距离$d$，都存在一定比例的注意力权重分配给距离为$d$的位置对。全局注意力适合捕获跨越长距离的语义关联、篇章级别的指代关系等。

在Transformer中，不同层和头表现出不同的注意力模式。靠近输入的层（浅层）往往表现出更多的局部注意力，捕获词汇级别和短语级别的特征；靠近输出的层（深层）往往表现出更多的全局注意力，捕获句子级别和篇章级别的特征。这种层级化的模式与人类语言的层级结构（词→短语→句子→篇章）具有良好的对应关系。

## 5.4.12 注意力头的专业化分工

在多头注意力中，不同的头可以专门负责建模不同类型的依赖关系。设头$h$的注意力权重矩阵为$A^{(h)}$，我们可以通过分析$A^{(h)}$的模式来推断该头负责的依赖类型。

某些头专门建模"语法依赖"。例如，主语-谓语依赖、形容词-名词依赖、修饰语-中心语依赖等。这些依赖通常是局部的（主语和谓语通常相距不太远），但方向性强（总是从主语指向谓语）。这类头往往表现出较强的局部注意力模式和明确的方向性。

某些头专门建模"语义依赖"。例如，代词与其指代对象的关联、实体与其属性的关联、事件与其论元的关联等。这些依赖可以是长距离的（代词可能距离指代对象很远），且不依赖于词序。语义头往往表现出全局注意力模式，注意力权重分布在语义相关的位置上。

某些头专门建模"位置依赖"。这类头对位置编码敏感，注意力权重与位置距离有明确的关系（如距离越近权重越大，或距离呈某种函数关系）。位置头适合捕获需要考虑序列顺序的依赖关系。

通过可视化注意力权重矩阵和统计分析注意力权重的分布特性，研究者发现训练良好的Transformer确实存在这种"专业化"现象。每个头学习到一种特定的"注意力策略"，不同的头协同工作，共同建模输入序列中的各种依赖关系。这种专业化是Transformer强大表达能力的重要来源。
## 5.4.13 层级化的依赖建模

在Transformer中，多个注意力层被堆叠在一起，每层都进行自注意力计算和前馈网络变换。这种层级化结构使得模型能够建模更加复杂的长程依赖。理解多层堆叠如何影响长程依赖建模，需要分析信息在层间传递的数学机制。

设第$l$层的输入为$X^{(l)}$，输出为$X^{(l+1)}$。自注意力的输出为：
$$
\text{Attention}^{(l)} = \text{Softmax}\left(\frac{X^{(l)}W_Q^{(l)} (X^{(l)}W_K^{(l)})^T}{\sqrt{d_k}}\right) X^{(l)}W_V^{(l)} \tag{5.4.8}
$$
加上残差连接和层归一化后：
$$
X^{(l+1)} = \text{LayerNorm}\left(X^{(l)} + \text{Attention}^{(l)}\right) \tag{5.4.9}
$$
这个递推关系展示了信息如何在层间流动。位置$i$在第$l+1$层的表示$X^{(l+1)}_i$​是$X^{(l)}_i$​（残差连接保留的原始信息）与$\text{Attention}^{(l)}_i$（从整个序列聚合的信息）的融合。

## 5.4.14 依赖传递的数学模型

为了分析多层注意力如何建模长程依赖，我们可以建立一个简化的数学模型。假设我们只关心某个特定的长程依赖关系，比如位置$1$和位置$n$之间的关联。在第$l$层，位置$1$的信息有多少能够传递到位置$n$？

在单层注意力中，位置$n$可以直接"看到"位置$1$，位置$n$的注意力权重$A_{n1}$​可能很小（如果模型认为它们不相关），但从信息流的角度，位置$1$的值向量$v_1$直接参与了位置$n$的输出计算：
$$
\text{output}_n = \sum_{j=1}^{n} A_{nj} v_j \tag{5.4.10}
$$
如果$A_{n1}$​较小，位置$1$的信息对位置$n$的贡献也较小。但这并不意味着信息没有传递——如果中间层能够调整注意力权重，使得后续层中$A_{n1}$变大，位置$1$的信息就可以"重新浮现"。

考虑一个两层的情况。第一层学习到某种"桥接"注意力模式：位置$n$关注位置$m$（$1 < m < n$），位置$m$关注位置$1$。设第一层的注意力权重为$A^{(1)}$，则：
$$
\text{output}_n^{(1)} = \sum_{j} A^{(1)}_{nj} v_j \approx A^{(1)}_{nm} v_m \tag{5.4.11}
$$
其中，$v_m$是位置$m$的值向量。位置$m$的值向量$v_m$包含了位置$m$从位置$1$聚合的信息：
$$
\text{output}_m^{(1)} = \sum_{k} A^{(1)}_{mk} v_k \approx A^{(1)}_{m1} v_1 \tag{5.4.12}
$$
因此，位置$1$的信息通过位置$m$作为"中继"，间接传递到了位置$n$。虽然这种传递不是直接的（需要两步），但它展示了多层注意力如何通过"信息中继"来建模超长距离的依赖关系。

## 5.4.15 残差连接与梯度流动

残差连接（Residual Connection）在长程依赖建模中发挥着关键作用。从信息流的角度，残差连接提供了一条"捷径"，允许信息直接从较浅的层流向较深的层，无需经过中间层的变换。

考虑一个$L$层的Transformer。位置$i$的原始嵌入$X_i^{(0)}$​通过残差连接直接参与到最终层的表示：
$$
X_i^{(L)} = X_i^{(0)} + \Delta X_i^{(1)} + \Delta X_i^{(2)} + \cdots + \Delta X_i^{(L)} \tag{5.4.13}
$$
其中，$\Delta X_i^{(l)}$​是第$l$层的残差贡献。位置$i$的最终表示保留了原始嵌入的直接成分，这确保了底层的信息不会被高层完全覆盖。

从梯度的角度，残差连接提供了一条梯度传播的"高速公路"。考虑损失函数$\mathcal{L}$关于$X_i^{(0)}$​的梯度：
$$
\frac{\partial \mathcal{L}}{\partial X_i^{(0)}} = \frac{\partial \mathcal{L}}{\partial X_i^{(L)}} \cdot \frac{\partial X_i^{(L)}}{\partial X_i^{(0)}} \tag{5.4.14}
$$
通过链式法则展开：
$$
\frac{\partial \mathcal{L}}{\partial X_i^{(0)}} = \frac{\partial \mathcal{L}}{\partial X_i^{(L)}} \cdot \left( I + \frac{\partial (\Delta X^{(1)})}{\partial X^{(0)}} + \cdots + \frac{\partial (\sum_{l=1}^{L} \Delta X^{(l)})}{\partial X^{(0)}} \right) \tag{5.4.15}
$$
残差连接确保了单位矩阵$I$的存在，这使得梯度可以"无损"地回传到第一层。即使中间层的梯度很小，原始输入的梯度贡献仍然保持完整。这解决了深层网络的梯度消失问题，使得模型能够稳定地训练。

## 5.4.16 层级特征与依赖跨度

Transformer的不同层捕获不同"层级"的特征，与之对应的是不同"跨度"的依赖关系。底层（靠近输入）主要捕获词汇级别和短语级别的特征，依赖跨度较小；顶层（靠近输出）主要捕获句子级别和篇章级别的特征，依赖跨度较大。

这种层级化可以通过信息论的工具进行量化。设$I(X_i^{(l)}; X_j^{(0)})$表示第$l$层位置$i$的表示与原始位置$j$嵌入之间的互信息。当$l = 0$时，互信息为1（完全相关）；当$l$增大时，互信息逐渐降低（信息经过变换后丢失了一些细节）。对于固定的$i$和$j$，$I(X_i^{(L)}; X_j^{(0)})$的大小可以衡量依赖关系的强度。

实证分析表明，对于近距离的位置对（如$|i - j| = 1$或$2$），互信息$I(X_i^{(L)}; X_j^{(0)})$在各层都较高，说明底层信息能够有效传递到高层。对于远距离的位置对（如$|i - j| = 50$或$100$），互信息在底层较低，但在高层可能升高，这意味着模型在高层通过注意力机制"重新发现"了远距离位置之间的关联。

这种模式揭示了Transformer处理长程依赖的策略：底层主要进行局部特征提取，高层在局部特征的基础上进行全局整合，捕获跨越长距离的语义关联。这种"先局部、后全局"的策略与人类处理语言的认知过程有相似之处。
## 5.4.17 位置编码的必要性

纯注意力机制是置换不变的（Permutation Invariant）：如果我们对输入序列进行重新排序，注意力权重的计算结果只依赖于新的顺序，不会感知到原始的顺序信息。这是因为注意力分数$QK^T$只依赖于查询和键向量的内容，不包含任何位置信息。

然而，语言具有强烈的顺序性——词序承载着重要的语法和语义信息。"狗咬人"和"人咬狗"由相同的词元组成，但意义截然不同。因此，位置编码（Positional Encoding）的引入是必要的，它为注意力机制注入了位置信息，使其能够区分不同位置的词元。

位置编码的数学形式为向原始嵌入添加一个与位置相关的编码向量：
$$
X_{\text{pos}} = X_{\text{emb}} + P \tag{5.4.16}
$$
其中，$P$是位置编码矩阵，$P_{i}$​是位置$i$的编码向量。位置编码的设计需要满足几个条件：唯一性（不同位置有不同的编码）、可泛化（能够处理训练时未见过的长度）、与内容编码兼容（不干扰内容信息的表示）。

## 5.4.18 正弦位置编码的数学性质

Transformer原始论文提出的正弦位置编码（Sinusoidal Positional Encoding）定义为：
$$
P_{(i, 2m)} = \sin\left(\frac{i}{10000^{2m/d_{\text{model}}}}\right), \quad P_{(i, 2m+1)} = \cos\left(\frac{i}{10000^{2m/d_{\text{model}}}}\right)\tag{5.4.17}
$$
其中，$i$是位置索引，$m$是维度索引。这种编码利用不同频率的正弦和余弦函数来唯一标识每个位置。

正弦位置编码的一个重要数学性质是"相对位置可学习性"。考虑两个位置$i$和$j$的编码向量$P_i$​和$P_j$。它们的点积为：
$$
P_i \cdot P_j = \sum_{m=0}^{d_{\text{model}}/2 - 1} \left[ \sin\left(\frac{i}{\omega_m}\right)\sin\left(\frac{j}{\omega_m}\right) + \cos\left(\frac{i}{\omega_m}\right)\cos\left(\frac{j}{\omega_m}\right) \right] \tag{5.4.18}
$$
其中，$\omega_m = 10000^{2m/d_{\text{model}}}$是各频率的周期。利用三角恒等式简化：
$$
P_i \cdot P_j = \sum_{m=0}^{d_{\text{model}}/2 - 1} \cos\left(\frac{i - j}{\omega_m}\right) \tag{5.4.19}
$$
这表明位置编码的点积只依赖于相对位置$i−j$，而非绝对位置。这使得模型能够学习到与相对位置相关的注意力模式，这是建模长程依赖的重要能力。例如，模型可以学习到"跳过10个词"或"跳过20个词"等模式化的注意力策略。

## 5.4.19 旋转位置编码与相对位置建模

旋转位置编码（Rotary Position Embedding，RoPE）是近年来广泛使用的位置编码方案，它通过在查询和键向量上应用旋转操作来编码位置信息。RoPE的数学定义为：
$$
\text{RoPE}(x_m, i) = \begin{pmatrix} x_m \cos(\theta_m i) - x_{m+1} \sin(\theta_m i) \\ x_m \sin(\theta_m i) + x_{m+1} \cos(\theta_m i) \end{pmatrix} \tag{5.4.20}
$$
其中，$x_m$和$x_{m+1}$是向量的第$m$和$m+1$个分量，$\theta_m = 10000^{-2m/d_{\text{model}}}$​是旋转角度。

RoPE的核心性质是相对位置的内积不变性。考虑两个应用了RoPE的向量$q_i$（位置$i$的查询）和$k_j$（位置$j$的键），它们的内积为：
$$
q_i \cdot k_j = \text{RoPE}(q, i) \cdot \text{RoPE}(k, j) = f(q, i) \cdot f(k, j) = g(q, k, i - j) \tag{5.4.21}
$$
这意味着注意力分数$q_i \cdot k_j$​只依赖于查询和键的相对位置$i−j$，而非它们的绝对位置。这正是建模长程依赖所需要的，模型可以学习到与距离相关的注意力权重模式，无论依赖关系出现在序列的哪个位置。

RoPE的这一性质使得Transformer能够自然地处理超出训练序列长度的输入（长度外推），这是正弦位置编码难以做到的。LLaMA、PaLM等现代大语言模型都采用了RoPE或其变体。

## 5.4.20 远程依赖的边界效应

在实际应用中，长程依赖的建模存在"边界效应"（Boundary Effect），序列开头和结尾的位置在建模远距离依赖时可能面临额外的挑战。对于位置$i$接近序列开头（如$i=1$）或接近序列结尾（如$i=n$）的位置，它们能够建立的远距离依赖关系较少，因为一边没有足够的"空间"。

边界效应在注意力权重的分布上有所体现。位置$1$无法关注位置$0$（不存在），只能关注位置$2$到$n$；位置$n$无法关注位置$n+1$，只能关注位置$1$到$n−1$。虽然这看起来是显然的，但当依赖关系跨越整个序列（如位置$1$和位置$n$之间的关联）时，边界位置需要"承担"更多的信息传递任务。

一种缓解边界效应的方法是使用循环位置编码（cyclic positional encoding）或对称位置编码。这些编码方式将位置映射到一个环面上，使得位置$1$和位置$n$成为"邻居"。这在理论上消除了边界，但实际效果因任务而异。

另一种方法是使用绝对位置编码的扩展，允许模型学习到位置$n+1$、$n+2$等的表示。这种方法在推理时能够处理更长的序列，但需要额外的训练策略来确保长度外推的性能。
## 5.4.21 注意力机制的函数空间

从函数逼近的角度来看，注意力机制定义了一类特定的函数。设输入序列为$X \in \mathbb{R}^{n \times d}$，注意力输出为$\text{Attention}(X) = \text{Softmax}(XW_Q W_K^T X^T / \sqrt{d_k}) XW_V$。这个函数可以表示为：
$$
f(X) = A(X) \cdot XW_V \tag{5.4.22}
$$
其中，$A(X) = \text{Softmax}(XW_Q W_K^T X^T / \sqrt{d_k})$是输入依赖的注意力矩阵。注意力机制的核心特性是$A(X)$是一个随机矩阵（行和为1），且$A(X)$的每一行只依赖于对应的查询行和整个键矩阵。

注意力机制能够表示的函数类与传统的循环网络或卷积网络有显著差异。RNN的函数类是"递归函数"，输出通过递归关系定义，信息必须逐步传递。卷积网络的函数类是"局部函数"。每个输出位置只依赖于输入的一个局部窗口。注意力机制的函数类是"全局函数"，每个输出位置可以依赖于整个输入序列。

这种全局性使得注意力机制在理论上能够建模任意复杂的长程依赖模式。只要存在一种注意力权重配置（由$W_Q, W_K, W_V$​参数化），使得对于每个位置$i$，注意力权重$A_{ij}$​正确地反映了位置$j$对位置$i$的重要性，模型就能够捕获任意位置对之间的依赖关系。

## 5.4.22 表达能力与计算能力界限

理论上，注意力机制具有强大的表达能力。我们可以分析它能够表示的函数复杂度。一个关键问题是：单层注意力能够建模任意复杂的长程依赖关系吗？

考虑一个简单的例子。设输入是一个二值序列$x_i \in \{0, 1\}$，我们希望输出序列$y_i$​满足$y_i = x_{i-10}$​（即输出是输入向左移动10位）。这种"延迟"操作是一种典型的长程依赖（跨度为10）。单层注意力能够表示这个函数吗？

注意力输出的第$i$个元素为：
$$
y_i = \sum_{j=1}^{n} A_{ij} v_j \tag{5.4.23}
$$
值向量$v_j$​是$x_j$​的线性变换：$v_j = W_V x_j$。因此：
$$
y_i = \sum_{j=1}^{n} A_{ij} W_V x_j \tag{5.4.24}
$$
如果我们希望$y_i = x_{i-10}$​，则需要$A_{ij} = \delta_{j, i-10}$（Kronecker delta）且$W_V = 1$。这意味着注意力权重必须是确定性的：对于每个$i$，只关注位置$i−10$。这种"硬"注意力可以通过学习实现。训练过程中，模型逐渐调整$W_Q$和$W_K$​，使得当$x_i$​是"当前位置"、$x_j$​是"10步前的位置"时，注意力分数特别高。

这个例子表明，理论上单层注意力能够建模任意"位置到位置"的映射（包括任意跨度的长程依赖）。关键在于模型是否能够学习到正确的注意力权重模式，而这取决于训练数据、模型容量和优化过程。

## 5.4.23 依赖建模的计算复杂度下界

从计算复杂度的角度，我们可以证明注意力机制在建模长程依赖方面的效率优势。考虑一个简单的长程依赖建模任务：给定序列$x_1, \ldots, x_n$计算所有位置对的某种关联度量，如距离矩阵$D_{ij} = d(x_i, x_j)$，其中$d(\cdot, \cdot)$是某个可学习的距离函数。

在RNN中，计算$D$需要$O(n^2)$次顺序计算（每次计算一行都需要遍历整个序列），总时间复杂度为$O(n^3)$（假设每次计算需要$O(n)$步）。在注意力机制中，$QK^T$计算一次性完成所有位置对的点积相似度，时间复杂度为$O(n^2 \cdot d)$，可以并行执行。

更一般地，假设我们有一个"oracle"函数（一个全知的函数，它**事先就知道序列中任意两个位置之间是否、以及多强地存在依赖关系**。）。注意力机制的学习目标是学习这个oracle的权重分配。在计算复杂度上，注意力机制达到了这个任务的下界，必须至少检查每一对位置一次，因此$O(n^2)$是理论上的最优复杂度。

然而，注意力机制的$O(n^2)$复杂度在超长序列场景下仍然是瓶颈。当$n$达到百万甚至十亿级别时，$n^2$的存储和计算都不可行。这催生了各种稀疏注意力和线性注意力变体的研究，旨在在保持长程依赖建模能力的同时降低计算复杂度。

## 5.4.24 注意力机制的归纳偏置

归纳偏置（Inductive Bias）是指学习算法对解空间的先验假设。不同的架构有不同的归纳偏置，这些偏置决定了模型擅长学习哪类函数、不擅长学习哪类函数。

注意力机制的归纳偏置包括：全连接假设（任意位置之间可以存在依赖关系）、内容驱动假设（依赖关系的强度由内容相似度决定）、位置编码假设（位置信息需要显式注入）。与RNN相比，注意力机制没有"序列顺序"的先验假设，它假设依赖关系可以跨越任意距离。与卷积网络相比，注意力机制没有"局部性"的先验假设，它不假设依赖关系只存在于邻近位置。

这种"弱归纳偏置"使得注意力机制具有很高的通用性，能够处理各种类型的依赖关系。但同时也意味着，如果没有足够的数据或适当的正则化，模型可能过拟合或学习到不合理的依赖模式。在实践中，位置编码、数据增强和正则化技术被用来引导模型学习有意义的依赖关系。

## 5.4.25 本节小结

本节深入分析了注意力机制建模长程依赖的数学原理。我们从RNN的梯度消失问题出发，揭示了传统循环结构在处理长距离依赖时的根本性困难，信息需要经过路径长度与依赖跨度成正比的逐步传递。

注意力机制通过全连接结构从根本上解决了这一问题。在自注意力中，任意两个位置之间可以直接建立联系，路径长度恒为1，与位置距离无关。这种"直接连接"特性是注意力机制建模长程依赖的数学基础。我们通过信息传递路径分析、计算复杂度对比等数学工具，量化地展示了这一优势。

我们进一步分析了注意力权重的分布特性，揭示了局部注意力与全局注意力的区别，以及不同头如何通过"专业化分工"来建模不同类型的依赖关系。通过多层堆叠的层级化结构，模型能够在底层捕获局部特征、在高层整合全局信息，实现复杂的长程依赖建模。

位置编码的引入为注意力机制注入了位置感知能力，使模型能够区分不同位置的词元、理解词序承载的信息。我们分析了正弦位置编码和旋转位置编码的数学性质，展示了它们如何支持相对位置相关的注意力模式。

最后，我们从函数空间和计算复杂度的角度进行了理论分析，证明注意力机制在理论上具有强大的长程依赖建模能力，同时也指出了其$O(n^2)$复杂度的局限性及相应的优化方向。

通过本节的学习，读者应该能够从数学层面深入理解注意力机制为何能够有效建模长程依赖，以及这种能力的来源和边界。下一节，我们将从谱性质的角度进一步分析注意力矩阵的数学特性。