# 第1章 数学基础
## 1.1 线性代数与张量运算

线性代数作为现代数学的重要分支，在深度学习和大语言模型的理论基础中占据核心地位。从Transformer架构的注意力机制到词嵌入的向量表示，从矩阵乘法的并行计算到张量运算的维度变换，线性代数为理解和构建深度学习模型提供了坚实的数学框架。本节系统阐述线性代数的基本概念、张量运算规则及其在深度学习中的应用。

### 1.1.1 线性代数的基本概念

向量是线性空间中最基本的元素，可理解为一维数组中的有序数集。在大语言模型中，词向量是典型的向量表示，每个词被映射为一个高维实数向量，维度编码语义或语法特征。矩阵是二维数组，是线性代数中最重要的研究对象。权重矩阵在模型中无处不在：嵌入层将词汇映射为向量，注意力机制中的查询、键、值投影通过矩阵乘法实现，前馈神经网络由多个权重矩阵堆叠而成。张量是多维数组的自然推广，一阶张量是向量，二阶张量是矩阵，三阶及以上张量表示更复杂的数据结构。现代深度学习框架中，张量是最基本的数据结构，输入通常为三维张量，维度分别代表批量大小、序列长度和嵌入维度。

### 1.1.2 基本运算规则

向量加法是逐分量运算，两个维度相同的向量可逐分量相加。设有两个 $n$ 维向量 $\mathbf{u} = (u_1, \ldots, u_n)$ 和 $\mathbf{v} = (v_1, \ldots, v_n)$，则其和为

$$\mathbf{u} + \mathbf{v} = (u_1 + v_1, \ldots, u_n + v_n)\tag{1.1.1}$$

向量加法满足交换律和结合律。残差连接是向量加法的典型应用，将子层输入与输出直接相加，为反向传播提供恒等映射路径，缓解梯度消失问题。标量乘法将向量每个分量乘以该标量，在权重初始化、学习率调整和梯度裁剪中有应用。矩阵乘法是最重要的运算，两个矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和 $\mathbf{B} \in \mathbb{R}^{n \times p}$ 的乘积 $\mathbf{C} = \mathbf{AB}$ 是 $m \times p$ 的矩阵，其中 $c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}$。矩阵乘法满足结合律和分配律，但不满足交换律。注意力机制包含多个矩阵乘法：查询与键矩阵乘积产生注意力分数，注意力权重与值矩阵乘积产生加权输出。矩阵转置将行和列互换，自注意力需要计算查询与转置键矩阵的乘积。矩阵求逆 $\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}$ 用于解线性方程组，线性回归的闭式解通过求解正规方程得到。

### 1.1.3 张量的表示与运算

张量是多维数组的抽象表示，标量是零阶张量，向量是一阶张量，矩阵是二阶张量。$k$ 阶张量有 $k$ 个索引，三阶张量 $\mathcal{X} \in \mathbb{R}^{I \times J \times K}$ 的元素表示为 $x_{ijk}$。Transformer输入张量形状为 $(B, S, D)$，批量大小 $B$、序列长度 $S$、隐藏维度 $D$。多头注意力中，张量重排为 $(B, H, S, D/H)$，注意力头数 $H$。张量运算包括逐元素运算、重塑、转置、广播和张量收缩。逐元素运算对每个独立元素应用相同函数，如ReLU激活函数 $f(x) = \max(0, x)$。重塑改变张量形状不改变数据元素，将 $(B, S \cdot D)$ 重塑为 $(B, S, D)$ 用于恢复嵌入序列。广播是自动对齐机制，当两个形状不同的张量进行逐元素运算时，较小的张量在某些维度上被视为重复以匹配较大的张量。广播规则从最后一个维度开始比较：对应维度相等或其中一个为1时可广播，维度数不同时在左侧自动补1。

张量收缩是矩阵乘法在高维张量上的推广，又称张量点积或广义矩阵乘法。收缩指定两个张量的若干维度配对相乘并求和。例如，两个三阶张量 $\mathcal{A} \in \mathbb{R}^{I \times J \times K}$ 和 $\mathcal{B} \in \mathbb{R}^{J \times K \times L}$ 在第二和第三维度上的收缩产生 $I \times L$ 的矩阵。注意力分数的计算可看作张量收缩操作，查询与键向量在特征维度上点积。张量分解是处理高维张量的重要技术，CP分解将张量表示为秩一张量和，Tucker分解将张量表示为核心张量与因子矩阵乘积，在模型压缩和参数高效微调中有重要应用。

### 1.1.4 特征空间的几何直觉

特征空间是描述数据表示的核心概念，$n$ 维特征空间可理解为 $n$ 维欧几里得空间 $\mathbb{R}^n$，空间中的点代表数据样本的表示。词嵌入空间是高维特征空间的典型例子，语义相近的词在空间中距离较近。嵌入空间是密集的，每个维度取连续实数值，维度选择是重要超参数：过低维度导致欠拟合，过高维度可能过拟合且计算效率下降。

向量距离和相似度是度量特征空间样本关系的基本工具。欧几里得距离 $d(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|_2$ 测量直线距离，余弦相似度 $\text{cosine}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\|_2 \|\mathbf{v}\|_2}$ 测量方向相似程度，与向量大小无关。词嵌入研究中余弦相似度最常用，能捕捉语义方向相似性。线性变换 $T: \mathbb{R}^n \to \mathbb{R}^m$ 可用矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 表示，满足 $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ 和 $T(c\mathbf{u}) = cT(\mathbf{u})$。线性变换的作用包括旋转、缩放、投影和剪切。子空间是特征空间中的重要结构，$k$ 维子空间是 $k$ 维线性子集。列空间是矩阵所有列向量的线性组合构成的空间，描述矩阵所能表示的所有输出；零空间是所有被映射到零向量的输入向量构成的空间，描述输入中的冗余信息。
![[fig_CH01_unit_circle_transform.png]]

### 1.1.5 特征值与特征向量

特征值和特征向量揭示线性变换的本质特征：某些特定方向（特征向量）在变换下只发生缩放而不改变方向，缩放倍数是特征值。给定方阵 $\mathbf{A} \in \mathbb{R}^{n \times n}$，若存在非零向量 $\mathbf{v} \in \mathbb{R}^n$ 和标量 $\lambda \in \mathbb{R}$ 满足 $\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$，则 $\mathbf{v}$ 是特征向量，$\lambda$ 是对应特征值。特征值通过求解特征方程 $\det(\mathbf{A} - \lambda\mathbf{I}) = 0$ 计算。对于大型矩阵，使用幂迭代、反幂迭代和QR算法计算特征值和特征向量。

在大语言模型中，特征值和特征向量有重要应用。首先，主成分分析（PCA）中特征值分解提取数据中方差最大的方向（主成分），保留最大 $k$ 个特征值对应的特征向量可实现降维同时最小化信息损失。其次，特征值与网络稳定性密切相关，线性动力系统 $\dot{\mathbf{x}} = \mathbf{A}\mathbf{x}$ 的稳定性由特征值决定：所有特征值实部为负则系统稳定。RNN和Transformer的稳定性分析有助于理解和解决梯度消失或爆炸问题，LSTM和GRU通过门控机制使等效变换矩阵具有接近1的特征值，缓解梯度消失。第三，谱范数（Spectral Norm）是矩阵最大奇异值，即 $\mathbf{A}^\top \mathbf{A}$ 最大特征值的平方根，用于权重归一化稳定训练，也是证明神经网络泛化性质的关键工具。谱归一化将权重矩阵谱范数归一化为1，约束网络函数Lipschitz常数。

### 1.1.6 奇异值与低秩近似

奇异值分解（SVD）是将任意矩阵分解为三个矩阵乘积的强大工具。对于任意矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$，存在正交矩阵 $\mathbf{U} \in \mathbb{R}^{m \times m}$、$\mathbf{V} \in \mathbb{R}^{n \times n}$ 和对角矩阵 $\mathbf{\Sigma} \in \mathbb{R}^{m \times n}$，使得 $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top$。其中 $\mathbf{\Sigma}$ 对角线元素 $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r > 0$（$r$ 为矩阵的秩）称为奇异值，是 $\mathbf{A}^\top \mathbf{A}$ 特征值的平方根。$\mathbf{U}$ 的列向量是左奇异向量，$\mathbf{V}$ 的列向量是右奇异向量。SVD适用于任意矩阵，无论方阵还是矩形阵，无论是否满秩。

低秩近似是SVD最重要的应用。根据Eckart-Young-Mirsky定理，对任何整数 $k \leq r$，用秩不超过 $k$ 的矩阵对 $\mathbf{A}$ 的最佳近似（在Frobenius范数和谱范数意义下）是保留前 $k$ 个最大奇异值及其对应奇异向量，即 $\mathbf{A}_k = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top$。在大语言模型中，低秩近似技术应用广泛。在模型压缩方面，权重矩阵近似为 $\mathbf{A} \approx \mathbf{U}_k \mathbf{V}_k^\top$，参数量从 $mn$ 减少到 $k(m+n)$。在参数高效微调中，LoRA假设权重更新可表示为低秩矩阵 $\Delta \mathbf{W} = \mathbf{B}\mathbf{A}$，只需训练 $k(m+n)$ 个参数而非 $mn$ 个参数。奇异值分解还可用于理解模型表达能力，分析奇异值分布可了解模型使用了多少"有效自由度"。

### 1.1.7 正交性与矩阵范数

两个向量 $\mathbf{u}$ 和 $\mathbf{v}$ 若满足 $\mathbf{u}^\top \mathbf{v} = 0$ 则正交。一组向量两两正交且都是单位向量则构成标准正交基。正交矩阵是满足 $\mathbf{Q}^\top \mathbf{Q} = \mathbf{Q}\mathbf{Q}^\top = \mathbf{I}$ 的方阵 $\mathbf{Q}$，其列向量（或行向量）构成标准正交基。正交矩阵保持向量范数不变：$\|\mathbf{Q}\mathbf{x}\|_2 = \|\mathbf{x}\|_2$，只旋转或反射空间而不改变向量长度。正交矩阵的逆等于其转置：$\mathbf{Q}^{-1} = \mathbf{Q}^\top$，行列式绝对值为1。正交初始化将权重矩阵初始化为正交矩阵，理论上可防止初始化时的梯度消失或爆炸问题，因为正交矩阵的奇异值全部为1。Gram-Schmidt正交化过程在特征提取和表示学习中使用，正交约束在优化中用于防止权重矩阵秩退化。

矩阵范数衡量矩阵"大小"，将矩阵映射到非负实数，满足非负性、齐次性和三角不等式。Frobenius范数定义为所有元素平方和的平方根：$\|\mathbf{A}\|_F = \sqrt{\sum_{i,j} a_{ij}^2}$，是矩阵展开为向量后的L2范数，具有旋转不变性，在权重衰减正则化中常用。谱范数是矩阵最大奇异值：$\|\mathbf{A}\|_2 = \sigma_{\max}(\mathbf{A})$，对应线性变换最大拉伸因子，在神经网络稳定性和Lipschitz约束中是核心概念。核范数是矩阵所有奇异值的和：$\|\mathbf{A}\|_* = \sum_i \sigma_i(\mathbf{A})$，是Frobenius范数和谱范数之间的折中，常用于矩阵补全和低秩矩阵恢复问题。1-范数和∞-范数分别是列范数和行范数：$\|\mathbf{A}\|_1 = \max_j \sum_i |a_{ij}|$ 和 $\|\mathbf{A}\|_\infty = \max_i \sum_j |a_{ij}|$，在稀疏优化和线性规划中有应用。

### 1.1.8 矩阵分解与Kronecker积

除SVD外，矩阵分解还包括LU分解、QR分解、特征值分解和Cholesky分解。LU分解将矩阵分解为下三角矩阵 $\mathbf{L}$ 和上三角矩阵 $\mathbf{U}$ 的乘积：$\mathbf{A} = \mathbf{L}\mathbf{U}$，用于高效求解线性方程组。QR分解将矩阵分解为正交矩阵 $\mathbf{Q}$ 和上三角矩阵 $\mathbf{R}$ 的乘积：$\mathbf{A} = \mathbf{Q}\mathbf{R}$，在线性最小二乘问题中有重要应用。特征值分解将方阵分解为 $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}$，在动力系统分析和谱聚类中有应用。Cholesky分解是LU分解的特例，适用于对称正定矩阵：$\mathbf{A} = \mathbf{L}\mathbf{L}^\top$，数值稳定性好，计算量约为LU分解的一半。

Kronecker积是两个矩阵之间的特殊运算，将 $m \times n$ 的矩阵与 $p \times q$ 的矩阵组合成 $mp \times nq$ 的大矩阵。对于矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和 $\mathbf{B} \in \mathbb{R}^{p \times q}$，Kronecker积定义为

$$\mathbf{A} \otimes \mathbf{B} = \begin{bmatrix} a_{11}\mathbf{B} & a_{12}\mathbf{B} & \cdots & a_{1n}\mathbf{B} \\ a_{21}\mathbf{B} & a_{22}\mathbf{B} & \cdots & a_{2n}\mathbf{B} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1}\mathbf{B} & a_{m2}\mathbf{B} & \cdots & a_{mn}\mathbf{B} \tag{1.1.2}\end{bmatrix}$$

Kronecker积满足结合律、混合积性质和转置性质。在深度学习中，Kronecker积应用于权重共享、参数高效微调和特定网络结构设计。MobileNet中的深度可分离卷积可用Kronecker积表示，在模型并行化中帮助将大矩阵运算分解为多个小矩阵运算。向量化是将矩阵转换为列向量的操作：$\text{vec}(\mathbf{A})$，按列优先顺序排列元素。向量化与Kronecker积结合可简化矩阵方程表示：$\mathbf{X}\mathbf{A}\mathbf{B} = \mathbf{C}$ 可转化为 $(\mathbf{B}^\top \otimes \mathbf{A})\text{vec}(\mathbf{X}) = \text{vec}(\mathbf{C})$，在推导反向传播公式时非常有用。

### 1.1.9 计算效率与并行计算

批处理是深度学习训练和推理的基本范式，允许同时处理多个样本以提高计算效率。从张量角度看，批处理在现有维度外增加新批处理维度。单个样本特征表示是 $(S, D)$ 二维张量，批次表示为 $(B, S, D)$ 三维张量。批处理数学优势来自矩阵运算并行性，现代GPU架构非常适合批处理计算，不同样本的计算可分配到不同处理单元同时执行。注意力机制的批处理实现中，注意力分数计算 $\mathbf{Q}\mathbf{K}^\top$ 在批处理下变成 $(B, S, S)$ 三维张量运算。梯度累积模拟更大有效批大小，先计算若干小批次梯度累加，使用累加梯度进行参数更新，数学上等价于使用更大批次训练。

矩阵乘法计算复杂度是 $O(mnp)$，计算 $d \times d$ 权重矩阵与 $d \times 1$ 向量乘积需要 $O(d^2)$ 次标量乘法和加法。注意力机制计算复杂度与序列长度平方成正比：$\mathbf{Q}\mathbf{K}^\top$ 需要 $O(S^2 d)$，注意力权重与值向量乘积同样需要 $O(S^2 d)$。内存效率不仅与参数数量有关，还与激活值存储需求有关。梯度检查点通过只保存部分层激活值、反向传播时重新计算省略的激活值来节省内存。混合精度训练使用半精度浮点数进行大部分计算，内存消耗减半，计算速度提高近一倍。模型并行化将大模型分布在多个计算设备上：张量并行在宽度维度划分参数，流水线并行在深度维度划分层，数据并行复制整个模型在多个数据批次上并行训练。

### 1.1.10 本节小结

本节系统阐述了深度学习中的线性代数基础与张量运算。从向量、矩阵、张量的基本定义出发，介绍了线性代数基本运算规则及其在模型中的直接应用。特征空间的几何直觉为理解数据表示提供了直观框架，特征值、特征向量、奇异值分解揭示了线性变换的本质性质。正交性和矩阵范数在网络设计和优化中具有重要作用，Kronecker积和向量化简化了复杂矩阵运算的表达。批处理策略和并行计算技术是实现高效训练的关键，体现了数学理论向工程实践的转化。这些数学基础构成了理解深度学习模型工作原理和设计高效算法的理论基石。

