## 1.2 概率论与统计

概率论是研究随机现象规律性的数学分支，为大语言模型提供了处理不确定性的理论基础。文本生成本质上是随机过程：给定前文条件下，下一个词的选择遵循概率分布。语言模型的训练目标就是学习这个条件概率分布，理解随机变量和概率分布的概念是掌握语言模型概率本质的必要前提。

### 1.2.1 随机变量与概率分布

随机变量是样本空间到实数集的映射，将随机试验的结果数值化。随机变量分为离散和连续两类。离散随机变量只能取有限或可数无穷个值，如词汇表中的词索引。连续随机变量可取任意实数值，如嵌入向量、注意力权重、神经网络的激活值。大语言模型同时处理这两类变量：词索引是离散的，嵌入表示和隐藏状态是连续的。

概率质量函数（PMF）描述离散随机变量的概率分布。对于离散随机变量 $X$，其PMF记为 $P(X = x) = p(x)$，满足非负性 $p(x) \geq 0$ 和归一性 $\sum_{x} p(x) = 1$。语言模型输出层的预测分布是典型的PMF，词汇表大小为 $V$ 的模型输出是 $V$ 维的概率向量。

伯努利分布是最简单的离散分布，描述单次二元试验的成功概率。设随机变量 $X \sim \text{Bernoulli}(p)$，则 $P(X = 1) = p$，$P(X = 0) = 1-p$，期望 $E[X] = p$，方差 $\text{Var}(X) = p(1-p)$。语言模型中可用于描述二元随机事件。

二项分布是 $n$ 次独立伯努利试验中成功次数的分布。设 $X \sim \text{Binomial}(n, p)$，则 $P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$，期望 $E[X] = np$，方差 $\text{Var}(X) = np(1-p)$。

类别分布是伯努利分布向多值情况的推广。设 
$$X \sim \text{Categorical}(p_1, p_2, \ldots, p_K)\tag{1.2.1}$$则 $P(X = k) = p_k$，其中 $p_k \geq 0$ 且 $\sum_{k=1}^{K} p_k = 1$。语言模型输出层正是类别分布的应用，Softmax函数将原始输出转换为类别概率分布。

连续随机变量由概率密度函数（PDF）描述。设 $X$ 的PDF为 $f(x)$，则 $P(a \leq X \leq b) = \int_a^b f(x) dx$，PDF满足非负性 $f(x) \geq 0$ 和归一性 $\int_{-\infty}^{\infty} f(x) dx = 1$。

均匀分布是最简单的连续分布。设 $X \sim \text{Uniform}(a, b)$，则PDF为 $f(x) = \frac{1}{b-a}$（当 $a \leq x \leq b$），期望 $E[X] = \frac{a+b}{2}$，方差 $\text{Var}(X) = \frac{(b-a)^2}{12}$。语言模型中可用于权重初始化和随机采样策略。

指数分布描述独立事件发生的时间间隔。设 $X \sim \text{Exponential}(\lambda)$，则PDF为 $f(x) = \lambda e^{-\lambda x}$（当 $x \geq 0$），期望 $E[X] = \frac{1}{\lambda}$，方差 $\text{Var}(X) = \frac{1}{\lambda^2}$。

### 1.2.2 期望、方差与协方差

期望、方差和协方差是概率论中最重要的统计量，刻画随机变量的集中趋势、离散程度和相互关系。

期望是随机变量的加权平均，描述"中心"位置。对于离散随机变量 $E[X] = \sum_x x \cdot p(x)$，对于连续随机变量 $E[X] = \int_{-\infty}^{\infty} x \cdot f(x) dx$。期望具有线性性质：$E[aX + bY] = aE[X] + bE[Y]$。条件期望 $E[Y | X = x]$ 是 $x$ 的函数，全期望公式为 $E[Y] = E[E[Y | X]]$。

方差衡量随机变量取值的离散程度，定义为 $\text{Var}(X) = E[(X - E[X])^2]$，可展开为 $\text{Var}(X) = E[X^2] - (E[X])^2$。方差性质包括：非负性 $\text{Var}(X) \geq 0$；对常数 $c$ 有 $\text{Var}(cX) = c^2 \text{Var}(X)$；若 $X$ 和 $Y$ 独立，则 $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$。标准差 $\sigma(X) = \sqrt{\text{Var}(X)}$ 与 $X$ 量纲相同。

协方差衡量两个随机变量之间的线性相关程度，定义为 $$\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\tag{1.2.2}$$可展开为 $\text{Cov}(X, Y) = E[XY] - E[X]E[Y]$。协方差大于0表示同向变化，小于0表示反向变化，等于0表示线性无关。

协方差矩阵是多维随机变量的二阶中心矩描述。对于 $n$ 维随机向量 $\mathbf{X} = (X_1, X_2, \ldots, X_n)^\top$，协方差矩阵 $\mathbf{\Sigma}$ 是 $n \times n$ 对称半正定矩阵，元素 $\Sigma_{ij} = \text{Cov}(X_i, X_j)$。对角线元素是各变量方差，非对角线元素是协方差。

相关系数是标准化的协方差，定义为 $\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sigma(X)\sigma(Y)}$，取值范围 $[-1, 1]$。矩是随机变量幂次期望的统称，$k$ 阶原点矩 $m_k = E[X^k]$，$k$ 阶中心矩 $\mu_k = E[(X - E[X])^k]$。矩母函数 $M(t) = E[e^{tX}]$ 唯一确定随机变量分布。

### 1.2.3 高斯分布与多元高斯

高斯分布（正态分布）是概率论中最重要的连续分布，核心地位源于中心极限定理、易于处理的数学性质和广泛的应用场景。

一维高斯分布的概率密度函数为 $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$，记为 $X \sim \mathcal{N}(\mu, \sigma^2)$。其中 $\mu$ 是均值参数，$\sigma^2$ 是方差参数。标准高斯分布 $\mathcal{N}(0, 1)$ 的PDF为 $\phi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}$，任何高斯随机变量可通过 $Z = \frac{X-\mu}{\sigma}$ 标准化。高斯分布的期望为 $\mu$，方差为 $\sigma^2$，熵为 $H(X) = \frac{1}{2}\log(2\pi e\sigma^2)$。

多元高斯分布是一维向多维的推广。设 $\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})$，其中 $\boldsymbol{\mu} \in \mathbb{R}^d$ 是均值向量，$\mathbf{\Sigma} \in \mathbb{R}^{d \times d}$ 是对称半正定协方差矩阵，则

$$f(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} |\mathbf{\Sigma}|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right)\tag{1.2.3}$$

多元高斯分布性质：边缘分布仍是高斯分布；线性变换保持高斯性，即 $\mathbf{Y} = \mathbf{A}\mathbf{X} + \mathbf{b} \sim \mathcal{N}(\mathbf{A}\boldsymbol{\mu} + \mathbf{b}, \mathbf{A}\mathbf{\Sigma}\mathbf{A}^\top)$。将 $\mathbf{X}$ 划分为 $\mathbf{X}_a$ 和 $\mathbf{X}_b$，均值和协方差分块为

$$\boldsymbol{\mu} = \begin{bmatrix} \boldsymbol{\mu}_a \\ \boldsymbol{\mu}_b \end{bmatrix}, \quad \mathbf{\Sigma} = \begin{bmatrix} \mathbf{\Sigma}_{aa} & \mathbf{\Sigma}_{ab} \\ \mathbf{\Sigma}_{ba} & \mathbf{\Sigma}_{bb} \tag{1.2.4}\end{bmatrix}$$

给定 $\mathbf{X}_b$ 时 $\mathbf{X}_a$ 的条件分布为

$$\mathbf{X}_a | \mathbf{X}_b = \mathbf{x}_b \sim \mathcal{N}(\boldsymbol{\mu}_{a|b}, \mathbf{\Sigma}_{a|b})\tag{1.2.5}$$

其中条件均值 $\boldsymbol{\mu}_{a|b} = \boldsymbol{\mu}_a + \mathbf{\Sigma}_{ab}\mathbf{\Sigma}_{bb}^{-1}(\mathbf{x}_b - \boldsymbol{\mu}_b)$，条件协方差 $\mathbf{\Sigma}_{a|b} = \mathbf{\Sigma}_{aa} - \mathbf{\Sigma}_{ab}\mathbf{\Sigma}_{bb}^{-1}\mathbf{\Sigma}_{ba}$。

大语言模型中，高斯分布在权重初始化（Xavier、He初始化）、Dropout正则化、VAE潜在空间、高斯过程回归等方面有重要应用。

![[fig_probability_expectation_wabisabi.png]]

### 1.2.4 KL散度与交叉熵

KL散度和交叉熵是信息论核心概念，用于衡量概率分布差异，是定义损失函数的重要工具。

信息熵衡量随机变量不确定性大小。对于离散随机变量 $X$ 和概率分布 $p(x)$，熵定义为 $H(X) = H(p) = -\sum_x p(x) \log p(x)$。联合熵 $H(X, Y) = -\sum_x \sum_y p(x, y) \log p(x, y)$，条件熵 $H(X | Y) = H(X, Y) - H(Y)$。互信息 $I(X; Y) = H(X) + H(Y) - H(X, Y)$ 衡量信息共享程度。

KL散度衡量两个概率分布之间的差异。对于离散分布 $p$ 和 $q$，KL散度定义为

$$D_{\text{KL}}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}\tag{1.2.6}$$

KL散度非负，$D_{\text{KL}}(p \| q) \geq 0$，当且仅当 $p = q$ 时为0，但不对称。对于连续分布，$D_{\text{KL}}(p \| q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx$。

交叉熵与KL散度密切相关，定义为 $H(p, q) = -\sum_x p(x) \log q(x)$，可分解为 $H(p, q) = H(p) + D_{\text{KL}}(p \| q)$。语言模型训练中，交叉熵损失为 $-\log q(y_t^* | y_{<t})$，最小化交叉熵等价于最大化对数似然。交叉熵损失关于logits的梯度为 $p_i - q_i$。

标签平滑将硬标签替换为软标签，防止模型过度自信。KL散度在变分推断、VAE、强化学习PPO、知识蒸馏中有重要应用。

### 1.2.5 统计推断与参数估计

统计推断是利用样本数据对总体特征进行推断的过程，包括参数估计和假设检验。

最大似然估计（MLE）是最基本的参数估计方法。设观测数据 $\mathcal{D} = \{x_1, x_2, \ldots, x_n\}$ 独立同分布 $p(x | \theta)$，似然函数 $L(\theta) = \prod_{i=1}^n p(x_i | \theta)$，对数似然 $\ell(\theta) = \sum_{i=1}^n \log p(x_i | \theta)$，MLE为 $\hat{\theta}_{\text{MLE}} = \arg\max_\theta \ell(\theta)$。语言模型训练使用MLE最大化训练语料的对数似然。MLE具有相合性、渐近正态性和渐近有效性。

贝叶斯估计将参数视为随机变量，后验分布为

$$p(\theta | \mathcal{D}) = \frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})}\tag{1.2.7}$$

共轭先验使后验分布与先验分布同分布族，如高斯分布的共轭先验是高斯分布，多项分布的共轭先验是Dirichlet分布。变分推断将后验推断转化为优化问题，通过最小化近似分布与真实后验的KL散度找到最好近似。

期望传播通过匹配矩近似后验分布。自助法通过重采样估计统计量方差，在语言模型评估中用于估计指标置信区间。

### 1.2.6 极限定理

大数定律和中心极限定理描述大量随机变量之和的渐近行为，为机器学习方法提供理论依据。

大数定律指出独立同分布随机变量的样本均值依概率收敛于期望。设 $X_1, X_2, \ldots, X_n$ 独立同分布，$E[X_i] = \mu$，则样本均值 $\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$ 满足 $\lim_{n \to \infty} P(|\bar{X}_n - \mu| > \epsilon) = 0$。语言模型训练中，批量大小增加使梯度估计方差减小，优化更稳定。

中心极限定理指出大量独立同分布随机变量之和趋向正态分布。设 $X_1, X_2, \ldots, X_n$ 独立同分布，$E[X_i] = \mu$，$\text{Var}(X_i) = \sigma^2$，则标准化和 $Z_n = \frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}}$ 的分布趋向标准正态分布。中心极限定理表明正态分布在自然界无处不在，用于分析梯度分布、初始化策略和批量归一化效果。

批量归一化利用中心极限定理原理稳定训练，当批量足够大时，批量内样本的均值和方差是整体的良好估计。验证集性能指标根据大数定律和中心极限定理可构建置信区间。

### 1.2.7 假设检验与模型评估

假设检验用于根据样本数据判断总体假设是否成立，在语言模型开发和研究中用于比较模型性能和验证改进措施。

假设检验框架包括原假设 $H_0$ 和备择假设 $H_1$。根据样本数据计算检验统计量和p值，若p值小于显著性水平则拒绝原假设。配对t检验比较两个相关样本均值，用于评估两个语言模型在相同测试集上的性能差异。

效应量衡量差异的实际大小，如Cohen's d。置信区间比p值提供更多信息，给出差异大小的可能范围。多重比较时需使用校正方法控制总体错误率。置换检验是非参数方法，适用于样本量不大或分布未知的情况。

### 1.2.8 Logits与概率分布的生成

大语言模型首先构建Logits向量（未归一化实数向量），是连接神经网络输出与概率分布的桥梁。设词汇表大小为 $V$，Logits向量 $\mathbf{z} \in \mathbb{R}^V$，分量 $z_i \in (-\infty, +\infty)$ 代表模型对第 $i$ 个词元的原始置信度评分。

从Logits到概率分布通过Softmax函数实现，将 $\mathbb{R}^V$ 映射到概率单纯形 $\Delta^{V-1}$

$$p_i = P(X = i) = \text{Softmax}(\mathbf{z})_i = \frac{\exp(z_i)}{\sum_{j=1}^{V} \exp(z_j)}\tag{1.2.8}$$

Softmax确保输出的概率分布满足归一性条件 $\sum_{i=1}^{V} p_i = 1$，具有"软最大化"特性，为所有类别分配非零概率。实际实现中通常计算Log-Softmax以提高数值稳定性：$\log \text{Softmax}(\mathbf{z})_i = z_i - \log \sum_{j=1}^{V} \exp(z_j)$。

交叉熵损失关于 $z_k$ 的偏导数为 $\frac{\partial L}{\partial z_k} = p_k - y_k$，具有"预测减真实"的残差结构。温度参数 $T$ 调节Softmax锐度：$p_i = \frac{\exp(z_i / T)}{\sum_{j=1}^{V} \exp(z_j / T)}$。Top-k和Top-p采样等解码策略在概率分布层面操作。

### 1.2.9 本节小结

本节系统阐述了概率论与统计在大语言模型中的基础应用。从随机变量和概率分布的基本概念出发，介绍了离散分布（伯努利、二项、类别分布）和连续分布（均匀、指数、高斯分布）的性质。期望、方差、协方差等统计量刻画随机变量的特征，KL散度和交叉熵是信息论核心概念和损失函数基础。最大似然估计和贝叶斯估计是参数估计的基本方法，大数定律和中心极限定理为优化和评估提供理论保障。Logits和Softmax是连接模型输出与概率分布的关键变换，构成了语言模型生成机制的理论基础。