微积分是研究变化率和累积效应的数学分支，为理解和优化大语言模型提供了核心的数学工具。大语言模型的训练本质上是一个优化问题：我们需要找到一组模型参数，使得模型在训练数据上的损失函数值最小化。这个优化过程涉及对损失函数求导（计算梯度），然后根据梯度方向更新参数。理解偏导数、链式法则、梯度和Hessian等概念，是深入掌握深度学习优化算法的必要基础。

## 1.3.1 偏导数与链式法则

偏导数是多元函数对单个变量的导数，它描述了当其他变量保持不变时，函数值随某一特定变量变化的速率。设$f: \mathbb{R}^n \to \mathbb{R}$是一个多元函数，$f(x_1, x_2, \ldots, x_n)$，则$f$关于变量$x_i$在点$\mathbf{x} = (x_1, x_2, \ldots, x_n)$处的偏导数定义为：
$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_i, \ldots, x_n)}{h}
$$
从几何角度来看，偏导数可以理解为多元函数图像与包含坐标轴$x_i$的平面相交得到的一元函数曲线的切线斜率。在大语言模型的语境下，当我们计算损失函数关于某个特定参数的偏导数时，我们实际上是在问：如果只改变这个参数而保持其他参数不变，损失函数会以多快的速度变化。这个信息对于确定参数更新的方向和幅度至关重要。

偏导数的计算遵循与单变量导数相同的规则。常数的偏导数为零，常数因子的偏导数等于该因子乘以函数的偏导数，和的偏导数等于偏导数的和。对于乘积$\frac{\partial (uv)}{\partial x} = u \frac{\partial v}{\partial x} + v \frac{\partial u}{\partial x}$​，商的偏导数$\frac{\partial (u/v)}{\partial x} = \frac{v \frac{\partial u}{\partial x} - u \frac{\partial v}{\partial x}}{v^2}$​​，链式法则将在下文详细讨论。复合函数的偏导数法则与单变量情况类似，只是需要明确哪个变量被当作中间变量。

链式法则（Chain Rule）是微积分中最重要的法则之一，它告诉我们如何计算复合函数的导数。在深度学习中，神经网络本质上是一个嵌套的复合函数，链式法则使得我们能够计算损失函数相对于任意深层参数的梯度，这一过程称为反向传播。链式法则的基本形式可以这样表述：如果$y = f(g(x))$，则$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$，其中$\frac{dy}{dg}$是外层函数关于中间变量的导数，$\frac{dg}{dx}$是内层函数关于自变量的导数。

对于多元函数的复合，情况稍微复杂一些。设$y = f(u_1, u_2, \ldots, u_m)$，其中每个$u_i = g_i(x_1, x_2, \ldots, x_n)$，则$y$关于$x_j$的偏导数为：
$$
\frac{\partial y}{\partial x_j} = \sum_{i=1}^{m} \frac{\partial y}{\partial u_i} \cdot \frac{\partial u_i}{\partial x_j}
$$
这个公式表明，$x_j$对$y$的总影响是通过所有中间变量$u_i$传导的，每个中间变量的贡献是它对$y$的偏导数与它对$x_j$的偏导数的乘积之和。

在深度学习的反向传播算法中，链式法则的应用更加系统化。考虑一个简单的两层神经网络：$\mathbf{h} = f(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1)$，$\mathbf{y} = g(\mathbf{W}_2 \mathbf{h} + \mathbf{b}_2)$，其中$f$和$g$是激活函数，$\mathbf{W}_1, \mathbf{W}_2$是权重矩阵，$\mathbf{b}_1, \mathbf{b}_2$是偏置向量，$\mathbf{x}$是输入，$\mathbf{y}$是输出。损失函数$L(\mathbf{y}, \mathbf{t})$衡量输出$\mathbf{y}$与目标$\mathbf{t}$之间的差异。反向传播从输出层开始，首先计算$\frac{\partial L}{\partial \mathbf{y}}$​，然后逐层向后传递：$\frac{\partial L}{\partial \mathbf{h}} = \frac{\partial L}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{h}}$，接着计算$\frac{\partial L}{\partial \mathbf{W}_2} = \frac{\partial L}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{W}_2}$​，以此类推直到输入层。
![[backprop.drawio.png]]

矩阵形式的链式法则在深度学习中尤为重要。设$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$，$y = f(\mathbf{z})$，则梯度$\frac{\partial y}{\partial \mathbf{W}}$是一个与$\mathbf{W}$形状相同的三阶张量，而$\frac{\partial y}{\partial \mathbf{x}} = \mathbf{W}^\top \frac{\partial y}{\partial \mathbf{z}}$​。为了高效地实现这些计算，我们使用雅可比矩阵（Jacobian Matrix）来组织偏导数。雅可比矩阵包含所有一阶偏导数，对于函数$\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$，雅可比矩阵 $\mathbf{J}$是一个$m \times n$的矩阵，其中$J_{ij} = \frac{\partial f_i}{\partial x_j}$​。

在计算图中，链式法则的应用更加直观和系统。计算图是表示数学表达式的有向无环图，其中节点表示变量或操作，边表示依赖关系。反向传播算法沿着计算图反向遍历，对于每个操作节点，计算其输出关于输入的梯度，然后使用链式法则组合这些梯度。典型的操作包括矩阵乘法、加法、元素级函数（如ReLU、Sigmoid、Softmax）和归一化操作，每种操作都有预定义的梯度计算规则。

自动微分（Automatic Differentiation）是现代深度学习框架的核心功能，它利用链式法则自动计算复杂函数的导数。与数值微分（使用有限差分近似导数）和符号微分（使用代数规则显式推导导数公式）不同，自动微分将函数分解为基本操作的序列，然后应用链式法则累加梯度。自动微分结合了数值计算的效率和符号计算的精确性，是训练神经网络的关键技术基础。

## 1.3.2 多元函数梯度与Hessian

梯度是多元函数最陡上升方向的向量，包含了函数关于所有自变量的偏导数信息。设$f: \mathbb{R}^n \to \mathbb{R}$是一个可微函数，则$f$在点$\mathbf{x} = (x_1, x_2, \ldots, x_n)$处的梯度定义为：
$$
\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
$$
梯度向量指向函数增长最快的方向，其长度（范数）反映了增长的速率。负梯度方向则是函数下降最快的方向，这就是梯度下降算法的理论基础。在大语言模型中，梯度向量指导着参数的更新方向：每个参数分量根据其在梯度中的值进行调整，使得整体损失函数朝着下降的方向移动。

梯度的性质对于理解优化过程至关重要。梯度与等值面（或等值线）正交：在函数$f(\mathbf{x}) = c$的等值面上，梯度向量与等值面垂直。这意味着如果我们沿着梯度方向移动，我们会以最快的速度离开当前的等值面，进入函数值更高的区域。驻点（Critical Point）是梯度为零向量的点，即$\nabla f(\mathbf{x}) = 0$，这些点可能是极小值、极大值或鞍点。

方向导数（Directional Derivative）衡量函数沿特定方向的变化率。设$\mathbf{d}$是一个单位向量，则$f$沿$\mathbf{d}$的方向导数为$\nabla_{\mathbf{d}} f(\mathbf{x}) = \nabla f(\mathbf{x})^\top \mathbf{d} = \|\nabla f(\mathbf{x})\| \cos\theta$，其中$\theta$是梯度与方向$\mathbf{d}$之间的夹角。这个公式清楚地表明，方向导数在梯度方向（$\theta = 0$）上取得最大值，在与梯度相反的方向（$\theta = \pi$）上取得最小值（负的最大值），在与梯度垂直的方向（$\theta = \pi/2$）上为零。

Hessian矩阵是多元函数的二阶导数矩阵，包含了关于所有自变量的二阶偏导数信息。对于函数$f: \mathbb{R}^n \to \mathbb{R}$，Hessian矩阵定义为：
$$
\mathbf{H} = \nabla^2 f(\mathbf{x}) = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}​​​
$$
如果函数$f$的二阶偏导数连续（这是多数实际应用中的常见假设），则Hessian矩阵是对称的，即$\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$​。对称性大大简化了Hessian矩阵的分析和计算。

Hessian矩阵在优化中的作用主要体现在以下几个方面。首先，通过Hessian矩阵可以判断驻点的性质：如果Hessian正定（所有特征值大于零），则该点是局部极小值；如果Hessian负定，则该点是局部极大值；如果Hessian既有正特征值又有负特征值，则该点是鞍点；如果Hessian半正定或半负定，则需要进一步分析。

其次，Hessian矩阵决定了泰勒展开的二次项，影响局部优化的收敛速度。在点$\mathbf{x}_0$​附近，函数$f$的二阶泰勒展开为：
$$
f(\mathbf{x}) \approx f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0)^\top (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^\top \mathbf{H}(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0)
$$
这个近似在局部区域非常准确，是理解牛顿法等二阶优化方法的基础。

第三，Hessian矩阵的奇异值分解揭示了函数的局部曲率结构。Hessian的特征值表示函数在不同特征方向上的曲率（凹凸程度），特征向量表示这些曲率对应的方向。在深度学习优化中，损失函数的Hessian通常具有极值的特征值分布：少数几个特征值很大（对应"平坦"方向），多数特征值很小（对应"陡峭"方向），这种结构影响着优化算法的行为。

共轭梯度法利用Hessian的信息来加速收敛，而不显式地计算和存储完整的Hessian矩阵。在深度学习中，由于参数数量巨大（可达数十亿），显式计算Hessian是不现实的，因此共轭梯度法和拟牛顿法等方法在处理大规模优化问题时具有优势。
![[CombinedScene_ManimCE_v0.19.1.png]]

K-FAC（Kronecker-Factored Approximate Curvature）是一种用于深度学习的Hessian近似方法，它将Hessian矩阵近似为两个小矩阵的Kronecker积，从而大幅降低计算和存储成本。K-FAC基于层间独立性假设，假设每层的参数可以独立地近似其Hessian块，这使得在保持二阶信息的同时实现了可扩展性。

高斯-牛顿矩阵是Hessian的一个常用近似，在最小二乘问题中特别有用。对于残差形式的问题 $L(\mathbf{w}) = \frac{1}{2} \| \mathbf{r}(\mathbf{w}) \|^2$，高斯-牛顿矩阵为$\mathbf{J}^\top \mathbf{J}$，其中$\mathbf{J}$是残差函数的雅可比矩阵。与完整的Hessian相比，高斯-牛顿矩阵省略了二阶导数项，通常是正定的，并且计算成本更低。

拉普拉斯近似利用Hessian矩阵在峰值附近对概率分布进行高斯近似。在贝叶斯深度学习和变分推断中，拉普拉斯近似用于构建后验分布的高斯代理，使得预测具有不确定性估计。这种方法在模型压缩和少样本学习中有一定的应用价值。

## 1.3.3 梯度下降与凸优化基础

梯度下降是最基本也最广泛使用的优化算法，它利用负梯度方向作为搜索方向来迭代地最小化目标函数。基本梯度下降算法的更新规则为$\mathbf{w}_{t+1} = \mathbf{w}_t - \eta_t \nabla L(\mathbf{w}_t)$，其中$\mathbf{w}_t$是第$t$步的参数，$\eta_t$是学习率（步长），$\nabla L(\mathbf{w}_t)$是损失函数在当前参数处的梯度。学习率是一个关键的超参数，它决定了每次更新的幅度：学习率太小会导致收敛缓慢，学习率太大可能导致跳过最优解甚至发散。

学习率调度（Learning Rate Scheduling）是在训练过程中动态调整学习率的策略。常见的学习率调度方法包括：阶梯衰减（Step Decay），每经过若干个epoch将学习率降低一个因子；指数衰减（Exponential Decay），学习率按指数函数递减；余弦退火（Cosine Annealing），学习率按余弦曲线从初始值逐渐减小到零；Warmup，在训练初期逐渐增加学习率，然后再衰减。这些调度策略的目标是在训练初期快速收敛，在后期精细调优，最终达到较好的收敛效果。

![[grad.png]]
凸优化是研究凸函数在凸集上最小化问题的数学分支，具有优美的理论性质和高效的算法。在凸优化问题中，任何局部最优解都是全局最优解，且最优解集是凸集。凸函数是一类特殊的函数，其上方的图构成一个凸集。函数$f$是凸函数的充要条件是对于任意$\mathbf{x}, \mathbf{y}$和任意$\theta \in [0, 1]$，有$f(\theta \mathbf{x} + (1-\theta)\mathbf{y}) \leq \theta f(\mathbf{x}) + (1-\theta) f(\mathbf{y})$。几何上，这意味着函数图上任意两点的连线位于函数图的上方。

严格凸函数是凸性更强的形式，要求上述不等式在$\mathbf{x} \neq \mathbf{y}$和$\theta \in (0,1)$时取严格小于号。严格凸函数至多有一个全局最小值点。可微函数是凸函数的充要条件是其梯度单调不减：$(\nabla f(\mathbf{y}) - \nabla f(\mathbf{x}))^\top (\mathbf{y} - \mathbf{x}) \geq 0$。对于二次可微函数，凸性的充要条件是Hessian矩阵半正定。

梯度下降在凸优化中的收敛性有完善的理论保证。对于具有L-利普希茨连续梯度的凸函数（即$\|\nabla f(\mathbf{x}) - \nabla f(\mathbf{y})\| \leq L \|\mathbf{x} - \mathbf{y}\|$），标准梯度下降以$O(1/t)$的速度收敛，即经过$t$次迭代后，目标函数值与最优值的差距为$O(1/t)$。对于强凸函数（即存在$\mu > 0$使得$f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) + \frac{\mu}{2} \|\mathbf{y} - \mathbf{x}\|^2$，梯度下降以线性速度收敛，即误差按几何级数衰减。

动量方法（Momentum）是加速梯度下降的重要技术，它累积历史梯度来平滑参数更新。动量更新的公式为：$\mathbf{v}_{t+1} = \gamma \mathbf{v}_t + \eta_t \nabla L(\mathbf{w}_t)$，$\mathbf{w}_{t+1} = \mathbf{w}_t - \mathbf{v}_{t+1}$​，其中$\mathbf{v}_t$是速度向量，$\gamma \in [0, 1)$是动量系数。动量方法可以看作是对梯度进行指数移动平均，使得更新方向更加平滑，从而加速收敛并减少震荡。Nesterov动量是动量方法的一个变体，它在更新之前先对梯度进行校正，通常能提供更好的收敛性质。

自适应学习率方法为每个参数单独调整学习率，是深度学习优化中最重要的进展之一。AdaGrad为每个参数维护一个累积梯度平方和，并相应地调整学习率：$\mathbf{w}_{t+1} = \mathbf{w}_t - \frac{\eta}{\sqrt{\mathbf{G}_{t} + \epsilon}} \odot \nabla L(\mathbf{w}_t)$，其中$\mathbf{G}_t$是逐元素累积的梯度平方，$\odot$表示逐元素乘法。AdaGrad特别适合处理稀疏特征和非均匀分布的梯度。

RMSprop是AdaGrad的改进版本，使用指数移动平均来累积梯度平方：$\mathbf{G}_{t} = \rho \mathbf{G}_{t-1} + (1-\rho) \nabla L(\mathbf{w}_t) \odot \nabla L(\mathbf{w}_t)$，其中$\rho$是衰减系数。这解决了AdaGrad学习率单调递减的问题，使得算法能够适应非平稳的目标函数。

Adam（Adaptive Moment Estimation）是最流行的自适应优化算法，它结合了动量方法和RMSprop的优点。Adam维护两个指数移动平均：$m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla L(\mathbf{w}_t)$（一阶矩估计，即动量）和$v_t = \beta_2 v_{t-1} + (1-\beta_2) \nabla L(\mathbf{w}_t) \odot \nabla L(\mathbf{w}_t)$（二阶矩估计，即未中心化的方差）。为了校正偏差，使用$\hat{m}_t = m_t / (1-\beta_1^t)$和$\hat{v}_t = v_t / (1-\beta_2^t)$。更新规则为$\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon)$。Adam的超参数$\beta_1, \beta_2$和$\epsilon$通常使用默认值$\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$。

尽管Adam在实践中表现优异，但最近的研究表明，在某些情况下，传统的带动量SGD可能优于Adam。AMSGrad是Adam的一个修改版本，通过保持$v_t$的单调性来保证收敛性。RAdam（Rectified Adam）通过引入一个校正因子来修复Adam在训练早期的方差问题。这些变体反映了深度学习优化研究的持续进展。

大语言模型的优化面临独特的挑战。首先，模型参数量巨大（数十亿甚至万亿级），标准优化算法的内存开销巨大。梯度检查点技术通过在前向传播时只保存部分激活值，在反向传播时重新计算其余激活值，以计算换内存。其次，混合精度训练使用半精度浮点数进行计算以节省内存和加速，但仍需要保持某些计算的精度。

第三，学习率预热（Warmup）在训练大语言模型中尤为重要。warmup策略在训练初期逐渐增加学习率，从很小的值线性增长到目标学习率，然后再进行衰减。这被认为有助于模型在早期阶段找到一个更好的参数区域，避免在随机初始化阶段受到过大梯度的干扰。第四，梯度裁剪（Gradient Clipping）限制梯度的范数以防止梯度爆炸，这对于训练深层网络和循环神经网络尤为重要。

非凸优化是深度学习面临的现实挑战。与凸优化问题不同，深度神经网络的损失函数通常是非凸的，存在大量的局部最小值、鞍点和平坦区域。非凸性意味着梯度下降可能收敛到局部最小值而非全局最小值。然而，实践表明深度学习模型通常能够找到性能不错的解，即使不能保证达到全局最优。关于为什么深度神经网络能够有效优化，有多种理论解释，包括损失函数的"伪凸"性质、随机梯度下降的隐式正则化效应，以及局部最小值通常具有相似的损失值等。

鞍点逃离是大规模非凸优化中的一个重要问题。在高维空间中，鞍点比局部最小值更为常见。动量方法和噪声梯度有助于算法逃离鞍点。SGD的随机性本身就提供了一种隐式的噪声源，有助于逃离平坦区域。研究表明，在适当的噪声水平下，SGD倾向于收敛到平坦的局部最小值，这些最小值通常具有更好的泛化能力。

局部最小值的等价性是大规模深度学习中的一个有趣现象。研究发现，对于不同的随机初始化，深度网络往往收敛到具有相似损失值的局部最小值，即使这些最小值在参数空间中相距甚远。这可能是因为神经网络具有大量的对称性（如神经元排列的不变性），使得不同的参数化方式可以产生相同的函数。此外，损失函数的等值面在高维空间中可能比直觉上预期的更加连通，允许梯度下降在不同局部最小值之间"穿行"。