## 9.5 稳定性控制的统一数学视角
深度学习优化的稳定性控制是训练成功的关键因素。从数学角度看，各种稳定性技术（如归一化、正则化、Dropout等）都可以统一在一个框架下理解：它们都通过引入受控的随机性或确定性变换，在保持期望输出的同时改变输出的方差分布，从而实现对优化过程的稳定化。本节将从期望保持、噪声模型和与SGD噪声的关系三个角度，建立稳定性控制的统一数学框架。

### 9.5.1 期望保持性质的统一框架
期望保持性质是稳定性控制技术的核心数学特性。无论是归一化层、Dropout还是其他随机化技术，它们都满足一个共同的性质：在统计意义上保持输出的期望值。设输入为$\mathbf{x}$，变换操作记为$T$，输出为$\mathbf{y} = T(\mathbf{x})$，则期望保持性质可以统一表示为：
$$
\mathbb{E}[\mathbf{y}] = \mathbf{x} \tag{9.5.1}
$$
对于有放缩Dropout，变换为$T(\mathbf{x}) = \frac{1}{p} \mathbf{m} \odot \mathbf{x}$，其中$\mathbf{m}$是伯努利掩码，满足$\mathbb{E}[\mathbf{m}] = p \mathbf{1}$，因此$\mathbb{E}[T(\mathbf{x})] = \mathbf{x}$。对于层归一化，变换为$T(\mathbf{x}) = \frac{\mathbf{x} - \mu}{\sigma} \odot \boldsymbol{\gamma} + \boldsymbol{\beta}$，由于$\frac{\mathbf{x} - \mu}{\sigma}$的期望为零，经过仿射变换后期望恢复为原始输入的某种平移。对于L2正则化，虽然它不直接作用于前向传播的输出，但在反向传播中保持梯度的某种一致性。
**定理 9.5.1** 如果连续应用两个期望保持变换$T_1$和$T_2$，则复合变换$T_2 \circ T_1$也是期望保持的。证明：$\mathbb{E}[T_2(T_1(\mathbf{x}))] = \mathbb{E}[T_1(\mathbf{x})] = \mathbf{x}$。这个性质意味着在深度网络中，可以逐层应用多种稳定性技术而不破坏整体的期望一致性。
期望保持性质的实践意义在于：它确保了稳定性变换不会改变模型在期望意义上的表达能力。模型可以学习与没有这些变换时相同的函数映射，只是优化过程变得更加稳定。这种设计原则指导着各种稳定性技术的实现方式，使得不同技术可以在同一个模型中协同工作。

### 9.5.2 等价噪声模型与稳定性机制
各种稳定性控制技术都可以等价为在模型中引入特定结构的噪声。这种噪声模型视角提供了一个统一的框架来理解和比较不同的稳定性技术。
对于Dropout，输出的随机性可以直接建模为乘性噪声：
$$
\mathbf{y} = \mathbf{x} \odot \boldsymbol{\epsilon} \tag{9.5.2}
$$
其中$\epsilon_j \sim \text{Bernoulli}(p)$或$\epsilon_j \sim \mathcal{N}(p, \sigma^2)$。噪声的统计特性由保留概率$p$控制，方差为$p(1-p)$或$\sigma^2$。这种乘性噪声的强度与输入信号成正比，对于较大的激活值引入较大的噪声方差。
对于批量归一化，等价噪声模型为加性噪声加上尺度变换。训练阶段的统计量$\mu_{\mathcal{B}}$和$\sigma_{\mathcal{B}^2}$是对总体均值和方差的估计，存在估计误差：
$$
\mu_{\mathcal{B}} = \mu + \delta_\mu, \quad \sigma_{\mathcal{B}}^2 = \sigma^2 + \delta_\sigma \tag{9.5.3}
$$
其中$\delta_\mu$和$\delta_\sigma$是随机噪声，其方差与批量大小成反比。较大的批量产生更准确的统计估计，引入的噪声更小。
样本均值的估计误差满足$\text{Var}(\hat{\mu}) = \sigma^2 / N$，其中$N$是批量大小。这意味着为了将均值的估计误差减半，需要将批量大小增加四倍。
**表 9.5.1 稳定性技术的等价噪声模型对比**

| 技术类型 | 噪声类型 | 噪声强度控制参数 | 期望保持方式 |
| ------- | -------- | --------------- | ------------ |
| Dropout | 乘性伯努利噪声 | 保留概率$p$ | 训练时放缩$1/p$ |
| 批量归一化 | 统计量估计噪声 | 批量大小$N$ | 移动平均近似 |
| 高斯Dropout | 乘性高斯噪声 | 方差参数$\sigma^2$ | 设置均值$\mu = p$ |

### 9.5.3 与SGD噪声的统一建模
SGD中的随机性来源于小批量采样，其噪声模型与Dropout存在深刻的数学联系。这种联系揭示了随机正则化与随机优化之间的统一性。
SGD的梯度噪声可以表示为：
$$
g_B(\theta) = \nabla \mathcal{L}(\theta) + \epsilon_B(\theta) \tag{9.5.4}
$$
其中$\epsilon_B(\theta)$是批量大小为$B$时的梯度噪声，其协方差矩阵与批量大小成反比。设$\Sigma_1(\theta)$为单个样本的梯度协方差矩阵，则：
$$
\Sigma_B(\theta) = \frac{1}{B} \Sigma_1(\theta) \tag{9.5.5}
$$
这个关系与Dropout中噪声强度与保留概率的关系形式上相似：$\text{Var}_{\text{Dropout}} \propto p(1-p)$，$\text{Var}_{\text{SGD}} \propto 1/B$。两者都表明噪声强度可以通过一个超参数调节。
**定理 9.5.2（随机正则化的统一框架）** Dropout引入的噪声方差与SGD引入的梯度噪声方差具有相似的统计结构。设Dropout的保留概率为$p = 1/B_{\text{eff}}$，则Dropout可以视为一种$B_{\text{eff}} = 1/(1-p)$的"有效批量"噪声注入。这个等价性意味着：Dropout的正则化效果等价于使用更小批量进行SGD训练的效果。
这种统一视角有几个重要推论。首先，Dropout和SGD噪声可以相互补偿，当批量较小时，SGD噪声已经足够大，不需要额外的Dropout正则化；当批量较大时，可能需要添加Dropout来引入额外的正则化。其次，两者的噪声都是各向异性的，其协方差结构由数据的几何性质决定，而非简单的各向同性噪声。最后，噪声的温度参数由有效批量大小或保留概率控制，较低的"温度"（较大的批量或较高的保留概率）导致更确定的优化轨迹，较高的"温度"（较小的批量或较低的保留概率）导致更多的随机探索。

### 9.5.4 本节小结
本节从统一数学视角分析了深度学习中的稳定性控制技术。
期望保持性质是稳定性控制技术的核心特性，统一表示为$\mathbb{E}[\mathbf{y}] = \mathbf{x}$。这个性质确保了稳定性变换不会改变模型在期望意义上的表达能力，使得各种技术可以在同一个模型中协同工作。期望保持的传递性意味着可以逐层应用多种稳定性技术。稳定性技术可以统一建模为噪声注入过程。Dropout引入乘性伯努利噪声，批量归一化引入统计量估计噪声，高斯Dropout引入乘性高斯噪声。噪声强度由相应的超参数（保留概率、批量大小、方差参数）控制，这种统一建模便于比较和选择不同的稳定性技术。SGD的梯度噪声与Dropout噪声具有相似的统计结构。Dropout可以视为一种"有效批量"的噪声注入，其等价性揭示了随机正则化与随机优化的统一性。这种统一框架指导着批量大小和Dropout保留概率的协同调优策略。