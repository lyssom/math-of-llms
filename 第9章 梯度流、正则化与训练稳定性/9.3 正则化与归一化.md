## 9.3 正则化与归一化
归一化技术和正则化方法是深度学习中最具影响力的技术创新。归一化技术通过重参数化改变损失函数的几何景观，使得优化过程更加稳定、收敛速度更快；正则化技术则通过在损失函数中添加惩罚项或修改优化过程来约束模型复杂度，从而改善泛化性能。本节将从数学角度深入分析归一化方法的统一框架、L2正则化与权重衰减的梯度分析、L1正则化与稀疏性的数学原理，以及正则化如何改变损失曲面的几何结构。通过严格的数学推导，我们将揭示这些技术的深层机制，理解它们如何加速收敛并提升模型性能。

### 9.3.1 归一化技术的统一框架

归一化技术的核心思想是在某个特定的维度轴上计算统计量（均值和方差），然后对输入进行标准化处理。设输入张量为$\mathbf{X} \in \mathbb{R}^{N \times D}$，其中$N$为批量大小，$D$为特征维度。对于卷积神经网络，输入张量通常是四维的$\mathbf{X} \in \mathbb{R}^{N \times C \times H \times W}$；对于Transformer模型，输入张量为$\mathbf{X} \in \mathbb{R}^{N \times L \times D}$。无论输入张量的维度如何，归一化操作的核心思想是相同的。

归一化的通用数学表达式可以写为：

$$
\mathbf{y} = \frac{\mathbf{x} - \boldsymbol{\mu}}{\boldsymbol{\sigma}} \odot \boldsymbol{\gamma} + \boldsymbol{\beta} \tag{9.3.1}
$$

其中各符号的定义如下：$\mathbf{x}$为归一化前的输入张量，$\mathbf{y}$为归一化后的输出张量，$\boldsymbol{\mu}$为计算得到的均值向量，$\boldsymbol{\sigma}$为计算得到的标准差向量，$\boldsymbol{\gamma}$为可学习的缩放参数向量，$\boldsymbol{\beta}$为可学习的平移参数向量，$\odot$表示逐元素乘法。这个表达式揭示了归一化的三个核心步骤：首先是去均值化操作$\mathbf{x} - \boldsymbol{\mu}$，消除输入分布的偏移；其次是缩放归一化操作$\frac{\mathbf{x} - \boldsymbol{\mu}}{\boldsymbol{\sigma}}$，将输入分布标准化为单位方差；最后是仿射变换$\boldsymbol{\gamma} \odot \hat{\mathbf{x}} + \boldsymbol{\beta}$，恢复模型的表示能力。如果忽略仿射变换参数$\boldsymbol{\gamma}$和$\boldsymbol{\beta}$，即令$\boldsymbol{\gamma} = \mathbf{1}$，$\boldsymbol{\beta} = \mathbf{0}$，则归一化后的输出$\hat{\mathbf{x}}$满足零均值和单位方差的统计性质。设$\mu = \mathbb{E}[\mathbf{x}]$，$\sigma^2 = \text{Var}(\mathbf{x})$，则有$\mathbb{E}[\hat{\mathbf{x}}] = 0$和$\text{Var}(\hat{\mathbf{x}}) = 1$。证明可以直接从定义推导得出。批归一化是最早提出的归一化技术，其核心思想是在每个特征通道维度上，对整个小批量数据计算均值和方差。设输入张量为$\mathbf{X} \in \mathbb{R}^{N \times D}$，批归一化首先计算第$j$个特征在当前批量上的样本均值：

$$
\mu_j = \frac{1}{N} \sum_{i=1}^{N} x_{ij} \tag{9.3.2}
$$
然后计算第$j$个特征的样本方差：

$$
\sigma_j^2 = \frac{1}{N} \sum_{i=1}^{N} (x_{ij} - \mu_j)^2 \tag{9.3.3}
$$

利用计算得到的均值和方差，可以对输入进行标准化处理$\hat{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}$。批归一化在推理阶段使用训练过程中维护的全局统计量的移动平均，而不是当前批量的统计量。层归一化的核心思想是：对每个样本的所有特征维度计算统计量，而非对每个特征维度计算批量统计量。层归一化首先计算第$i$个样本的均值：

$$
\mu_i = \frac{1}{D} \sum_{j=1}^{D} x_{ij} \tag{9.3.4}
$$

然后计算第$i$个样本的方差$\sigma_i^2 = \frac{1}{D} \sum_{j=1}^{D} (x_{ij} - \mu_i)^2$。层归一化是样本条件独立的，即每个样本的归一化只依赖于该样本本身的特征，不受批量中其他样本的影响。这使得层归一化在处理变长序列和单个样本时更加稳定。均方根归一化是对层归一化的重要改进，其核心洞见是：层归一化中的去均值化操作在许多情况下是不必要的。RMSNorm计算第$i$个样本的均方根统计量：

$$
\text{RMS}_i = \sqrt{\frac{1}{D} \sum_{j=1}^{D} x_{ij}^2} \tag{9.3.5}
$$

然后对输入进行归一化$\hat{x}_{ij} = \frac{x_{ij}}{\text{RMS}_i}$。RMSNorm移除了去均值化步骤，只保留缩放参数$\gamma$，移除了平移参数$\beta$。这种设计使得RMSNorm具有缩放不变性，计算量约为层归一化的一半。

### 9.3.2 L2正则化与权重衰减的梯度分析

L2正则化（也称为权重衰减）是深度学习中最常用的正则化技术之一。其基本思想是在损失函数中添加与权重范数平方成正比的惩罚项。设原始损失函数为$\mathcal{L}_0(\boldsymbol{\theta})$，其中$\boldsymbol{\theta} \in \mathbb{R}^d$为模型参数向量，则L2正则化后的总损失函数为：

$$
\mathcal{L}_{\text{reg}}(\boldsymbol{\theta}) = \mathcal{L}_0(\boldsymbol{\theta}) + \frac{\lambda}{2} \|\boldsymbol{\theta}\|^2 \tag{9.3.6}
$$

其中$\lambda > 0$为正则化系数，控制正则化的强度，$\|\boldsymbol{\theta}\|^2 = \boldsymbol{\theta}^{\top}\boldsymbol{\theta} = \sum_{i=1}^{d} \theta_i^2$为参数向量的欧几里得范数的平方。
L2正则化项的梯度为$\nabla_{\boldsymbol{\theta}}\left(\frac{\lambda}{2} \|\boldsymbol{\theta}\|^2\right) = \lambda \boldsymbol{\theta}$。证明直接来自于对$\|\boldsymbol{\theta}\|^2 = \sum_{i=1}^{d} \theta_i^2$求偏导数：$\frac{\partial}{\partial \theta_j}\left(\sum_{i=1}^{d} \theta_i^2\right) = 2\theta_j$，因此$\nabla_{\boldsymbol{\theta}}(\|\boldsymbol{\theta}\|^2) = 2\boldsymbol{\theta}$，乘以系数$\lambda/2$后得$\lambda \boldsymbol{\theta}$。
应用L2正则化后，参数更新的梯度为：

$$
\nabla_{\boldsymbol{\theta}} \mathcal{L}_{\text{reg}} = \nabla_{\boldsymbol{\theta}} \mathcal{L}_0 + \lambda \boldsymbol{\theta} \tag{9.3.7}
$$
考虑标准的梯度下降更新规则：

$$
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \eta \nabla_{\boldsymbol{\theta}} \mathcal{L}_{\text{reg}}(\boldsymbol{\theta}_t) \tag{9.3.8}
$$
代入正则化后的梯度表达式并整理后得到：

$$
\boldsymbol{\theta}_{t+1} = (1 - \eta\lambda) \boldsymbol{\theta}_t - \eta \nabla_{\boldsymbol{\theta}} \mathcal{L}_0(\boldsymbol{\theta}_t) \tag{9.3.9}
$$

**定理 9.3.1（权重衰减的收缩效应）** 梯度更新中的因子$(1 - \eta\lambda)$对参数向量施加了均匀的收缩。当$\eta\lambda < 1$时，每次更新都会将参数向量按比例缩小，导致参数范数随时间指数衰减。经过$T$步更新后，$\|\boldsymbol{\theta}_T\| \approx (1 - \eta\lambda)^T \|\boldsymbol{\theta}_0\|$。
这个结果表明，L2正则化在每次梯度更新时都会对参数进行收缩，推动参数向量向原点靠近。从几何角度看，这种收缩效应使得优化过程倾向于找到范数较小的解，而这些解通常具有更好的泛化性质。实际上起作用的是乘积$\eta\lambda$，当学习率$\eta$较大时，较小的正则化系数$\lambda$就能产生显著的权重衰减效应。
在Adam等自适应优化器中，L2正则化的表现与普通梯度下降有所不同。Adam的更新规则涉及梯度的一阶矩估计$\mathbf{m}_t$和二阶矩估计$\mathbf{v}_t$。当应用L2正则化时，正则化项通过$\lambda \theta_j$进入梯度，但由于自适应学习率的存在，其效果被缩放。这意味着在Adam中，参数更新的方向不仅取决于正则化梯度本身，还取决于该参数的历史梯度方差。

### 9.3.3 L1正则化与稀疏梯度
L1正则化与L2正则化有着本质的区别。L1正则化使用参数向量的一阶范数作为惩罚项：
$$
\mathcal{L}_{\text{reg}}(\boldsymbol{\theta}) = \mathcal{L}_0(\boldsymbol{\theta}) + \lambda \|\boldsymbol{\theta}\|_1 = \mathcal{L}_0(\boldsymbol{\theta}) + \lambda \sum_{i=1}^{d} |\theta_i| \tag{9.3.10}
$$
L1正则化的关键数学特性在于其不可微性——范数函数在$\theta_i = 0$处不可导。为了处理这种情况，需要使用次梯度（Subgradient）概念。对于L1范数$|\theta|$，其在$\theta \neq 0$处的次梯度为$\partial |\theta| = \{1\}$（若$\theta > 0$）或$\partial |\theta| = \{-1\}$（若$\theta < 0$）；而在$\theta = 0$处，次梯度是一个区间$\partial |0| = [-1, 1]$。
**定理 9.3.2（L1正则化的梯度表达式）** L1正则化后的损失函数关于参数$\theta_j$的次梯度为：

$$
\partial \mathcal{L}_{\text{reg}} / \partial \theta_j = \partial \mathcal{L}_0 / \partial \theta_j + \lambda \cdot \text{sign}(\theta_j) \tag{9.3.11}
$$

当$\theta_j = 0$时，梯度表达式中的正则化项为$\lambda \cdot \gamma_j$，其中$\gamma_j \in [-1, 1]$是任意次梯度选择。
L1正则化最显著的效果是诱导参数稀疏性，即使得许多参数精确为零。这种稀疏性是L2正则化所不具备的，因为L2正则化只是将参数缩小，但不会使其精确为零。
**定理 9.3.3（L1诱导稀疏性的数学证明）** 考虑单参数优化问题。设原始损失$\mathcal{L}_0(\theta)$在$\theta = 0$附近可微，且$\mathcal{L}'_0(0) = a$。应用L1正则化后，当$|a| < \lambda$时，最优解为$\theta^* = 0$。这意味着L1正则化相当于对每个参数应用了一个硬阈值滤波器，只有当梯度超过$\lambda$时，参数才能保持非零值。
从信号处理的角度看，L1正则化实现了一种"软阈值"（Soft Thresholding）操作。对于线性回归问题$\min_\theta \frac{1}{2}(y - \theta)^2 + \lambda |\theta|$，最优解为：

$$
\theta^* = \begin{cases} y - \lambda & \text{if } y > \lambda \\ 0 & \text{if } |y| \leq \lambda \\ y + \lambda & \text{if } y < -\lambda \end{cases} \tag{9.3.12}
$$
这个闭式解展示了L1正则化如何将接近零的参数推向精确为零，只有当信号强度超过阈值$\lambda$时，参数才被保留。
**表 9.3.1 L1与L2正则化的数学特性对比**

| 特性 | L1 正则化 | L2 正则化 |
| --- | --- | --- |
| 惩罚项 | $\lambda \sum_i \lvert \theta_i \rvert$ | $\frac{\lambda}{2}\sum_i \theta_i^2$ |
| 梯度形式 | 次梯度 $\lambda \cdot \mathrm{sign}(\theta_i)$ | $\lambda \theta_i$ |
| 参数解 | 稀疏（许多为零） | 稠密（接近零但不为零） |
| 几何形状 | 菱形等高线 | 圆形等高线 |

### 9.3.4 正则化与损失曲面的几何变化
正则化通过在损失函数中添加惩罚项来改变损失曲面的几何形状。考虑一个简单的二维优化问题，原始损失函数$\mathcal{L}_0(\theta_1, \theta_2)$的等高线是椭圆（假设为凸二次函数）。添加L2正则化后：

$$
\mathcal{L}_{\text{reg}}(\theta_1, \theta_2) = \mathcal{L}_0(\theta_1, \theta_2) + \frac{\lambda}{2}(\theta_1^2 + \theta_2^2) \tag{9.3.13}
$$

正则化项的等高线是同心圆，因此总损失函数的等高线是原始椭圆等高线与圆形的"混合"。在原点附近，等高线趋近于圆形；在远离原点的区域，原始椭圆等高线占主导。添加L2正则化后，损失曲面的等高线从原始的椭圆变为"圆角化"的椭圆。正则化系数$\lambda$越大，等高线越接近圆形；$\lambda = 0$时，等高线恢复为原始椭圆。这种几何变换对优化过程有重要影响：当等高线接近圆形时，梯度的方向更加一致，优化路径更加平滑。正则化不仅改变等高线的形状，还会改变极小值的位置。考虑线性回归问题$\mathcal{L}_0(\boldsymbol{\theta}) = \frac{1}{2} \|\mathbf{X}\boldsymbol{\theta} - \mathbf{y}\|^2$，添加L2正则化后：

$$
\mathcal{L}_{\text{reg}}(\boldsymbol{\theta}) = \frac{1}{2} \|\mathbf{X}\boldsymbol{\theta} - \mathbf{y}\|^2 + \frac{\lambda}{2} \|\boldsymbol{\theta}\|^2 \tag{9.3.14}
$$

**定理 9.3.4** L2正则化线性回归（岭回归）的解析解为：

$$
\boldsymbol{\theta}_{\text{ridge}} = (\mathbf{X}^{\top}\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^{\top}\mathbf{y} \tag{9.3.15}
$$

岭回归解与普通最小二乘解的关系为$\boldsymbol{\theta}_{\text{ridge}} = (\mathbf{I} + \lambda (\mathbf{X}^{\top}\mathbf{X})^{-1})^{-1} \boldsymbol{\theta}_{\text{OLS}}$。由于$(\mathbf{I} + \lambda \mathbf{A})^{-1}$的特征值小于1，岭回归解在所有方向上都比OLS解更接近原点。特征值较大的方向（主成分方向）收缩较少，特征值较小的方向收缩较多。
正则化还改变损失曲面的曲率分布。曲率由Hessian矩阵描述：

$$
\mathbf{H} = \nabla^2_{\boldsymbol{\theta}} \mathcal{L}_{\text{reg}} = \mathbf{H}_0 + \lambda \mathbf{I} \tag{9.3.16}
$$

其中$\mathbf{H}_0 = \nabla^2_{\boldsymbol{\theta}} \mathcal{L}_0$是原始损失的Hessian，$\lambda \mathbf{I}$是正则化项的贡献。
**定理 9.3.5** 添加L2正则化后，Hessian矩阵的所有特征值都增加了$\lambda$。原始Hessian的条件数为$\kappa(\mathbf{H}_0) = \mu_1 / \mu_d$，正则化后条件数为$\kappa(\mathbf{H}) = \frac{\mu_1 + \lambda}{\mu_d + \lambda} \leq \kappa(\mathbf{H}_0)$。当$\lambda$较大时，$\kappa(\mathbf{H}) \approx 1$，Hessian接近单位矩阵的倍数，损失曲面接近球形。条件数的改善直接转化为优化速度的提升。

### 9.3.5 隐式正则化与谱正则化
正则化的作用远不止显式的惩罚项，许多训练技术和模型架构会引入隐式的正则化效应。Dropout通过在训练过程中随机丢弃神经元来防止过拟合，数学上可以证明Dropout与L2正则化存在等价性。考虑一个简单的线性模型$y = \mathbf{w}^{\top}\mathbf{x}$，应用Dropout后的期望损失为：

$$
\mathcal{L}_{\text{Dropout}} = \mathbb{E}_{\mathbf{m}}\left[(y - \hat{y})^2\right] = \mathcal{L}_0 + \lambda_{\text{eff}} \|\mathbf{w}\|^2 \tag{9.3.17}
$$

其中$\lambda_{\text{eff}}$是与Dropout保留概率$p$相关的等效正则化系数。这种等价性意味着Dropout在优化过程中引入了隐式的正则化效应。
BatchNorm虽然在概念上不属于"正则化"技术，但它对损失曲面的几何结构有显著影响。BatchNorm的数学效应可以分解为两步：首先，通过减去均值和除以标准差，将输入分布标准化为单位方差；然后，通过仿射变换恢复表达能力。BatchNorm对Hessian矩阵有两个主要影响：一是收缩效应，通过将激活值标准化到单位方差限制了激活值的尺度；二是重缩放效应，参数$\gamma$控制了输出激活值的尺度。
谱正则化是一类直接作用于权重矩阵奇异值或特征值的正则化技术。设权重矩阵为$\mathbf{W} \in \mathbb{R}^{m \times n}$，其奇异值分解为$\mathbf{W} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\top}$，其中$\boldsymbol{\Sigma} = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_{\min(m,n)})$包含所有奇异值。矩阵$\mathbf{W}$的谱范数定义为最大奇异值：

$$
\|\mathbf{W}\|_2 = \sigma_{\max}(\mathbf{W}) = \sqrt{\lambda_{\max}(\mathbf{W}^{\top}\mathbf{W})} \tag{9.3.18}
$$

**定理 9.3.6** 谱范数与神经网络函数的Lipschitz常数密切相关。如果激活函数$\sigma$是$\gamma$-Lipschitz的，则网络$f(\mathbf{x}) = \mathbf{W}\sigma(\mathbf{x})$的Lipschitz常数为$\text{Lip}(f) \leq \|\mathbf{W}\|_2 \cdot \gamma$。这表明通过限制$\|\mathbf{W}\|_2$，我们可以控制整个网络的Lipschitz常数，从而控制输入扰动对输出的影响。
核范数是矩阵奇异值之和$\|\mathbf{W}\|_* = \sum_{k=1}^{r} \sigma_k$，核范数正则化是L1范数在矩阵空间的推广，它倾向于产生低秩解。这种特性在推荐系统和矩阵补全任务中尤为重要。
**表 9.3.2 矩阵范数正则化的对比**

| 正则化类型     | 范数定义              | 惩罚对象     | 效果      |
| ---------- | ----------------- | -------- | ------- |
| L2/Frobenius | $\sum \theta_k^2$   | 所有参数均匀   | 权重收缩    |
| 谱范数        | $\sigma_{\max}$     | 最大奇异值    | 谱半径控制   |
| 核范数        | $\sum \sigma_k$     | 奇异值之和    | 低秩诱导    |

### 9.3.6 本节小结

本节系统地介绍了正则化与归一化技术的数学原理及其对优化过程的影响。
归一化技术通过重参数化改变损失函数的几何景观，其通用表达式为$\mathbf{y} = \frac{\mathbf{x} - \boldsymbol{\mu}}{\boldsymbol{\sigma}} \odot \boldsymbol{\gamma} + \boldsymbol{\beta}$。批归一化在批量维度上计算统计量，适用于CNN但对批量大小敏感；层归一化在特征维度上计算统计量，适用于RNN和Transformer；均方根归一化移除去均值化步骤，计算效率更高且保持良好性能。三种方法的本质区别在于统计量的计算轴不同。L2正则化通过添加$\frac{\lambda}{2}\|\boldsymbol{\theta}\|^2$实现权重衰减，其梯度为$\lambda \boldsymbol{\theta}$。在梯度下降中，参数更新包含$(1 - \eta\lambda)$的收缩因子，推动参数向原点靠近。在Adam等自适应优化器中，正则化效应被历史梯度方差缩放，表现出不同的行为特征。L1正则化通过次梯度计算实现稀疏性，其软阈值操作$\text{sign}(y)\max(|y| - \lambda, 0)$将接近零的参数推向精确为零。当$|\partial \mathcal{L}_0 / \partial \theta_j| < \lambda$时，L1正则化会完全"压制"该参数，使其为零。这种稀疏性是L2正则化所不具备的。L2正则化通过"圆角化"椭圆等高线改善条件数，岭回归的闭式解揭示了极小值向原点的选择性收缩。Hessian特征值的提升直接改善了条件数，加速了优化收敛。核范数正则化则通过奇异值之和的惩罚诱导低秩结构。Dropout与L2正则化存在数学等价性，BatchNorm通过标准化激活值改善损失曲面的几何结构，谱正则化通过限制权重矩阵的谱范数控制网络的Lipschitz常数。这些隐式正则化机制与显式正则化技术协同作用，共同提升模型的收敛速度和泛化性能。