{"documentCount":1,"nextId":2,"documentIds":{"1":"1.1-线性代数与张量运算.html"},"fieldIds":{"title":0,"aliases":1,"headers":2,"tags":3,"path":4,"content":5},"fieldLength":{"1":[2,1,26,1,3,871]},"averageFieldLength":[2,1,26,1,3,871],"storedFields":{"1":{"title":"1.1 线性代数与张量运算","aliases":[],"headers":["1.1.1 线性代数的核心地位","1.1.2 基本运算与运算规则","1.1.3 张量的表示与运算","1.1.4 特征空间的几何直觉","1.1.5 特征值与特征向量","1.1.6 奇异值与低秩近似","1.1.7 正交性与正交矩阵","1.1.8 矩阵范数与度量","1.1.9 矩阵分解的深化理解","1.1.10 Kronecker积与向量化","1.1.11 张量网络与高维运算","1.1.12 批处理与并行计算的张量视图","1.1.13 计算复杂度与内存效率"],"tags":[],"path":"1.1-线性代数与张量运算.html"}},"dirtCount":1,"index":[["需要",{"5":{"1":1}}],["隐藏维度",{"5":{"1":1}}],["隐藏维度通常在数千量级",{"5":{"1":1}}],["次标量乘法和加法",{"5":{"1":1}}],["单个样本的注意力计算涉及形状为",{"5":{"1":1}}],["单个样本的注意力计算涉及形状为的查询",{"5":{"1":1}}],["降低到",{"5":{"1":1}}],["到",{"5":{"1":1}}],["成立",{"5":{"1":1}}],["对所有向量",{"5":{"1":1}}],["对于序列长度",{"5":{"1":1}}],["对于序列长度和注意力维度",{"5":{"1":1}}],["对于设计和实现高效的大规模训练系统至关重要",{"5":{"1":2}}],["对于深层网络和长序列",{"5":{"1":2}}],["对于大语言模型中的全连接层",{"5":{"1":2}}],["对于大型矩阵",{"5":{"1":2}}],["对于一个批次",{"5":{"1":2}}],["对于矩阵",{"5":{"1":1}}],["对于矩阵方程",{"5":{"1":2}}],["对于矩阵和",{"5":{"1":1}}],["对于对称矩阵",{"5":{"1":2}}],["对于方阵",{"5":{"1":2}}],["对于任何整数",{"5":{"1":2}}],["对于任意矩阵",{"5":{"1":2}}],["对于实对称矩阵",{"5":{"1":2}}],["个秩一张量的和",{"5":{"1":1}}],["个参数",{"5":{"1":1}}],["个参数而不是",{"5":{"1":1}}],["个索引",{"5":{"1":1}}],["时",{"5":{"1":1}}],["减少到",{"5":{"1":1}}],["测量两个向量方向的相似程度",{"5":{"1":1}}],["测量两个向量之间的直线距离",{"5":{"1":1}}],["欧几里得距离",{"5":{"1":1}}],["欧几里得距离测量两个向量之间的直线距离",{"5":{"1":1}}],["视为被重复",{"5":{"1":1}}],["​",{"5":{"1":3}}],["​​",{"5":{"1":2}}],["阶张量有",{"5":{"1":1}}],["通常在数千量级",{"5":{"1":1}}],["通常不等于",{"5":{"1":1}}],["通过仔细规划张量的内存布局和运算顺序",{"5":{"1":2}}],["通过将大型权重张量分解为张量网络的形式",{"5":{"1":2}}],["通过将大型权重张量分解为若干低秩张量",{"5":{"1":2}}],["通过向量化操作可以将其转化为线性方程",{"5":{"1":2}}],["通过惩罚大权重来防止过拟合",{"5":{"1":2}}],["通过gram",{"5":{"1":2}}],["通过分析权重矩阵的奇异值分布",{"5":{"1":2}}],["通过保留最大的k个特征值对应的特征向量",{"5":{"1":2}}],["通过逆运算解线性方程组",{"5":{"1":2}}],["通过张量运算",{"5":{"1":2}}],["维向量",{"5":{"1":1}}],["维度的选择是一个重要的超参数",{"5":{"1":2}}],["流水线并行",{"5":{"1":2}}],["模型并行化将大模型分布在多个计算设备上",{"5":{"1":2}}],["模型能够高效地并行处理大量数据",{"5":{"1":2}}],["并将计算速度提高近一倍",{"5":{"1":2}}],["并有效缓解梯度消失问题",{"5":{"1":2}}],["进行大部分计算",{"5":{"1":2}}],["混合精度利用了现代gpu对低精度运算的专门优化",{"5":{"1":2}}],["混合精度训练可以将内存消耗减半",{"5":{"1":2}}],["混合精度训练是另一种重要的效率优化技术",{"5":{"1":2}}],["混合积性质",{"5":{"1":2}}],["代价是增加额外的计算量",{"5":{"1":2}}],["激活值的内存消耗可能超过参数本身",{"5":{"1":2}}],["还与激活值的存储需求有关",{"5":{"1":2}}],["内存效率不仅与参数数量有关",{"5":{"1":2}}],["稀疏注意力和分层注意力",{"5":{"1":2}}],["输出",{"5":{"1":2}}],["输入张量的形状通常是",{"5":{"1":2}}],["又不受单批次大小的限制",{"5":{"1":2}}],["又称为张量点积或广义矩阵乘法",{"5":{"1":2}}],["有效批大小",{"5":{"1":2}}],["有效自由度",{"5":{"1":2}}],["既保证了一次更新使用的样本数量",{"5":{"1":2}}],["然后使用累加的梯度进行一次参数更新",{"5":{"1":2}}],["梯度检查点",{"5":{"1":2}}],["梯度累积在数学上等价于使用更大的批次进行训练",{"5":{"1":2}}],["梯度累积是另一种与批处理相关的技术",{"5":{"1":2}}],["梯度裁剪就是将梯度向量的模长限制在某个阈值之内",{"5":{"1":2}}],["用秩不超过k的矩阵对",{"5":{"1":1}}],["用秩不超过k的矩阵对的最佳近似",{"5":{"1":1}}],["用于处理超大序列或有限gpu内存的情况",{"5":{"1":2}}],["微批处理",{"5":{"1":2}}],["现代gpu的并行架构非常适合这种批处理计算",{"5":{"1":2}}],["现代gpu架构专门优化了张量运算",{"5":{"1":2}}],["×",{"5":{"1":2}}],["序列长度",{"5":{"1":2}}],["序列长度和嵌入维度",{"5":{"1":2}}],["设单个样本的特征表示是形状为",{"5":{"1":1}}],["设单个样本的特征表示是形状为的二维张量",{"5":{"1":1}}],["设有两个",{"5":{"1":1}}],["设有两个维向量",{"5":{"1":1}}],["设有一个三阶张量",{"5":{"1":2}}],["批处理相当于将多个独立的矩阵运算堆叠成一个大的张量运算",{"5":{"1":2}}],["批处理的数学优势来自于矩阵运算的并行性",{"5":{"1":2}}],["批处理是在现有维度之外增加一个新的批处理维度",{"5":{"1":2}}],["批处理",{"5":{"1":2}}],["批处理与并行计算的张量视图",{"2":{"1":1},"5":{"1":1}}],["理解这些并行策略的数学基础",{"5":{"1":2}}],["理解不同运算的资源需求对于优化模型设计和硬件利用至关重要",{"5":{"1":2}}],["理解张量运算的数学本质有助于编写高效的代码和诊断性能瓶颈",{"5":{"1":2}}],["理解它们的数学性质和计算特性对于优化模型性能和调试模型行为至关重要",{"5":{"1":2}}],["卷积运算和批处理操作",{"5":{"1":2}}],["更复杂的网络结构如树状张量网络",{"5":{"1":2}}],["参数复杂度从",{"5":{"1":1}}],["参数复杂度从降低到",{"5":{"1":1}}],["参数高效微调和特定网络结构的设计",{"5":{"1":2}}],["参数高效微调以及模型蒸馏等场景中具有重要应用",{"5":{"1":2}}],["被分解为多个低阶张量的组合",{"5":{"1":2}}],["被誉为",{"5":{"1":2}}],["值的交互",{"5":{"1":2}}],["值投影都是通过矩阵乘法实现的",{"5":{"1":2}}],["查询",{"5":{"1":2}}],["查询矩阵与键矩阵的乘积产生注意力分数",{"5":{"1":2}}],["组成",{"5":{"1":2}}],["按列优先的顺序排列矩阵元素",{"5":{"1":2}}],["记为",{"5":{"1":1}}],["记为或",{"5":{"1":1}}],["记作",{"5":{"1":1}}],["记作或",{"5":{"1":1}}],["vectorization",{"5":{"1":2}}],["value",{"5":{"1":2}}],["kronecker积可以帮助我们将大矩阵运算分解为多个小矩阵运算的组合",{"5":{"1":2}}],["kronecker积的主要应用包括权重共享",{"5":{"1":2}}],["kronecker积具有许多有用的代数性质",{"5":{"1":2}}],["kronecker积是两个矩阵之间的特殊二元运算",{"5":{"1":2}}],["kronecker积与向量化",{"2":{"1":1},"5":{"1":1}}],["计算量约为lu分解的一半",{"5":{"1":2}}],["计算复杂度与内存效率",{"2":{"1":1},"5":{"1":1}}],["主成分分析和谱聚类中都有应用",{"5":{"1":2}}],["且特征向量矩阵是正交的",{"5":{"1":2}}],["才能进行这种分解",{"5":{"1":2}}],["包括矩阵乘法",{"5":{"1":2}}],["包括结合律",{"5":{"1":2}}],["包括gram",{"5":{"1":2}}],["包含所有可以通过原点的k维平面上的点",{"5":{"1":2}}],["带置换矩阵",{"5":{"1":2}}],["每一层的输入和输出都需要被保存用于反向传播",{"5":{"1":2}}],["每种分解都有其特定的应用场景和数值性质",{"5":{"1":2}}],["每个微批次独立计算",{"5":{"1":2}}],["每个边代表张量的一个维度",{"5":{"1":2}}],["每个节点是一个张量",{"5":{"1":2}}],["每个秩一矩阵对应一个主成分方向",{"5":{"1":2}}],["每个索引对应张量的一个维度",{"5":{"1":2}}],["每个词被映射为一个高维实数向量",{"5":{"1":2}}],["qr分解将矩阵分解为正交矩阵",{"5":{"1":1}}],["qr分解将矩阵分解为正交矩阵和上三角矩阵的乘积",{"5":{"1":1}}],["qr分解在线性最小二乘问题中有重要应用",{"5":{"1":2}}],["qr分解有多种计算方法",{"5":{"1":2}}],["qr分解",{"5":{"1":2}}],["除了前面介绍的奇异值分解",{"5":{"1":2}}],["micro",{"5":{"1":2}}],["mirsky定理",{"5":{"1":2}}],["mobilenet中的深度可分离卷积可以用kronecker积来表示",{"5":{"1":2}}],["matrix",{"5":{"1":2}}],["核范数正则化可以鼓励解的低秩性质",{"5":{"1":2}}],["核范数是frobenius范数和谱范数之间的折中",{"5":{"1":2}}],["核范数",{"5":{"1":2}}],["核范数和1",{"5":{"1":2}}],["weight",{"5":{"1":2}}],["wise",{"5":{"1":2}}],["定义为所有元素平方和的平方根",{"5":{"1":2}}],["∞",{"5":{"1":2}}],["范数​",{"5":{"1":1}}],["范数分别是列范数和行范数",{"5":{"1":2}}],["范数和∞",{"5":{"1":2}}],["范数",{"5":{"1":5}}],["常用于矩阵补全",{"5":{"1":2}}],["常用的矩阵范数包括frobenius范数",{"5":{"1":2}}],["常见的张量分解方法包括",{"5":{"1":2}}],["齐次性和三角不等式等性质",{"5":{"1":2}}],["满足",{"5":{"1":2}}],["满足非负性",{"5":{"1":2}}],["满足和",{"5":{"1":1}}],["大小",{"5":{"1":2}}],["大语言模型中的输入通常是一个三维张量",{"5":{"1":2}}],["大语言模型中的所有参数最终都以矩阵或张量的形式存储和计算",{"5":{"1":2}}],["虽然自然语言处理很少直接使用傅里叶变换",{"5":{"1":2}}],["傅里叶变换和正交基扩展是信号处理和自然语言处理中的重要工具",{"5":{"1":2}}],["第四",{"5":{"1":2}}],["第三",{"5":{"1":8}}],["鼓励权重矩阵的列向量相互正交",{"5":{"1":2}}],["与随机高斯初始化或xavier初始化不同",{"5":{"1":2}}],["与向量的大小无关",{"5":{"1":2}}],["构成标准正交基",{"5":{"1":2}}],["或",{"5":{"1":2}}],["或更低精度",{"5":{"1":2}}],["或行向量",{"5":{"1":2}}],["或其中一个维度为",{"5":{"1":2}}],["具体的",{"5":{"1":2}}],["具体来说",{"5":{"1":6}}],["具体做法是如果梯度的范数超过阈值",{"5":{"1":2}}],["具有完整特征向量基",{"5":{"1":2}}],["具有旋转不变性",{"5":{"1":2}}],["具有重要的几何意义和计算优势",{"5":{"1":2}}],["说明模型的表达能力主要集中在少数方向上",{"5":{"1":2}}],["假设一个",{"5":{"1":1}}],["假设一个的权重矩阵被近似为​",{"5":{"1":1}}],["假设预训练模型的权重更新可以表示为低秩矩阵",{"5":{"1":2}}],["adaptation",{"5":{"1":2}}],["alt=",{"5":{"1":2}}],["典型的低秩适配方法如lora",{"5":{"1":2}}],["低秩更新是一种核心技术",{"5":{"1":2}}],["低秩近似技术有着广泛的应用",{"5":{"1":2}}],["低秩近似是奇异值分解最重要的应用之一",{"5":{"1":2}}],["fp32",{"5":{"1":2}}],["fp16",{"5":{"1":2}}],["frobenius范数因其可导性好而最常用",{"5":{"1":2}}],["frobenius范数常用于权重衰减",{"5":{"1":2}}],["frobenius范数是最直观的矩阵范数",{"5":{"1":2}}],["fine",{"5":{"1":2}}],["flow",{"5":{"1":4}}],["efficient",{"5":{"1":2}}],["当",{"5":{"1":1}}],["当批大小受限于gpu内存时",{"5":{"1":2}}],["当单个样本太大以至于无法在gpu内存中容纳时",{"5":{"1":2}}],["当我们计算批次中所有样本的前向传播时",{"5":{"1":2}}],["当维度匹配时",{"5":{"1":2}}],["当时",{"5":{"1":1}}],["当两个形状不同的张量进行逐元素运算时",{"5":{"1":2}}],["即对所有向量成立",{"5":{"1":1}}],["即",{"5":{"1":10}}],["即通常不等于",{"5":{"1":1}}],["young",{"5":{"1":2}}],["根据eckart",{"5":{"1":2}}],["无论是否满秩",{"5":{"1":2}}],["无论是方阵还是矩形阵",{"5":{"1":2}}],["正则化",{"5":{"1":2}}],["正交变换仍然是有价值的分析工具",{"5":{"1":2}}],["正交约束在优化中被用于防止权重矩阵的秩退化",{"5":{"1":2}}],["正交初始化将权重矩阵初始化为正交矩阵",{"5":{"1":2}}],["正交初始化是一种常用的权重初始化方法",{"5":{"1":2}}],["正交矩阵是满足",{"5":{"1":1}}],["正交矩阵是满足的方阵",{"5":{"1":1}}],["正交矩阵的行列式的绝对值为1",{"5":{"1":2}}],["正交矩阵的逆矩阵等于其转置矩阵",{"5":{"1":2}}],["正交矩阵保持向量的范数不变",{"5":{"1":2}}],["正交矩阵有几个重要的性质",{"5":{"1":2}}],["正交性有着多方面的应用",{"5":{"1":2}}],["正交性是线性代数中一个核心概念",{"5":{"1":2}}],["正交性与正交矩阵",{"2":{"1":1},"5":{"1":1}}],["正是因为它对任何矩阵都成立",{"5":{"1":2}}],["另外",{"5":{"1":2}}],["称为奇异值",{"5":{"1":2}}],["称为欠拟合",{"5":{"1":2}}],["存在正交矩阵",{"5":{"1":2}}],["奇异值分解可以用于理解模型的表达能力",{"5":{"1":2}}],["奇异值分解之所以强大",{"5":{"1":2}}],["奇异值分解与特征值分解有密切关系",{"5":{"1":2}}],["奇异值分解",{"5":{"1":2}}],["奇异值与低秩近似",{"2":{"1":1},"5":{"1":1}}],["gpu可以同时计算批次中所有样本的注意力分数",{"5":{"1":2}}],["gradient",{"5":{"1":2}}],["gram",{"5":{"1":2}}],["graph",{"5":{"1":6}}],["gan",{"5":{"1":2}}],["谱归一化",{"5":{"1":4}}],["谱范数最重要",{"5":{"1":2}}],["谱范数是核心概念",{"5":{"1":2}}],["谱范数在深度学习中有重要应用",{"5":{"1":2}}],["谱范数",{"5":{"1":6}}],["也是为什么许多研究致力于开发高效的注意力变体",{"5":{"1":2}}],["也是证明神经网络泛化性质的关键工具",{"5":{"1":2}}],["也决定了它在网络中的具体作用方式",{"5":{"1":2}}],["network",{"5":{"1":2}}],["normalization",{"5":{"1":4}}],["norm",{"5":{"1":6}}],["nuclear",{"5":{"1":2}}],["null",{"5":{"1":2}}],["numpy",{"5":{"1":2}}],["使得不同特征方向捕捉不同的信息",{"5":{"1":2}}],["使得",{"5":{"1":3}}],["使得等效的变换矩阵具有接近1的特征值",{"5":{"1":2}}],["使梯度能够绕过复杂的非线性变换直接传播到较浅层",{"5":{"1":2}}],["类似的稳定性分析有助于理解和解决梯度消失或梯度爆炸问题",{"5":{"1":2}}],["rank",{"5":{"1":2}}],["r为矩阵的秩",{"5":{"1":2}}],["rnn",{"5":{"1":2}}],["reshape",{"5":{"1":2}}],["<br>",{"5":{"1":1}}],["<img",{"5":{"1":2}}],["再用非线性方法进行二维或三维嵌入",{"5":{"1":2}}],["先通过pca进行初步降维",{"5":{"1":2}}],["就是典型的逐元素运算",{"5":{"1":1}}],["就是保留前k个最大的奇异值及其对应的奇异向量",{"5":{"1":2}}],["就可以实现对大型语言模型的有效微调",{"5":{"1":2}}],["就依赖于这种思想",{"5":{"1":2}}],["就将梯度向量乘以一个缩放因子使其范数等于阈值",{"5":{"1":2}}],["深度学习框架也提供了高效的函数来计算矩阵的特征值分解和奇异值分解",{"5":{"1":2}}],["深度学习框架会在不显式复制数据的情况下",{"5":{"1":2}}],["int8",{"5":{"1":2}}],["inverse",{"5":{"1":2}}],["iteration",{"5":{"1":4}}],["反幂迭代",{"5":{"1":2}}],["直接求解特征方程通常是不切实际的",{"5":{"1":2}}],["给定一个方阵",{"5":{"1":2}}],["数学上",{"5":{"1":6}}],["某些特定方向",{"5":{"1":2}}],["遗忘",{"5":{"1":2}}],["权重矩阵与",{"5":{"1":1}}],["权重矩阵的低秩近似可以显著减少参数量和计算量",{"5":{"1":2}}],["权重矩阵的列空间决定了该层能够表示的特征空间的范围",{"5":{"1":2}}],["权重矩阵无处不在",{"5":{"1":2}}],["零空间",{"5":{"1":2}}],["零阶张量",{"5":{"1":2}}],["列空间",{"5":{"1":2}}],["子空间是特征空间中的重要结构",{"5":{"1":2}}],["全连接层就是典型的线性变换加上非线性激活函数的组合",{"5":{"1":2}}],["剪切则是一种保持体积但改变形状的变换",{"5":{"1":2}}],["投影将向量映射到低维子空间",{"5":{"1":2}}],["投影和剪切等",{"5":{"1":2}}],["只在关键步骤使用全精度",{"5":{"1":2}}],["只适用于对称正定矩阵",{"5":{"1":2}}],["只需要训练",{"5":{"1":1}}],["只需要训练个参数而不是个参数",{"5":{"1":1}}],["只需进行转置操作",{"5":{"1":2}}],["只改变方向",{"5":{"1":2}}],["只有可对角化矩阵",{"5":{"1":2}}],["只有满秩",{"5":{"1":2}}],["只有方阵才可能存在逆矩阵",{"5":{"1":2}}],["旋转保持向量的模长不变",{"5":{"1":2}}],["缩放的倍数就是特征值",{"5":{"1":2}}],["缩放改变向量的模长",{"5":{"1":2}}],["缩放",{"5":{"1":2}}],["表示",{"5":{"1":2}}],["余弦相似度",{"5":{"1":1}}],["余弦相似度是最常用的相似度度量",{"5":{"1":2}}],["余弦相似度测量两个向量方向的相似程度",{"5":{"1":1}}],["过高的维度则可能导致过拟合和计算效率的下降",{"5":{"1":2}}],["过低的维度可能导致表达能力的损失",{"5":{"1":2}}],["嵌入维度",{"5":{"1":2}}],["嵌入空间的维度通常远小于词表大小",{"5":{"1":2}}],["嵌入空间通常是密集的",{"5":{"1":2}}],["嵌入层将词汇映射为向量",{"5":{"1":2}}],["首先",{"5":{"1":10}}],["语义不同的词则距离较远",{"5":{"1":2}}],["语义相近的词在空间中距离较近",{"5":{"1":2}}],["词嵌入的降维可视化",{"5":{"1":2}}],["词嵌入空间具有一些重要的几何性质",{"5":{"1":2}}],["词嵌入空间就是一个典型的高维特征空间",{"5":{"1":2}}],["词向量就是典型的向量表示",{"5":{"1":2}}],["空间中的每个点代表一个数据样本在该空间中的表示",{"5":{"1":2}}],["可以看作是查询张量和键张量的收缩",{"5":{"1":1}}],["可以用一个矩阵",{"5":{"1":1}}],["可以用于表示高阶交互关系",{"5":{"1":2}}],["可以将批次进一步划分为更小的微批次",{"5":{"1":2}}],["可以一次性完成所有样本的计算",{"5":{"1":2}}],["可以显著减少不必要的内存分配和数据拷贝",{"5":{"1":2}}],["可以显著减少参数量和计算量",{"5":{"1":2}}],["可以在保持模型性能的同时显著减少参数量和计算开销",{"5":{"1":2}}],["可以理解为一维数组中的有序数集",{"5":{"1":2}}],["分解则将张量表示为一个核心张量与多个因子矩阵在各模上的乘积",{"5":{"1":2}}],["分解将一个张量表示为若干个秩一张量的和",{"5":{"1":2}}],["分解",{"5":{"1":4}}],["checkpointing",{"5":{"1":2}}],["cholesky分解的数值稳定性好",{"5":{"1":2}}],["cholesky分解是lu分解的特例",{"5":{"1":2}}],["candecomp",{"5":{"1":2}}],["cp分解可以将一个",{"5":{"1":1}}],["cp分解可以将一个的三阶张量表示为个秩一张量的和",{"5":{"1":1}}],["cp",{"5":{"1":4}}],["completion",{"5":{"1":2}}],["component",{"5":{"1":2}}],["column",{"5":{"1":2}}],["contraction",{"5":{"1":2}}],["不当使用容易引入隐蔽且难以察觉的",{"5":{"1":2}}],["加法",{"5":{"1":2}}],["它对应于从",{"5":{"1":1}}],["它对应于从到的线性变换中最大的拉伸因子",{"5":{"1":1}}],["它使用半精度浮点数",{"5":{"1":2}}],["它通过在前向传播时只保存部分层的激活值",{"5":{"1":2}}],["它通过将权重矩阵除以其谱范数来约束网络函数的lipschitz常数",{"5":{"1":2}}],["它允许同时处理多个样本以提高计算效率",{"5":{"1":2}}],["它们的kronecker积定义为",{"5":{"1":2}}],["它们揭示了线性变换的本质特征",{"5":{"1":2}}],["它可以看作是矩阵展开为向量后的l2范数",{"5":{"1":2}}],["它可以显著简化代码",{"5":{"1":2}}],["它只旋转或反射空间而不改变向量的长度",{"5":{"1":2}}],["它用于权重归一化以稳定训练过程",{"5":{"1":2}}],["它描述了输入中的冗余信息",{"5":{"1":2}}],["它描述了矩阵所能表示的所有输出",{"5":{"1":2}}],["它将一个",{"5":{"1":1}}],["它将一个的矩阵与一个的矩阵组合成一个的大矩阵",{"5":{"1":1}}],["它将矩阵分解为下三角矩阵与其转置的乘积",{"5":{"1":2}}],["它将权重矩阵的谱范数归一化为1",{"5":{"1":2}}],["它将标量",{"5":{"1":2}}],["它将某一子层的输入与该子层的输出直接相加",{"5":{"1":2}}],["pipeline",{"5":{"1":2}}],["processing",{"5":{"1":2}}],["partial",{"5":{"1":2}}],["parallelism",{"5":{"1":6}}],["parameter",{"5":{"1":2}}],["parafac",{"5":{"1":2}}],["peft",{"5":{"1":2}}],["pca",{"5":{"1":6}}],["power",{"5":{"1":2}}],["pytorch",{"5":{"1":2}}],["png",{"5":{"1":8}}],["若两个张量的维度数不同",{"5":{"1":2}}],["否则形状不兼容",{"5":{"1":2}}],["则",{"5":{"1":1}}],["则参数量从",{"5":{"1":1}}],["则参数量从减少到",{"5":{"1":1}}],["则复制整个模型在多个数据批次上并行训练",{"5":{"1":2}}],["则一个批次的表示就是形状为",{"5":{"1":2}}],["则是一个的列向量",{"5":{"1":1}}],["则plu分解总是存在",{"5":{"1":2}}],["则lu分解存在",{"5":{"1":2}}],["则称",{"5":{"1":1}}],["则称为标准正交基",{"5":{"1":2}}],["则称它们是正交的",{"5":{"1":2}}],["则称是的特征向量",{"5":{"1":1}}],["则模型利用了更多的参数自由度",{"5":{"1":2}}],["则系统是不稳定的",{"5":{"1":2}}],["则系统是稳定的",{"5":{"1":2}}],["则会在较小张量的左侧自动补",{"5":{"1":2}}],["则该维度可以广播",{"5":{"1":2}}],["则其元素可以表示为",{"5":{"1":1}}],["则其元素可以表示为​",{"5":{"1":1}}],["则其和为",{"5":{"1":1}}],["则其和为向量加法满足交换律和结合律",{"5":{"1":1}}],["以保持数值稳定性",{"5":{"1":2}}],["以匹配较大的张量",{"5":{"1":2}}],["以transformer模型为例",{"5":{"1":2}}],["将形状较小的张量在某些维度上",{"5":{"1":1}}],["将形状较小的张量在某些维度上视为被重复",{"5":{"1":1}}],["将它们累加起来",{"5":{"1":2}}],["将离散的符号表示转化为连续的数值表示",{"5":{"1":2}}],["逐元素运算对张量中的每个独立元素应用相同的函数",{"5":{"1":2}}],["等于矩阵",{"5":{"1":1}}],["等于矩阵的最大特征值的平方根",{"5":{"1":1}}],["等现代深度学习框架均支持广播机制",{"5":{"1":2}}],["等",{"5":{"1":2}}],["bug",{"5":{"1":2}}],["bias",{"5":{"1":2}}],["broadcasting",{"5":{"1":2}}],["batch",{"5":{"1":6}}],["广播规则通常是从最后一个维度开始逐维比较",{"5":{"1":2}}],["广播是张量逐元素运算中的一种自动对齐机制",{"5":{"1":2}}],["广播",{"5":{"1":2}}],["重塑操作改变张量的形状但不改变其包含的数据元素",{"5":{"1":2}}],["重塑",{"5":{"1":2}}],[">",{"5":{"1":2}}],["tensor",{"5":{"1":4}}],["tree",{"5":{"1":2}}],["transpose",{"5":{"1":2}}],["transformer中的注意力机制可以被看作一个三阶张量",{"5":{"1":2}}],["transformer",{"5":{"1":2}}],["tuning",{"5":{"1":2}}],["tucker",{"5":{"1":6}}],["target=",{"5":{"1":2}}],["data",{"5":{"1":2}}],["decay",{"5":{"1":2}}],["decomposition",{"5":{"1":4}}],["dynamical",{"5":{"1":2}}],["drawio",{"5":{"1":8}}],["dimension",{"5":{"1":2}}],["lu分解将矩阵分解为下三角矩阵",{"5":{"1":1}}],["lu分解将矩阵分解为下三角矩阵和上三角矩阵的乘积",{"5":{"1":1}}],["lu分解的主要用途是高效求解线性方程组",{"5":{"1":2}}],["low",{"5":{"1":2}}],["lstm和gru通过门控机制设计",{"5":{"1":2}}],["llm",{"5":{"1":4}}],["length",{"5":{"1":2}}],["便于进行残差连接和层归一化操作",{"5":{"1":2}}],["经过注意力计算后的输出张量会与输入张量形状相同",{"5":{"1":2}}],["经过线性投影并拆分注意力头后",{"5":{"1":2}}],["hierarchical",{"5":{"1":2}}],["hidden",{"5":{"1":2}}],["householder变换和givens旋转",{"5":{"1":2}}],["html",{"4":{"0":1,"1":1}}],["sum",{"5":{"1":2}}],["schmidt正交化",{"5":{"1":2}}],["schmidt正交化可以移除表示中的冗余成分",{"5":{"1":2}}],["schmidt正交化过程是构建正交基的标准算法",{"5":{"1":2}}],["svd",{"5":{"1":2}}],["singular",{"5":{"1":2}}],["size",{"5":{"1":2}}],["spectral",{"5":{"1":8}}],["space",{"5":{"1":4}}],["system",{"5":{"1":2}}],["sne和umap",{"5":{"1":2}}],["self",{"5":{"1":2}}],["sequence",{"5":{"1":2}}],["src=",{"5":{"1":2}}],["二阶张量",{"5":{"1":2}}],["二阶张量是矩阵",{"5":{"1":2}}],["而不是高斯消元的",{"5":{"1":1}}],["而数据并行",{"5":{"1":2}}],["而奇异值分解适用于任意矩阵",{"5":{"1":2}}],["而零空间则包含了可能被",{"5":{"1":2}}],["而是取连续的实数值",{"5":{"1":2}}],["而",{"5":{"1":2}}],["而正规方程的求解等价于计算矩阵的伪逆",{"5":{"1":2}}],["而且并非所有方阵都有逆矩阵",{"5":{"1":2}}],["线性变换对特征空间的作用包括旋转",{"5":{"1":2}}],["线性变换是理解特征空间中数据变换的关键概念",{"5":{"1":2}}],["线性回归的闭式解就是通过求解正规方程得到的",{"5":{"1":2}}],["线性代数的瑞士军刀",{"5":{"1":2}}],["线性代数的基本运算包括向量加法",{"5":{"1":2}}],["线性代数的核心地位",{"2":{"1":1},"5":{"1":1}}],["线性代数为理解和构建深度学习模型提供了坚实的数学框架",{"5":{"1":2}}],["线性代数作为现代数学的重要分支",{"5":{"1":2}}],["线性代数与张量运算",{"0":{"1":1},"4":{"0":1,"1":1},"5":{"1":1}}],["但在深度学习中使用较少",{"5":{"1":2}}],["但在处理序列数据的某些场景下",{"5":{"1":2}}],["但奇异值总是非负的",{"5":{"1":2}}],["但又有所不同",{"5":{"1":2}}],["但又足够高以捕获丰富的语义信息",{"5":{"1":2}}],["但由于广播可能在语法上合法而在语义上错误",{"5":{"1":2}}],["但矩阵求逆的数学思想",{"5":{"1":2}}],["但不满足交换律",{"5":{"1":2}}],["因为前向和后向替换的复杂度是",{"5":{"1":1}}],["因为前向和后向替换的复杂度是而不是高斯消元的",{"5":{"1":1}}],["因为梯度是线性的",{"5":{"1":2}}],["因为长序列可能消耗大量内存",{"5":{"1":2}}],["因为正规方程",{"5":{"1":1}}],["因为正规方程的解等价于求解",{"5":{"1":1}}],["因为正交矩阵的奇异值全部为1",{"5":{"1":2}}],["因为它可以将不同样本的计算分配到不同的处理单元上同时执行",{"5":{"1":2}}],["因为它将复杂的矩阵运算转化为标准的矩阵",{"5":{"1":2}}],["因为它决定了信息放大或衰减的上界",{"5":{"1":2}}],["因为它能够捕捉语义方向的相似性而不受词频等因素的影响",{"5":{"1":2}}],["因为在数值上计算高阶多项式的根非常困难",{"5":{"1":2}}],["因为数值稳定性较差且计算成本高昂",{"5":{"1":2}}],["因此单个层的计算量是相当可观的",{"5":{"1":2}}],["因此现代gpu都针对矩阵运算进行了专门的硬件优化",{"5":{"1":2}}],["因此深入掌握线性代数的基本概念和运算规则是理解模型工作原理的必要前提",{"5":{"1":2}}],["我们得到形状为",{"5":{"1":1}}],["我们得到形状为的张量",{"5":{"1":1}}],["我们先计算若干个小批次的梯度",{"5":{"1":2}}],["我们关心如何用少量的奇异值最好地近似原始矩阵",{"5":{"1":2}}],["我们在优化目标中加入正交性惩罚项",{"5":{"1":2}}],["我们可以通过梯度累积来模拟更大的有效批大小",{"5":{"1":2}}],["我们可以了解模型使用了多少",{"5":{"1":2}}],["我们可以实现降维同时最小化信息损失",{"5":{"1":2}}],["我们使用迭代算法如幂迭代",{"5":{"1":2}}],["我们很少直接计算矩阵的逆",{"5":{"1":2}}],["我们需要首先明确几个基本定义",{"5":{"1":2}}],["的计算量",{"5":{"1":1}}],["的查询",{"5":{"1":1}}],["的二维张量",{"5":{"1":1}}],["的二维张量重塑为",{"5":{"1":1}}],["的三阶张量表示为",{"5":{"1":1}}],["的三维张量运算",{"5":{"1":1}}],["的三维张量",{"5":{"1":3}}],["的列向量",{"5":{"1":1}}],["的列向量称为右奇异向量",{"5":{"1":2}}],["的列向量称为左奇异向量",{"5":{"1":2}}],["的大矩阵",{"5":{"1":1}}],["的解等价于求解",{"5":{"1":1}}],["的线性变换中最大的拉伸因子",{"5":{"1":1}}],["的方阵",{"5":{"1":1}}],["的方阵才是可逆的",{"5":{"1":2}}],["的权重矩阵被近似为",{"5":{"1":1}}],["的最佳近似",{"5":{"1":1}}],["的最大特征值的平方根",{"5":{"1":1}}],["的特征值的平方根",{"5":{"1":1}}],["的特征值决定",{"5":{"1":1}}],["的特征向量",{"5":{"1":1}}],["的稳定性由矩阵",{"5":{"1":1}}],["的稳定性由矩阵的特征值决定",{"5":{"1":1}}],["的多项式方程",{"5":{"1":1}}],["的张量",{"5":{"1":2}}],["的矩阵组合成一个",{"5":{"1":1}}],["的矩阵与一个",{"5":{"1":1}}],["的矩阵",{"5":{"1":3}}],["的乘积",{"5":{"1":3}}],["的乘法可以看作是收缩掉两个张量的一个公共维度",{"5":{"1":2}}],["的标准",{"5":{"1":2}}],["的对角线元素",{"5":{"1":2}}],["的信息",{"5":{"1":2}}],["的概念推广到任意维度",{"5":{"1":2}}],["的运算",{"5":{"1":2}}],["行列式非零",{"5":{"1":2}}],["行数和列数",{"5":{"1":2}}],["此外",{"5":{"1":2}}],["自注意力机制需要计算查询向量与键向量的相似度",{"5":{"1":2}}],["转置运算满足",{"5":{"1":1}}],["转置运算满足和",{"5":{"1":1}}],["转置性质",{"5":{"1":2}}],["转置",{"5":{"1":2}}],["转置操作也经常用于梯度的反向传播计算",{"5":{"1":2}}],["转置和求逆等",{"5":{"1":2}}],["注意力分数的计算",{"5":{"1":2}}],["注意力分数的计算需要的计算量",{"5":{"1":1}}],["注意力分数的计算在批处理情况下变成的三维张量运算",{"5":{"1":1}}],["注意力分数的计算可以看作是一个张量收缩操作",{"5":{"1":2}}],["注意力权重矩阵",{"5":{"1":1}}],["注意力权重矩阵可以看作是查询张量和键张量的收缩",{"5":{"1":1}}],["注意力权重与值向量的乘积同样需要",{"5":{"1":2}}],["注意力权重与值矩阵的乘积产生加权输出",{"5":{"1":2}}],["注意力机制的计算复杂度与序列长度的平方成正比",{"5":{"1":2}}],["注意力机制的计算过程就包含了多个矩阵乘法操作",{"5":{"1":2}}],["注意力机制的批处理实现是理解这一概念的良好例子",{"5":{"1":2}}],["注意力机制中的查询",{"5":{"1":2}}],["其列向量",{"5":{"1":2}}],["其次",{"5":{"1":10}}],["其中每个词被映射为空间中的一个点",{"5":{"1":2}}],["其中每个元素",{"5":{"1":2}}],["其中查询向量与键向量在特征维度上进行点积运算",{"5":{"1":2}}],["其中是序列长度",{"5":{"1":1}}],["其中是批大小",{"5":{"1":1}}],["其中是批量大小",{"5":{"1":1}}],["其中是注意力头的数量",{"5":{"1":1}}],["其中是单位矩阵",{"5":{"1":1}}],["其中",{"5":{"1":15}}],["其维度分别代表批量大小",{"5":{"1":2}}],["两个向量",{"5":{"1":1}}],["两个向量和如果满足",{"5":{"1":1}}],["两个三阶张量",{"5":{"1":1}}],["两个三阶张量和在第二和第三维度上的收缩产生一个的矩阵",{"5":{"1":1}}],["两个矩阵",{"5":{"1":1}}],["两个矩阵和的乘积",{"5":{"1":1}}],["两个二阶张量",{"5":{"1":2}}],["两个维度相同的向量可以逐分量相加",{"5":{"1":2}}],["学习率调整和梯度裁剪中都有应用",{"5":{"1":2}}],["结果是将向量的每个分量都乘以该标量",{"5":{"1":2}}],["相乘",{"5":{"1":2}}],["实数",{"5":{"1":2}}],["实际上是一个巨大的查找矩阵",{"5":{"1":2}}],["显著提升了深层模型的可训练性",{"5":{"1":2}}],["残差连接是向量加法的典型应用",{"5":{"1":2}}],["中",{"5":{"1":6}}],["如relu激活函数",{"5":{"1":1}}],["如relu激活函数就是典型的逐元素运算",{"5":{"1":1}}],["如何将矩阵运算分解为可以在不同设备上独立执行的子运算",{"5":{"1":2}}],["如bf16",{"5":{"1":2}}],["如线性注意力",{"5":{"1":2}}],["如t",{"5":{"1":2}}],["如果满足",{"5":{"1":1}}],["如果存在非零向量",{"5":{"1":1}}],["如果存在非零向量和标量满足",{"5":{"1":1}}],["如果使用张量运算而非循环",{"5":{"1":2}}],["如果",{"5":{"1":2}}],["如果进行行交换",{"5":{"1":2}}],["如果主元都不为零",{"5":{"1":2}}],["如果奇异值分布比较均匀",{"5":{"1":2}}],["如果只有少数几个奇异值很大",{"5":{"1":2}}],["如果有特征值的实部为正",{"5":{"1":2}}],["如果所有特征值的实部都为负",{"5":{"1":2}}],["如果对应维度相等",{"5":{"1":2}}],["如",{"5":{"1":2}}],["如pytorch和tensorflow中",{"5":{"1":2}}],["和注意力维度",{"5":{"1":1}}],["和上三角矩阵",{"5":{"1":2}}],["和标量",{"5":{"1":1}}],["和分层张量分解",{"5":{"1":2}}],["和分配律",{"5":{"1":2}}],["和连接节点的边",{"5":{"1":2}}],["和低秩矩阵恢复问题",{"5":{"1":2}}],["和对角矩阵",{"5":{"1":2}}],["和防止对抗攻击都很重要",{"5":{"1":2}}],["和transformer中",{"5":{"1":2}}],["和qr算法来计算特征值和特征向量",{"5":{"1":2}}],["和张量收缩",{"5":{"1":2}}],["和矩阵",{"5":{"1":2}}],["和",{"5":{"1":12}}],["向量的乘积需要",{"5":{"1":1}}],["向量加法满足交换律和结合律",{"5":{"1":1}}],["向量加法是逐分量",{"5":{"1":2}}],["向量乘积",{"5":{"1":2}}],["向量化操作与kronecker积结合可以简化矩阵方程的表示",{"5":{"1":2}}],["向量化",{"5":{"1":2}}],["向量距离和相似度是度量特征空间中样本关系的基本工具",{"5":{"1":2}}],["向量",{"5":{"1":2}}],["向量是线性空间中最基本的元素",{"5":{"1":2}}],["标量乘法是将一个向量与一个标量",{"5":{"1":2}}],["标量乘法",{"5":{"1":2}}],["这是一个关于",{"5":{"1":1}}],["这是一个关于的多项式方程",{"5":{"1":1}}],["这就是为什么transformer在处理长序列时面临计算挑战",{"5":{"1":2}}],["这有助于理解其参数效率",{"5":{"1":2}}],["这涉及到部分和",{"5":{"1":2}}],["这在理论上可以防止初始化时的梯度消失或爆炸问题",{"5":{"1":2}}],["这在将展平后的向量恢复为嵌入序列时非常有用",{"5":{"1":2}}],["这使得求解正交矩阵的逆矩阵变得极其简单",{"5":{"1":2}}],["这意味着计算一个",{"5":{"1":1}}],["这意味着计算一个权重矩阵与向量的乘积需要次标量乘法和加法",{"5":{"1":1}}],["这意味着正交变换是一种保距变换",{"5":{"1":2}}],["这意味着每个维度都不是二元的或稀疏的",{"5":{"1":2}}],["这样",{"5":{"1":4}}],["这个运算被高度优化",{"5":{"1":2}}],["这个近似将原始矩阵分解为k个秩一矩阵的和",{"5":{"1":2}}],["这个向量的每一个维度都编码了某种语义或语法特征",{"5":{"1":2}}],["这对训练生成对抗网络",{"5":{"1":2}}],["这种技术在训练大语言模型时尤为重要",{"5":{"1":2}}],["这种转化在推导反向传播公式和优化算法时非常有用",{"5":{"1":2}}],["这种约束有助于提高模型的稳定性和泛化能力",{"5":{"1":2}}],["这种分析对于诊断模型过拟合和理解模型容量都很有价值",{"5":{"1":2}}],["这种方法的数学基础正是低秩近似理论",{"5":{"1":2}}],["这种压缩是显著的",{"5":{"1":2}}],["这种密集表示使得模型能够在连续空间中学习平滑的语义关系",{"5":{"1":2}}],["这种几何表示使得我们可以利用空间中的几何关系来理解和操作语义信息",{"5":{"1":2}}],["这种运算在模型的权重初始化",{"5":{"1":2}}],["这本质上就是计算查询矩阵与转置后的键矩阵的乘积",{"5":{"1":2}}],["这些运算的吞吐量通常是全精度运算的数倍",{"5":{"1":2}}],["这些运算在大语言模型中有着直接的应用场景",{"5":{"1":2}}],["这些范数在稀疏优化和线性规划中有重要应用",{"5":{"1":2}}],["这些方向称为主成分",{"5":{"1":2}}],["这些矩阵乘法的计算效率直接影响模型的整体性能",{"5":{"1":2}}],["这也是大语言模型能够实现高效训练和推理的关键技术基础",{"5":{"1":2}}],["几乎所有的计算都在张量之上进行",{"5":{"1":2}}],["三阶及更高阶的张量则可以表示更复杂的数据结构",{"5":{"1":2}}],["一组向量如果两两正交且都是单位向量",{"5":{"1":2}}],["一个",{"5":{"1":1}}],["一个张量网络由若干个节点",{"5":{"1":2}}],["一个线性变换",{"5":{"1":1}}],["一个线性变换可以用一个矩阵",{"5":{"1":1}}],["一个线性",{"5":{"1":2}}],["一个k维子空间是特征空间的一个k维线性子集",{"5":{"1":2}}],["一个n维特征空间可以理解为一个n维欧几里得空间",{"5":{"1":2}}],["一个阶张量有个索引",{"5":{"1":1}}],["一个300维的词向量可以看作是一个从自然语言到连续向量空间的映射",{"5":{"1":2}}],["一阶张量",{"5":{"1":2}}],["一阶张量是向量",{"5":{"1":2}}],["是",{"5":{"1":3}}],["是注意力头的数量",{"5":{"1":1}}],["是批大小",{"5":{"1":1}}],["是批量大小",{"5":{"1":1}}],["是批处理的一个变体",{"5":{"1":2}}],["是单位矩阵",{"5":{"1":1}}],["是训练超大规模语言模型的必要技术",{"5":{"1":2}}],["是深度学习训练和推理的基本范式",{"5":{"1":2}}],["是的矩阵",{"5":{"1":1}}],["是的特征值的平方根",{"5":{"1":1}}],["是将矩阵转换为列向量的操作",{"5":{"1":2}}],["是将任意矩阵分解为三个矩阵乘积的强大工具",{"5":{"1":2}}],["是一个",{"5":{"1":2}}],["是一个的矩阵",{"5":{"1":1}}],["是一种常用的内存优化技术",{"5":{"1":2}}],["是一种常用的权重归一化技术",{"5":{"1":2}}],["是一种重要的权重归一化技术",{"5":{"1":2}}],["是对应的特征值",{"5":{"1":2}}],["是所有被矩阵映射到零向量的输入向量构成的空间",{"5":{"1":2}}],["是矩阵所有奇异值的和",{"5":{"1":2}}],["是矩阵所有列向量的线性组合构成的空间",{"5":{"1":2}}],["是矩阵的最大奇异值",{"5":{"1":4}}],["是矩阵概念在更高维度的延伸",{"5":{"1":2}}],["是隐藏维度",{"5":{"1":2}}],["是序列长度",{"5":{"1":3}}],["是线性代数中最重要的研究对象之一",{"5":{"1":2}}],["决定了它所代表的线性变换的类型",{"5":{"1":2}}],["前馈神经网络中的全连接层更是由多个权重矩阵堆叠而成",{"5":{"1":2}}],["键和值张量",{"5":{"1":2}}],["键",{"5":{"1":4}}],["例如将一个",{"5":{"1":1}}],["例如将一个的二维张量重塑为的三维张量",{"5":{"1":1}}],["例如",{"5":{"1":20}}],["在批处理情况下变成",{"5":{"1":1}}],["在第二和第三维度上的收缩产生一个",{"5":{"1":1}}],["在深度维度上划分层",{"5":{"1":2}}],["在深度学习中有着广泛的应用",{"5":{"1":2}}],["在深度学习中",{"5":{"1":6}}],["在反向传播时重新计算被省略的激活值来节省内存",{"5":{"1":2}}],["在前向传播过程中",{"5":{"1":2}}],["在pytorch中",{"5":{"1":2}}],["在物理学和机器学习中都有重要应用",{"5":{"1":2}}],["在模型的宽度维度上划分参数",{"5":{"1":2}}],["在模型并行化中",{"5":{"1":2}}],["在模型压缩和低秩近似中",{"5":{"1":2}}],["在模型压缩方面",{"5":{"1":2}}],["在高斯过程回归和二次优化中有广泛应用",{"5":{"1":2}}],["在正则化中",{"5":{"1":2}}],["在分析梯度流和稳定性时",{"5":{"1":2}}],["在推荐系统和缺失数据插补中",{"5":{"1":2}}],["在神经网络的稳定性分析和lipschitz约束中",{"5":{"1":2}}],["在神经网络中",{"5":{"1":2}}],["在某些正则化方法中",{"5":{"1":2}}],["在对比学习中",{"5":{"1":2}}],["在特征提取和表示学习中经常被使用",{"5":{"1":2}}],["在参数高效微调",{"5":{"1":2}}],["在frobenius范数和谱范数意义下",{"5":{"1":2}}],["在循环神经网络",{"5":{"1":2}}],["在主成分分析",{"5":{"1":2}}],["在实际实现中",{"5":{"1":2}}],["在实践中",{"5":{"1":2}}],["在实现反向传播时",{"5":{"1":2}}],["在变换下只发生缩放而不改变方向",{"5":{"1":2}}],["在词嵌入研究中",{"5":{"1":2}}],["在注意力机制中",{"5":{"1":2}}],["在多头注意力中",{"5":{"1":2}}],["在优化算法和正则化方法中有着重要的应用",{"5":{"1":2}}],["在现代深度学习框架中",{"5":{"1":2}}],["在大语言模型",{"5":{"1":2}}],["在大语言模型中",{"5":{"1":18}}],["在大语言模型的架构设计中",{"5":{"1":2}}],["在大语言模型的实践中",{"5":{"1":2}}],["在大语言模型的语境下",{"5":{"1":2}}],["在大语言模型的理论基础中占据着不可替代的核心地位",{"5":{"1":2}}],["在线性代数的学习路径中",{"5":{"1":2}}],["从张量的角度来看",{"5":{"1":2}}],["从张量网络的角度来看",{"5":{"1":2}}],["从几何角度来看",{"5":{"1":2}}],["从而实现高效的存储和计算",{"5":{"1":2}}],["从而在多个计算设备上分布式执行",{"5":{"1":2}}],["从而在反向传播时为梯度提供了一条恒等映射路径",{"5":{"1":2}}],["从而约束了网络函数的lipschitz常数",{"5":{"1":2}}],["从而缓解了梯度消失问题",{"5":{"1":2}}],["从而使得机器学习算法能够在这些向量上进行运算和优化",{"5":{"1":2}}],["从矩阵乘法的并行计算到张量运算的维度变换",{"5":{"1":2}}],["从transformer架构的注意力机制到词嵌入的向量表示",{"5":{"1":2}}],["张量可重排为",{"5":{"1":1}}],["张量可重排为的张量",{"5":{"1":1}}],["张量并行",{"5":{"1":2}}],["张量运算的高效实现是现代深度学习框架的核心能力之一",{"5":{"1":2}}],["张量运算的基本操作包括逐元素运算",{"5":{"1":2}}],["张量网络在模型压缩和高效推理中有着重要的应用价值",{"5":{"1":2}}],["张量网络的思维方式有助于理解复杂的计算结构",{"5":{"1":2}}],["张量网络的核心思想是用多个小张量的乘积之和来表示一个巨大的高维张量",{"5":{"1":2}}],["张量网络是用低维张量通过网络结构表示高维张量的方法",{"5":{"1":2}}],["张量网络与高维运算",{"2":{"1":1},"5":{"1":1}}],["张量分解在模型压缩",{"5":{"1":2}}],["张量分解是处理高维张量的重要技术",{"5":{"1":2}}],["张量收缩指定两个张量的若干维度进行配对相乘并求和",{"5":{"1":2}}],["张量收缩是矩阵乘法在高维张量上的推广",{"5":{"1":2}}],["张量的维度通常具有明确的语义含义",{"5":{"1":2}}],["张量的表示与运算",{"2":{"1":1},"5":{"1":1}}],["张量是多维数组在数学上的抽象表示",{"5":{"1":2}}],["张量是多维数组的自然推广",{"5":{"1":2}}],["张量是最基本的数据结构",{"5":{"1":2}}],["矩阵和张量运算的计算复杂度和内存效率是核心关注点",{"5":{"1":2}}],["矩阵",{"5":{"1":2}}],["矩阵分解还包括lu分解",{"5":{"1":2}}],["矩阵分解的深化理解",{"2":{"1":1},"5":{"1":1}}],["矩阵范数的选择取决于具体问题的需求",{"5":{"1":2}}],["矩阵范数将矩阵映射到非负实数",{"5":{"1":2}}],["矩阵范数是衡量矩阵",{"5":{"1":2}}],["矩阵范数与度量",{"2":{"1":1},"5":{"1":1}}],["矩阵的求逆是找到一个矩阵",{"5":{"1":1}}],["矩阵的求逆是找到一个矩阵使得",{"5":{"1":1}}],["矩阵的转置是将矩阵的行和列互换",{"5":{"1":2}}],["矩阵的形状",{"5":{"1":2}}],["矩阵乘法的计算复杂度是",{"5":{"1":2}}],["矩阵乘法是连接两个张量的最基本操作",{"5":{"1":2}}],["矩阵乘法是最重要也是计算量最大的运算",{"5":{"1":2}}],["矩阵乘法满足结合律",{"5":{"1":2}}],["矩阵乘法",{"5":{"1":2}}],["矩阵是二维数组",{"5":{"1":2}}],["9",{"2":{"1":1},"5":{"1":1}}],["8",{"2":{"1":1},"5":{"1":1}}],["7",{"2":{"1":1},"5":{"1":1}}],["6",{"2":{"1":1},"5":{"1":1}}],["特征向量",{"5":{"1":2}}],["特征值是实数",{"5":{"1":2}}],["特征值可以是负数",{"5":{"1":2}}],["特征值分解在动力系统分析",{"5":{"1":2}}],["特征值分解总是可行的",{"5":{"1":2}}],["特征值分解将方阵分解为特征向量矩阵和特征值对角矩阵的乘积",{"5":{"1":2}}],["特征值分解和cholesky分解等多种方法",{"5":{"1":2}}],["特征值分解只适用于方阵且要求矩阵可对角化",{"5":{"1":2}}],["特征值分解用于提取数据中方差最大的方向",{"5":{"1":2}}],["特征值与网络稳定性密切相关",{"5":{"1":2}}],["特征值与特征向量",{"2":{"1":1},"5":{"1":1}}],["特征值和特征向量有着多方面的应用",{"5":{"1":2}}],["特征值和特征向量是线性代数中最深刻也最有应用价值的概念之一",{"5":{"1":2}}],["特征值的计算需要求解特征方程",{"5":{"1":2}}],["特征空间是机器学习和深度学习中描述数据表示的核心概念",{"5":{"1":2}}],["特征空间的几何直觉",{"2":{"1":1},"5":{"1":1}}],["5",{"2":{"1":1},"5":{"1":1}}],["4",{"2":{"1":1},"5":{"1":1}}],["3",{"2":{"1":1},"5":{"1":1}}],["基本运算与运算规则",{"2":{"1":1},"5":{"1":1}}],["2",{"2":{"1":1},"5":{"1":1}}],["13",{"2":{"1":1},"5":{"1":1}}],["12",{"2":{"1":1},"5":{"1":1}}],["11",{"2":{"1":1},"5":{"1":1}}],["10",{"2":{"1":1},"5":{"1":1}}],["1",{"0":{"1":2},"2":{"1":27},"4":{"0":2,"1":2},"5":{"1":35}}]],"serializationVersion":2}