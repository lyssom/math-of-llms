{"createdTime":1767861774437,"shownInTree":["第1章-数学基础/1.1-线性代数与张量运算.html","第1章-数学基础/1.2-概率论与统计.html","第1章-数学基础/1.3-微积分与优化基础.html","第2章-前馈网络数学/2.1-神经元的数学模型.html","第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","第2章-前馈网络数学/2.3-前向传播的数学本质.html","第2章-前馈网络数学/2.4-反向传播梯度推导.html","第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","大模型中的数学.html"],"attachments":["site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/293fd13dbca5a3e450ef.woff2","site-lib/fonts/085cb93e613ba3d40d2b.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/global-variable-styles.css","site-lib/styles/main-styles.css","graph/xorscene_manimce_v0.19.1.png","graph/linearinseparablescene_manimce_v0.19.1.png","graph/backprop.drawio.png","graph/combinedscene_manimce_v0.19.1.png","graph/grad.png","graph/dist_relation.drawio.png","graph/gauss_dist.drawio.png","graph/msg_relation.drawio.png","graph/llm_flow.drawio.png","graph/pca.drawio.png","site-lib/rss.xml","graph/learn_msg.drawio.png","graph/comp_graph.drawio.png","graph/backward.drawio.png"],"allFiles":["大模型中的数学.html","第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","第2章-前馈网络数学/2.4-反向传播梯度推导.html","graph/backward.drawio.png","第2章-前馈网络数学/2.3-前向传播的数学本质.html","第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","第2章-前馈网络数学/2.1-神经元的数学模型.html","第1章-数学基础/1.3-微积分与优化基础.html","第1章-数学基础/1.2-概率论与统计.html","第1章-数学基础/1.1-线性代数与张量运算.html","site-lib/scripts/graph-wasm.wasm","site-lib/fonts/94f2f163d4b698242fef.otf","site-lib/fonts/72505e6a122c6acd5471.woff2","site-lib/fonts/2d5198822ab091ce4305.woff2","site-lib/fonts/c8ba52b05a9ef10f4758.woff2","site-lib/fonts/cb10ffd7684cd9836a05.woff2","site-lib/fonts/293fd13dbca5a3e450ef.woff2","site-lib/fonts/085cb93e613ba3d40d2b.woff2","site-lib/fonts/b5f0f109bc88052d4000.woff2","site-lib/fonts/cbe0ae49c52c920fd563.woff2","site-lib/fonts/535a6cf662596b3bd6a6.woff2","site-lib/fonts/70cc7ff27245e82ad414.ttf","site-lib/fonts/454577c22304619db035.ttf","site-lib/fonts/52ac8f3034507f1d9e53.ttf","site-lib/fonts/05b618077343fbbd92b7.ttf","site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","site-lib/media/6155340132a851f6089e.svg","site-lib/media/2308ab1944a6bfa5c5b8.svg","site-lib/fonts/mathjax_zero.woff","site-lib/fonts/mathjax_main-regular.woff","site-lib/fonts/mathjax_main-bold.woff","site-lib/fonts/mathjax_math-italic.woff","site-lib/fonts/mathjax_main-italic.woff","site-lib/fonts/mathjax_math-bolditalic.woff","site-lib/fonts/mathjax_size1-regular.woff","site-lib/fonts/mathjax_size2-regular.woff","site-lib/fonts/mathjax_size3-regular.woff","site-lib/fonts/mathjax_size4-regular.woff","site-lib/fonts/mathjax_ams-regular.woff","site-lib/fonts/mathjax_calligraphic-regular.woff","site-lib/fonts/mathjax_calligraphic-bold.woff","site-lib/fonts/mathjax_fraktur-regular.woff","site-lib/fonts/mathjax_fraktur-bold.woff","site-lib/fonts/mathjax_sansserif-regular.woff","site-lib/fonts/mathjax_sansserif-bold.woff","site-lib/fonts/mathjax_sansserif-italic.woff","site-lib/fonts/mathjax_script-regular.woff","site-lib/fonts/mathjax_typewriter-regular.woff","site-lib/fonts/mathjax_vector-regular.woff","site-lib/fonts/mathjax_vector-bold.woff","site-lib/html/file-tree-content.html","site-lib/scripts/webpage.js","site-lib/scripts/graph-wasm.js","site-lib/scripts/graph-render-worker.js","site-lib/media/favicon.png","site-lib/styles/obsidian.css","site-lib/styles/global-variable-styles.css","site-lib/styles/main-styles.css"],"webpages":{"第1章-数学基础/1.1-线性代数与张量运算.html":{"title":"1.1 线性代数与张量运算","icon":"","description":"线性代数作为现代数学的重要分支，在大语言模型的理论基础中占据着不可替代的核心地位。从Transformer架构的注意力机制到词嵌入的向量表示，从矩阵乘法的并行计算到张量运算的维度变换，线性代数为理解和构建深度学习模型提供了坚实的数学框架。大语言模型中的所有参数最终都以矩阵或张量的形式存储和计算，因此深入掌握线性代数的基本概念和运算规则是理解模型工作原理的必要前提。在线性代数的学习路径中，我们需要首先明确几个基本定义。向量是线性空间中最基本的元素，可以理解为一维数组中的有序数集。在大语言模型的语境下，词向量就是典型的向量表示，每个词被映射为一个高维实数向量，这个向量的每一个维度都编码了某种语义或语法特征。例如，一个300维的词向量可以看作是一个从自然语言到连续向量空间的映射，将离散的符号表示转化为连续的数值表示，从而使得机器学习算法能够在这些向量上进行运算和优化。矩阵是二维数组，是线性代数中最重要的研究对象之一。在大语言模型中，权重矩阵无处不在：嵌入层将词汇映射为向量，实际上是一个巨大的查找矩阵；注意力机制中的查询、键、值投影都是通过矩阵乘法实现的；前馈神经网络中的全连接层更是由多个权重矩阵堆叠而成。矩阵的形状（行数和列数）决定了它所代表的线性变换的类型，也决定了它在网络中的具体作用方式。张量是多维数组的自然推广，是矩阵概念在更高维度的延伸。一阶张量是向量，二阶张量是矩阵，三阶及更高阶的张量则可以表示更复杂的数据结构。在现代深度学习框架中，如PyTorch和TensorFlow中，张量是最基本的数据结构，几乎所有的计算都在张量之上进行。大语言模型中的输入通常是一个三维张量，其维度分别代表批量大小、序列长度和嵌入维度。通过张量运算，模型能够高效地并行处理大量数据，这也是大语言模型能够实现高效训练和推理的关键技术基础。线性代数的基本运算包括向量加法、标量乘法、矩阵乘法、转置和求逆等。这些运算在大语言模型中有着直接的应用场景，理解它们的数学性质和计算特性对于优化模型性能和调试模型行为至关重要。向量加法是逐分量（component-wise）的运算，两个维度相同的向量可以逐分量相加。设有两个维向量 和，则其和为向量加法满足交换律和结合律。在大语言模型（如 Transformer）中，残差连接是向量加法的典型应用。它将某一子层的输入与该子层的输出直接相加，从而在反向传播时为梯度提供了一条恒等映射路径，使梯度能够绕过复杂的非线性变换直接传播到较浅层，显著提升了深层模型的可训练性，并有效缓解梯度消失问题。标量乘法是将一个向量与一个标量（实数）相乘，结果是将向量的每个分量都乘以该标量。这种运算在模型的权重初始化、学习率调整和梯度裁剪中都有应用。例如，梯度裁剪就是将梯度向量的模长限制在某个阈值之内，具体做法是如果梯度的范数超过阈值，就将梯度向量乘以一个缩放因子使其范数等于阈值。矩阵乘法是最重要也是计算量最大的运算。两个矩阵和的乘积 是一个的矩阵，其中每个元素。矩阵乘法满足结合律 和分配律，但不满足交换律，即通常不等于。在大语言模型中，注意力机制的计算过程就包含了多个矩阵乘法操作：查询矩阵与键矩阵的乘积产生注意力分数，注意力权重与值矩阵的乘积产生加权输出。这些矩阵乘法的计算效率直接影响模型的整体性能，因此现代GPU都针对矩阵运算进行了专门的硬件优化。矩阵的转置是将矩阵的行和列互换，记作或。转置运算满足和。在大语言模型中，自注意力机制需要计算查询向量与键向量的相似度，这本质上就是计算查询矩阵与转置后的键矩阵的乘积。此外，在实现反向传播时，转置操作也经常用于梯度的反向传播计算。矩阵的求逆是找到一个矩阵使得，其中是单位矩阵。只有方阵才可能存在逆矩阵，而且并非所有方阵都有逆矩阵——只有满秩（行列式非零）的方阵才是可逆的。在深度学习中，我们很少直接计算矩阵的逆，因为数值稳定性较差且计算成本高昂。但矩阵求逆的数学思想——通过逆运算解线性方程组——在优化算法和正则化方法中有着重要的应用。例如，线性回归的闭式解就是通过求解正规方程得到的，而正规方程的求解等价于计算矩阵的伪逆。张量是多维数组在数学上的抽象表示，它将标量（零阶张量）、向量（一阶张量）和矩阵（二阶张量）的概念推广到任意维度。一个阶张量有个索引，每个索引对应张量的一个维度。设有一个三阶张量，则其元素可以表示为​，其中，，。在大语言模型的实践中，张量的维度通常具有明确的语义含义。以Transformer模型为例，输入张量的形状通常是 ，其中是批量大小（batch size），是序列长度（sequence length），是隐藏维度（hidden dimension）。在多头注意力中，经过线性投影并拆分注意力头后，张量可重排为的张量，其中是注意力头的数量。经过注意力计算后的输出张量会与输入张量形状相同，便于进行残差连接和层归一化操作。\n<img alt=\"llm_flow.drawio.png\" src=\"graph/llm_flow.drawio.png\" target=\"_self\">\n张量运算的基本操作包括逐元素运算、重塑（reshape）、转置（transpose）、广播（broadcasting）和张量收缩（contraction）等。逐元素运算对张量中的每个独立元素应用相同的函数，如ReLU激活函数就是典型的逐元素运算。重塑操作改变张量的形状但不改变其包含的数据元素，例如将一个的二维张量重塑为的三维张量，这在将展平后的向量恢复为嵌入序列时非常有用。广播是张量逐元素运算中的一种自动对齐机制。当两个形状不同的张量进行逐元素运算时，深度学习框架会在不显式复制数据的情况下，将形状较小的张量在某些维度上视为被重复，以匹配较大的张量。广播规则通常是从最后一个维度开始逐维比较：如果对应维度相等，或其中一个维度为 1，则该维度可以广播；否则形状不兼容。若两个张量的维度数不同，则会在较小张量的左侧自动补 1。 NumPy、PyTorch 等现代深度学习框架均支持广播机制，它可以显著简化代码（例如 bias 加法），但由于广播可能在语法上合法而在语义上错误，不当使用容易引入隐蔽且难以察觉的 bug。张量收缩是矩阵乘法在高维张量上的推广，又称为张量点积或广义矩阵乘法。张量收缩指定两个张量的若干维度进行配对相乘并求和。例如，两个三阶张量和在第二和第三维度上的收缩产生一个的矩阵。在注意力机制中，注意力分数的计算可以看作是一个张量收缩操作，其中查询向量与键向量在特征维度上进行点积运算。张量分解是处理高维张量的重要技术。常见的张量分解方法包括 CP 分解（CANDECOMP/PARAFAC） 和 Tucker 分解。CP 分解将一个张量表示为若干个秩一张量的和，而 Tucker 分解则将张量表示为一个核心张量与多个因子矩阵在各模上的乘积。张量分解在模型压缩、参数高效微调以及模型蒸馏等场景中具有重要应用，通过将大型权重张量分解为若干低秩张量，可以在保持模型性能的同时显著减少参数量和计算开销。特征空间是机器学习和深度学习中描述数据表示的核心概念。从几何角度来看，一个n维特征空间可以理解为一个n维欧几里得空间，空间中的每个点代表一个数据样本在该空间中的表示。在大语言模型中，词嵌入空间就是一个典型的高维特征空间，其中每个词被映射为空间中的一个点，语义相近的词在空间中距离较近，语义不同的词则距离较远。这种几何表示使得我们可以利用空间中的几何关系来理解和操作语义信息。词嵌入空间具有一些重要的几何性质。首先，嵌入空间通常是密集的，这意味着每个维度都不是二元的或稀疏的，而是取连续的实数值。这种密集表示使得模型能够在连续空间中学习平滑的语义关系。其次，嵌入空间的维度通常远小于词表大小，但又足够高以捕获丰富的语义信息。维度的选择是一个重要的超参数，过低的维度可能导致表达能力的损失（称为欠拟合），过高的维度则可能导致过拟合和计算效率的下降。向量距离和相似度是度量特征空间中样本关系的基本工具。欧几里得距离测量两个向量之间的直线距离。余弦相似度测量两个向量方向的相似程度，与向量的大小无关。在词嵌入研究中，余弦相似度是最常用的相似度度量，因为它能够捕捉语义方向的相似性而不受词频等因素的影响。线性变换是理解特征空间中数据变换的关键概念。一个线性变换可以用一个矩阵 表示，满足和 。线性变换对特征空间的作用包括旋转、缩放、投影和剪切等。旋转保持向量的模长不变，只改变方向；缩放改变向量的模长；投影将向量映射到低维子空间；剪切则是一种保持体积但改变形状的变换。在神经网络中，全连接层就是典型的线性变换加上非线性激活函数的组合。子空间是特征空间中的重要结构。一个k维子空间是特征空间的一个k维线性子集，包含所有可以通过原点的k维平面上的点。列空间（Column Space）是矩阵所有列向量的线性组合构成的空间，它描述了矩阵所能表示的所有输出。零空间（Null Space）是所有被矩阵映射到零向量的输入向量构成的空间，它描述了输入中的冗余信息。在大语言模型中，权重矩阵的列空间决定了该层能够表示的特征空间的范围，而零空间则包含了可能被\"遗忘\"的信息。特征值和特征向量是线性代数中最深刻也最有应用价值的概念之一。它们揭示了线性变换的本质特征：某些特定方向（特征向量）在变换下只发生缩放而不改变方向，缩放的倍数就是特征值。数学上，给定一个方阵，如果存在非零向量和标量满足，则称是的特征向量，是对应的特征值。特征值的计算需要求解特征方程，这是一个关于的多项式方程。对于大型矩阵，直接求解特征方程通常是不切实际的，因为在数值上计算高阶多项式的根非常困难。在实践中，我们使用迭代算法如幂迭代（Power Iteration）、反幂迭代（Inverse Iteration）和QR算法来计算特征值和特征向量。深度学习框架也提供了高效的函数来计算矩阵的特征值分解和奇异值分解。在大语言模型中，特征值和特征向量有着多方面的应用。首先，在主成分分析（PCA）中，特征值分解用于提取数据中方差最大的方向，这些方向称为主成分。通过保留最大的k个特征值对应的特征向量，我们可以实现降维同时最小化信息损失。词嵌入的降维可视化（如t-SNE和UMAP）就依赖于这种思想，先通过PCA进行初步降维，再用非线性方法进行二维或三维嵌入。<br>\n<img alt=\"pca.drawio.png\" src=\"graph/pca.drawio.png\" target=\"_self\">\n其次，特征值与网络稳定性密切相关。一个线性 dynamical system 的稳定性由矩阵的特征值决定：如果所有特征值的实部都为负，则系统是稳定的；如果有特征值的实部为正，则系统是不稳定的。在循环神经网络（RNN）和Transformer中，类似的稳定性分析有助于理解和解决梯度消失或梯度爆炸问题。例如，LSTM和GRU通过门控机制设计，使得等效的变换矩阵具有接近1的特征值，从而缓解了梯度消失问题。第三，谱范数（Spectral Norm）是矩阵的最大奇异值，等于矩阵的最大特征值的平方根。谱范数在深度学习中有重要应用：它用于权重归一化以稳定训练过程，也是证明神经网络泛化性质的关键工具。谱归一化（Spectral Normalization）是一种常用的权重归一化技术，它将权重矩阵的谱范数归一化为1，从而约束了网络函数的Lipschitz常数，这对训练生成对抗网络（GAN）和防止对抗攻击都很重要。奇异值分解（Singular Value Decomposition，SVD）是将任意矩阵分解为三个矩阵乘积的强大工具，被誉为\"线性代数的瑞士军刀\"。对于任意矩阵，存在正交矩阵、和对角矩阵，使得。其中，的对角线元素（r为矩阵的秩）称为奇异值，是的特征值的平方根。的列向量称为左奇异向量，的列向量称为右奇异向量。奇异值分解与特征值分解有密切关系，但又有所不同。特征值分解只适用于方阵且要求矩阵可对角化，而奇异值分解适用于任意矩阵。另外，特征值可以是负数（对于实对称矩阵，特征值是实数），但奇异值总是非负的。奇异值分解之所以强大，正是因为它对任何矩阵都成立，无论是方阵还是矩形阵，无论是否满秩。低秩近似是奇异值分解最重要的应用之一。根据Eckart-Young-Mirsky定理，对于任何整数，用秩不超过k的矩阵对的最佳近似（在Frobenius范数和谱范数意义下）就是保留前k个最大的奇异值及其对应的奇异向量，即。这个近似将原始矩阵分解为k个秩一矩阵的和，每个秩一矩阵对应一个主成分方向。在大语言模型中，低秩近似技术有着广泛的应用。首先，在模型压缩方面，权重矩阵的低秩近似可以显著减少参数量和计算量。假设一个的权重矩阵被近似为​，其中，，则参数量从减少到。当时，这种压缩是显著的。其次，在参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）中，低秩更新是一种核心技术。典型的低秩适配方法如LoRA（Low-Rank Adaptation）假设预训练模型的权重更新可以表示为低秩矩阵，即，其中，。这样，只需要训练个参数而不是个参数，就可以实现对大型语言模型的有效微调。这种方法的数学基础正是低秩近似理论。第三，奇异值分解可以用于理解模型的表达能力。通过分析权重矩阵的奇异值分布，我们可以了解模型使用了多少\"有效自由度\"。如果只有少数几个奇异值很大，说明模型的表达能力主要集中在少数方向上；如果奇异值分布比较均匀，则模型利用了更多的参数自由度。这种分析对于诊断模型过拟合和理解模型容量都很有价值。正交性是线性代数中一个核心概念，具有重要的几何意义和计算优势。两个向量和如果满足，则称它们是正交的。一组向量如果两两正交且都是单位向量，则称为标准正交基。正交矩阵是满足的方阵，其列向量（或行向量）构成标准正交基。正交矩阵有几个重要的性质。首先，正交矩阵保持向量的范数不变，即对所有向量成立。这意味着正交变换是一种保距变换，它只旋转或反射空间而不改变向量的长度。其次，正交矩阵的逆矩阵等于其转置矩阵，即，这使得求解正交矩阵的逆矩阵变得极其简单，只需进行转置操作。第三，正交矩阵的行列式的绝对值为1，即。在大语言模型的架构设计中，正交性有着多方面的应用。首先，正交初始化是一种常用的权重初始化方法。与随机高斯初始化或Xavier初始化不同，正交初始化将权重矩阵初始化为正交矩阵，这在理论上可以防止初始化时的梯度消失或爆炸问题，因为正交矩阵的奇异值全部为1。其次，Gram-Schmidt正交化过程是构建正交基的标准算法，在特征提取和表示学习中经常被使用。例如，在对比学习中，通过Gram-Schmidt正交化可以移除表示中的冗余成分，使得不同特征方向捕捉不同的信息。第三，正交约束在优化中被用于防止权重矩阵的秩退化。在某些正则化方法中，我们在优化目标中加入正交性惩罚项，鼓励权重矩阵的列向量相互正交。这种约束有助于提高模型的稳定性和泛化能力。第四，傅里叶变换和正交基扩展是信号处理和自然语言处理中的重要工具。虽然自然语言处理很少直接使用傅里叶变换，但在处理序列数据的某些场景下，正交变换仍然是有价值的分析工具。矩阵范数是衡量矩阵\"大小\"的标准，在深度学习中有着广泛的应用。矩阵范数将矩阵映射到非负实数，满足非负性、齐次性和三角不等式等性质。常用的矩阵范数包括Frobenius范数、谱范数、核范数和1-范数/∞-范数。Frobenius范数是最直观的矩阵范数，定义为所有元素平方和的平方根：​​。它可以看作是矩阵展开为向量后的L2范数，具有旋转不变性。在深度学习中，Frobenius范数常用于权重衰减（Weight Decay）正则化，通过惩罚大权重来防止过拟合。谱范数（Spectral Norm）是矩阵的最大奇异值：。它对应于从到的线性变换中最大的拉伸因子。在神经网络的稳定性分析和Lipschitz约束中，谱范数是核心概念。谱归一化（Spectral Normalization）是一种重要的权重归一化技术，它通过将权重矩阵除以其谱范数来约束网络函数的Lipschitz常数。核范数（Nuclear Norm）是矩阵所有奇异值的和：。核范数是Frobenius范数和谱范数之间的折中，常用于矩阵补全（Matrix Completion）和低秩矩阵恢复问题。在推荐系统和缺失数据插补中，核范数正则化可以鼓励解的低秩性质。1-范数和∞-范数分别是列范数和行范数：和。这些范数在稀疏优化和线性规划中有重要应用，但在深度学习中使用较少。矩阵范数的选择取决于具体问题的需求。在模型压缩和低秩近似中，我们关心如何用少量的奇异值最好地近似原始矩阵，这涉及到部分和（Partial Sum）范数​。在分析梯度流和稳定性时，谱范数最重要，因为它决定了信息放大或衰减的上界。在正则化中，Frobenius范数因其可导性好而最常用。除了前面介绍的奇异值分解，矩阵分解还包括LU分解、QR分解、特征值分解和Cholesky分解等多种方法。每种分解都有其特定的应用场景和数值性质。LU分解将矩阵分解为下三角矩阵和上三角矩阵的乘积：。对于方阵，如果主元都不为零，则LU分解存在；如果进行行交换（带置换矩阵），则PLU分解总是存在。LU分解的主要用途是高效求解线性方程组，因为前向和后向替换的复杂度是而不是高斯消元的。QR分解将矩阵分解为正交矩阵和上三角矩阵的乘积：。QR分解有多种计算方法，包括Gram-Schmidt正交化、Householder变换和Givens旋转。QR分解在线性最小二乘问题中有重要应用，因为正规方程的解等价于求解。特征值分解将方阵分解为特征向量矩阵和特征值对角矩阵的乘积：。只有可对角化矩阵（具有完整特征向量基）才能进行这种分解。对于对称矩阵，特征值分解总是可行的，且特征向量矩阵是正交的。特征值分解在动力系统分析、主成分分析和谱聚类中都有应用。Cholesky分解是LU分解的特例，只适用于对称正定矩阵。它将矩阵分解为下三角矩阵与其转置的乘积：。Cholesky分解的数值稳定性好，计算量约为LU分解的一半，在高斯过程回归和二次优化中有广泛应用。Kronecker积是两个矩阵之间的特殊二元运算，它将一个的矩阵与一个的矩阵组合成一个的大矩阵。对于矩阵和，它们的Kronecker积定义为：Kronecker积具有许多有用的代数性质，包括结合律、混合积性质 （当维度匹配时）、转置性质。在深度学习中，Kronecker积的主要应用包括权重共享、参数高效微调和特定网络结构的设计。例如，MobileNet中的深度可分离卷积可以用Kronecker积来表示，这有助于理解其参数效率。在模型并行化中，Kronecker积可以帮助我们将大矩阵运算分解为多个小矩阵运算的组合，从而在多个计算设备上分布式执行。向量化（Vectorization）是将矩阵转换为列向量的操作，记为或。如果 是的矩阵，则是一个的列向量，按列优先的顺序排列矩阵元素。向量化操作与Kronecker积结合可以简化矩阵方程的表示。具体来说，对于矩阵方程，通过向量化操作可以将其转化为线性方程。这种转化在推导反向传播公式和优化算法时非常有用，因为它将复杂的矩阵运算转化为标准的矩阵-向量乘积。张量网络是用低维张量通过网络结构表示高维张量的方法，在物理学和机器学习中都有重要应用。一个张量网络由若干个节点（每个节点是一个张量）和连接节点的边（每个边代表张量的一个维度）组成。张量网络的核心思想是用多个小张量的乘积之和来表示一个巨大的高维张量，从而实现高效的存储和计算。在大语言模型中，张量网络的思维方式有助于理解复杂的计算结构。Transformer中的注意力机制可以被看作一个三阶张量（查询、键、值的交互）被分解为多个低阶张量的组合。具体的，注意力权重矩阵可以看作是查询张量和键张量的收缩。张量网络在模型压缩和高效推理中有着重要的应用价值。通过将大型权重张量分解为张量网络的形式，可以显著减少参数量和计算量。例如，CP分解可以将一个的三阶张量表示为个秩一张量的和，参数复杂度从降低到。从张量网络的角度来看，矩阵乘法是连接两个张量的最基本操作。两个二阶张量（矩阵）的乘法可以看作是收缩掉两个张量的一个公共维度。更复杂的网络结构如树状张量网络（Tree Tensor Network）和分层张量分解（Hierarchical Tucker Decomposition）可以用于表示高阶交互关系。张量运算的高效实现是现代深度学习框架的核心能力之一。现代GPU架构专门优化了张量运算，包括矩阵乘法、卷积运算和批处理操作。理解张量运算的数学本质有助于编写高效的代码和诊断性能瓶颈。例如，在PyTorch中，通过仔细规划张量的内存布局和运算顺序，可以显著减少不必要的内存分配和数据拷贝。批处理（Batch Processing）是深度学习训练和推理的基本范式，它允许同时处理多个样本以提高计算效率。从张量的角度来看，批处理是在现有维度之外增加一个新的批处理维度。设单个样本的特征表示是形状为的二维张量（序列长度 × 嵌入维度），则一个批次的表示就是形状为 的三维张量，其中是批大小。批处理的数学优势来自于矩阵运算的并行性。当我们计算批次中所有样本的前向传播时，如果使用张量运算而非循环，可以一次性完成所有样本的计算。现代GPU的并行架构非常适合这种批处理计算，因为它可以将不同样本的计算分配到不同的处理单元上同时执行。数学上，批处理相当于将多个独立的矩阵运算堆叠成一个大的张量运算。注意力机制的批处理实现是理解这一概念的良好例子。单个样本的注意力计算涉及形状为的查询、键和值张量，其中是序列长度。对于一个批次，我们得到形状为的张量。注意力分数的计算在批处理情况下变成的三维张量运算。在实际实现中，这个运算被高度优化，GPU可以同时计算批次中所有样本的注意力分数。微批处理（Micro-batch）是批处理的一个变体，用于处理超大序列或有限GPU内存的情况。当单个样本太大以至于无法在GPU内存中容纳时，可以将批次进一步划分为更小的微批次，每个微批次独立计算。这种技术在训练大语言模型时尤为重要，因为长序列可能消耗大量内存。梯度累积是另一种与批处理相关的技术。当批大小受限于GPU内存时，我们可以通过梯度累积来模拟更大的有效批大小。具体来说，我们先计算若干个小批次的梯度，将它们累加起来，然后使用累加的梯度进行一次参数更新。这样，既保证了一次更新使用的样本数量（有效批大小），又不受单批次大小的限制。梯度累积在数学上等价于使用更大的批次进行训练，因为梯度是线性的。在大语言模型中，矩阵和张量运算的计算复杂度和内存效率是核心关注点。理解不同运算的资源需求对于优化模型设计和硬件利用至关重要。矩阵乘法的计算复杂度是，其中，，输出。这意味着计算一个权重矩阵与向量的乘积需要次标量乘法和加法。对于大语言模型中的全连接层，隐藏维度通常在数千量级，因此单个层的计算量是相当可观的。注意力机制的计算复杂度与序列长度的平方成正比。具体来说，对于序列长度和注意力维度 ，注意力分数的计算需要的计算量，注意力权重与值向量的乘积同样需要。这就是为什么Transformer在处理长序列时面临计算挑战，也是为什么许多研究致力于开发高效的注意力变体，如线性注意力、稀疏注意力和分层注意力。内存效率不仅与参数数量有关，还与激活值的存储需求有关。在前向传播过程中，每一层的输入和输出都需要被保存用于反向传播。对于深层网络和长序列，激活值的内存消耗可能超过参数本身。梯度检查点（Gradient Checkpointing）是一种常用的内存优化技术，它通过在前向传播时只保存部分层的激活值，在反向传播时重新计算被省略的激活值来节省内存，代价是增加额外的计算量。混合精度训练是另一种重要的效率优化技术。它使用半精度浮点数（FP16）或更低精度（如BF16、INT8）进行大部分计算，只在关键步骤使用全精度（FP32）以保持数值稳定性。混合精度训练可以将内存消耗减半，并将计算速度提高近一倍。数学上，混合精度利用了现代GPU对低精度运算的专门优化，这些运算的吞吐量通常是全精度运算的数倍。模型并行化将大模型分布在多个计算设备上，是训练超大规模语言模型的必要技术。张量并行（Tensor Parallelism）在模型的宽度维度上划分参数，流水线并行（Pipeline Parallelism）在深度维度上划分层，而数据并行（Data Parallelism）则复制整个模型在多个数据批次上并行训练。理解这些并行策略的数学基础——如何将矩阵运算分解为可以在不同设备上独立执行的子运算——对于设计和实现高效的大规模训练系统至关重要。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1.1.1 线性代数的核心地位","level":2,"id":"1.1.1_线性代数的核心地位_0"},{"heading":"1.1.2 基本运算与运算规则","level":2,"id":"1.1.2_基本运算与运算规则_0"},{"heading":"1.1.3 张量的表示与运算","level":2,"id":"1.1.3_张量的表示与运算_0"},{"heading":"1.1.4 特征空间的几何直觉","level":2,"id":"1.1.4_特征空间的几何直觉_0"},{"heading":"1.1.5 特征值与特征向量","level":2,"id":"1.1.5_特征值与特征向量_0"},{"heading":"1.1.6 奇异值与低秩近似","level":2,"id":"1.1.6_奇异值与低秩近似_0"},{"heading":"1.1.7 正交性与正交矩阵","level":2,"id":"1.1.7_正交性与正交矩阵_0"},{"heading":"1.1.8 矩阵范数与度量","level":2,"id":"1.1.8_矩阵范数与度量_0"},{"heading":"1.1.9 矩阵分解的深化理解","level":2,"id":"1.1.9_矩阵分解的深化理解_0"},{"heading":"1.1.10 Kronecker积与向量化","level":2,"id":"1.1.10_Kronecker积与向量化_0"},{"heading":"1.1.11 张量网络与高维运算","level":2,"id":"1.1.11_张量网络与高维运算_0"},{"heading":"1.1.12 批处理与并行计算的张量视图","level":2,"id":"1.1.12_批处理与并行计算的张量视图_0"},{"heading":"1.1.13 计算复杂度与内存效率","level":2,"id":"1.1.13_计算复杂度与内存效率_0"}],"links":[],"author":"","coverImageURL":"graph/llm_flow.drawio.png","fullURL":"第1章-数学基础/1.1-线性代数与张量运算.html","pathToRoot":"..","attachments":["graph/llm_flow.drawio.png","graph/pca.drawio.png"],"createdTime":1767062570796,"modifiedTime":1767773409182,"sourceSize":33194,"sourcePath":"第1章 数学基础/1.1 线性代数与张量运算.md","exportPath":"第1章-数学基础/1.1-线性代数与张量运算.html","showInTree":true,"treeOrder":1,"backlinks":[],"type":"markdown"},"第1章-数学基础/1.2-概率论与统计.html":{"title":"1.2 概率论与统计","icon":"","description":"概率论是研究随机现象规律性的数学分支，为大语言模型提供了处理不确定性的理论基础。在自然语言处理中，文本生成本质上是一个随机过程：给定前文的条件下，下一个词的选择遵循某种概率分布。语言模型的训练目标就是学习这个条件概率分布，使得生成的文本既流畅又符合语义逻辑。理解随机变量和概率分布的概念，是掌握语言模型概率本质的必要前提。随机变量是样本空间到实数集的映射，它将随机试验的结果数值化。根据取值方式的不同，随机变量分为离散随机变量和连续随机变量。离散随机变量只能取有限或可数无穷个值，如抛硬币的结果（正面或反面）、掷骰子的点数（1到6）、词汇表中的词索引等。连续随机变量可以取任意实数值或实数区间内的值，如词语的嵌入向量、注意力权重、神经网络的激活值等。在大语言模型中，我们同时处理这两类随机变量：词索引是离散的，而连续空间中的嵌入表示和隐藏状态则是连续的。概率质量函数（Probability Mass Function，PMF）是描述离散随机变量概率分布的函数。对于离散随机变量，其PMF记为，表示取特定值的概率。PMF必须满足两个基本性质：非负性对所有成立，以及归一性。在大语言模型中，词汇表上的概率分布就是典型的PMF。对于词汇表大小为的语言模型，输出层的预测分布是一个维的概率向量，其元素表示生成词汇表中每个词的概率。伯努利分布（Bernoulli Distribution）是最简单的离散分布，描述单次二元试验的成功概率。设随机变量，则，，其中是成功概率。伯努利分布的期望为，方差为。在语言模型中，伯努利分布可用于描述二元随机事件，如判断一个词是否属于某个类别、一个词是否被mask掉、或经过Dropout操作后某个神经元是否被激活等。二项分布（Binomial Distribution）是n次独立伯努利试验中成功次数的分布。设，则，其中。二项分布的期望为，方差为。二项分布在语言模型中的应用包括描述一批样本中被mask的词的数量、计算分类任务中的正确预测数量等。类别分布（Categorical Distribution）是伯努利分布向多值情况的推广，也称为多项分布（Multinomial Distribution）的一次试验版本。设随机变量在个类别上服从类别分布，则，其中且。语言模型的输出层正是类别分布的一个典型应用：模型为词汇表中的每个词分配一个概率，表示生成该词的可能性。Softmax函数将模型的原始输出（logits）转换为有效的类别概率分布。连续随机变量由概率密度函数（Probability Density Function，PDF）描述。与PMF不同，PDF在单点的取值不代表概率，概率由PDF在区间上的积分给出。设连续随机变量的PDF为，则 落在区间的概率为。PDF同样满足非负性和归一性。均匀分布（Uniform Distribution）是最简单的连续分布。设，则其PDF为当，否则为0。均匀分布的期望为，方差为​。在语言模型中，均匀分布可用于权重初始化、随机采样策略的设计，以及某些正则化技术的理论基础。指数分布（Exponential Distribution）描述独立事件发生的时间间隔。设，则其PDF为当，其中是率参数。指数分布的期望为​，方差为​。指数分布在语言模型中的应用包括描述某些随机过程的到达时间间隔，以及在概率图模型中作为共轭先验使用。\n<img alt=\"dist_relation.drawio.png\" src=\"graph/dist_relation.drawio.png\" target=\"_self\">期望、方差和协方差是概率论中最重要的一阶和二阶统计量，它们刻画了随机变量的集中趋势、离散程度和相互关系。这些统计量在机器学习的理论和实践中都有广泛应用，从损失函数的定义到模型性能的评价，从梯度计算的期望到模型不确定性的估计，都离不开这些基本概念。期望（Expectation）是随机变量取值的加权平均，描述了随机变量的\"中心\"位置。对于离散随机变量，其期望定义为。对于连续随机变量，期望定义为。期望具有线性性质：对任意常数和随机变量，有。这一性质在反向传播算法的推导中至关重要，因为我们可以将期望操作与梯度操作交换次序。条件期望是在给定某些信息的条件下对随机变量的期望。条件期望是的函数，记为 。条件期望的一个重要性质是全期望公式：。在语言模型中，条件期望用于描述在给定上下文的条件下，下一个词的条件概率分布的期望特性。例如，困惑度（Perplexity）的计算就隐含了条件期望的概念。方差（Variance）衡量随机变量取值的离散程度，定义为。根据期望的线性性质，方差可以展开为，这个公式在计算方差时更加实用。方差的平方根称为标准差（Standard Deviation），记为​，它与随机变量具有相同的量纲，便于解释和比较。方差的性质包括：非负性；对常数有；若和独立，则。需要注意的是，一般情况下，只有当和不相关时等号才成立。在深度学习中，方差用于分析梯度的波动、权重初始化的尺度选择，以及模型输出的不确定性估计。协方差（Covariance）衡量两个随机变量之间的线性相关程度。对于两个随机变量和，协方差定义为。协方差可以展开为。当时，和倾向于同向变化；当时，和倾向于反向变化；当时，和线性无关。协方差矩阵是多维随机变量的二阶中心矩描述。对于维随机向量，其协方差矩阵是一个的对称半正定矩阵，其中第个元素为。协方差矩阵的对角线元素是各随机变量的方差，非对角线元素是变量之间的协方差。协方差矩阵在大语言模型中的应用包括：描述词嵌入向量的分布特性、分析不同层之间隐藏状态的相关性，以及在变分自编码器中定义潜在空间的先验分布。相关系数（Correlation Coefficient）是标准化的协方差，定义为​。相关系数的取值范围为，其中表示完全正相关，表示完全负相关，表示线性无关。相关系数消除了量纲的影响，使得不同变量对之间的相关性可以直接比较。在语言模型研究中，相关系数可用于分析不同词嵌入维度之间的冗余程度，以及评估模型对不同类型输入的响应一致性。矩（Moment）是随机变量幂次期望的统称。阶原点矩定义为，阶中心矩定义为。一阶原点矩是期望，二阶中心矩是方差。偏度（Skewness）由三阶中心矩归一化得到，描述分布的对称性；峰度（Kurtosis）由四阶中心矩归一化得到，描述分布的尾部厚度。这些高阶矩在统计分析中有一定应用，但在深度学习中的直接应用较少。矩母函数（Moment Generating Function，MGF）是，它唯一确定了随机变量的概率分布（在其存在的范围内）。通过对矩母函数求导并在处求值，可以得到各阶矩：。特征函数是矩母函数的复数形式，它总是存在的，且同样唯一确定概率分布。在某些深度学习应用中，如分析神经网络输出的分布特性，特征函数提供了有用的分析工具。高斯分布（Gaussian Distribution），也称为正态分布（Normal Distribution），是概率论中最重要的连续分布，在统计学和机器学习中有着核心地位。高斯分布之所以如此重要，源于多个方面的原因：从理论上看，中心极限定理表明大量独立随机变量之和趋向于高斯分布；从应用上看，许多自然现象和测量误差都近似服从高斯分布；从数学上看，高斯分布在卷积、傅里叶变换和微分方程等运算下保持封闭形式，这使得高斯分布成为最易于处理的分布之一。一维高斯分布的概率密度函数为，记为。其中是均值参数，是方差参数，是标准差。标准高斯分布（或称单位高斯分布）是均值为0、方差为1的特殊高斯分布，记为，其PDF为。任何高斯随机变量都可以通过标准化变换转换为标准高斯随机变量。高斯分布的期望为，方差为，这从参数命名就可以看出。高斯分布的众数（概率密度最大的点）和中位数都等于均值。高斯分布的偏度为0（完全对称），峰度为3（相对于标准正态分布）。高斯分布的熵（稍后详述）为，这表明在给定方差的所有连续分布中，高斯分布具有最大的熵，即最大的不确定性。<br>\n<img alt=\"gauss_dist.drawio.png\" src=\"graph/gauss_dist.drawio.png\" target=\"_self\">\n多元高斯分布是一维高斯分布向多维空间的推广。设是一个维随机向量，若服从多元高斯分布，则记为，其中是均值向量，是协方差矩阵（必须是对称半正定的）。多元高斯分布的PDF为：其中 是协方差矩阵的行列式，是协方差矩阵的逆矩阵（称为精度矩阵）。二次型 称为马氏距离（Mahalanobis Distance），它考虑了各变量之间的相关性。多元高斯分布具有许多重要的性质。首先，边缘分布仍然是高斯分布：若，则任意分量服从一维高斯分布。其次，线性变换保持高斯性：若，则对任意矩阵 和向量，有。这一性质在变分推断和高斯过程回归中非常重要。条件分布是多元高斯分布最优美也最实用的性质之一。考虑将划分为两个子向量和​，均值和协方差相应地分块为：则给定时的条件分布仍然是高斯分布：其中条件均值为，条件协方差为​。注意条件协方差不依赖于观测值，这是一个重要的性质。条件分布在高斯过程回归、卡尔曼滤波和贝叶斯线性回归中有着核心应用。在大语言模型中，高斯分布扮演着多种重要角色。在权重初始化方面，Xavier初始化和He初始化都假设权重服从某种高斯分布（或均匀分布），以确保信号在各层之间平稳传播。具体而言，He初始化使用均值为0、方差为的高斯分布，其中是输入神经元的数量。在正则化技术方面，Dropout可以被解释为对神经网络进行高斯近似。具体来说，当Dropout率为 时，等效的高斯Dropout在权重上引入方差为的高斯噪声。变分Dropout（Variational Dropout）进一步将Dropout解释为贝叶斯推断，噪声的参数通过变分推断学习得到。在输出建模方面，高斯输出分布用于回归任务，但在语言模型中更常用的是分类分布。某些语言模型变体使用高斯混合模型来表示输出分布，以捕获更复杂的多模态特性。在对话系统中，高斯分布可用于建模响应的不确定性，使得模型能够生成多样化的回复。在高斯过程和贝叶斯优化中，高斯过程是函数的先验分布，用于建模连续的随机过程。高斯过程回归在超参数优化、神经架构搜索和语言模型微调中有着应用。高斯过程的一个优势是它提供了预测的不确定性估计，这对于主动学习和探索-利用权衡很有价值。在变分自编码器和生成模型中，高斯分布通常作为潜在空间的先验分布。标准VAE假设潜在变量服从标准高斯分布，通过编码器学习将输入映射到该分布的参数（均值和对数方差），再通过重参数化技巧进行采样，最后通过解码器重建原始输入。大语言模型的某些扩展（如结合VAE的表示学习）也采用了类似的思想。KL散度（Kullback-Leibler Divergence）和交叉熵（Cross Entropy）是信息论中的核心概念，在机器学习特别是大语言模型中有着广泛应用。它们用于衡量两个概率分布之间的差异，是定义损失函数和分析模型行为的重要工具。信息论的基本概念始于熵（Entropy）的定义。对于离散随机变量及其概率分布，熵定义为。熵衡量了随机变量不确定性的大小，单位为比特（bit，当使用以2为底的对数时）或奈特（nat，当使用自然对数时）。直观上，熵越大，分布越\"平坦\"（不确定性高）；熵越小，分布越\"尖锐\"（确定性高）。对于确定性分布（某个的概率为1，其他为0），熵为0。联合熵（Joint Entropy）是两个随机变量的联合分布的熵：。条件熵（Conditional Entropy）是给定一个随机变量时另一个随机变量的熵：。链式法则给出了多个随机变量的联合熵分解：。互信息（Mutual Information）衡量两个随机变量之间的信息共享程度：。互信息始终非负，当且仅当和独立时互信息为0。在语言模型中，互信息可用于分析不同位置或不同层之间的信息流动，以及评估词嵌入中捕获的语义信息量。KL散度衡量两个概率分布之间的\"距离\"（严格来说不是真正的距离，因为它不满足对称性和三角不等式）。对于两个离散概率分布和，KL散度定义为：KL散度可以理解为使用分布来编码来自分布的样本时所需要的额外信息量。从公式可以看出，KL散度是非负的（由Jensen不等式保证），即，且当且仅当时KL散度为0。然而，KL散度不对称，即一般情况下。对于连续随机变量（分布用PDF表示），KL散度的定义为：在大语言模型中，KL散度有多种重要应用。在变分推断中，我们希望用简单的近似分布来逼近复杂的后验分布，优化目标就是最小化或，具体选择取决于变分推断的目标。在VAE中，损失函数包含重构项和KL正则项，后者就是编码器分布与标准高斯先验之间的KL散度。在强化学习与语言模型的结合中，KL散度用于限制策略更新的幅度，确保新策略不会偏离旧策略太远。这在近端策略优化（PPO）等算法中非常重要。在知识蒸馏中，KL散度用于衡量学生模型和教师模型输出分布之间的差异，使学生模型能够学习教师模型的知识。交叉熵与KL散度密切相关。对于两个分布和，交叉熵定义为。交叉熵可以分解为熵与KL散度之和：。当是真实分布而是模型预测分布时，是常数（因为真实分布固定），因此最小化交叉熵等价于最小化KL散度。<br>\n<img alt=\"msg_relation.drawio.png\" src=\"graph/msg_relation.drawio.png\" target=\"_self\">在大语言模型的训练中，交叉熵损失是最常用的损失函数。具体而言，对于每个位置，语言模型预测下一个词的概率分布为，而真实分布是\"正确答案\"的one-hot编码。该位置的交叉熵损失为，其中​ 是真实词元。整个序列或批次的损失是这些位置损失的均值或和。交叉熵损失的优势在于它直接与对数似然相关，最小化交叉熵等价于最大化数据的对数似然。从优化的角度看，交叉熵损失相对于模型输出（logits）的梯度具有简洁的形式。设是第个类别的logit，softmax输出为​，真实标签为（在多类分类中为one-hot编码），则交叉熵损失相对于的梯度为​。这个简洁的梯度形式是交叉熵损失被广泛采用的重要原因之一。标签平滑（Label Smoothing）是一种常用的正则化技术，它将硬标签（one-hot编码）替换为软标签，即在真实类别上赋予的概率，在其他类别上均匀分配的概率。标签平滑可以防止模型对训练数据过度自信，提高泛化能力。从KL散度的角度看，标签平滑相当于在训练时使用真实分布与均匀分布的混合作为目标分布。KL散度在不同场景下的不同形式反映了其灵活性。正向鼓励覆盖的所有模式（即的支持集必须包含的支持集），而反向则鼓励找到一个主要的模式并紧密地拟合它。在变分推断中，选择哪种KL形式取决于我们希望对近似分布施加什么样的约束。在生成模型中，这种选择影响着生成样本的多样性和质量。统计推断是利用样本数据对总体特征进行推断的过程，包括参数估计和假设检验两大类方法。在大语言模型中，统计推断的思想贯穿于模型训练、参数优化和性能评估的全过程。理解最大似然估计、贝叶斯估计和期望传播等统计推断方法，对于深入理解语言模型的工作原理和改进模型设计都有重要意义。最大似然估计（Maximum Likelihood Estimation，MLE）是最基本也是最重要的参数估计方法。设我们有观测数据，假设这些数据独立同分布（i.i.d.）于某个概率分布，其中是未知参数。似然函数定义为，对数似然函数为。最大似然估计就是找到使对数似然最大的参数值：。在大语言模型中，MLE是训练的标准方法。语言模型的训练目标是最大化训练语料库的对数似然，即给定前文的条件下预测下一个词的对数概率之和。由于训练数据通常被组织为序列，对数似然可以分解为序列中每个位置的条件对数概率之和。训练过程就是使用随机梯度下降等优化算法来最大化这个目标函数。最大似然估计具有一些重要的渐近性质。在正则条件下，当样本量时，MLE是相合的（收敛于真实参数值）、渐近正态的（服从以真实参数为均值、费希尔信息矩阵逆为方差的正态分布）和渐近有效的（方差达到克拉美-罗下界）。这些性质为MLE在大样本场景下的应用提供了理论保障。贝叶斯估计是另一种参数估计方法，它将参数视为随机变量并使用贝叶斯公式进行推断。设参数 的先验分布为，在观测数据后，参数的后验分布为：其中是似然函数，是边际似然（或证据）。贝叶斯估计使用后验分布进行预测，而不是点估计。在语言模型中，贝叶斯方法可用于模型选择（通过边际似然）、不确定性量化和小样本学习。共轭先验是使后验分布与先验分布同分布族的先验选择，这使得贝叶斯推断在计算上更加方便。例如，高斯分布的共轭先验还是高斯分布，二项分布的共轭先验是Beta分布，多项分布的共轭先验是Dirichlet分布。在语言模型中，Beta先验和Dirichlet先验可用于建模词汇概率的不确定性。变分推断是一类近似贝叶斯推断的方法，它将后验分布的推断转化为优化问题。变分推断假设后验分布可以用某个简单的分布族（如高斯分布）来近似，然后通过最小化近似分布与真实后验分布之间的KL散度来找到最好的近似。在大语言模型中，变分推断被用于VAE的训练、主题模型的推断和某些贝叶斯神经网络方法。期望传播（Expectation Propagation）是另一种近似推断方法，它通过匹配矩（而不是最小化全局KL散度）来近似后验分布。期望传播在某些情况下比变分推断更准确，特别是当目标分布是高度非高斯的时候。在语言模型的某些应用中，如基于贝叶斯方法的超参数优化，期望传播提供了有用的近似工具。自助法（Bootstrap）是一种通过重采样来估计统计量方差的非参数方法。基本思想是从原始数据中有放回地抽取与原数据等大的样本，重复多次，计算每次的统计量估计，然后使用这些估计的分布来表征原始统计量的不确定性。在语言模型的评估中，自助法可用于估计困惑度、精确率等指标的置信区间，帮助我们理解模型性能评估的可靠性。大数定律和中心极限定理是概率论中最重要的极限定理，它们描述了大量随机变量之和（或平均值）的渐近行为。这些定理为机器学习中许多方法的合理性提供了理论依据，也指导着我们理解和解释模型训练过程中的各种现象。大数定律（Law of Large Numbers）指出，当独立同分布的随机变量样本量趋向无穷时，样本均值趋向于随机变量的期望。设是独立同分布的随机变量，期望有限，则样本均值依概率收敛于对任意成立。弱大数定律保证样本均值以概率收敛于期望，而强大数定律保证样本均值几乎必然收敛于期望。在机器学习训练中，大数定律解释了为什么使用更大的批量（batch）进行梯度估计时，梯度的方向更加稳定。随着批量大小的增加，梯度估计的方差减小，优化过程变得更加稳定。在语言模型训练中，我们通常使用随机梯度下降或其变体，每次只使用一小批样本来估计梯度。根据大数定律，当批量大小足够大时，这一小批样本的梯度是整体数据梯度的一个良好近似。然而，当批量太小时，梯度估计的方差较大，可能导致训练过程不稳定或收敛较慢。中心极限定理（Central Limit Theorem，CLT）是概率论中最深刻的定理之一，它指出大量独立同分布随机变量之和（适当标准化后）趋向于正态分布。设是独立同分布的随机变量，期望有限，方差有限，则标准化和的分布趋向于标准正态分布：，其中是标准正态分布的CDF。中心极限定理的重要性在于它表明正态分布在自然界中无处不在，无论原始随机变量的分布是什么，只要样本量足够大，它们的均值（或和）就近似服从正态分布。在深度学习中，这一性质被用于分析梯度分布、初始化策略和批量归一化的效果。批量归一化（Batch Normalization）是深度学习中最重要的技术之一，它利用中心极限定理的原理来稳定训练过程。批量归一化对每一层的输入进行标准化，使其均值接近0、方差接近1。从中心极限定理的角度看，当批量大小足够大时，该批量内样本的均值和方差是整体数据均值和方差的良好估计，因此标准化操作是合理的。在分析神经网络梯度的分布时，中心极限定理提供了重要的洞见。假设网络中有大量独立的噪声源（如随机初始化、Dropout噪声等），则梯度的分布趋向于高斯分布。这种近似在分析神经网络的学习动态、设计优化算法和理解泛化性质时非常有用。大数定律和中心极限定理也指导着模型评估策略。当我们计算验证集上的性能指标时，该指标是总体性能的一个估计。根据大数定律，验证集越大，这个估计越可靠。根据中心极限定理，我们可以构建置信区间来量化估计的不确定性。在实际应用中，我们需要平衡验证集大小和计算成本，同时理解评估结果的统计显著性。假设检验是统计推断的重要工具，用于根据样本数据判断关于总体的假设是否成立。在大语言模型的开发和研究中，假设检验被用于比较不同模型的性能、验证改进措施的有效性以及检测数据中的系统性偏差。假设检验的基本框架包括原假设（）和备择假设（​）。原假设通常是我们想要拒绝的假设（如\"两个模型没有差异\"），备择假设是我们想要支持的假设（如\"模型A优于模型B\"）。我们根据样本数据计算检验统计量，确定在原假设下观察到该统计量或更极端情况的概率（p值）。如果p值小于预先设定的显著性水平（如0.05），则拒绝原假设。配对t检验是比较两个相关样本均值的常用方法。在语言模型评估中，如果我们使用相同的测试集评估两个模型，配对t检验可以判断两个模型性能的差异是否显著。具体来说，对于每个测试样本，我们计算两个模型的性能差异，然后检验这些差异的均值是否显著不为零。显著性检验的解读需要谨慎。\"统计显著\"不等于\"实际显著\"或\"重要\"。一个很大的模型可能在某些指标上有统计显著的改进，但改进幅度可能很小以至于在实际应用中可以忽略。p值受样本量影响很大：样本量很大时，即使是微小的差异也可能是统计显著的；样本量很小时，即使很大的差异也可能不显著。效应量（Effect Size）衡量差异的实际大小，独立于样本量。常用的效应量包括Cohen's d（标准化均值差异）和相关系数。在比较语言模型性能时，报告效应量可以帮助读者理解改进的实际意义。此外，置信区间比p值提供更多信息：它不仅告诉我们差异是否显著，还给出了差异大小的可能范围。多重比较问题是在进行多个假设检验时需要考虑的重要问题。如果同时检验多个假设，即使所有原假设都为真，至少有一个被拒绝的概率也会大大增加（这是所谓的\"多重比较谬误\"）。在大语言模型评估中，当我们比较多个模型或在多个测试集上评估时，需要使用适当的多重比较校正方法（如Bonferroni校正或False Discovery Rate控制）来控制总体错误率。在比较语言模型时，置换检验（Permutation Test）是一种非参数方法，特别适用于样本量不大或分布未知的情况。置换检验通过随机打乱标签来生成置换样本，在原假设下所有排列是等可能的。通过计算真实排列的统计量在置换分布中的位置，可以得到p值。置换检验在比较两个模型在特定测试集上的性能时很有用。A/B测试是互联网公司和研究机构常用的实验方法，用于比较两个版本（如两个模型或两种策略）的效果。A/B测试的核心是随机分组和控制变量，确保比较的公平性。在大语言模型的在线评估中，A/B测试可以用于评估模型对用户交互的实际影响，如用户满意度、任务完成率等指标。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1.2.1 随机变量与概率分布","level":2,"id":"1.2.1_随机变量与概率分布_0"},{"heading":"1.2.2 期望、方差与协方差","level":2,"id":"1.2.2_期望、方差与协方差_0"},{"heading":"1.2.3 高斯分布与多元高斯","level":2,"id":"1.2.3_高斯分布与多元高斯_0"},{"heading":"1.2.4 KL散度与交叉熵","level":2,"id":"1.2.4_KL散度与交叉熵_0"},{"heading":"1.2.5 统计推断与参数估计","level":2,"id":"1.2.5_统计推断与参数估计_0"},{"heading":"1.2.6 大数定律与中心极限定理","level":2,"id":"1.2.6_大数定律与中心极限定理_0"},{"heading":"1.2.7 假设检验与模型评估","level":2,"id":"1.2.7_假设检验与模型评估_0"}],"links":[],"author":"","coverImageURL":"graph/dist_relation.drawio.png","fullURL":"第1章-数学基础/1.2-概率论与统计.html","pathToRoot":"..","attachments":["graph/dist_relation.drawio.png","graph/gauss_dist.drawio.png","graph/msg_relation.drawio.png"],"createdTime":1767062570795,"modifiedTime":1767773432423,"sourceSize":31796,"sourcePath":"第1章 数学基础/1.2 概率论与统计.md","exportPath":"第1章-数学基础/1.2-概率论与统计.html","showInTree":true,"treeOrder":2,"backlinks":[],"type":"markdown"},"第1章-数学基础/1.3-微积分与优化基础.html":{"title":"1.3 微积分与优化基础","icon":"","description":"微积分是研究变化率和累积效应的数学分支，为理解和优化大语言模型提供了核心的数学工具。大语言模型的训练本质上是一个优化问题：我们需要找到一组模型参数，使得模型在训练数据上的损失函数值最小化。这个优化过程涉及对损失函数求导（计算梯度），然后根据梯度方向更新参数。理解偏导数、链式法则、梯度和Hessian等概念，是深入掌握深度学习优化算法的必要基础。偏导数是多元函数对单个变量的导数，它描述了当其他变量保持不变时，函数值随某一特定变量变化的速率。设是一个多元函数，，则关于变量在点处的偏导数定义为：从几何角度来看，偏导数可以理解为多元函数图像与包含坐标轴的平面相交得到的一元函数曲线的切线斜率。在大语言模型的语境下，当我们计算损失函数关于某个特定参数的偏导数时，我们实际上是在问：如果只改变这个参数而保持其他参数不变，损失函数会以多快的速度变化。这个信息对于确定参数更新的方向和幅度至关重要。偏导数的计算遵循与单变量导数相同的规则。常数的偏导数为零，常数因子的偏导数等于该因子乘以函数的偏导数，和的偏导数等于偏导数的和。对于乘积​，商的偏导数​​，链式法则将在下文详细讨论。复合函数的偏导数法则与单变量情况类似，只是需要明确哪个变量被当作中间变量。链式法则（Chain Rule）是微积分中最重要的法则之一，它告诉我们如何计算复合函数的导数。在深度学习中，神经网络本质上是一个嵌套的复合函数，链式法则使得我们能够计算损失函数相对于任意深层参数的梯度，这一过程称为反向传播。链式法则的基本形式可以这样表述：如果，则，其中是外层函数关于中间变量的导数，是内层函数关于自变量的导数。对于多元函数的复合，情况稍微复杂一些。设，其中每个，则关于的偏导数为：这个公式表明，对的总影响是通过所有中间变量传导的，每个中间变量的贡献是它对的偏导数与它对的偏导数的乘积之和。在深度学习的反向传播算法中，链式法则的应用更加系统化。考虑一个简单的两层神经网络：，，其中和是激活函数，是权重矩阵，是偏置向量，是输入，是输出。损失函数衡量输出与目标之间的差异。反向传播从输出层开始，首先计算​，然后逐层向后传递：，接着计算​，以此类推直到输入层。\n<img alt=\"backprop.drawio.png\" src=\"graph/backprop.drawio.png\" target=\"_self\">矩阵形式的链式法则在深度学习中尤为重要。设，，则梯度是一个与形状相同的三阶张量，而​。为了高效地实现这些计算，我们使用雅可比矩阵（Jacobian Matrix）来组织偏导数。雅可比矩阵包含所有一阶偏导数，对于函数，雅可比矩阵 是一个的矩阵，其中​。在计算图中，链式法则的应用更加直观和系统。计算图是表示数学表达式的有向无环图，其中节点表示变量或操作，边表示依赖关系。反向传播算法沿着计算图反向遍历，对于每个操作节点，计算其输出关于输入的梯度，然后使用链式法则组合这些梯度。典型的操作包括矩阵乘法、加法、元素级函数（如ReLU、Sigmoid、Softmax）和归一化操作，每种操作都有预定义的梯度计算规则。自动微分（Automatic Differentiation）是现代深度学习框架的核心功能，它利用链式法则自动计算复杂函数的导数。与数值微分（使用有限差分近似导数）和符号微分（使用代数规则显式推导导数公式）不同，自动微分将函数分解为基本操作的序列，然后应用链式法则累加梯度。自动微分结合了数值计算的效率和符号计算的精确性，是训练神经网络的关键技术基础。梯度是多元函数最陡上升方向的向量，包含了函数关于所有自变量的偏导数信息。设是一个可微函数，则在点处的梯度定义为：梯度向量指向函数增长最快的方向，其长度（范数）反映了增长的速率。负梯度方向则是函数下降最快的方向，这就是梯度下降算法的理论基础。在大语言模型中，梯度向量指导着参数的更新方向：每个参数分量根据其在梯度中的值进行调整，使得整体损失函数朝着下降的方向移动。梯度的性质对于理解优化过程至关重要。梯度与等值面（或等值线）正交：在函数的等值面上，梯度向量与等值面垂直。这意味着如果我们沿着梯度方向移动，我们会以最快的速度离开当前的等值面，进入函数值更高的区域。驻点（Critical Point）是梯度为零向量的点，即，这些点可能是极小值、极大值或鞍点。方向导数（Directional Derivative）衡量函数沿特定方向的变化率。设是一个单位向量，则沿的方向导数为，其中是梯度与方向之间的夹角。这个公式清楚地表明，方向导数在梯度方向（）上取得最大值，在与梯度相反的方向（）上取得最小值（负的最大值），在与梯度垂直的方向（）上为零。Hessian矩阵是多元函数的二阶导数矩阵，包含了关于所有自变量的二阶偏导数信息。对于函数，Hessian矩阵定义为：如果函数的二阶偏导数连续（这是多数实际应用中的常见假设），则Hessian矩阵是对称的，即​。对称性大大简化了Hessian矩阵的分析和计算。Hessian矩阵在优化中的作用主要体现在以下几个方面。首先，通过Hessian矩阵可以判断驻点的性质：如果Hessian正定（所有特征值大于零），则该点是局部极小值；如果Hessian负定，则该点是局部极大值；如果Hessian既有正特征值又有负特征值，则该点是鞍点；如果Hessian半正定或半负定，则需要进一步分析。其次，Hessian矩阵决定了泰勒展开的二次项，影响局部优化的收敛速度。在点​附近，函数的二阶泰勒展开为：这个近似在局部区域非常准确，是理解牛顿法等二阶优化方法的基础。第三，Hessian矩阵的奇异值分解揭示了函数的局部曲率结构。Hessian的特征值表示函数在不同特征方向上的曲率（凹凸程度），特征向量表示这些曲率对应的方向。在深度学习优化中，损失函数的Hessian通常具有极值的特征值分布：少数几个特征值很大（对应\"平坦\"方向），多数特征值很小（对应\"陡峭\"方向），这种结构影响着优化算法的行为。共轭梯度法利用Hessian的信息来加速收敛，而不显式地计算和存储完整的Hessian矩阵。在深度学习中，由于参数数量巨大（可达数十亿），显式计算Hessian是不现实的，因此共轭梯度法和拟牛顿法等方法在处理大规模优化问题时具有优势。<br>\n<img alt=\"CombinedScene_ManimCE_v0.19.1.png\" src=\"graph/combinedscene_manimce_v0.19.1.png\" target=\"_self\">K-FAC（Kronecker-Factored Approximate Curvature）是一种用于深度学习的Hessian近似方法，它将Hessian矩阵近似为两个小矩阵的Kronecker积，从而大幅降低计算和存储成本。K-FAC基于层间独立性假设，假设每层的参数可以独立地近似其Hessian块，这使得在保持二阶信息的同时实现了可扩展性。高斯-牛顿矩阵是Hessian的一个常用近似，在最小二乘问题中特别有用。对于残差形式的问题 ，高斯-牛顿矩阵为，其中是残差函数的雅可比矩阵。与完整的Hessian相比，高斯-牛顿矩阵省略了二阶导数项，通常是正定的，并且计算成本更低。拉普拉斯近似利用Hessian矩阵在峰值附近对概率分布进行高斯近似。在贝叶斯深度学习和变分推断中，拉普拉斯近似用于构建后验分布的高斯代理，使得预测具有不确定性估计。这种方法在模型压缩和少样本学习中有一定的应用价值。梯度下降是最基本也最广泛使用的优化算法，它利用负梯度方向作为搜索方向来迭代地最小化目标函数。基本梯度下降算法的更新规则为，其中是第步的参数，是学习率（步长），是损失函数在当前参数处的梯度。学习率是一个关键的超参数，它决定了每次更新的幅度：学习率太小会导致收敛缓慢，学习率太大可能导致跳过最优解甚至发散。学习率调度（Learning Rate Scheduling）是在训练过程中动态调整学习率的策略。常见的学习率调度方法包括：阶梯衰减（Step Decay），每经过若干个epoch将学习率降低一个因子；指数衰减（Exponential Decay），学习率按指数函数递减；余弦退火（Cosine Annealing），学习率按余弦曲线从初始值逐渐减小到零；Warmup，在训练初期逐渐增加学习率，然后再衰减。这些调度策略的目标是在训练初期快速收敛，在后期精细调优，最终达到较好的收敛效果。<br><img alt=\"grad.png\" src=\"graph/grad.png\" target=\"_self\">\n凸优化是研究凸函数在凸集上最小化问题的数学分支，具有优美的理论性质和高效的算法。在凸优化问题中，任何局部最优解都是全局最优解，且最优解集是凸集。凸函数是一类特殊的函数，其上方的图构成一个凸集。函数是凸函数的充要条件是对于任意和任意，有。几何上，这意味着函数图上任意两点的连线位于函数图的上方。严格凸函数是凸性更强的形式，要求上述不等式在和时取严格小于号。严格凸函数至多有一个全局最小值点。可微函数是凸函数的充要条件是其梯度单调不减：。对于二次可微函数，凸性的充要条件是Hessian矩阵半正定。梯度下降在凸优化中的收敛性有完善的理论保证。对于具有L-利普希茨连续梯度的凸函数（即），标准梯度下降以的速度收敛，即经过次迭代后，目标函数值与最优值的差距为。对于强凸函数（即存在使得，梯度下降以线性速度收敛，即误差按几何级数衰减。动量方法（Momentum）是加速梯度下降的重要技术，它累积历史梯度来平滑参数更新。动量更新的公式为：，​，其中是速度向量，是动量系数。动量方法可以看作是对梯度进行指数移动平均，使得更新方向更加平滑，从而加速收敛并减少震荡。Nesterov动量是动量方法的一个变体，它在更新之前先对梯度进行校正，通常能提供更好的收敛性质。自适应学习率方法为每个参数单独调整学习率，是深度学习优化中最重要的进展之一。AdaGrad为每个参数维护一个累积梯度平方和，并相应地调整学习率：，其中是逐元素累积的梯度平方，表示逐元素乘法。AdaGrad特别适合处理稀疏特征和非均匀分布的梯度。RMSprop是AdaGrad的改进版本，使用指数移动平均来累积梯度平方：，其中是衰减系数。这解决了AdaGrad学习率单调递减的问题，使得算法能够适应非平稳的目标函数。Adam（Adaptive Moment Estimation）是最流行的自适应优化算法，它结合了动量方法和RMSprop的优点。Adam维护两个指数移动平均：（一阶矩估计，即动量）和（二阶矩估计，即未中心化的方差）。为了校正偏差，使用和。更新规则为。Adam的超参数和通常使用默认值。尽管Adam在实践中表现优异，但最近的研究表明，在某些情况下，传统的带动量SGD可能优于Adam。AMSGrad是Adam的一个修改版本，通过保持的单调性来保证收敛性。RAdam（Rectified Adam）通过引入一个校正因子来修复Adam在训练早期的方差问题。这些变体反映了深度学习优化研究的持续进展。大语言模型的优化面临独特的挑战。首先，模型参数量巨大（数十亿甚至万亿级），标准优化算法的内存开销巨大。梯度检查点技术通过在前向传播时只保存部分激活值，在反向传播时重新计算其余激活值，以计算换内存。其次，混合精度训练使用半精度浮点数进行计算以节省内存和加速，但仍需要保持某些计算的精度。第三，学习率预热（Warmup）在训练大语言模型中尤为重要。warmup策略在训练初期逐渐增加学习率，从很小的值线性增长到目标学习率，然后再进行衰减。这被认为有助于模型在早期阶段找到一个更好的参数区域，避免在随机初始化阶段受到过大梯度的干扰。第四，梯度裁剪（Gradient Clipping）限制梯度的范数以防止梯度爆炸，这对于训练深层网络和循环神经网络尤为重要。非凸优化是深度学习面临的现实挑战。与凸优化问题不同，深度神经网络的损失函数通常是非凸的，存在大量的局部最小值、鞍点和平坦区域。非凸性意味着梯度下降可能收敛到局部最小值而非全局最小值。然而，实践表明深度学习模型通常能够找到性能不错的解，即使不能保证达到全局最优。关于为什么深度神经网络能够有效优化，有多种理论解释，包括损失函数的\"伪凸\"性质、随机梯度下降的隐式正则化效应，以及局部最小值通常具有相似的损失值等。鞍点逃离是大规模非凸优化中的一个重要问题。在高维空间中，鞍点比局部最小值更为常见。动量方法和噪声梯度有助于算法逃离鞍点。SGD的随机性本身就提供了一种隐式的噪声源，有助于逃离平坦区域。研究表明，在适当的噪声水平下，SGD倾向于收敛到平坦的局部最小值，这些最小值通常具有更好的泛化能力。局部最小值的等价性是大规模深度学习中的一个有趣现象。研究发现，对于不同的随机初始化，深度网络往往收敛到具有相似损失值的局部最小值，即使这些最小值在参数空间中相距甚远。这可能是因为神经网络具有大量的对称性（如神经元排列的不变性），使得不同的参数化方式可以产生相同的函数。此外，损失函数的等值面在高维空间中可能比直觉上预期的更加连通，允许梯度下降在不同局部最小值之间\"穿行\"。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"1.3.1 偏导数与链式法则","level":2,"id":"1.3.1_偏导数与链式法则_0"},{"heading":"1.3.2 多元函数梯度与Hessian","level":2,"id":"1.3.2_多元函数梯度与Hessian_0"},{"heading":"1.3.3 梯度下降与凸优化基础","level":2,"id":"1.3.3_梯度下降与凸优化基础_0"}],"links":[],"author":"","coverImageURL":"graph/backprop.drawio.png","fullURL":"第1章-数学基础/1.3-微积分与优化基础.html","pathToRoot":"..","attachments":["graph/backprop.drawio.png","graph/combinedscene_manimce_v0.19.1.png","graph/grad.png"],"createdTime":1767062570789,"modifiedTime":1767858763109,"sourceSize":19202,"sourcePath":"第1章 数学基础/1.3 微积分与优化基础.md","exportPath":"第1章-数学基础/1.3-微积分与优化基础.html","showInTree":true,"treeOrder":3,"backlinks":[],"type":"markdown"},"第2章-前馈网络数学/2.1-神经元的数学模型.html":{"title":"2.1 神经元的数学模型","icon":"","description":"神经网络（Neural Network）的数学基础建立在对生物神经元（Biological Neuron）的抽象与简化之上。1943年，神经生理学家沃伦·麦肯罗皮层（Warren McCulloch）和数学家沃尔特·皮茨（Walter Pitts）发表了开创性的论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity），首次提出了人工神经元的数学模型，标志着计算神经科学和人工智能的诞生。本节将从数学建模的角度，系统阐述神经元从生物原型到数学抽象的演化过程，深入分析单层神经元的数学结构，并建立与后续章节的联系。在深入数学细节之前，我们需要首先理解生物神经元的基本结构及其数学抽象过程。生物神经元是一种复杂的细胞系统，由细胞体（Soma）、树突（Dendrites）、轴突（Axon）和突触（Synapse）等部分组成。树突负责接收来自其他神经元的信号，细胞体对这些输入信号进行整合处理，当输入信号的累积效应超过某个阈值时，神经元被激活并通过轴突向其他神经元传递信号。突触是神经元之间传递信息的连接点，其连接强度决定了信号传递的效率。定义2.1.1（麦肯罗皮层神经元模型）：麦肯罗皮层神经元（McCulloch-Pitts Neuron）是一种二值神经元模型，其数学定义为：其中为输入信号，为连接权重，为激活阈值，为阶跃激活函数，为神经元输出。阶跃激活函数的数学定义为：麦肯罗皮层神经元模型虽然简单，却抓住了生物神经元信息处理的核心特征：加权求和信息整合与阈值触发机制。这种抽象使得我们可以利用线性代数和逻辑代数的工具来分析神经网络的行为。值得注意的是，麦肯罗皮层神经元实际上是一个线性分类器，它将输入空间划分为两个半空间，由超平面所界定。定理2.1.1（麦肯罗皮层神经元的表达能力）：单个麦肯罗皮层神经元可以实现所有的基本逻辑运算，包括逻辑与（AND）、逻辑或（OR）和逻辑非（NOT）。证明：我们通过构造具体的权重和阈值来说明。假设输入为。对于逻辑与运算（​），设权重、，阈值。则当且仅当两个输入均为1时，加权和为，输出为1；其他情况输出为0。对于逻辑或运算（​），设权重、，阈值。只要至少有一个输入为1，加权和至少为1，输出为1；只有两个输入均为0时输出为0。对于逻辑非运算（），设权重，阈值。当时，加权和为，输出为1；当时，加权和为，输出为0。由于任意布尔函数都可以由AND、OR、NOT组合表示，单个神经元可以模拟任意单变量布尔函数。这个定理表明，尽管单个神经元结构极其简单，但它已经具备了一定的逻辑推理能力。然而，麦肯罗皮层神经元模型存在明显的局限性：它是离散的二值模型，权重和阈值需要人工设定，无法通过数据自动学习。1958年，弗兰克·罗森布拉特（Frank Rosenblatt）提出的感知机（Perceptron）模型克服了这些局限性，引入了可学习的参数和连续可微的激活函数，为神经网络的现代发展奠定了基础。感知机是麦肯罗皮层神经元的重要推广，其核心创新在于引入了参数学习机制，使得神经元能够从数据中自动学习最优的连接权重。与麦肯罗皮层神经元使用阶跃函数不同，感知机通常采用连续可微的激活函数，这为使用梯度下降等优化算法提供了数学基础。定义2.1.2（感知机模型）：设为输入向量，为权重向量，为偏置项（Bias）。感知机的数学定义为：其中为激活函数，称为净输入（Net Input），为输出向量。表达式是线性变换与平移的组合，在几何上称为仿射变换（Affine Transformation）。仿射变换是线性变换的推广，它保持了空间的平行性和比例关系，但不要求保持原点不变。定义2.1.3（仿射变换）：从到的仿射变换定义为：其中为变换矩阵，为平移向量。当时，仿射变换退化为线性变换。定理2.1.2（仿射变换的几何意义）：仿射变换在几何上等价于先进行线性变换，再进行平移变换。对于超平面，仿射变换将其映射为另一个超平面。证明：考虑超平面。经过线性变换后，中任意点变为。设，其中为的摩尔-彭罗斯广义逆，则。因此，线性变换后的点集满足，仍为超平面。平移变换则将整个超平面平移，不改变其超平面性质。在神经网络的语境下，仿射变换承担着特征提取和信息整合的功能。权重矩阵决定了不同特征之间的线性组合方式，偏置项则提供了灵活的阈值调节能力。理解仿射变换的几何本质，对于分析神经网络的能力边界和训练动态至关重要。定义2.1.4（齐次坐标表示）：为了将仿射变换统一表示为线性变换的形式，我们引入齐次坐标（Homogeneous Coordinates）。将输入向量扩展为齐次坐标，将权重矩阵和偏置合并为扩展权重矩阵：则仿射变换可以统一表示为：齐次坐标表示的优势在于它将仿射变换转化为单一的矩阵乘法运算，使得我们可以使用线性代数的标准工具进行分析。在计算机图形学和计算机视觉中，齐次坐标被广泛用于处理平移、旋转、缩放等仿射变换。仿射变换本身是线性变换，无论堆叠多少层，复合后的结果仍然是线性变换。激活函数（Activation Function）通过引入非线性变换，打破了神经网络的线性瓶颈，使得神经网络能够拟合任意复杂的非线性函数。这是神经网络强大表达能力的关键来源。定理2.1.3（多层线性网络的恒等性）：设有一个层的前馈网络，每一层仅进行仿射变换而不应用激活函数，即。则整个网络等价于一个单一的仿射变换：其中，。证明：通过数学归纳法证明。当时，结论显然成立。假设对于层网络，结论成立，即：其中，。则第层的输出为：这正是层网络的等效仿射变换形式。\n这个定理揭示了一个深刻的事实：没有激活函数的神经网络无论多深，其表达能力等价于单层线性变换。这意味着堆叠多层网络不会带来任何额外的表达能力——这是不可能拟合复杂非线性函数的原因。激活函数通过引入非线性打破了这一限制。定义2.1.5（常用激活函数）：以下是深度学习中常用的激活函数及其数学定义：（1）Sigmoid函数：（2）双曲正切函数（Tanh）：（3）修正线性单元（ReLU）：（4）GELU激活函数：其中为标准正态分布的累积分布函数，为误差函数。这些激活函数各有特点，将在第三章进行详细的数学分析。这里我们关注激活函数在神经元模型中的整体角色。从数学角度看，激活函数将神经元的净输入映射到输出，完成了从输入空间到输出空间的非线性变换。激活函数的设计需要满足以下几个条件：非线性（保证网络的表达能力）、连续可微（保证梯度计算的可操作性）、数值稳定性（避免数值溢出或下溢）、以及计算效率（满足大规模训练的需求）。从几何视角来看，单个神经元执行的是一个超平面分类（或回归）的操作。理解这一几何本质对于分析神经网络的能力和局限性至关重要。定义2.1.6（决策边界）：对于二分类问题，考虑使用sigmoid激活函数的神经元。决策边界（Decision Boundary）定义为使后验概率等于0.5的输入点集：这是一个超平面，将输入空间划分为两个区域：的区域对应类别1，的区域对应类别0。定理2.1.4（神经元的几何分离能力）：单个神经元（感知机）只能解决线性可分（Linearly Separable）的问题。对于线性不可分的数据集（如异或问题），单个神经元无法找到正确的分类边界。证明：考虑异或（XOR）问题，其真值表为：点和属于类别0，点和属于类别1。任何超平面都只能将平面划分为两个半平面，无法将类别0的点放在一侧而将类别1的点放在另一侧（因为异或问题的正例点位于对角位置）。因此，单个神经元无法解决异或问题。\n<img alt=\"graph/XORScene_ManimCE_v0.19.1.png\" src=\"graph/xorscene_manimce_v0.19.1.png\" target=\"_self\">\n这个定理揭示了单层神经网络的根本局限性：它只能处理线性可分的问题。这一局限性直到多层神经网络（多层感知机）的出现才得到解决——通过在隐藏层引入非线性激活函数，多层神经网络可以学习复杂的非线性决策边界，从而解决异或等线性不可分问题。定义2.1.7（神经元的特征空间映射）：从特征学习的角度来看，神经元可以理解为将输入向量从原始输入空间映射到新的特征空间的变换。这个变换包含两个步骤：首先通过仿射变换进行线性投影，然后通过激活函数进行非线性变换。<br>\n<img alt=\"graph/LinearInseparableScene_ManimCE_v0.19.1.png\" src=\"graph/linearinseparablescene_manimce_v0.19.1.png\" target=\"_self\">数学上，这个特征映射可以表示为复合函数，其中是仿射变换算子。特征空间的维度由神经元的数量决定，不同的神经元学习不同的特征表示，多个神经元的组合形成了一个丰富的特征基。神经元模型不仅可以从几何角度理解，还可以从概率论的角度进行分析。这种视角对于理解神经网络的输出解释和损失函数设计具有重要意义。定义2.1.8（伯努利分布与sigmoid激活）：考虑二分类问题，设为类别标签。假设给定输入时，标签服从伯努利分布（Bernoulli Distribution）：sigmoid函数恰好可以将神经元的净输入映射为概率值：定理2.1.5（sigmoid神经元与伯努利分布的对应）：sigmoid神经元的输出可以解释为给定输入时，类别为1的条件概率。证明：sigmoid函数的输出范围为，满足概率的基本要求。设，则：当时，；当时，。这与概率的边界行为一致。更重要的是，通过选择适当的权重和偏置，sigmoid函数可以拟合任意单调递增的概率函数。这种概率解释在逻辑回归（Logistic Regression）中得到了充分的应用。逻辑回归模型正是使用sigmoid神经元进行二分类，其损失函数为交叉熵损失（Cross-Entropy Loss），这将在第四章详细讨论。定义2.1.9（softmax神经元与多项分布）：对于多分类问题，设为个类别之一。Softmax函数将个神经元的输出归一化为概率分布：定理2.1.6（softmax的数学性质）：Softmax函数具有以下重要性质：（1）输出值均为正且和为1：。（2）平移不变性：对任意常数，有，其中是全1向量。（3）单调性：若​，则。证明：对于性质（1），直接计算：对于性质（2）：对于性质（3），考虑比值：故。softmax函数是大语言模型（如GPT、BERT等）中最常用的输出激活函数，用于将模型的logits输出转换为概率分布。这与第六章讨论的交叉熵损失有着深刻的联系——softmax和交叉熵的组合在数学上等价于最大似然估计。在实际应用中，我们通常需要同时计算多个神经元的输出。这可以通过矩阵运算高效地实现。定义2.1.10（层操作）：设输入矩阵包含个样本，每个样本为维向量。设有个神经元的层，其权重矩阵为，偏置向量为。则该层的输出为：其中是全1向量，是净输入矩阵，是输出矩阵。这种矩阵表示的优势在于它可以利用现代硬件（特别是GPU）进行高度并行化的矩阵运算，大大提高了计算效率。深度学习框架（如PyTorch、TensorFlow）的核心正是通过这种矩阵表示实现高效的神经网络计算。定理2.1.7（矩阵运算与批量处理的等价性）：上述矩阵运算等价于对每个样本单独进行神经元计算后的结果堆叠。证明：设的第行为，的第行为为行向量。则：其中是的第列。这正是单样本仿射变换的定义。因此，矩阵运算一次性处理了所有个样本，结果等价于逐样本计算后按行堆叠。批量处理（Batch Processing）是深度学习训练的标准范式。通过一次矩阵运算同时处理多个样本，GPU的并行计算能力得到充分利用，显著提高了训练效率。同时，批量处理还提供了隐式的正则化效果——在计算损失和梯度时，多个样本的信息被平均，这有助于提高模型的泛化能力。本节系统地阐述了神经元的数学模型，从麦肯罗皮层神经元的二值模型出发，逐步推广到现代深度学习中常用的连续激活神经元。我们建立了仿射变换的数学框架，证明了激活函数对于突破神经网络线性瓶颈的关键作用，并从几何和概率两个角度分析了神经元的行为。本节的内容可以概括为以下几个核心要点：第一，神经元是神经网络的基本计算单元，其数学形式为，包含仿射变换和非线性激活两个组成部分。仿射变换进行线性特征组合，偏置项提供灵活的阈值调节。第二，没有激活函数的神经网络等价于单层线性变换，无法拟合复杂的非线性函数。激活函数通过引入非线性，赋予神经网络万能逼近能力——理论上可以拟合任意连续函数。第三，从几何角度看，单个神经元实现了一个超平面分类器，只能处理线性可分问题。从概率角度看，sigmoid和softmax激活函数的输出可以解释为分类概率，这为损失函数的设计提供了理论基础。第四，神经元的矩阵表示使得批量处理成为可能，这是深度学习高效训练的关键技术基础。理解矩阵运算与逐样本计算的一致性，对于理解深度学习框架的工作原理至关重要。掌握本节的数学基础后，我们将在下一节讨论如何将多个神经元组织成层，以及整个神经网络的矩阵表示形式。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"2.1.1 从生物神经元到数学抽象","level":2,"id":"2.1.1_从生物神经元到数学抽象_0"},{"heading":"2.1.2 感知机与仿射变换","level":2,"id":"2.1.2_感知机与仿射变换_0"},{"heading":"2.1.3 激活函数的数学角色","level":2,"id":"2.1.3_激活函数的数学角色_0"},{"heading":"2.1.4 神经元的几何解释","level":2,"id":"2.1.4_神经元的几何解释_0"},{"heading":"2.1.5 概率视角下的神经元模型","level":2,"id":"2.1.5_概率视角下的神经元模型_0"},{"heading":"2.1.6 神经元模型的矩阵表示","level":2,"id":"2.1.6_神经元模型的矩阵表示_0"},{"heading":"2.1.7 本节小结","level":2,"id":"2.1.7_本节小结_0"}],"links":[],"author":"","coverImageURL":"graph/xorscene_manimce_v0.19.1.png","fullURL":"第2章-前馈网络数学/2.1-神经元的数学模型.html","pathToRoot":"..","attachments":["graph/xorscene_manimce_v0.19.1.png","graph/linearinseparablescene_manimce_v0.19.1.png"],"createdTime":1767062570851,"modifiedTime":1767861363873,"sourceSize":21030,"sourcePath":"第2章 前馈网络数学/2.1 神经元的数学模型.md","exportPath":"第2章-前馈网络数学/2.1-神经元的数学模型.html","showInTree":true,"treeOrder":5,"backlinks":[],"type":"markdown"},"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html":{"title":"2.2 神经网络的矩阵形式","icon":"","description":"在上一节中，我们建立了单个神经元的数学模型，理解了仿射变换与激活函数的组合如何实现从输入到输出的非线性映射。然而，实际的神经网络由大量神经元组成，这些神经元按照层次结构组织。如果我们对每个神经元单独进行数学描述和计算，不仅会使得表达式繁琐冗长，更会错失利用矩阵运算简化计算的机会。本节将系统阐述如何将神经网络表示为矩阵运算的形式，这种表示方法不仅是深度学习框架实现高效计算的理论基础，也是理解反向传播算法的必要前提。单个神经元的能力是有限的。正如我们在4.1节中看到的，单个感知机只能解决线性可分的问题，对于异或等线性不可分的模式无能为力。这一局限性的根本原因在于，单个神经元只形成了一个超平面决策边界，无论权重如何调整，这个决策边界始终是线性的。为了突破这一限制，我们需要组合多个神经元，形成多层网络结构。定义2.2.1（神经网络层）：设有一组神经元，它们共享相同的输入，并各自产生输出​。这组神经元构成神经网络的一个层（Layer），记为：其中为权重矩阵，为偏置向量，为逐元素应用的激活函数，为该层的输出向量（也称为隐藏表示或隐藏激活）。从几何角度看，一层中的个神经元实际上定义了个超平面。这些超平面的组合形成了一个复杂的决策边界，其表达能力远超单个超平面。例如，两个神经元的组合可以解决异或问题：第一个神经元学习\"左上-右下\"的对角线分类，第二个神经元学习\"左下-右上\"的对角线分类，两者的输出再通过某种方式组合，就可以实现异或逻辑。定理2.2.1（两层网络的异或解）：使用两层神经网络可以解决异或问题。证明：考虑一个两层网络，第一层有两个神经元，第二层有一个神经元。设输入为。设计权重和偏置如下：第一层神经元1：权重，偏置，激活函数为阈值函数。\n第一层神经元2：权重，偏置，激活函数为阈值函数。\n第二层：权重，偏置，激活函数为阈值函数。计算各层的输出：\n当时，第一层输出为，第二层输出为0。\n当时，第一层输出为，第二层输出为0。\n当时，第一层输出为，第二层输出为1。\n当时，第一层输出为，第二层输出为1。\n这正是异或函数的输出。这个定理虽然使用了简化的阈值激活函数，但其核心洞见对连续激活函数同样适用：多层网络的表达能力超越了单层网络。通过增加网络的深度，我们可以构建任意复杂的非线性函数——这正是深度学习\"深度\"二字的含义所在。层的概念在神经网络架构设计中具有核心地位。现代神经网络通常由以下类型的层组成：全连接层（Dense Layer或Fully Connected Layer），每一层的每个神经元与前一层的所有神经元相连；卷积层（Convolutional Layer），通过卷积核提取局部特征；循环层（Recurrent Layer），具有记忆功能，用于处理序列数据；注意力层（Attention Layer），通过注意力机制聚合信息。在本节中，我们主要关注全连接层，因为它的数学形式最为清晰，是理解其他层类型的基础。全连接层是神经网络中最基本的层类型。在全连接层中，每个输出神经元与所有输入神经元相连，因此得名\"全连接\"。这种连接模式可以用矩阵乘法简洁地表示。定义2.2.2（全连接层）：设输入向量为，输出向量为​。全连接层的数学定义为：其中为权重矩阵，​为偏置向量，为激活函数，在此处按元素应用于向量。定义2.2.3（权重矩阵的元素含义）：权重矩阵的元素​表示第个输入特征与第个输出神经元之间的连接强度，即：其中行索引对应输出神经元的索引，列索引对应输入特征的索引。定理2.2.2（全连接层的计算展开）：全连接层的矩阵运算等价于对每个输出神经元进行独立的仿射变换计算。证明：将矩阵乘法展开为元素形式。对于输出向量的第个元素：这正是4.1节中定义的单个神经元的数学形式。因此，全连接层的矩阵表示是多个神经元并行计算的紧凑写法。定理2.2.3（齐次坐标形式的单层网络）：使用齐次坐标（Homogeneous Coordinates），全连接层可以统一表示为单一的矩阵乘法。证明：将输入扩展为齐次坐标，将权重矩阵和偏置合并为扩展权重矩阵：则全连接层可以表示为：其中的计算结果与完全相同。齐次坐标表示在理论上具有优雅性，它将仿射变换统一为线性变换的形式。然而在实际实现中，我们通常保持权重矩阵和偏置向量的分离形式，原因在于：第一，偏置向量通常有特殊的初始化策略（如初始化为零），与权重矩阵区别对待；第二，反向传播时对权重和偏置的梯度更新可能使用不同的学习率或正则化系数；第三，现代深度学习框架的优化器（如Adam）可能对权重和偏置维护不同的优化状态。多层神经网络（Multi-Layer Neural Network）由多个全连接层堆叠而成，前一层的输出作为后一层的输入。这种层叠结构通过矩阵运算的复合来表示。定义2.2.4（L层前馈网络）：设有一个层的前馈神经网络，第层的输入为，输出为，其中为网络输入，​为网络输出。则各层的计算定义为：其中，和分别为第层的权重矩阵和偏置向量，为第层的激活函数。定理2.2.4（多层网络的复合函数表示）：层前馈网络的计算可以表示为一系列函数的复合：其中每个层映射​定义为：证明：直接由定义2.2.4的递归形式可得。将到的等式依次代入：$$\\hat{\\mathbf{y}} = \\mathbf{h}^{(L)} = f^{(L)}(f^{(L-1)}(\\cdots f^{(1)}(\\mathbf{x})\\cdots))P(y = k \\mid \\mathbf{x}) = \\frac{e^{z^{(L)}k}}{\\sum{j=1}^{K} e^{z^{(L)}_j}}, \\quad \\mathbf{z}^{(L)} = \\mathbf{W}^{(L)}\\mathbf{h}^{(L-1)} + \\mathbf{b}^{(L)}|\\mathbf{W}|F = \\sqrt{\\sum{i,j} w_{ij}^2} = \\sqrt{\\text{tr}(\\mathbf{W}^T\\mathbf{W})}​zj = \\sum{i=1}^{n{in}} w{ji} x_i + b_j​Var(xi)\\text{Var}(zj) = \\text{Var}\\left(\\sum{i=1}^{n{in}} w{ji} xi\\right) = \\sum{i=1}^{n{in}} \\text{Var}(w{ji}xi) = \\sum{i=1}^{n{in}} \\text{Var}(w{ji})\\text{Var}(x_i)\\text{Var}(zj) = n{in} \\sigma_w^2 \\sigma_x^2​\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{1}_B \\mathbf{b}^T, \\quad \\mathbf{H} = \\phi(\\mathbf{Z})\\mathbf{z}_i = \\mathbf{x}_i \\mathbf{W} + \\mathbf{b}^T\\text{FLOPs}{\\text{forward}} = \\sum{\\ell=1}^{L} \\left(2n{\\ell-1}n\\ell + n_\\ell\\right)\\text{FLOPs}{\\text{single}} = \\sum{\\ell=1}^{L} \\left(2n{\\ell-1}n\\ell + n\\ell\\right) \\approx 2\\sum{\\ell=1}^{L} n{\\ell-1}n\\ell\\text{Params} = \\sum{\\ell=1}^{L} \\left(n{\\ell-1}n\\ell + n\\ell\\right) = \\sum{\\ell=1}^{L} n\\ell(n_{\\ell-1} + 1)\\text{Params}_{\\text{FFN}} \\approx L \\times (n \\times 4n + 4n \\times n) \\approx 96 \\times (12288 \\times 49152 + 49152 \\times 12288) \\approx 1.15 \\times 10^{11}","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"2.2.1 引言：从神经元到层","level":2,"id":"2.2.1_引言：从神经元到层_0"},{"heading":"2.2.2 单层网络的矩阵表示","level":2,"id":"2.2.2_单层网络的矩阵表示_0"},{"heading":"2.2.3 多层网络的矩阵形式","level":2,"id":"2.2.3_多层网络的矩阵形式_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","pathToRoot":"..","attachments":[],"createdTime":1767062570847,"modifiedTime":1768188525931,"sourceSize":19881,"sourcePath":"第2章 前馈网络数学/2.2 神经网络的矩阵形式.md","exportPath":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","showInTree":true,"treeOrder":6,"backlinks":[],"type":"markdown"},"第2章-前馈网络数学/2.3-前向传播的数学本质.html":{"title":"2.3 前向传播的数学本质","icon":"","description":"前向传播（Forward Propagation）是神经网络信息流动的基本方式，它描述了输入数据如何逐层变换，最终产生网络输出的过程。从数学角度来看，前向传播本质上是一个复合函数的求值过程：输入数据依次通过每一层的仿射变换和非线性激活，最终映射到输出空间。理解前向传播的数学本质，不仅有助于我们把握神经网络的工作原理，更能为分析网络的表达能力、训练动态和泛化性能奠定理论基础。本节将从复合函数、链式法则、空间变换等多个数学视角，深入剖析前向传播的内在机制。在2.1节和2.2节中，我们已经从计算单元和结构组织的角度介绍了神经网络的数学描述。前向传播是这些数学描述在实际运行时的具体执行过程。然而，前向传播的意义远不止于\"计算\"本身——它本质上实现了从输入空间到输出空间的函数映射。当我们设计一个神经网络架构并训练其参数时，我们实际上是在寻找一个能够拟合目标函数的数学表达。设有一个层的前馈神经网络，其参数集合为。给定输入​，网络通过前向传播计算输出，其中是由网络参数定义的函数映射。前向传播的核心数学问题可以概括为：给定网络结构和输入，如何高效地计算这个复合函数的输出。这个问题看似简单——只需要按顺序执行各层的矩阵运算即可。但深入理解前向传播的数学本质，我们会发现它蕴含着丰富的理论内涵：它揭示了神经网络如何通过层次化的非线性变换逼近复杂的函数关系，展现了深度表示学习的数学基础，并为理解网络的表达能力和计算特性提供了理论工具。前向传播最本质的数学描述是复合函数（Composite Function）的逐层求值。每一层网络定义了一个从其输入空间到输出空间的映射，多个层的组合则构成了这些映射的复合。定义2.3.1（层映射）：设第层的输入为，输出为​。该层定义的映射​为：其中为权重矩阵，​为偏置向量，为激活函数（按元素应用）。定义2.3.2（复合函数）：设和为两个映射，它们的复合定义为：定理2.3.1（神经网络作为复合函数）：层前馈网络定义的函数​是各层映射的复合：对于任意输入，有：证明：直接由前向传播的计算顺序可得。设，根据层映射的定义，。递推地，，依此类推，直到。这正是复合函数的定义。这个定理揭示了前向传播的数学本质：它是一个复合函数的逐层求值过程。网络的学习目标——调整参数以使逼近目标函数——可以理解为寻找能够最好地逼近目标映射的复合函数形式。定理2.3.2（复合函数的梯度——链式法则）：设，其中和均为可微函数。则对的导数为：或等价地，使用莱布尼茨记号表示为：证明：这是微积分中的基本定理。由导数的定义：利用微分中值定理，存在介于和之间，使得：当时，，故：即​。链式法则在反向传播算法中扮演着核心角色，它使得我们能够高效地计算复合函数对底层变量的梯度。在第2.4节中，我们将看到如何利用链式法则推导出反向传播的梯度计算公式。定义2.3.3（雅可比矩阵）：对于映射，其雅可比矩阵（Jacobian Matrix）定义为：定理2.3.3（复合函数的雅可比矩阵）：设，，则对的雅可比矩阵为：其中矩阵乘法的顺序不可交换。证明：根据矩阵微积分的链式法则，，故：这正是矩阵乘法的定义，即。前向传播不仅是一个代数计算过程，更是一个几何变换过程。从几何角度看，每一层网络执行了一次从输入空间到输出空间的变换，这些变换的复合最终将输入数据映射到所需的输出空间。理解这一几何本质对于分析神经网络的表达能力和特征学习机制至关重要。定义2.3.4（仿射变换的几何意义）：仿射变换在几何上等价于线性变换（由描述）和平移变换（由描述）的复合。线性变换可以分解为旋转、缩放和剪切等基本变换的组合。定理2.3.4（线性变换的奇异值分解）：任意矩阵可以分解为：其中和是正交矩阵，是对角矩阵（其对角元素为非负实数，称为奇异值）。证明：这是线性代数中的基本定理（奇异值分解定理）。矩阵是半正定对称矩阵，可以正交对角化为，其中的列向量是的特征向量，奇异值为对角线上特征值的平方根。则（是的伪逆）可验证为正交矩阵，且满足分解式。奇异值分解揭示了线性变换的几何本质：执行旋转（将输入向量旋转到奇异向量方向），执行缩放（按奇异值拉伸或压缩各方向），执行另一个旋转（将结果旋转到输出空间）。因此，一个仿射变换可以理解为：先将输入向量旋转到\"主轴方向\"，然后在各主轴方向上进行缩放和平移，最后旋转回原坐标系。定义2.3.5（激活函数的几何作用）：激活函数在几何上对仿射变换的结果执行了逐元素的非线性\"切割\"或\"折叠\"操作。以ReLU激活函数为例，将输入空间按超平面划分为两个区域：的区域保持不变，的区域被压缩到零点。定理2.3.5（ReLU激活的几何效果）：ReLU激活函数将空间按个坐标轴分割成个象限（orthant）。在每个象限内，ReLU网络表现为一个线性函数。证明：ReLU对每个坐标独立操作：。对于固定的符号模式（当时可归入任一模式），有，且：因此，对于固定的符号模式，网络输出是输入的线性函数。由于本身是线性的，故也是的线性函数。这个定理揭示了ReLU网络的一个重要性质：尽管整体上是非线性的，但网络在局部区域（由激活模式定义）上表现为线性函数。这与样条函数（Spline Function）的分段线性逼近有相似之处，只是ReLU网络的\"分片\"边界是由数据决定的，而非预先指定的。定义2.3.6（深度网络的特征空间层次）：在层网络中，信息经过次非线性变换后到达输出层。每一层的输出可以视为输入数据在该层定义的\"特征空间\"中的表示。设为第层的输出，则可以理解为在​空间中的表示。定理2.3.6（表示空间的层次结构）：深度网络的表示空间具有层次化的结构：较低层（靠近输入）的表示捕获数据的局部、低级特征（如边缘、纹理），较高层（靠近输出）的表示编码全局、高级语义特征（如物体类别、语句含义）。证明：这一结论虽然难以给出严格的数学证明，但可以从信息论和函数逼近的角度理解。仿射变换本质上是对输入的线性投影，不同的权重矩阵定义了不同的投影方向。浅层网络的线性组合较为简单，只能捕获输入的线性可分特征。随着层数增加，非线性激活使得网络能够学习越来越复杂的特征组合，从而捕获更抽象的语义信息。这一观点与深度学习中的\"特征层次假说\"相一致。<img alt=\"learn_msg.drawio.png\" src=\"graph/learn_msg.drawio.png\" target=\"_self\">从前向传播的数学描述到实际代码实现，需要一种中间表示来桥接理论模型和计算程序。计算图（Computational Graph）是这种中间表示的标准形式，它将数学表达式显式地表示为节点和边的有向无环图。定义2.3.7（计算图）：计算图是一个有向无环图（DAG），其中：节点（Node）表示数学运算（如矩阵乘法、加法、激活函数）；边（Edge）表示数据（如张量、标量）的流动；输入节点表示网络的输入数据；输出节点表示网络的最终输出。例2.3.1（单层网络的计算图）：考虑单层全连接网络。其计算图包含以下节点：输入节点、权重节点、偏置节点；矩阵乘法节点；加法节点；激活节点。定理2.3.7（计算图的前向遍历）：前向传播对应于计算图的拓扑排序（Topological Order）遍历：按从输入到输出的顺序依次计算各节点的值。<br>\n<img alt=\"comp_graph.drawio.png\" src=\"graph/comp_graph.drawio.png\" target=\"_self\">\n证明：计算图是有向无环图，节点的值只依赖于其前驱节点。拓扑排序保证每个节点在其所有前驱节点之后、其后继节点之前被处理。因此，按拓扑排序遍历可以保证计算每个节点时，其所有依赖值已经就绪。 定义2.3.8（自动微分）：自动微分（Automatic Differentiation）是一种利用计算图高效计算导数的技术。与数值微分（利用有限差分近似）和符号微分（利用代数规则求导）不同，自动微分利用链式法则和计算图的结构，可以精确地计算任意复杂函数的导数。自动微分分为前向模式（Forward Mode）和反向模式（Reverse Mode）两种。反向模式自动微分是深度学习中反向传播算法的理论基础，我们将在第2.4节详细讨论。前向模式自动微分虽然计算效率较低，但在某些场景（如Jacobian矩阵计算）中有其独特优势。定理2.3.8（前向模式的微分计算）：在前向模式自动微分中，我们伴随值（Adjoint Value）表示​，并按与前向传播相同的顺序计算。证明：考虑函数。设的伴随值为，中间变量的伴随值为。根据链式法则​，前向模式计算​。对于复合函数，这一过程可以与前向传播并行进行。定理2.3.9（反向模式的微分计算）：在反向模式自动微分中，我们计算每个节点对其直接后继的梯度，并按与前向传播相反的顺序传播这些梯度。证明：反向模式的核心是链式法则​。设​为后向传播的中间值，则的梯度为​。按后向顺序计算可以保证每次计算时所需的依赖值已经就绪。表2-2：前向模式与反向模式自动微分的比较理解前向传播的计算复杂度对于评估网络效率、设计模型架构和优化计算资源至关重要。定义2.3.9（浮点运算数FLOPs）：浮点运算数（Floating Point Operations）是衡量计算复杂度的标准指标。一次乘法或一次加法计为一次FLOPs。定理2.3.10（单层前向传播的FLOPs）：对于输入​、权重、偏置的全连接层，单次前向传播的FLOPs为：其中来自矩阵乘法（每个输出元素需要次乘法和​次加法），​来自偏置加法。证明：考虑单个输出元素​（第个样本的第个输出）：这个求和需要次乘法和次加法，总计约​次FLOPs。个样本、​个输出元素的总FLOPs为​（偏置加法），即。推论2.3.1（多层网络的FLOPs）：对于层全连接网络，总FLOPs为各层FLOPs之和：例2.3.2（GPT规模模型的FLOPs估算）：考虑一个简化的Transformer层，包含注意力机制和前馈网络。设隐藏维度，注意力头数为32（每头维度），前馈扩展维度4d=16384。单层Transformer的FLOPs约为：\n注意力计算：（为序列长度）\n前馈网络：\n对于GPT-3规模的模型（层，），单次前向传播的FLOPs约为量级，需要大量GPU并行计算才能在合理时间内完成。现代GPU和TPU等硬件加速器擅长并行计算。前向传播的矩阵运算形式天然具有良好的并行性，可以充分利用硬件的并行计算能力。定理2.3.11（样本级并行）：对于批量输入，不同样本之间的计算是相互独立的，可以完全并行执行。证明：考虑全连接层的计算。的第行仅依赖于的第行和，与其他行无关。因此，个样本的计算可以分解为个独立的子计算。定理2.3.12（层内并行）：在同一层内，不同输出神经元（同一batch内）的计算也是并行的。证明：的第列仅依赖于的第列和的所有列。不同列之间的计算不相互依赖，可以并行执行。定理2.3.13（模型并行）：对于超大规模模型，可以将权重矩阵按列（或行）分割到多个计算设备上，实现模型并行。证明：将按列分割，则。每个子矩阵​可以分配给不同的设备，计算​后合并结果。这种分割不会影响计算的数学正确性。表4-3：前向传播的并行性层次本节从多个数学视角深入分析了前向传播的本质。前向传播在数学上是一个复合函数的逐层求值过程，它将输入数据依次通过层非线性变换，最终映射到输出空间。本节的核心内容可以概括为以下几点：第一，前向传播的本质是复合函数的求值。神经网络可以表示为各层映射的复合，链式法则是分析这一复合过程的核心数学工具。第二，前向传播实现了从输入空间到输出空间的几何变换。仿射变换负责旋转、缩放和平移，激活函数负责非线性切割，两者的组合使得网络能够学习复杂的非线性决策边界。第三，计算图是连接数学理论和工程实现的桥梁。通过构建计算图，可以系统地组织前向传播的计算步骤，并为反向传播的梯度计算提供数据结构支持。第四，前向传播具有良好的并行性，可以从样本级、神经元级、矩阵运算级和模型级等多个层次进行并行优化。这种并行性是深度学习在大规模数据和模型上高效训练的基础。理解前向传播的数学本质，为我们进一步学习反向传播算法（第2.4节）和其他深度学习高级主题奠定了坚实的理论基础。下一节我们将讨论损失函数与优化目标，为理解反向传播提供必要的背景知识。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"2.3.1 引言：从计算到函数逼近","level":2,"id":"2.3.1_引言：从计算到函数逼近_0"},{"heading":"2.3.2 复合函数视角下的前向传播","level":2,"id":"2.3.2_复合函数视角下的前向传播_0"},{"heading":"2.3.3 信息流动的几何描述","level":2,"id":"2.3.3_信息流动的几何描述_0"},{"heading":"2.3.4 计算图的表示与实现","level":2,"id":"2.3.4_计算图的表示与实现_0"},{"heading":"2.3.5 前向传播的计算复杂度","level":2,"id":"2.3.5_前向传播的计算复杂度_0"},{"heading":"2.3.6 前向传播的并行性","level":2,"id":"2.3.6_前向传播的并行性_0"},{"heading":"2.3.7 本节小结","level":2,"id":"2.3.7_本节小结_0"}],"links":[],"author":"","coverImageURL":"graph/learn_msg.drawio.png","fullURL":"第2章-前馈网络数学/2.3-前向传播的数学本质.html","pathToRoot":"..","attachments":["graph/learn_msg.drawio.png","graph/comp_graph.drawio.png"],"createdTime":1767062570843,"modifiedTime":1768199986514,"sourceSize":21005,"sourcePath":"第2章 前馈网络数学/2.3 前向传播的数学本质.md","exportPath":"第2章-前馈网络数学/2.3-前向传播的数学本质.html","showInTree":true,"treeOrder":7,"backlinks":[],"type":"markdown"},"第2章-前馈网络数学/2.4-反向传播梯度推导.html":{"title":"2.4 反向传播梯度推导","icon":"","description":"反向传播（Backpropagation）是深度学习中最核心的算法之一，它使得大规模神经网络的端到端训练成为可能。从数学角度来看，反向传播本质上是链式法则在复合函数梯度计算中的高效应用。1986年，杰弗里·辛顿（Geoffrey Hinton）、大卫·鲁梅尔哈特（David Rumelhart）和罗纳德·威廉姆斯（Ronald Williams）在《自然》杂志上发表了关于反向传播算法的开创性论文，证明了通过错误信号的反向传播可以有效训练多层神经网络，从而开启了深度学习的现代时代。本节将从数学推导的角度，系统阐述反向传播算法的完整理论框架。我们将从链式法则的矩阵形式出发，逐步推导出各层参数的梯度计算公式，并分析反向传播的计算复杂度。最后，我们将讨论反向传播与自动微分的关系，以及实际实现中的数值稳定性问题。在2.3节中，我们讨论了前向传播的数学本质：输入数据通过多层非线性变换，最终产生网络输出。然而，前向传播只是完成了\"推理\"的过程——它告诉我们给定当前参数下网络的输出是什么，但并没有告诉我们如何调整参数以改善输出。神经网络的训练目标是最小化某个损失函数，其中是网络预测，是真实标签。参数优化需要计算损失函数对各参数的梯度：由于神经网络是一个深度复合函数，直接对复合函数求导会导致计算量爆炸。反向传播通过巧妙地利用链式法则和动态规划思想，将梯度计算的时间复杂度从指数级降低到线性级，使得训练深层网络成为可能。问题设定：设有一个层前馈网络，第层的计算定义为：其中为输入，​为输出。损失函数度量预测与真实标签之间的差异。我们的目标是计算和对所有层。反向传播的核心数学工具是链式法则。在多层神经网络中，我们需要将链式法则从标量情形推广到向量和矩阵情形。定理2.4.1（标量链式法则）：设，，则：证明：这是微积分基本定理的直接推论，已在2.3节中证明。定义2.4.1（梯度与雅可比矩阵）：设标量函数，其梯度为行向量：对于向量值函数，其雅可比矩阵为：定理2.4.2（向量链式法则）：设，，则：其中矩阵乘法的顺序不可交换。证明：考虑的第个分量​和的第个分量​：这正是矩阵乘法的定义。定理2.4.3（标量对矩阵的链式法则）：设为标量损失函数，，其中为矩阵变量。则对的梯度为：其中假设，为输入向量。证明：考虑梯度矩阵的元素​。由链式法则：其中​，故（为克罗内克函数）。代入得：将所有元素写为矩阵形式，即。这个定理是反向传播中权重梯度计算的核心公式。它告诉我们：损失函数对权重的梯度可以分解为损失函数对净输入的梯度（称为\"误差信号\"或\"梯度\"）与输入向量的外积。在反向传播中，我们定义一个关键中间变量——误差信号（Error Signal），它表示损失函数对各层净输入的梯度。这个变量在反向传播过程中起着桥梁作用，将输出层的损失信息传递到输入层。定义2.4.2（第层误差信号）：设​为第层的误差信号，即损失函数对该层净输入的梯度。误差信号包含了损失函数对该层激活值变化的敏感程度信息。通过理解误差信号的传播机制，我们可以清楚地看到损失信息如何从输出层反向流动到输入层。定理2.4.4（输出层误差信号）：对于输出层（），误差信号为：其中表示逐元素乘法（Hadamard积），为第层激活函数的导数。证明：由链式法则和雅可比矩阵的性质：由于是逐元素函数，雅可比矩阵​是对角矩阵，其对角元素为。因此，矩阵乘法退化为逐元素乘法：推论2.4.1（常见输出层误差信号）：对于不同的输出层配置，误差信号有不同的简化形式：（1）回归任务（恒等激活）：（均方误差损失）。（2）二分类任务（Sigmoid激活）：（交叉熵损失）。（3）多分类任务（Softmax激活）：（交叉熵损失），其中​是概率分布。证明：以二分类为例，损失函数为，其中。计算梯度：代入并化简，可得。多分类情况的证明类似。这个推论揭示了一个重要的简化：对于常见的分类和回归任务，使用交叉熵或均方误差损失时，输出层误差信号与预测误差成正比。这一性质大大简化了反向传播的实现。定理2.4.5（隐藏层误差信号传播）：对于隐藏层（），误差信号从前一层传播到当前层：证明：根据误差信号的定义和链式法则：第一项为。第二项中，，故。第三项为对角矩阵。因此：注意矩阵乘法的方向：，​，故的维度为​。写成向量形式即：这个定理揭示了反向传播的本质机制：误差信号从输出层向输入层逐层传播，每一层通过权重矩阵的转置将误差\"路由\"到对应的输入维度，然后通过激活函数的导数进行缩放。如果激活函数的导数接近零（饱和区域），误差信号会被衰减，导致梯度消失。在获得各层误差信号后，我们可以计算损失函数对各层参数的梯度。这是反向传播的最后一步，也是参数更新的直接输入。定理2.4.6（权重梯度）：第层权重的梯度为：证明：根据定理2.4.3的直接应用，有：其中是行向量还是列向量需要注意维度匹配。定理2.4.7（偏置梯度）：第层偏置的梯度为：证明：偏置项直接加到净输入上，故：因为（克罗内克函数）。因此，梯度向量就是误差信号本身。算法2.4.1（反向传播算法）：综合以上结果，反向传播算法可以描述为：输入：网络参数​，前向传播保存的中间值，损失函数，真实标签。输出：各层参数的梯度​。步骤：（1）前向传播：执行完整的前向传播，保存所有层的和。（2）输出层误差：计算（使用推论2.4.1的简化形式）。（3）反向传播循环：对：\n计算权重梯度：\n计算偏置梯度：\n若，计算前一层误差：\n（4）返回：所有参数的梯度。\n<img alt=\"backward.drawio.png\" src=\"graph/backward.drawio.png\" target=\"_self\">\n这个算法清晰地展示了反向传播的执行流程：先通过前向传播保存必要的中间值，然后从输出层开始，逐层计算误差信号和参数梯度，最后将梯度用于参数更新。在实际训练中，我们通常使用小批量随机梯度下降（Mini-Batch Stochastic Gradient Descent）。这需要对上述单样本梯度计算进行扩展，以处理批量输入。定义2.4.3（批量前向传播）：设批量输入，批量标签​（或对于分类任务）。批量损失通常定义为各样本损失的平均值：定理2.4.8（批量误差信号）：对于批量输入，第层的误差信号矩阵为，其第行为样本的误差信号​。输出层误差为：其中是批量预测矩阵。证明：批量损失对净输入的梯度为：因此，误差信号矩阵需要除以批量大小。对于常见的均方误差和交叉熵损失，输出层误差简化为预测误差的平均值。定理2.4.9（批量权重梯度）：批量情况下，第层权重的梯度为：其中是批量隐藏激活矩阵。证明：考虑批量中每个样本的权重梯度：将求和写为矩阵形式，即为。推论2.4.2（批量偏置梯度）：批量情况下，第层偏置的梯度为：即误差信号矩阵在批量维度上的平均值。理解反向传播的计算复杂度对于评估训练效率和设计高效算法至关重要。定理2.4.10（反向传播的FLOPs）：对于层全连接网络，单次反向传播的浮点运算数为：证明：反向传播的主要计算包括两部分：（1）误差信号传播：对于每层，计算。矩阵乘法的FLOPs约为，逐元素乘法约为​。（2）参数梯度计算：计算的FLOPs约为​，计算偏置梯度约为​。将各层求和，反向传播的总FLOPs约为前向传播的两倍。表4-4：前向传播与反向传播的FLOPs比较推论2.4.3（训练迭代的总复杂度）：每次训练迭代（包含前向传播、反向传播和参数更新）的总FLOPs约为前向传播的4倍：一次前向传播加上一次反向传播（约为2倍前向传播），再加上参数更新的少量计算。反向传播是自动微分（Automatic Differentiation）的一种特殊情况。理解这一关系有助于我们从更高的视角理解梯度的计算原理。定义2.4.4（自动微分的分类）：自动微分主要分为两种模式：（1）前向模式（Forward Mode）：沿计算图的前向方向累积计算导数。（2）反向模式（Reverse Mode）：沿计算图的反向方向累积计算导数。定理2.4.11（反向传播是反向模式自动微分）：反向传播算法对应于反向模式自动微分在神经网络中的应用。反向模式自动微分从输出节点开始，沿计算图的边反向传播梯度。对于神经网络，计算图是前向传播构建的 DAG，反向传播正是沿此图的反向边计算各节点对损失的梯度。这与反向模式自动微分的定义完全一致。表4-5：前向模式与反向模式的比较定义2.4.5（梯度检查）：梯度检查（Gradient Checking）是一种验证反向传播实现正确性的数值方法。其基本思想是用有限差分近似计算梯度，并与反向传播计算的梯度进行比较：如果两种方法计算的梯度足够接近（相对误差小于），则可以认为反向传播实现正确。在实际实现反向传播时，需要特别注意数值稳定性问题。深度神经网络的训练涉及大量的梯度计算和参数更新，不当的实现可能导致数值溢出、梯度消失或梯度爆炸等问题。定义2.4.6（梯度裁剪）：梯度裁剪（Gradient Clipping）是一种防止梯度爆炸的技术。常见的裁剪方式包括：（1）按值裁剪：对梯度向量逐元素限制在范围内：（2）按范数裁剪：如果梯度范数超过阈值，则按比例缩放：定理2.4.12（梯度裁剪的边界效应）：梯度裁剪将梯度范数限制在范围内，确保参数更新不会过大。证明：对于按范数裁剪，若，则；若，则。因此，裁剪后的梯度范数始终不超过。定义2.4.7（混合精度训练中的梯度缩放）：在混合精度训练中，梯度可能因精度限制而下溢。梯度缩放（Gradient Scaling）通过在反向传播时放大损失值来间接放大梯度，从而避免下溢：其中是缩放因子，是缩放后损失的梯度。表4-6：常见的数值稳定性问题与解决方案本节系统阐述了反向传播算法的数学理论基础，从链式法则的矩阵形式出发，逐步推导出完整的梯度计算公式。本节的核心内容可以概括为以下几点：第一，反向传播的核心是链式法则的高效应用。通过定义误差信号，我们将复杂的复合函数求导问题分解为误差信号的前向传播和参数梯度的局部计算两个步骤。第二，误差信号从前向传播的最后一层（输出层）开始，通过公式逐层传播到输入层。这一过程实现了损失信息从输出到输入的逆向流动。第三，参数梯度可以通过误差信号与前向传播保存的激活值计算：权重梯度为，偏置梯度为。第四，反向传播的计算复杂度约为前向传播的两倍，但这是实现高效梯度计算的必要代价。通过批量处理，可以将多个样本的梯度计算合并为矩阵运算，充分利用硬件并行性。第五，数值稳定性是反向传播实现中的关键考虑。梯度裁剪、混合精度训练、梯度缩放等技术都是保证大规模神经网络训练稳定性的重要手段。掌握本节的数学基础后，读者应能够理解反向传播的工作原理，并有能力实现自己的深度学习框架或深入理解现有框架的底层机制。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"2.4.1 引言：为什么需要反向传播","level":2,"id":"2.4.1_引言：为什么需要反向传播_0"},{"heading":"2.4.2 链式法则的矩阵形式","level":2,"id":"2.4.2_链式法则的矩阵形式_0"},{"heading":"2.4.3 误差信号的传播","level":2,"id":"2.4.3_误差信号的传播_0"},{"heading":"2.4.4 参数梯度的完整推导","level":2,"id":"2.4.4_参数梯度的完整推导_0"},{"heading":"2.4.5 批量处理的梯度计算","level":2,"id":"2.4.5_批量处理的梯度计算_0"},{"heading":"2.4.6 计算复杂度分析","level":2,"id":"2.4.6_计算复杂度分析_0"},{"heading":"2.4.7 反向传播与自动微分的关系","level":2,"id":"2.4.7_反向传播与自动微分的关系_0"},{"heading":"2.4.8 数值稳定性与实现细节","level":2,"id":"2.4.8_数值稳定性与实现细节_0"},{"heading":"2.4.9 本节小结","level":2,"id":"2.4.9_本节小结_0"}],"links":[],"author":"","coverImageURL":"graph/backward.drawio.png","fullURL":"第2章-前馈网络数学/2.4-反向传播梯度推导.html","pathToRoot":"..","attachments":["graph/backward.drawio.png"],"createdTime":1767062570838,"modifiedTime":1768202108194,"sourceSize":23952,"sourcePath":"第2章 前馈网络数学/2.4 反向传播梯度推导.md","exportPath":"第2章-前馈网络数学/2.4-反向传播梯度推导.html","showInTree":true,"treeOrder":8,"backlinks":["大模型中的数学.html"],"type":"markdown"},"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html":{"title":"3.1 激活函数的数学角色","icon":"","description":"激活函数是神经网络能够拟合复杂非线性函数的核心数学组件。在线性模型中，无论堆叠多少层神经元，网络的表达能力始终局限于输入的线性变换——这正是\"万有逼近定理\"所揭示的深刻数学原理。激活函数的引入打破了这种线性局限，使得神经网络能够表示任意复杂的非线性映射关系。本节将从数学的角度系统性地分析激活函数的理论基础、概率解释以及信息论意义，为理解大语言模型中非线性变换的数学本质奠定坚实基础。神经网络之所以能够拟合任意复杂的函数模式，其根本原因在于激活函数引入的非线性变换。为了深刻理解这一数学本质，我们需要从线性映射的局限性出发，逐步揭示非线性变换的必要性和数学价值。这种分析不仅具有理论意义，也为理解Transformer架构中非线性组件的设计提供了数学基础。考虑一个没有激活函数的浅层神经网络，其数学表示可以清晰地揭示线性变换的本质局限性。设输入向量为，网络包含一个隐藏层，该隐藏层有个神经元，输出层有个神经元。则网络的前向传播可以表示为两个连续的线性变换：其中是输入层到隐藏层的权重矩阵，是相应的偏置向量，是隐藏层到输出层的权重矩阵，是输出层的偏置向量。通过矩阵乘法的结合律和分配律，上述表达式可以简化为：其中是两个权重矩阵的乘积，是组合偏置向量。这个结果表明，即使网络包含多个隐藏层，只要不使用激活函数，整个网络仍然等价于一个简单的线性变换。这个数学事实揭示了一个深刻的原理：线性变换的复合仍然是线性变换。定义3.1.1（线性映射） 一个映射是线性的，当且仅当对于任意向量和任意标量，满足以下两个条件：这个定义包含了线性映射的两个核心性质：\n齐次性（Homogeneity）\n可加性（Additivity）。\n线性映射可以用矩阵乘法完全描述，即，其中是变换矩阵，是平移向量。\n从几何角度理解，线性映射描述了输入空间通过旋转、缩放和反射（由矩阵描述）变换到输出空间的过程，然后通过平移（由向量描述）调整位置。这种变换保持了几何结构的许多性质：直线映射为直线，平行线保持平行，原点保持不动（除非有平移分量）。然而，正是这些保持的性质限制了线性映射的表达能力——它无法表示复杂的曲线、曲面或更复杂的几何结构。线性模型虽然具有优美的数学性质，如凸优化问题和解析解的存在性，但其表达能力受到根本性的限制。设真实的数据生成过程涉及两个变量的交互效应，例如​，这是一个简单的乘法交互。线性模型只能学习形如的函数，完全无法捕获和之间的乘积交互。这种局限性在处理现实世界的复杂数据时尤为突出，因为语言、图像等数据中充满了非线性模式。考虑一个二维空间中的分类问题，数据分布为两个交织的螺旋形状。要将这两个类别分开，需要一条非线性的决策边界——可能是曲线、折线或更复杂的形状。线性分类器（如逻辑回归、支持向量机）只能学习直线或超平面作为决策边界，对于这种复杂的分布，无论训练多长时间、调整多少超参数，都无法达到良好的分类效果。这个例子直观地说明了线性模型的表达能力边界。从数学上分析，线性模型的学习能力可以用函数空间的维度来刻画。设输入空间为，线性模型的函数空间是所有仿射函数的集合，其维度为（个权重加 1 个偏置）。无论训练数据有多少，只要模型结构固定（线性），其能表示的函数就被限制在这个有限维的函数空间中。当真实的数据生成过程超出这个空间时，线性模型必然产生系统性误差。激活函数的引入彻底改变了这一局面。设隐藏层的输出为，其中是非线性激活函数。此时网络的输出为：由于是非线性函数，不再能够简化为的形式。网络现在是分段线性或非线性的，其表达能力不再受限于线性变换的复合。这种非线性的引入使得神经网络能够学习任意复杂的输入-输出映射，从简单的曲线到高维空间中的复杂流形。与第五章注意力的联系：在Transformer的自注意力机制中，Query、Key、Value的投影计算是线性的，但Softmax激活函数引入了关键的非线性。注意力权重的计算公式为，Softmax的非线性确保了注意力权重具有归一化的概率解释，同时使得注意力机制能够学习非线性的Token交互模式。没有Softmax的线性归一化将无法实现注意力的\"软选择\"功能，模型将退化为简单的加权平均，无法捕获Token之间的复杂依赖关系。从数学角度看，激活函数将线性变换的输出通过一个非线性函数进行变换，然后在下一层再次进行线性变换。这种\"线性-非线性-线性-非线性\"的交替结构是深度学习成功的关键数学基础。每一次非线性变换都扩展了网络能够表示的函数空间，多层堆叠使得网络能够表示极其复杂的函数。定义3.1.2（逐元素非线性变换） 设激活函数逐元素应用于向量，则输出向量为：这种逐元素应用的方式保持了向量结构，同时对每个维度施加了非线性变换。在反向传播中，这种结构使得梯度计算可以高效地逐元素进行。神经网络之所以被称为\"通用逼近器\"，是因为在适当条件下，单层神经网络可以逼近任意连续函数到任意精度。激活函数在这一逼近能力中扮演着核心角色——正是非线性激活函数赋予了网络这种强大的表达能力。本节将从函数逼近论的角度，深入分析激活函数的数学角色，揭示其作为\"函数逼近基石\"的理论基础。神经网络通用逼近能力的核心定理表明，只要激活函数满足一定的数学条件，单隐藏层神经网络就可以以任意精度逼近任意连续函数。这是神经网络理论中最深刻的结果之一，它从数学上证明了深度学习的表达能力基础。定理3.1.1（通用逼近定理） 设是任意非常数的有界连续函数，是紧集（封闭有界集合）。对于任意连续函数和任意，存在一个单隐藏层神经网络：其中是输入权重向量，是偏置，是输出权重，使得：这个定理的数学意义极其深远。它指出：只要激活函数是\"非平凡\"的（非常数），就具有通用逼近能力。这意味着Sigmoid、Tanh、ReLU、GELU等激活函数在理论上都具有同等的表达能力——它们都可以作为通用逼近器的核心组件。定理的证明思路基于Stone-Weierstrass逼近定理。Stone-Weierstrass定理指出，如果一组函数满足以下条件：\n(1) 包含所有常数函数，\n(2) 对加法和乘法封闭，\n(3) 能区分任意两点，则它们的线性组合可以逼近任意连续函数。\n激活函数构成的函数族在适当条件下可以满足这些要求，从而通过Stone-Weierstrass定理推导出通用逼近能力。通用逼近定理的核心数学洞见在于：非线性激活函数的\"非单调性\"或\"有界振荡性\"使得函数族能够在输入空间中形成\"局部基函数\"。每个神经元 可以被视为一个\"局部探测器\"——它在输入空间的某个区域激活，在其他区域抑制。大量这样的局部探测器通过线性组合，可以精确重构任意复杂的连续函数模式。从几何角度理解，每个激活函数在输入空间中定义了一个\"超平面\" 。这个超平面将输入空间分为两个半空间：在一侧，激活函数的输出值较高；在另一侧，输出值较低。因此，每个神经元本质上是在输入空间中划分一个\"决策边界\"，将空间分为\"激活区\"和\"抑制区\"。多个这样的超平面组合，可以形成复杂的决策边界，从而逼近任意形状的函数曲面。考虑一个简单的例子：使用Sigmoid函数作为激活函数。每个Sigmoid神经元在输入空间中形成一个\"S形\"曲面。当多个这样的曲面进行线性组合时，可以通过调整每个曲面的位置、方向和幅度来构造任意复杂的曲面。这种\"局部曲面叠加\"的能力正是通用逼近能力的几何解释。激活函数类型与逼近效率：通用逼近定理指出\"任意非常数有界连续函数\"都可以作为激活函数，但不同激活函数的逼近效率是不同的。对于相同的逼近精度，使用\"表达能力更强\"的激活函数可能需要更少的神经元。历史上，常用的激活函数如Sigmoid和Tanh被选择是因为它们具有良好的数学性质（可微、有界、光滑），但近年来ReLU的流行表明，简单性有时比数学上的\"优雅\"更有价值。ReLU虽然数学形式简单（分段线性），但其分段线性的性质使得它在大规模网络中表现优异，因为它的稀疏激活特性和线性区域的恒定导数有利于深层网络的训练。通用逼近定理表明，足够宽的单层网络可以逼近任意函数。然而，在实践中，我们通常使用较窄但较深的网络来达到相同的表达能力。这种\"深度优先\"的设计选择背后有深刻的数学原因，理解这些原因对于理解现代大语言模型的架构设计至关重要。在高维输入空间中，若目标函数具有组合（compositional）或层次化结构，则深度神经网络在表示效率上可能显著优于浅层网络。已有理论结果表明，对于某些函数类，浅层网络需要指数级数量的神经元才能逼近，而深度网络仅需多项式规模。从直观上看，深度网络通过逐层的非线性变换，可以学习从低级局部模式到高级抽象语义的层次化表示。这种结构与许多自然数据（如图像、语音、语言）中的生成机制相契合，因此在实践中表现出更强的表示能力和泛化性能。从数学上分析，深度网络能够表示的函数空间远大于同等参数量的浅层网络。设一个深度为、宽度为的全连接网络，其参数数量为（考虑权重和偏置）。研究表明，深度为的网络可以等效于宽度为指数级别的浅层网络，即。这就是著名的\"深度指数优势\"——深度网络用指数级更少的参数可以达到与浅层网络相同的表达能力。与Transformer架构的联系：Transformer采用了深度网络的架构设计，其注意力层和前馈层堆叠形成深层结构。每个注意力层学习Token之间的交互模式，不同层的注意力可能关注不同类型的关系（如词汇关系、句法关系、语义关系）。这种层次化的注意力学习是大语言模型能力的重要来源。激活函数（如GELU）在每一层提供必要的非线性变换，使得注意力模式可以逐层组合，形成更复杂的Token交互模式。定义3.1.3（层次化特征学习） 设是第层的表示向量，则层次化特征学习可以表示为：其中每一层的变换和激活将前一层的表示转换为新的表示。理想的层次化学习应该满足：捕获比更抽象、更语义化的特征。这种抽象化过程正是深度学习成功的关键数学机制。从概率论的角度，许多激活函数可以解释为某种概率分布的参数变换。这种概率解释不仅加深了我们对激活函数数学本质的理解，也为设计新的激活函数和理解训练动态提供了理论指导。本节将系统性地分析主要激活函数的概率解释，揭示它们与统计学和信息论之间的深层联系。Sigmoid函数将实数域映射到区间，这正好是概率值的取值范围。这种映射关系使得Sigmoid函数在概率建模中具有直接的应用价值。在伯努利分布中，成功概率的对数几率（Log-Odds）定义为​。Sigmoid函数的逆函数正是对数几率的变换：这个恒等式可以从代数上验证。将代入对数几率的表达式：因此，Sigmoid函数可以将任意实数解释为伯努利分布的成功概率。在神经网络中，Sigmoid输出常用于二分类任务的概率预测——网络的原始输出（logits）经过Sigmoid变换后，得到样本属于正类的概率估计。定义3.1.4（Logits与概率的变换） 设网络的原始输出为（称为logit），则对应的概率为。这个变换的逆变换为​。Logits可以理解为\"对数几率\"，它是一个无界的实数，可以取任意值；概率是归一化后的值，限制在区间内。从贝叶斯推断的角度，Sigmoid变换可以理解为将先验信息（用logits表示）转换为后验概率的过程。在逻辑回归中，参数的后验分布与似然函数和先验的乘积成正比。Sigmoid函数将线性组合转换为概率，完成了从线性预测到概率预测的关键一步。Softmax函数是Sigmoid在多类别情况下的推广，它将维向量映射到概率单纯形。这与多项式分布的参数空间完全一致。在多项式分布中，概率参数的对数几率为（以类别为基准）。Softmax变换正是这种对数几率的指数化和归一化。定义3.1.5（Softmax函数与概率单纯形） 对于输入向量，Softmax函数定义为：Softmax的输出满足概率分布的所有公理：对所有成立，且。因此 是概率单纯形中的点。从多项式分布的角度，Softmax输出的概率表示样本属于第类的概率估计。多项式分布的似然函数为：其中是网络的原始输出（logits）。对数似然为：这个表达式在分类任务的训练中至关重要，它正是交叉熵损失的核心组成部分。与第四章损失函数的联系：概率解释为损失函数的设计提供了坚实的理论基础。第四章详细讨论的交叉熵损失正是衡量两个概率分布（真实分布和预测分布）差异的信息论度量。当预测分布 由Softmax给出时，交叉熵损失具有清晰的概率解释——它是负对数似然的期望。激活函数的概率解释与损失函数的信息论基础共同构成了分类任务训练的数学框架。在需要从离散分布采样的场景中（如变分自编码器的离散潜在变量、策略网络的离散动作选择），Gumbel-Softmax提供了一种可微的采样近似。这种技术使得离散随机变量的重参数化成为可能，极大地促进了离散分布表示学习的发展。定义3.1.6（Gumbel-Softmax分布） 设是 logits，Gumbel-Softmax样本定义为：其中是Gumbel噪声，是独立同分布的均匀随机变量，是温度参数。Gumbel-Softmax的数学性质由温度参数控制。当时，中最大值的指数增长占主导，Gumbel-Softmax趋向于one-hot采样（对应Hardmax）；当时，所有指数项趋向于1，Gumbel-Softmax趋向于均匀分布。温度参数提供了从\"软\"（接近均匀分布）到\"硬\"（接近one-hot采样）的连续调节能力。从概率论角度，Gumbel-Softmax是Gumbel分布和Softmax分布的结合。Gumbel分布是极值分布的一种，用于模拟独立随机变量序列的最大值分布。通过向logits添加Gumbel噪声并进行Softmax变换，我们得到了一个可以近似离散采样的连续分布。这个分布在重参数化梯度计算中扮演关键角色：样本可以表示为确定函数，其中是随机噪声源，因此 可以通过采样来估计。从信息论的角度，激活函数可以被理解为对输入信息的某种非线性编码和变换。这种视角揭示了激活函数在信息处理中的深层作用，也为分析神经网络的信息流和表示学习提供了数学工具。信息论提供了量化信息、熵和互信息的数学框架，将这些概念应用于激活函数分析，可以揭示其信息处理能力的本质。许多激活函数（如Sigmoid、Tanh）将实数轴压缩到有限的区间或。这种压缩本质上是信息的有损编码——输入的无限精度被截断为有限范围的输出。然而，这种有损压缩在机器学习中是有益的：它提供了数值稳定性，限制了输出值的范围，使得不同层之间的信息传递更加可控。定义3.1.7（互信息） 两个随机变量和之间的互信息定义为：互信息衡量的是知道后关于的信息量（反之亦然），它度量了和之间的依赖程度。互信息是非负的，当且仅当和独立时互信息为零。从信息论角度，激活函数可以视为一种信息瓶颈（Information Bottleneck）。设输入为，经过激活函数的输出为。信息瓶颈理论认为，良好的表示应该最大化（保留关于目标的信息），同时最小化（去除关于输入的冗余信息）。激活函数的非线性变换正是实现这种信息筛选的机制。ReLU的信息选择性：ReLU函数 的行为类似于信息选择器——它保留正信息（），完全抑制负信息（）。从信息论角度，ReLU可以被解释为一种\"阈值化\"操作：只有超过阈值的信号才能通过。这种选择性与生物神经元的行为类似——生物神经元在膜电位超过阈值时发放动作电位。ReLU的稀疏激活特性（在任何输入下，约有一半的神经元输出为零）使得网络具有稀疏表示的能力。稀疏表示在信息论上具有几个优势：首先，稀疏编码通常更高效地利用参数——少量的活跃神经元可以编码大量的信息；其次，稀疏表示对噪声更加鲁棒——单个噪声样本不太可能同时激活多个稀疏神经元；第三，稀疏表示提供了更好的可解释性——活跃的神经元可以被解释为检测到特定模式的\"探测器\"。在自监督学习中，神经网络的目标通常是最大化输入不同部分之间的互信息。激活函数在这个过程中扮演关键角色，它们决定了信息如何被编码和传递。考虑一个简单的互信息估计问题：给定输入和其上下文（如同一序列中相邻的Token），我们希望最大化。神经网络通过编码器和分别编码和，然后使用激活函数（如ReLU或GELU）处理编码向量，最后通过对比损失（如InfoNCE，参见第四章）估计和最大化互信息。激活函数的非线性变换增加了编码的丰富性，使得和能够捕获输入之间的复杂依赖关系。定理3.1.2（互信息的下界估计） InfoNCE损失提供了互信息的下界估计：其中是负样本的数量。这个不等式表明，最小化InfoNCE损失等价于最大化互信息的下界。激活函数的选择影响这个下界的紧致程度——表达能力更强的激活函数可能提供更紧的下界，从而更准确地估计和最大化互信息。与位置编码的信息论联系：在Transformer中，位置编码（参见第六章）为序列中的每个位置添加位置信息。位置编码的数学设计（如正弦余弦编码的频率分解）使得不同位置具有可区分的编码，同时相邻位置之间具有平滑的插值性质。从信息论角度，位置编码可以被理解为对\"位置信息\"的显式注入——它将位置信息从隐式（通过序列顺序隐含）转换为显式（通过编码向量表示）。位置编码与激活函数的结合决定了位置信息如何在网络中传递。正弦余弦编码的频率分解特性意味着不同频率的成分携带不同尺度的位置信息——低频成分编码粗粒度的位置关系，高频成分编码细粒度的位置关系。激活函数（如GELU）对位置编码的处理会保留或抑制不同频率的成分，从而影响模型对位置模式的捕获能力。GELU的平滑特性有助于保持相邻位置信息的连续性，这对于学习位置敏感的Token交互模式至关重要。信息几何是研究概率分布空间几何性质的理论框架。在这个框架下，概率分布被视为一个流形，而各种信息量（如Fisher信息、KL散度）定义了流形上的几何结构。激活函数在这个几何框架中扮演着重要角色，它们定义了从输入空间到表示空间的映射，而这个映射的几何性质决定了神经网络的学习动态和表达能力。定义3.1.8（Fisher信息度量） 在参数化的概率分布族上，Fisher信息矩阵定义为：Fisher信息矩阵定义了概率流形上的黎曼度量。在这个度量下，两个概率分布之间的\"距离\"不是欧几里得距离，而是由Fisher信息加权的距离。激活函数通过影响概率分布的形状，间接影响Fisher信息度量的几何结构。对于Softmax激活函数，输出分布关于输入的Fisher信息矩阵正好是Softmax雅可比矩阵与自身转置的乘积：。这个矩阵的谱结构决定了梯度下降在概率空间中的几何行为——较大的奇异值对应于\"陡峭\"的方向，较小的奇异值对应于\"平坦\"的方向。理解这个几何结构对于设计有效的优化算法至关重要。与损失函数的联系：在第四章中，我们将详细分析损失函数的优化性质。交叉熵损失 的Hessian矩阵与Fisher信息矩阵密切相关。具体而言，，即交叉熵损失的曲率正好由Softmax分布的Fisher信息度量给出。这种联系揭示了为什么Softmax与交叉熵的组合在优化上具有特殊的性质——它们的联合结构使得损失景观具有特定的几何形状，有利于梯度下降的收敛。激活函数在Transformer架构中的应用体现了深度学习组件之间数学协同的设计哲学。Transformer中的激活函数选择不是随意的，而是经过深思熟虑的数学设计。本节将系统分析激活函数与Transformer其他组件（注意力机制、位置编码、前馈网络）之间的数学协同关系。Transformer中的前馈网络（Feed-Forward Network，FFN）定义为：其中是激活函数。现代Transformer（如GPT、BERT、LLaMA）普遍选择GELU作为。这个选择背后有深刻的数学考量。GELU的定义表明它是输入与其通过标准正态累积分布函数权重的乘积。这种定义使得GELU具有概率解释：它可以被理解为\"以概率 通过\"。这种概率解释与注意力机制中的Softmax概率分布形成了数学上的呼应——两者都涉及概率加权的信号传递。从梯度角度分析，GELU的导数 在整个实数轴上都是正的，这意味着GELU不会像ReLU那样产生\"死神经元\"问题。同时，GELU的导数不会过于接近零（除了在极端负值区域），这避免了Sigmoid和Tanh的梯度饱和问题。这种\"非饱和但有界\"的梯度特性使得GELU特别适合训练非常深的网络。Transformer的自注意力机制本身不包含显式的激活函数，但Softmax操作本质上是另一种形式的非线性激活。注意力权重的计算将Query-Key相似度分数转换为概率分布，这个过程与Sigmoid将logits转换为概率的过程在数学上是相似的。定义3.1.9（缩放点积注意力） Scaled Dot-Product Attention定义为：这里，计算的是Query和Key之间的相似度矩阵，Softmax将每一行归一化为概率分布，最后用这个分布对Value进行加权平均。注意力机制中的Softmax和FFN中的GELU形成了\"双重非线性\"结构。Softmax提供Token间的交互非线性（学习Token之间的注意力模式），GELU提供Token内部的特征非线性（对注意力输出进行非线性变换）。这种双重非线性的组合使得Transformer能够学习极其复杂的函数映射。位置编码（参见第六章）为序列中的每个位置分配数学表示。绝对位置编码（如正弦余弦编码）定义为：这种编码将位置信息嵌入到固定范围的正弦和余弦函数中。正弦和余弦函数的取值范围是，这与GELU输入的范围需求相匹配。从信息论角度，位置编码的频率分解意味着不同维度携带不同尺度的位置信息——偶数维度（低频）和奇数维度（高频）分别编码粗粒度和细粒度的位置关系。激活函数对这种频率分解信息的处理决定了位置信息如何在网络中传递。GELU的平滑特性有助于保持相邻位置信息的连续性，这对于学习位置敏感的Token交互模式至关重要。定义3.1.10（位置感知的特征表示） 设是位置编码矩阵，是Token嵌入矩阵，则位置感知的嵌入表示为：这个公式表明，位置信息和Token信息通过不同的权重矩阵投影后相加，然后经过GELU激活。GELU的非线性变换将这种位置感知的嵌入转换为新的表示，该表示保留了位置信息，同时通过非线性组合创造了位置-Token的交互特征。本节从多个数学视角系统性地分析了激活函数的角色。从线性映射的局限性出发，我们揭示了非线性激活函数打破表达瓶颈的数学必然性。从函数逼近论的角度，我们论证了激活函数作为通用逼近器核心组件的数学基础。从概率论的角度，我们分析了Sigmoid、Softmax等激活函数与概率分布的内在联系。从信息论的角度，我们探讨了激活函数的信息压缩、选择和互信息最大化作用。最后，我们分析了激活函数与Transformer架构（注意力机制、位置编码、前馈网络）的数学协同关系。这些分析为后续章节（特别是第五章注意力机制、第六章位置编码、第四章损失函数）奠定了理论基础，揭示了大语言模型中非线性变换的数学本质。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"3.1.1 非线性的数学必要性","level":2,"id":"3.1.1_非线性的数学必要性_0"},{"heading":"线性映射的数学结构","level":3,"id":"线性映射的数学结构_0"},{"heading":"线性模型表达能力的根本局限","level":3,"id":"线性模型表达能力的根本局限_0"},{"heading":"非线性变换打破表达瓶颈","level":3,"id":"非线性变换打破表达瓶颈_0"},{"heading":"3.1.2 激活函数作为函数逼近的基石","level":2,"id":"3.1.2_激活函数作为函数逼近的基石_0"},{"heading":"通用逼近定理的数学表述","level":3,"id":"通用逼近定理的数学表述_0"},{"heading":"局部基函数与函数重构","level":3,"id":"局部基函数与函数重构_0"},{"heading":"深度与宽度的数学权衡","level":3,"id":"深度与宽度的数学权衡_0"},{"heading":"3.1.3 概率视角下的激活函数","level":2,"id":"3.1.3_概率视角下的激活函数_0"},{"heading":"Sigmoid函数与伯努利分布","level":3,"id":"Sigmoid函数与伯努利分布_0"},{"heading":"Softmax函数与多项式分布","level":3,"id":"Softmax函数与多项式分布_0"},{"heading":"Gumbel-Softmax与离散分布采样","level":3,"id":"Gumbel-Softmax与离散分布采样_0"},{"heading":"3.1.4 信息论视角与激活函数","level":2,"id":"3.1.4_信息论视角与激活函数_0"},{"heading":"激活函数的信息压缩作用","level":3,"id":"激活函数的信息压缩作用_0"},{"heading":"激活函数与互信息最大化","level":3,"id":"激活函数与互信息最大化_0"},{"heading":"激活函数的信息几何视角","level":3,"id":"激活函数的信息几何视角_0"},{"heading":"3.1.5 激活函数与Transformer架构的数学协同","level":2,"id":"3.1.5_激活函数与Transformer架构的数学协同_0"},{"heading":"前馈网络中的激活函数选择","level":3,"id":"前馈网络中的激活函数选择_0"},{"heading":"注意力机制与激活函数的协同","level":3,"id":"注意力机制与激活函数的协同_0"},{"heading":"激活函数与位置编码的配合","level":3,"id":"激活函数与位置编码的配合_0"},{"heading":"3.1.6 本节小结","level":2,"id":"3.1.6_本节小结_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","pathToRoot":"..","attachments":[],"createdTime":1767062570863,"modifiedTime":1768205677505,"sourceSize":30764,"sourcePath":"第3章 激活函数与非线性数学/3.1 激活函数的数学角色.md","exportPath":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","showInTree":true,"treeOrder":10,"backlinks":["大模型中的数学.html"],"type":"markdown"},"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html":{"title":"3.2 导数推导与梯度特性","icon":"","description":"激活函数的导数性质是理解神经网络训练动力学的核心数学基础。在反向传播算法中，损失函数关于网络参数的梯度需要通过激活函数的导数逐层传递。激活函数的导数结构直接决定了梯度如何在不同层之间传播，进而影响模型的收敛速度、稳定性和最终性能。本节将从数学的角度系统性地推导各类激活函数的导数公式，分析其几何意义和梯度特性，并深入探讨激活函数导数对神经网络训练的影响。这些数学分析为理解大语言模型的训练过程提供了必要的理论基础，也为分析和改进现有优化算法提供了理论工具。Sigmoid函数是神经网络中最经典的激活函数之一，它将实数映射到 区间，天然具有概率解释。虽然在大规模语言模型中Sigmoid已被ReLU及其变体取代，但它在理解激活函数导数性质方面具有重要的教学价值，其导数的\"自化\"形式在数学上非常优美，也使得Sigmoid在某些特定场景下仍有应用价值。定义3.2.1（Sigmoid函数） Sigmoid函数（也称为逻辑函数）定义为：其定义域为，值域为。Sigmoid函数是单调递增函数，其图像呈标准的S形曲线，在处有拐点，。从几何上看，Sigmoid函数将整个实数轴压缩到区间，将输入的尺度归一化，同时保留了输入的相对顺序信息。Sigmoid函数的反函数为对数几率函数（Logit函数），定义为：这个反函数在逻辑回归中具有重要意义，它将概率值转换回实数域（logits空间），使得参数估计可以在无界的实数空间中进行。从极限的角度分析Sigmoid函数的边界行为：当时，，因此；当 时，，因此。这种渐近行为表明Sigmoid函数在正无穷处趋向于饱和值1，在负无穷处趋向于饱和值0。Sigmoid函数的导数具有一个极为优美的性质：导数可以用函数自身来表示，这一性质被称为\"自化\"（Self-Derivative Property）。这种数学上的简洁性不仅在理论上令人赞叹，在实际计算中也具有重要的应用价值——在反向传播中，我们不需要存储或重新计算导数，只需要知道当前的值即可。定理3.2.1（Sigmoid导数的自化形式） Sigmoid函数的导数为：证明：将Sigmoid函数重写为，使用链式法则求导：利用​，我们可以将导数表示为：证毕。这个证明揭示了Sigmoid导数自化性质的数学根源：Sigmoid函数的分母是分子的积分（从某种意义上），这种积分-微分关系导致了导数的简洁表示。从几何角度看， 表示在Sigmoid曲线上各点的斜率，斜率的最大值出现在处，此时，。Sigmoid函数的高阶导数可以递归地通过低阶导数表示，这种递推关系揭示了Sigmoid非线性变换的深层数学结构。定理3.2.2（Sigmoid的二阶导数） Sigmoid函数的二阶导数为：证明：使用乘积法则对求导：证毕。二阶导数的几何意义表示Sigmoid曲线的曲率变化。在处，，这表明拐点处的曲率为零，曲线在该点由上凸转为下凸。\n当时，，二阶导数为正，曲线下凸；\n当时，，二阶导数为负，曲线上凸。三阶导数：类似地，可以推导出Sigmoid的三阶导数：高阶导数的复杂结构反映了Sigmoid非线性变换的深层特性。随着阶数增加，导数表达式变得越来越复杂，但它们都保持着用 和表示的形式，这体现了Sigmoid函数的代数闭合性。从几何角度分析，表示Sigmoid曲线在各点的斜率。在 时，，，曲线趋于水平；在 时，，，曲线也趋于水平。在处，，，这是曲线的\"最陡点\"。Sigmoid导数的这种\"钟形\"结构意味着激活值在0.5附近时变化最快，而在极端值（接近0或1）时变化最慢。定义3.2.2（梯度饱和） 对于激活函数，如果存在输入区域使得，则在该区域输入的微小变化不会引起输出的显著变化，这种现象称为梯度饱和。Sigmoid函数的导数取值范围是，最大值在处取得。当输入的绝对值很大时，非常接近 0 。具体而言，当 时，；当时，。这意味着，如果神经元的输入（加权求和结果）落在饱和区域，其激活值对输入的微小变化不敏感，梯度几乎为零。与梯度消失问题的联系：在深层网络中，这种饱和效应会逐层累积放大。考虑一个层的前馈网络，每层的激活输出为。在反向传播中，梯度从输出层向输入层传播时，如果每一层的激活都处于饱和状态（即 接近 0），则梯度会按指数级衰减。设平均每层的梯度衰减因子为，经过层后，梯度衰减为原来的 。当且时，梯度衰减为，在数值上完全不可检测，这就 是所谓的\"梯度消失\"问题。Tanh函数（双曲正切函数）是另一个经典的激活函数，它将实数映射到区间。Tanh与Sigmoid有密切的数学关系——Tanh可以视为Sigmoid的缩放和平移版本。Tanh的零中心性质（输出均值为0）使得它在实践中通常比Sigmoid表现更好，减少了梯度偏移问题，加速了训练收敛。定义3.2.3（Tanh函数） Tanh函数（双曲正切函数）定义为：其定义域为，值域为。Tanh函数是奇函数，满足，在处有。与Sigmoid相比，Tanh的输出以0为中心，这带来了两个重要的数学优势。首先，零中心输出减少了梯度偏移问题。在Sigmoid中，输出总是正的，这意味着梯度总是非负的（在反向传播中，，如果为正，则只能为正或零）。这种单向梯度可能导致优化过程的\"zig-zag\"行为，浪费计算资源。Tanh的零中心输出消除了这个问题，梯度可以正可以负，优化更加灵活高效。其次，零中心输出使得各层的输入分布更加稳定，减少了协变量偏移问题。当上一层输出的均值不为零时，下一层的输入分布会发生偏移，可能导致训练不稳定。Tanh的零中心性质缓解了这个问题。Tanh和Sigmoid存在直接的数学关系，通过这个关系可以清晰地看到Tanh如何从Sigmoid演化而来。定理3.2.3（Tanh与Sigmoid的关系） Tanh函数可以表示为Sigmoid函数的缩放和平移：证明：设，则：证毕。这个关系表明，Tanh本质上是Sigmoid的\"零中心版本\"。Sigmoid输出区间，Tanh输出区间；Sigmoid以0.5为中心，Tanh以0为中心。从变换的角度，Tanh先对输入进行2倍缩放，应用Sigmoid，然后将输出平移和缩移到区间。定理3.2.4（Tanh的导数） Tanh函数的导数为：证明：使用商的求导法则对求导：利用平方差公式：因此：其中是双曲正割函数。使用自身表示导数：证毕。与Sigmoid导数的比较：Tanh导数 与Sigmoid导数在形式上非常相似。两者都是函数值与其\"补\"的乘积：对于Sigmoid，\"补\"是；对于Tanh，\"补\"是。这种相似性反映了两个函数在数学结构上的内在联系。Tanh导数的取值范围：Tanh导数的取值范围是，最大值在处取得，此时 。与Sigmoid相比，Tanh的导数峰值更大（1 vs 0.25），这意味着在附近，Tanh的梯度\"更强\"。然而，当增大时，Tanh导数同样会迅速衰减至 0——当时， 。这种快速饱和特性仍然是Tanh的主要局限性。与Sigmoid的梯度比较：设输入的绝对值较大时，Sigmoid导数，Tanh导数，两者都存在梯度饱和问题。但由于Tanh的导数峰值是1而Sigmoid只有0.25，Tanh在激活值接近零的区域（未饱和区域）有更强的梯度信号，这使得Tanh在训练初期通常比Sigmoid收敛更快。然而，当网络较深时，Tanh的饱和问题依然严重，导致深层网络的训练困难。ReLU（Rectified Linear Unit，修正线性单元）是目前深度学习中最广泛使用的激活函数。ReLU的数学形式极其简单：，正是这种简单性带来了卓越的计算效率和优异的性能。ReLU及其变体（包括Leaky ReLU、ELU、GELU等）构成了现代神经网络激活函数的主流选择，在Transformer架构中扮演着关键角色。定义3.2.4（ReLU函数） ReLU函数定义为：\\frac{d}{dx}\\text{ReLU}(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\ 0 &amp; \\text{if } x &lt; 0 \\end{cases}​f(y) \\geq f(x) + g^T (y - x), \\quad \\forall y\\text{LeakyReLU}(x) = \\max(\\alpha x, x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\ \\alpha x &amp; \\text{if } x \\leq 0 \\end{cases}​\\text{LeakyReLU}(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\ \\alpha &amp; \\text{if } x &lt; 0 \\end{cases}​\\text{PReLU}(x) = \\max(0, x) + \\alpha \\min(0, x)\\text{ELU}(x) = \\begin{cases} x &amp; \\text{if } x &gt; 0 \\ \\alpha (e^x - 1) &amp; \\text{if } x \\leq 0 \\end{cases}​\\text{ELU}(x) = \\begin{cases} 1 &amp; \\text{if } x &gt; 0 \\ \\alpha e^x &amp; \\text{if } x \\leq 0 \\end{cases}\\text{GELU}(x) = x \\cdot \\Phi(x) = \\frac{x}{2} \\left[ 1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right) \\right]\\frac{d}{dx}\\text{GELU}(x) = \\Phi(x) + x \\cdot \\phi(x)\\text{GELU}(x) \\approx 0.5x \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}}\\left(x + 0.044715x^3\\right)\\right)\\right)\\hat{p}i = \\text{Softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum{j=1}^C \\exp(z_j)}, \\quad i = 1, 2, \\ldots, C\\hat{p}_1 = \\frac{\\exp(z_1)}{\\exp(z_1) + \\exp(z_2)}, \\quad \\hat{p}_2 = \\frac{\\exp(z_2)}{\\exp(z_1) + \\exp(z_2)}​J_{ij} = \\frac{\\partial \\hat{p}_i}{\\partial z_j} = \\begin{cases} \\hat{p}_i (1 - \\hat{p}_i) &amp; \\text{if } i = j \\ -\\hat{p}_i \\hat{p}_j &amp; \\text{if } i \\neq j \\end{cases}​\\begin{align}\n\\frac{\\partial \\hat{p}_i}{\\partial z_i} &amp;= \\frac{\\partial}{\\partial z_i} \\left( \\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\right) \\ &amp;= \\frac{\\exp(z_i) \\sum_k \\exp(z_k) - \\exp(z_i) \\exp(z_i)}{(\\sum_k \\exp(z_k))^2} \\ &amp;= \\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\left( 1 - \\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\right) \\ &amp;= \\hat{p}_i (1 - \\hat{p}_i) \\end{align}\\begin{align}\n\\frac{\\partial \\hat{p}_i}{\\partial z_j} &amp;= \\frac{\\partial}{\\partial z_j} \\left( \\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\right) \\ &amp;= \\exp(z_i) \\cdot \\left( -\\frac{\\exp(z_j)}{(\\sum_k \\exp(z_k))^2} \\right) \\ &amp;= -\\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)} \\cdot \\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)} \\ &amp;= -\\hat{p}_i \\hat{p}_j \\end{align}J_{\\text{Softmax}}(z) = \\text{diag}(\\hat{p}) - \\hat{p} \\hat{p}^T\\sum{j=1}^C J{ij} = \\hat{p}i (1 - \\hat{p}_i) + \\sum{j \\neq i} (-\\hat{p}i \\hat{p}_j) = \\hat{p}_i - \\hat{p}_i \\sum{j=1}^C \\hat{p}_j = \\hat{p}_i - \\hat{p}_i \\cdot 1 = 0\\begin{align}\nv^T J v &amp;= v^T (\\text{diag}(\\hat{p}) - \\hat{p} \\hat{p}^T) v \\ &amp;= \\sum{i=1}^C \\hat{p}_i v_i^2 - \\left(\\sum{i=1}^C \\hat{p}i v_i\\right)^2 \\ &amp;= \\mathbb{E}{\\hat{p}}[v^2] - (\\mathbb{E}{\\hat{p}}[v])^2 \\ &amp;= \\text{Var}{\\hat{p}}(v) \\geq 0 \\end{align}\\frac{\\partial L}{\\partial z_k} = \\hat{p}_k - y_k​\\begin{align}\n\\frac{\\partial L}{\\partial zk} &amp;= \\sum_i \\frac{\\partial L}{\\partial \\hat{p}_i} \\frac{\\partial \\hat{p}_i}{\\partial z_k} \\ &amp;= \\sum_i \\left( -\\frac{y_i}{\\hat{p}_i} \\right) J{ik} \\ &amp;= -\\frac{yk}{\\hat{p}_k} \\hat{p}_k (1 - \\hat{p}_k) - \\sum{i \\neq k} \\frac{yi}{\\hat{p}_i} (-\\hat{p}_i \\hat{p}_k) \\ &amp;= -y_k (1 - \\hat{p}_k) + \\hat{p}_k \\sum{i \\neq k} y_i \\ &amp;= -y_k + y_k \\hat{p}_k + \\hat{p}_k (1 - y_k) \\ &amp;= \\hat{p}_k - y_k \\end{align}J_\\sigma(h) = \\frac{\\partial \\sigma(h)}{\\partial h} = \\text{diag}(\\sigma'(h))J_{h \\to \\hat{h}} = \\frac{\\partial \\hat{h}}{\\partial h} = \\frac{\\partial \\hat{h}}{\\partial z} \\frac{\\partial z}{\\partial h} = \\text{diag}(\\sigma'(z)) W\\frac{\\partial L}{\\partial h} = \\frac{\\partial L}{\\partial \\hat{h}} \\odot \\sigma'(h)\\left|\\frac{\\partial L}{\\partial h}\\right|_2 \\leq \\max_i |\\sigma'(h_i)| \\cdot \\left|\\frac{\\partial L}{\\partial \\hat{h}}\\right|_2 \\cdot |W|_2h = \\sigma(W_{pe} PE + W_e E)","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"3.2.1 Sigmoid函数的导数与性质","level":2,"id":"3.2.1_Sigmoid函数的导数与性质_0"},{"heading":"Sigmoid函数的数学定义","level":3,"id":"Sigmoid函数的数学定义_0"},{"heading":"导数推导与自化性质","level":3,"id":"导数推导与自化性质_0"},{"heading":"高阶导数与泰勒展开","level":3,"id":"高阶导数与泰勒展开_0"},{"heading":"几何解释与梯度特性","level":3,"id":"几何解释与梯度特性_0"},{"heading":"3.2.2 Tanh函数的导数与性质","level":2,"id":"3.2.2_Tanh函数的导数与性质_0"},{"heading":"Tanh函数的数学定义与性质","level":3,"id":"Tanh函数的数学定义与性质_0"},{"heading":"与Sigmoid的数学关系","level":3,"id":"与Sigmoid的数学关系_0"},{"heading":"导数推导与性质分析","level":3,"id":"导数推导与性质分析_0"},{"heading":"3.2.3 ReLU函数族的导数与性质","level":2,"id":"3.2.3_ReLU函数族的导数与性质_0"},{"heading":"ReLU函数的定义与导数","level":3,"id":"ReLU函数的定义与导数_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","pathToRoot":"..","attachments":[],"createdTime":1767062570860,"modifiedTime":1768209060257,"sourceSize":35297,"sourcePath":"第3章 激活函数与非线性数学/3.2 导数推导与梯度特性.md","exportPath":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","showInTree":true,"treeOrder":11,"backlinks":["大模型中的数学.html"],"type":"markdown"},"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html":{"title":"3.3 梯度饱和与梯度爆炸的数学根源","icon":"","description":"梯度饱和与梯度爆炸是深度学习训练中的两大核心挑战，它们直接决定了神经网络能否有效学习。本节将从数学的角度深入分析这两种现象的根源，揭示它们与激活函数特性、网络架构设计之间的深层联系。通过严格的数学推导和几何直觉，我们将建立对梯度动力学的系统性理解，为设计和改进大语言模型的训练策略提供理论指导。梯度饱和是深度学习训练中的核心挑战之一，它指的是在训练过程中梯度值变得非常小，导致参数更新极其缓慢甚至停止。理解梯度饱和的数学机制对于设计有效的激活函数和训练策略至关重要。本节将从多个角度深入分析梯度饱和的根源，揭示其与激活函数导数特性的内在联系。在分析梯度饱和之前，我们首先需要建立饱和现象的严格数学定义。激活函数的饱和可以从两个层面理解：输出饱和和梯度饱和。输出饱和关注的是激活函数输出值进入平坦区域的物理现象，而梯度饱和则关注导数趋近于零导致的梯度消失问题。定义3.3.1（输出饱和） 对于激活函数 ，如果存在输入区域使得趋近于其渐近值（即当时​，当时，则在该区域输入的饱和称为输出饱和。定义3.3.2（梯度饱和） 对于激活函数，如果存在输入区域使得，则在该区域输入的微小变化不会引起输出的显著变化，这种现象称为梯度饱和。这两个定义密切相关但并不等价。输出饱和是激活函数值域边界的行为，而梯度饱和是激活函数导数的行为。当激活函数输出接近其渐近边界时，其导数通常也接近零，因此输出饱和往往伴随梯度饱和。然而，梯度饱和可以在输出尚未饱和时发生，例如在Sigmoid函数的中间区域（），导数达到最大值而非最小值。Sigmoid函数是分析梯度饱和的经典案例。从导数公式可以看出，导数的值取决于和的乘积。当接近0或1时，导数趋近于零；当时，导数达到最大值0.25。定理3.3.1（Sigmoid的饱和区域） Sigmoid函数的梯度饱和区域定义为：对于任意，饱和区域可以精确计算。设，则：解这个不等式得到两个解，饱和区域为。当 时，即进入显著饱和区域；当时，进入深度饱和区域。从几何角度理解，Sigmoid曲线在其两端进入平坦区域。在时，曲线趋近于渐近线，斜率趋近于零；在时，曲线趋近于渐近线，斜率同样趋近于零。这种\"S形\"曲线两端的平坦性正是梯度饱和的几何表现。饱和深度的量化分析：定义饱和深度函数来量化输入值距离饱和区域的远近：当较小时，接近饱和区域；当较大时，处于激活区域。对于Sigmoid函数，当时，，处于深度饱和状态；当时，，处于充分激活状态。Tanh函数的饱和特性与Sigmoid类似，但由于其输出范围是，饱和行为呈现对称分布。Tanh的导数为，当接近1时，导数趋近于零。定理3.3.2（Tanh的饱和区域） Tanh函数的梯度饱和区域为：与Sigmoid相比，Tanh有两个显著差异。首先，Tanh的导数峰值是1而非0.25，这意味着在未饱和区域（较小时），Tanh的梯度信号更强。其次，Tanh的输出以零为中心，这使得负输入也会进入饱和区域（当时，）。Tanh与Sigmoid的饱和比较：设为饱和阈值，对于Sigmoid，饱和区域为；对于Tanh，饱和区域为（因为，）。这表明Tanh比Sigmoid更早进入饱和区域，但其饱和程度较轻（导数最小值接近0而非0）。在深层网络中，饱和效应会逐层累积放大，这是导致深层神经网络训练困难的根本原因之一。考虑一个层的前馈网络，每层的激活输出为。在反向传播中，梯度从输出层向输入层传播。定理3.3.3（梯度累积效应） 考虑三层网络的简单情况，损失函数关于第一层权重的梯度为：其中。如果每一层的激活都处于饱和状态，即，则每层的梯度传递因子约为。累积衰减模型：设平均每层的梯度传递因子为，经过层后，梯度衰减为原来的。对于Sigmoid激活函数，平均（考虑权重归一化后）。当时，梯度衰减为，在数值上完全不可检测。饱和的网络层叠效应：从信息论角度，梯度饱和意味着信息在反向传播过程中丢失。当梯度信号被压缩到接近零时，前层的参数几乎无法接收到关于损失函数的信息，导致这些层无法学习。深层网络因此陷入\"僵尸状态\"——前层参数保持随机初始化状态，只有后层能够学习。与权重初始化的关系：饱和效应与权重初始化密切相关。如果初始权重过大，输入到激活函数的线性组合可能直接落在饱和区域，导致训练开始时梯度就很小。Xavier初始化和He初始化正是为了避免这个问题而设计的。Xavier初始化将权重方差设置为​，使得Sigmoid和Tanh激活后的输出方差保持稳定；He初始化将权重方差设置为​，针对ReLU激活函数进行了优化，考虑了ReLU激活一半神经元输出为零的特性。与梯度饱和相反，梯度爆炸指的是在反向传播过程中梯度值变得非常大，导致参数更新幅度过大，训练不稳定甚至发散。梯度爆炸在循环神经网络和Transformer中尤为突出，是训练这些模型的主要挑战之一。本节将系统分析梯度爆炸的数学根源，揭示其与网络架构和初始化策略的关系。定义3.3.3（梯度爆炸） 在反向传播中，如果梯度矩阵的范数随层数增加而指数级增长，即存在常数使得，则称该网络存在梯度爆炸问题。梯度爆炸的表现包括：损失函数值出现NaN或Inf；参数更新幅度过大导致模型权重溢出；训练曲线剧烈振荡无法收敛。在极端情况下，单次参数更新就可能将权重推向无穷大或负无穷，使模型完全崩溃。循环神经网络（RNN）是分析梯度爆炸的经典模型。RNN的隐藏状态更新为，在时间反向传播（BPTT）中，梯度需要沿时间步反向传播。定理3.3.4（RNN的梯度范数增长） 考虑简化情况（无激活函数、无输入），则​。梯度关于的雅可比矩阵的范数满足：如果，则沿时间步反向传播后，梯度范数约为，呈指数级增长。矩阵谱半径与稳定性：对于线性系统​，其解为​。矩阵的谱半径决定了系统的稳定性。如果，则随指数增长，导致梯度爆炸；如果，则随指数衰减，导致梯度消失。谱半径等于1是临界状态。谱半径与特征值的关系：谱半径是特征值模的最大值。对于对称矩阵，谱半径等于最大奇异值；对于非对称矩阵，谱半径可能大于或小于最大奇异值。在实践中，计算矩阵的谱半径通常需要数值方法。Transformer架构虽然解决了RNN的长期依赖问题，但引入了新的梯度挑战。梯度爆炸可能通过多个渠道发生，需要仔细分析。QK点积的尺度问题：在自注意力机制中，注意力分数 的尺度与相关。Query和Key向量的点积期望方差为：如果很大而没有进行适当的缩放，注意力分数的方差会很大。当的标准差为1时，的分布范围可能达到或更宽，导致Softmax的输出接近one-hot分布。这种极端分布使得反向传播时产生大的梯度。Softmax的极端梯度：当Softmax输出接近one-hot分布时，雅可比矩阵的对角元素非常小（因为或），而非对角元素也非常小。这看似不会导致梯度爆炸，但当这种极端分布与后续的线性变换结合时，可能产生问题。多头注意力的梯度复合：在多头注意力中，多个头的梯度会复合。设个头的输出拼接为，则总损失。每个头的梯度可能具有不同的范数尺度，当这些梯度在拼接处合并时，可能产生范数增长。残差连接（Residual Connection）是Transformer中缓解梯度问题的重要技术，它从根本上改变了梯度传播的路径，为深层网络的稳定训练提供了数学保障。定义3.3.4（残差连接） 设网络的一层为，残差连接定义为。在反向传播中：即使很小（导致梯度消失），确保了梯度可以直接通过恒等映射传递。残差的梯度流分析：考虑残差网络的反向传播。设为损失函数，为第层的输入，。则：这个递推关系的解为：如果，则的特征值接近1，梯度可以稳定传播。残差路径的恒等保证：残差连接的核心数学保证是：梯度至少可以通过恒等路径（部分）无衰减地传播。即使的梯度完全消失（），梯度仍然可以通过路径传递到前一层。这种\"梯度高速公路\"效应使得非常深的网络成为可能。定理3.3.5（残差的梯度下界） 对于残差连接，梯度的范数满足：如果，则：这个下界确保了即使的梯度很小，原始梯度信息仍然可以部分保留。在实际训练中，需要检测梯度爆炸并采取应对措施。梯度裁剪是最常用的策略，它通过限制梯度的范数来防止过度更新。定义3.3.5（全局梯度裁剪） 全局梯度裁剪将梯度的范数裁剪到一个固定的上界：其中是裁剪阈值，通常设置为1或5。裁剪后的梯度方向与原梯度方向相同，但范数被限制在以下。按范数裁剪与按值裁剪：梯度裁剪有两种主要形式。按范数裁剪保持梯度的方向，只缩放其大小：按值裁剪将每个梯度元素裁剪到区间：按范数裁剪保持了梯度的方向信息，但按值裁剪可能改变梯度的方向。在实践中，按范数裁剪更为常用，因为它保留了梯度的主要方向信息。裁剪阈值的选择：裁剪阈值的选择是一个超参数调优问题。较小的确保稳定性但可能限制学习速度；较大的允许快速学习但可能容忍不稳定。实践中常用的策略是从较小的值（如0.5）开始，逐渐增加直到找到稳定的最大允许值。梯度流（Gradient Flow）研究的是梯度在深度网络中传播时的行为，包括梯度如何衰减、放大或保持稳定。理解梯度流对于设计深层网络架构和训练策略至关重要。本节将从数学的角度分析梯度流的各种机制，揭示信息在网络中传递和保持的数学原理。神经网络的表达能力与其参数矩阵的有效秩密切相关。有效秩决定了网络能够捕获的信息维度，也影响梯度的传播特性。定义3.3.6（有效秩） 设参数矩阵的奇异值分解为，其中包含所有奇异值（按降序排列）。对于，有效秩定义为：有效秩衡量了矩阵中\"显著\"奇异值的数量，即具有实质贡献的维度数。有效秩与梯度衰减：在梯度传播过程中，奇异值大于1的方向会导致梯度放大，奇异值小于1的方向会导致梯度衰减。设​，则反向传播的梯度变换为​。奇异值决定了梯度在对应方向上的缩放因子。定理3.3.6（梯度范数与奇异值） 对于线性变换，反向传播的梯度满足：其中是的最大奇异值。如果，梯度范数可能放大；如果，梯度范数可能衰减。定义3.3.7（梯度范数边界） 对于线性层，输入梯度和输出梯度满足：其中和分别是的最小和最大奇异值。这个不等式给出了梯度范数的精确边界。条件数与梯度稳定性：矩阵的条件数定义为。条件数越大，梯度范数的可能变化范围越大，训练越不稳定。当时，即使输入梯度很小，在最小奇异值方向上可能产生相对较大的输出梯度；反之亦然。路径归一化与深度网络稳定性：在非常深的网络中，梯度的累积效应可能导致不稳定。路径归一化（Path Normalization）是一种通过调整权重来稳定训练的方法。对于网络中的一条前向路径，其\"路径深度\"可以定义为各层雅可比矩阵范数的乘积。路径归一化确保所有路径的深度相近，避免某些路径成为\"捷径\"或\"瓶颈\"。权重初始化的一个核心目标是确保各层的前向激活和反向梯度都具有稳定的方差。这种稳定性对于深层网络的成功训练至关重要。定义3.3.8（Xavier初始化） 对于Sigmoid和Tanh激活函数，Xavier初始化将权重初始化为：其中和分别是输入和输出的维度。这个初始化确保了激活值的方差在前向传播中保持稳定。Xavier初始化的方差推导：考虑线性层，如果的各元素独立同分布且方差为，则的各元素方差为：为了保持方差稳定（），需要​。类似地，在反向传播中需要​。Xavier初始化取两者的几何平均​，在实践中使用作为近似。定义3.3.9（He初始化） 对于ReLU激活函数，He初始化将权重初始化为：He初始化的推导考虑了ReLU的\"死神经元\"问题。由于约一半的神经元输出为零，实际参与传播的神经元数量约为​​，因此需要使用更大的权重方差来保持激活的稳定性。不同初始化策略的比较：在自注意力机制中，梯度流通过注意力权重矩阵进行。注意力权重矩阵的谱性质决定了梯度如何在不同位置之间传播。注意力权重的谱结构：注意力权重矩阵是非负行随机矩阵，其谱半径为1，所有特征值位于单位圆内。不同注意力分布对应截然不同的谱结构：当注意力分布趋于均匀时，接近秩-1 矩阵，除最大特征值外其余特征值趋近于零，导致表示过度平均；而当注意力分布趋于one-hot时，更接近置换或选择矩阵，通常保持较高秩，其特征值分布在单位圆附近，对应硬路由和弱混合行为。梯度流的信息瓶颈：注意力机制的梯度流可以视为一种信息瓶颈。设是隐藏状态，注意力输出为。反向传播的梯度为​。注意力权重矩阵的谱性质决定了梯度如何被\"混合\"和传递。如果的非主特征值很小，梯度主要集中在少数方向；如果接近均匀混合，梯度会均匀分布到所有位置。归一化技术是现代深度学习训练的核心组件，它们通过规范化激活值或梯度的分布来稳定训练过程。本节将系统分析各类归一化技术的数学原理及其对梯度稳定性的影响，揭示这些技术如何从根本上改善深层网络的训练动态。批归一化（Batch Normalization，BN）是最早提出且应用广泛的归一化技术，它通过对激活值进行归一化来稳定训练过程。批归一化的核心思想是规范化每层的输入分布，减少内部协变量偏移。定义3.3.10（批归一化） 设mini-batch中的激活值为，批归一化定义为：其中是批均值，是批方差，和是可学习的缩放和偏移参数，是防止除零的小常数。归一化的数学效应：批归一化将激活值的分布规范化到均值为0、方差为1的标准正态分布附近。这种归一化具有以下几个重要的数学效应。首先，它减少了内部协变量偏移（Internal Covariate Shift）——通过规范化每层的输入分布，减少了层与层之间的依赖关系。其次，它提供了一定的正则化效果——由于归一化使用mini-batch的统计量而非全局统计量，引入了一定的噪声，这类似于Dropout的正则化效果。第三，它允许使用更大的学习率——由于激活值的分布更加稳定，梯度的大小也更加稳定，因此可以使用更大的学习率进行训练。批归一化的梯度计算：批归一化的反向传播需要计算四个梯度：、、和​。这些梯度可以通过链式法则推导，其复杂性源于归一化操作的非线性。批归一化在卷积网络中的应用：在卷积网络中，批归一化通常应用于通道维度。设卷积输出的形状为，批归一化对每个通道独立计算统计量，对每个通道内的所有空间位置和batch应用相同的归一化参数。这种设计保持了卷积的空间结构，同时实现了通道间的归一化。层归一化（Layer Normalization，LN）是Transformer中使用的标准归一化技术。与批归一化不同，层归一化在特征维度上进行归一化，而非批维度。定义3.3.11（层归一化） 设是隐藏状态向量，层归一化定义为：其中是均值，是方差，和是可学习的逐元素缩放和偏移参数。层归一化的关键优势是它不依赖于batch size，可以在任何情况下使用，包括在线学习和自回归模型。层归一化的计算特性：层归一化在每个样本内部计算统计量，不依赖于其他样本。这使得它在以下场景中特别有用：在线学习（batch size可能为1）、自回归模型（不能使用未来信息）、分布式训练（不同设备处理不同样本）。相比之下，批归一化在这些场景中会遇到统计量估计不稳定的问题。与批归一化的比较：在Transformer中，层归一化通常部署在注意力层和前馈层之间（Pre-LN Transformer）或之后（Post-LN Transformer）。研究表明，Pre-LN结构更有利于训练非常深的网络。定义3.3.12（Pre-LN Transformer） 在Pre-LN Transformer中，第层的计算为：\n​这种结构将层归一化置于残差分支内部，确保了主路径（残差连接）的梯度可以直接传播，同时归一化稳定了分支路径的激活分布。Pre-LN的数学优势：在Pre-LN结构中，层归一化位于残差分支的输入端，这使得：1.残差路径的梯度可以直接通过恒等连接传递，不受层归一化的影响\n2.层归一化稳定了注意力计算和前馈计算的输入分布\n3.每一层的输入分布更加稳定，减小了协变量偏移Post-LN的挑战：在Post-LN Transformer中，层归一化位于残差分支的输出端：\n这种结构在训练初期可能不稳定，因为层归一化位于关键路径上，可能放大或缩小梯度。实践中通常需要仔细的学习率调度（如warmup）来稳定训练。RMSNorm（Root Mean Square Layer Normalization）是层归一化的一种简化变体，它移除了均值归一化，只保留RMS归一化。定义3.3.13（RMSNorm） RMSNorm定义为：其中是的均方根。RMSNorm的数学动机是：在许多场景下，均值信息可能包含重要的信号，不应被归一化消除。RMSNorm的数学性质：与层归一化相比，RMSNorm具有以下优势。首先，计算量减少——RMSNorm不需要计算均值，节省了加法和除法操作。其次，理论简洁——RMS范数是更简单的统计量，其数学性质更容易分析。第三，效果相当——实验表明，RMSNorm在保持性能的同时减少了计算量，已成为LLaMA等现代大语言模型的默认选择。与位置编码的联合设计：归一化技术与位置编码在Transformer中共同工作，确保位置信息在深层网络中稳定传递。位置编码（如RoPE）将位置信息嵌入到Token嵌入中，这些嵌入经过层归一化处理后，位置信息被缩放到合适的范围。归一化确保了不同位置的位置编码具有相似的数值尺度，避免了某些位置主导信息流的问题。归一化技术通过多种机制稳定梯度传播，这些机制可以从数学角度进行详细分析。激活分布的稳定化：归一化将激活值的分布规范化到固定范围（均值为0、方差为1，或均方根为1）。这种规范化意味着激活值的尺度不会随网络深度增加而累积变化，梯度也不会因为激活值的尺度变化而放大或缩小。梯度尺度的归一化：在反向传播中，归一化层的梯度计算涉及除以方差（或均方根），这具有某种\"归一化\"效应。具体而言，如果输入梯度的某些维度较大，归一化层会将其缩放，使得输出梯度更加平衡。定义3.3.14（梯度尺度归一化） 对于层归一化，反向传播的梯度满足某种形式的尺度约束。设 ​，则和。这些约束意味着归一化后的表示位于一个特定的流形上，梯度也被限制在这个流形的切空间中。基于前文的分析，我们可以总结出激活函数设计应遵循的数学原则。这些原则不仅解释了现有激活函数的设计逻辑，也为未来新型激活函数的发展提供了指导。原则3.3.1（避免饱和） 理想的激活函数应该在整个输入范围内都有非零导数，避免存在大范围的饱和区域。ReLU通过分段线性的设计实现了这一点——在线性区域，导数为常数1，不存在饱和问题。然而，ReLU的负区域完全抑制了信号，可能导致\"死神经元\"问题。Leaky ReLU、ELU、GELU等变体通过不同的策略处理负区域，在保持正区域良好梯度的同时，为负区域提供有限的梯度传递。数学评估指标：定义激活函数的饱和度量来量化其饱和程度：其中是输入分布的支持域，是饱和阈值。较小的值表示激活函数具有更好的抗饱和性能。原则3.3.2（梯度流稳定） 激活函数的导数不应该过大（导致梯度爆炸）或过小（导致梯度消失），在整个输入范围内应该接近1。对于深层网络，激活函数导数的范数应该接近1，以确保梯度可以稳定地反向传播。GELU的设计考虑了这一点——其导数在大多数区域都接近1，不会过度放大或抑制梯度。梯度稳定性分析：定义激活函数的梯度稳定性指标：较小的值表示激活函数的导数接近1，梯度流更加稳定。原则3.3.3（非线性表达能力） 激活函数需要能够表示复杂的非线性映射，过于简单的激活函数可能限制网络的表达能力。过于简单的激活函数（如ReLU）虽然训练高效，但表达能力可能受限。GELU通过引入高斯分布相关的非线性，增加了函数的\"弯曲\"程度，可能捕获更复杂的模式。然而，简单性与表达能力之间存在权衡——过于复杂的激活函数可能难以训练或过拟合。原则3.3.4（计算效率） 激活函数的计算成本直接影响训练和推理速度。ReLU的计算仅涉及比较和乘法，非常高效。GELU涉及误差函数的计算，计算成本较高，但现代硬件和库优化使得其计算开销在可接受范围内。在实际应用中，需要在数学特性和计算效率之间进行权衡。原则3.3.5（任务契合） 不同的下游任务可能需要不同特性的激活函数。对于分类任务，Softmax及其梯度特性与交叉熵损失的配合至关重要。对于语言模型，GELU的平滑性和概率解释使其成为自然的选择。对于视觉任务，ReLU的稀疏激活特性可能更有价值。在Transformer中，激活函数的选择与注意力机制的设计紧密相关。注意力输出需要经过前馈网络处理，前馈网络中的激活函数决定了信息如何被非线性变换。GELU被选择作为Transformer的激活函数，是因为它与注意力的概率输出特性相契合——注意力权重本身是一种概率分布，GELU可以平滑地处理这些概率信息，同时引入有益的非线性。本节从数学的角度系统性地分析了梯度饱和与梯度爆炸的根源。我们首先详细推导了Sigmoid和Tanh函数的饱和区域和饱和深度，揭示了激活函数导数特性与饱和现象的内在联系。然后，我们分析了梯度爆炸的数学机制，包括RNN的时间反向传播问题和Transformer中的梯度挑战。残差连接的数学分析展示了它如何通过恒等路径保证梯度流的稳定。我们进一步讨论了梯度流与信息保持的关系，包括有效秩、奇异值和权重初始化的数学原理。归一化技术（批归一化、层归一化、RMSNorm）的分析揭示了它们如何通过稳定激活分布来改善梯度传播。最后，我们总结了激活函数设计的数学原则，为未来研究和实践提供了理论指导。这些分析为理解大语言模型训练的动力学过程提供了坚实的数学基础。激活函数的研究仍在继续，以下是一些有前景的未来方向。\n自适应激活函数：Swish函数通过可学习的参数提供了自适应的激活特性。参数可以通过反向传播自动学习，使激活函数能够适应不同的数据分布和任务需求。\n神经架构搜索（NAS）：NAS被用于自动发现新的激活函数。通过定义搜索空间和评估指标，可以系统地探索激活函数的参数空间，发现性能更优的激活函数。\n特定任务激活函数：针对特定任务（如长文本建模、多模态学习）设计专门的激活函数，可能带来性能的提升。这需要深入理解任务的数学特性和激活函数的数学性质。","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"3.3.1 梯度饱和的数学机制","level":2,"id":"3.3.1_梯度饱和的数学机制_0"},{"heading":"饱和现象的数学定义","level":3,"id":"饱和现象的数学定义_0"},{"heading":"Sigmoid函数的饱和区域分析","level":3,"id":"Sigmoid函数的饱和区域分析_0"},{"heading":"Tanh函数的饱和特性","level":3,"id":"Tanh函数的饱和特性_0"},{"heading":"饱和效应的累积放大机制","level":3,"id":"饱和效应的累积放大机制_0"},{"heading":"3.3.2 梯度爆炸的数学机制","level":2,"id":"3.3.2_梯度爆炸的数学机制_0"},{"heading":"梯度爆炸的现象描述","level":3,"id":"梯度爆炸的现象描述_0"},{"heading":"简单RNN的梯度分析","level":3,"id":"简单RNN的梯度分析_0"},{"heading":"Transformer中的梯度爆炸机制","level":3,"id":"Transformer中的梯度爆炸机制_0"},{"heading":"残差连接的数学作用","level":3,"id":"残差连接的数学作用_0"},{"heading":"梯度爆炸的阈值与裁剪策略","level":3,"id":"梯度爆炸的阈值与裁剪策略_0"},{"heading":"3.3.3 梯度流与信息保持","level":2,"id":"3.3.3_梯度流与信息保持_0"},{"heading":"有效秩与信息传递","level":3,"id":"有效秩与信息传递_0"},{"heading":"奇异值与梯度范数分析","level":3,"id":"奇异值与梯度范数分析_0"},{"heading":"初始化与梯度流","level":3,"id":"初始化与梯度流_0"},{"heading":"与注意力机制的梯度流","level":3,"id":"与注意力机制的梯度流_0"},{"heading":"3.3.4 归一化技术与梯度稳定","level":2,"id":"3.3.4_归一化技术与梯度稳定_0"},{"heading":"批归一化的数学原理","level":3,"id":"批归一化的数学原理_0"},{"heading":"层归一化的数学原理","level":3,"id":"层归一化的数学原理_0"},{"heading":"层归一化与Transformer的配合","level":3,"id":"层归一化与Transformer的配合_0"},{"heading":"RMSNorm与计算优化","level":3,"id":"RMSNorm与计算优化_0"},{"heading":"归一化技术的梯度稳定机制","level":3,"id":"归一化技术的梯度稳定机制_0"},{"heading":"3.3.5 激活函数设计的数学原则","level":2,"id":"3.3.5_激活函数设计的数学原则_0"},{"heading":"避免梯度饱和区域","level":3,"id":"避免梯度饱和区域_0"},{"heading":"保持梯度流稳定性","level":3,"id":"保持梯度流稳定性_0"},{"heading":"提供足够的非线性表达能力","level":3,"id":"提供足够的非线性表达能力_0"},{"heading":"计算效率","level":3,"id":"计算效率_0"},{"heading":"与下游任务的契合","level":3,"id":"与下游任务的契合_0"},{"heading":"与注意力机制的设计协同","level":3,"id":"与注意力机制的设计协同_0"},{"heading":"3.3.6本节小结","level":2,"id":"3.3.6本节小结_0"}],"links":[],"author":"","coverImageURL":"","fullURL":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","pathToRoot":"..","attachments":[],"createdTime":1767062570855,"modifiedTime":1768211183928,"sourceSize":34219,"sourcePath":"第3章 激活函数与非线性数学/3.3 梯度饱和与梯度爆炸的数学根源.md","exportPath":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","showInTree":true,"treeOrder":12,"backlinks":["大模型中的数学.html"],"type":"markdown"},"大模型中的数学.html":{"title":"大模型中的数学","icon":"","description":"\n<a data-href=\"1.1 线性代数与张量运算\" href=\"第1章-数学基础/1.1-线性代数与张量运算.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1.1 线性代数与张量运算</a>\n<br><a data-href=\"1.2 概率论与统计\" href=\"第1章-数学基础/1.2-概率论与统计.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1.2 概率论与统计</a>\n<br><a data-href=\"1.3 微积分与优化基础\" href=\"第1章-数学基础/1.3-微积分与优化基础.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">1.3 微积分与优化基础</a> <br><a data-href=\"2.1 神经元的数学模型\" href=\"第2章-前馈网络数学/2.1-神经元的数学模型.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2.1 神经元的数学模型</a>\n<br><a data-href=\"2.2 神经网络的矩阵形式\" href=\"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2.2 神经网络的矩阵形式</a>\n<br><a data-href=\"2.3 前向传播的数学本质\" href=\"第2章-前馈网络数学/2.3-前向传播的数学本质.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2.3 前向传播的数学本质</a>\n<br><a data-href=\"2.4 反向传播梯度推导\" href=\"第2章-前馈网络数学/2.4-反向传播梯度推导.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">2.4 反向传播梯度推导</a> <br><a data-href=\"3.1 激活函数的数学角色\" href=\"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3.1 激活函数的数学角色</a>\n<br><a data-href=\"3.2 导数推导与梯度特性\" href=\"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3.2 导数推导与梯度特性</a>\n<br><a data-href=\"3.3 梯度饱和与梯度爆炸的数学根源\" href=\"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html\" class=\"internal-link\" target=\"_self\" rel=\"noopener nofollow\">3.3 梯度饱和与梯度爆炸的数学根源</a>\n","aliases":[],"inlineTags":[],"frontmatterTags":[],"headers":[{"heading":"Wir müssen wissen, wir werden wissen --David Hilbert","level":1,"id":"Wir_müssen_wissen,_wir_werden_wissen_--David_Hilbert_0"},{"heading":"我们必须知道，我们终将知道 --大卫·希尔伯特","level":5,"id":"我们必须知道，我们终将知道_--大卫·希尔伯特_0"},{"heading":"第一章 线性代数基础","level":2,"id":"第一章_线性代数基础_0"},{"heading":"第二章 前馈网络数学","level":2,"id":"第二章_前馈网络数学_0"},{"heading":"第三章 激活函数与非线性数学","level":2,"id":"第三章_激活函数与非线性数学_0"}],"links":["第1章-数学基础/1.1-线性代数与张量运算.html","第1章-数学基础/1.2-概率论与统计.html","第1章-数学基础/1.3-微积分与优化基础.html","第2章-前馈网络数学/2.1-神经元的数学模型.html","第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","第2章-前馈网络数学/2.3-前向传播的数学本质.html","第2章-前馈网络数学/2.4-反向传播梯度推导.html","第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html"],"author":"","coverImageURL":"","fullURL":"大模型中的数学.html","pathToRoot":".","attachments":[],"createdTime":1767691816397,"modifiedTime":1768211282688,"sourceSize":696,"sourcePath":"大模型中的数学.md","exportPath":"大模型中的数学.html","showInTree":true,"treeOrder":13,"backlinks":[],"type":"markdown"}},"fileInfo":{"第1章-数学基础/1.1-线性代数与张量运算.html":{"createdTime":1767062570796,"modifiedTime":1767773409182,"sourceSize":33194,"sourcePath":"第1章 数学基础/1.1 线性代数与张量运算.md","exportPath":"第1章-数学基础/1.1-线性代数与张量运算.html","showInTree":true,"treeOrder":1,"backlinks":["index.html"],"type":"markdown","data":null},"第1章-数学基础/1.2-概率论与统计.html":{"createdTime":1767062570795,"modifiedTime":1767773432423,"sourceSize":31796,"sourcePath":"第1章 数学基础/1.2 概率论与统计.md","exportPath":"第1章-数学基础/1.2-概率论与统计.html","showInTree":true,"treeOrder":2,"backlinks":["index.html"],"type":"markdown","data":null},"第1章-数学基础/1.3-微积分与优化基础.html":{"createdTime":1767062570789,"modifiedTime":1767858763109,"sourceSize":19202,"sourcePath":"第1章 数学基础/1.3 微积分与优化基础.md","exportPath":"第1章-数学基础/1.3-微积分与优化基础.html","showInTree":true,"treeOrder":3,"backlinks":["index.html"],"type":"markdown","data":null},"第2章-前馈网络数学/2.1-神经元的数学模型.html":{"createdTime":1767062570851,"modifiedTime":1767861363873,"sourceSize":21030,"sourcePath":"第2章 前馈网络数学/2.1 神经元的数学模型.md","exportPath":"第2章-前馈网络数学/2.1-神经元的数学模型.html","showInTree":true,"treeOrder":5,"backlinks":["index.html"],"type":"markdown","data":null},"site-lib/scripts/graph-wasm.wasm":{"createdTime":1767858711981,"modifiedTime":1767748737828.0103,"sourceSize":23655,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.wasm","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null},"site-lib/fonts/94f2f163d4b698242fef.otf":{"createdTime":1768211296126,"modifiedTime":1768211296126,"sourceSize":66800,"sourcePath":"","exportPath":"site-lib/fonts/94f2f163d4b698242fef.otf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/72505e6a122c6acd5471.woff2":{"createdTime":1768211296126,"modifiedTime":1768211296126,"sourceSize":104232,"sourcePath":"","exportPath":"site-lib/fonts/72505e6a122c6acd5471.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/2d5198822ab091ce4305.woff2":{"createdTime":1768211296127,"modifiedTime":1768211296127,"sourceSize":104332,"sourcePath":"","exportPath":"site-lib/fonts/2d5198822ab091ce4305.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/c8ba52b05a9ef10f4758.woff2":{"createdTime":1768211296147,"modifiedTime":1768211296147,"sourceSize":98868,"sourcePath":"","exportPath":"site-lib/fonts/c8ba52b05a9ef10f4758.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cb10ffd7684cd9836a05.woff2":{"createdTime":1768211296128,"modifiedTime":1768211296128,"sourceSize":106876,"sourcePath":"","exportPath":"site-lib/fonts/cb10ffd7684cd9836a05.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/293fd13dbca5a3e450ef.woff2":{"createdTime":1768211296128,"modifiedTime":1768211296128,"sourceSize":105924,"sourcePath":"","exportPath":"site-lib/fonts/293fd13dbca5a3e450ef.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/085cb93e613ba3d40d2b.woff2":{"createdTime":1768211296128,"modifiedTime":1768211296128,"sourceSize":112184,"sourcePath":"","exportPath":"site-lib/fonts/085cb93e613ba3d40d2b.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/b5f0f109bc88052d4000.woff2":{"createdTime":1768211296127,"modifiedTime":1768211296127,"sourceSize":105804,"sourcePath":"","exportPath":"site-lib/fonts/b5f0f109bc88052d4000.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/cbe0ae49c52c920fd563.woff2":{"createdTime":1768211296127,"modifiedTime":1768211296127,"sourceSize":106108,"sourcePath":"","exportPath":"site-lib/fonts/cbe0ae49c52c920fd563.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/535a6cf662596b3bd6a6.woff2":{"createdTime":1768211296127,"modifiedTime":1768211296127,"sourceSize":111708,"sourcePath":"","exportPath":"site-lib/fonts/535a6cf662596b3bd6a6.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/70cc7ff27245e82ad414.ttf":{"createdTime":1768211296148,"modifiedTime":1768211296148,"sourceSize":192740,"sourcePath":"","exportPath":"site-lib/fonts/70cc7ff27245e82ad414.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/454577c22304619db035.ttf":{"createdTime":1768211296148,"modifiedTime":1768211296148,"sourceSize":161376,"sourcePath":"","exportPath":"site-lib/fonts/454577c22304619db035.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/52ac8f3034507f1d9e53.ttf":{"createdTime":1768211296147,"modifiedTime":1768211296147,"sourceSize":191568,"sourcePath":"","exportPath":"site-lib/fonts/52ac8f3034507f1d9e53.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/05b618077343fbbd92b7.ttf":{"createdTime":1768211296149,"modifiedTime":1768211296149,"sourceSize":155288,"sourcePath":"","exportPath":"site-lib/fonts/05b618077343fbbd92b7.ttf","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2":{"createdTime":1768211296120,"modifiedTime":1768211296120,"sourceSize":7876,"sourcePath":"","exportPath":"site-lib/fonts/4bb6ac751d1c5478ff3a.woff2","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/media/6155340132a851f6089e.svg":{"createdTime":1768211296121,"modifiedTime":1768211296121,"sourceSize":315,"sourcePath":"","exportPath":"site-lib/media/6155340132a851f6089e.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/media/2308ab1944a6bfa5c5b8.svg":{"createdTime":1768211296122,"modifiedTime":1768211296122,"sourceSize":278,"sourcePath":"","exportPath":"site-lib/media/2308ab1944a6bfa5c5b8.svg","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/fonts/mathjax_zero.woff":{"createdTime":1768211296117,"modifiedTime":1768211296117,"sourceSize":1368,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_zero.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-regular.woff":{"createdTime":1768211296117,"modifiedTime":1768211296117,"sourceSize":34160,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-bold.woff":{"createdTime":1768211296117,"modifiedTime":1768211296117,"sourceSize":34464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-italic.woff":{"createdTime":1768211296117,"modifiedTime":1768211296117,"sourceSize":19360,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_main-italic.woff":{"createdTime":1768211296117,"modifiedTime":1768211296117,"sourceSize":20832,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_main-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_math-bolditalic.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":19776,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_math-bolditalic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size1-regular.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":5792,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size1-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size2-regular.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":5464,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size2-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size3-regular.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":3244,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size3-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_size4-regular.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":5148,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_size4-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_ams-regular.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":40808,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_ams-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-regular.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":9600,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_calligraphic-bold.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":9908,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_calligraphic-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-regular.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":21480,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_fraktur-bold.woff":{"createdTime":1768211296118,"modifiedTime":1768211296118,"sourceSize":22340,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_fraktur-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-regular.woff":{"createdTime":1768211296119,"modifiedTime":1768211296119,"sourceSize":12660,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-bold.woff":{"createdTime":1768211296119,"modifiedTime":1768211296119,"sourceSize":15944,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_sansserif-italic.woff":{"createdTime":1768211296119,"modifiedTime":1768211296119,"sourceSize":14628,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_sansserif-italic.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_script-regular.woff":{"createdTime":1768211296119,"modifiedTime":1768211296119,"sourceSize":11852,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_script-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_typewriter-regular.woff":{"createdTime":1768211296119,"modifiedTime":1768211296119,"sourceSize":17604,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_typewriter-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-regular.woff":{"createdTime":1768211296119,"modifiedTime":1768211296119,"sourceSize":1136,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-regular.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/fonts/mathjax_vector-bold.woff":{"createdTime":1768211296119,"modifiedTime":1768211296119,"sourceSize":1116,"sourcePath":"","exportPath":"site-lib/fonts/mathjax_vector-bold.woff","showInTree":false,"treeOrder":0,"backlinks":[],"type":"font","data":null},"site-lib/html/file-tree-content.html":{"createdTime":1768211296276,"modifiedTime":1768211296276,"sourceSize":6058,"sourcePath":"","exportPath":"site-lib/html/file-tree-content.html","showInTree":false,"treeOrder":0,"backlinks":[],"type":"html","data":null},"site-lib/scripts/webpage.js":{"createdTime":1768200167661,"modifiedTime":1768200167661,"sourceSize":110729,"sourcePath":"","exportPath":"site-lib/scripts/webpage.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-wasm.js":{"createdTime":1768200167661,"modifiedTime":1768200167661,"sourceSize":12885,"sourcePath":"","exportPath":"site-lib/scripts/graph-wasm.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/scripts/graph-render-worker.js":{"createdTime":1768200167661,"modifiedTime":1768200167661,"sourceSize":5681,"sourcePath":"","exportPath":"site-lib/scripts/graph-render-worker.js","showInTree":false,"treeOrder":0,"backlinks":[],"type":"script","data":null},"site-lib/media/favicon.png":{"createdTime":1768211296042,"modifiedTime":1768211296042,"sourceSize":1105,"sourcePath":"","exportPath":"site-lib/media/favicon.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/styles/obsidian.css":{"createdTime":1768211296191,"modifiedTime":1768211296191,"sourceSize":206058,"sourcePath":"","exportPath":"site-lib/styles/obsidian.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/global-variable-styles.css":{"createdTime":1768211296111,"modifiedTime":1768211296111,"sourceSize":305,"sourcePath":"","exportPath":"site-lib/styles/global-variable-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"site-lib/styles/main-styles.css":{"createdTime":1768200167683,"modifiedTime":1768200167683,"sourceSize":19521,"sourcePath":"","exportPath":"site-lib/styles/main-styles.css","showInTree":false,"treeOrder":0,"backlinks":[],"type":"style","data":null},"graph/xorscene_manimce_v0.19.1.png":{"createdTime":1767861131131,"modifiedTime":1767861075528,"sourceSize":93583,"sourcePath":"graph/XORScene_ManimCE_v0.19.1.png","exportPath":"graph/xorscene_manimce_v0.19.1.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/linearinseparablescene_manimce_v0.19.1.png":{"createdTime":1767860710245,"modifiedTime":1767860667896,"sourceSize":89466,"sourcePath":"graph/LinearInseparableScene_ManimCE_v0.19.1.png","exportPath":"graph/linearinseparablescene_manimce_v0.19.1.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/backprop.drawio.png":{"createdTime":1767771456575,"modifiedTime":1767771525432,"sourceSize":298927,"sourcePath":"graph/backprop.drawio.png","exportPath":"graph/backprop.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/combinedscene_manimce_v0.19.1.png":{"createdTime":1767858534721,"modifiedTime":1767858672081,"sourceSize":216961,"sourcePath":"graph/CombinedScene_ManimCE_v0.19.1.png","exportPath":"graph/combinedscene_manimce_v0.19.1.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/grad.png":{"createdTime":1767773281772,"modifiedTime":1767773281878,"sourceSize":265391,"sourcePath":"graph/grad.png","exportPath":"graph/grad.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/dist_relation.drawio.png":{"createdTime":1767756869104,"modifiedTime":1767765068902,"sourceSize":169619,"sourcePath":"graph/dist_relation.drawio.png","exportPath":"graph/dist_relation.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/gauss_dist.drawio.png":{"createdTime":1767755780562,"modifiedTime":1767765083603,"sourceSize":149158,"sourcePath":"graph/gauss_dist.drawio.png","exportPath":"graph/gauss_dist.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/msg_relation.drawio.png":{"createdTime":1767757688849,"modifiedTime":1767765099361,"sourceSize":199173,"sourcePath":"graph/msg_relation.drawio.png","exportPath":"graph/msg_relation.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/llm_flow.drawio.png":{"createdTime":1767684541018,"modifiedTime":1767765036789,"sourceSize":110895,"sourcePath":"graph/llm_flow.drawio.png","exportPath":"graph/llm_flow.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/pca.drawio.png":{"createdTime":1767685002403,"modifiedTime":1767764906858,"sourceSize":73023,"sourcePath":"graph/pca.drawio.png","exportPath":"graph/pca.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"site-lib/rss.xml":{"createdTime":1768211301377,"modifiedTime":1768211301377,"sourceSize":75754,"sourcePath":"","exportPath":"site-lib/rss.xml","showInTree":false,"treeOrder":0,"backlinks":[],"type":"other","data":null},"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html":{"createdTime":1767062570847,"modifiedTime":1768188525931,"sourceSize":19881,"sourcePath":"第2章 前馈网络数学/2.2 神经网络的矩阵形式.md","exportPath":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","showInTree":true,"treeOrder":6,"backlinks":["index.html"],"type":"markdown","data":null},"第2章-前馈网络数学/2.3-前向传播的数学本质.html":{"createdTime":1767062570843,"modifiedTime":1768199986514,"sourceSize":21005,"sourcePath":"第2章 前馈网络数学/2.3 前向传播的数学本质.md","exportPath":"第2章-前馈网络数学/2.3-前向传播的数学本质.html","showInTree":true,"treeOrder":7,"backlinks":["index.html"],"type":"markdown","data":null},"graph/learn_msg.drawio.png":{"createdTime":1768189747433,"modifiedTime":1768189785296,"sourceSize":86205,"sourcePath":"graph/learn_msg.drawio.png","exportPath":"graph/learn_msg.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"graph/comp_graph.drawio.png":{"createdTime":1768190415078,"modifiedTime":1768190415271,"sourceSize":49561,"sourcePath":"graph/comp_graph.drawio.png","exportPath":"graph/comp_graph.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null},"第2章-前馈网络数学/2.4-反向传播梯度推导.html":{"createdTime":1767062570838,"modifiedTime":1768202108194,"sourceSize":23952,"sourcePath":"第2章 前馈网络数学/2.4 反向传播梯度推导.md","exportPath":"第2章-前馈网络数学/2.4-反向传播梯度推导.html","showInTree":true,"treeOrder":8,"backlinks":["大模型中的数学.html"],"type":"markdown","data":null},"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html":{"createdTime":1767062570863,"modifiedTime":1768205677505,"sourceSize":30764,"sourcePath":"第3章 激活函数与非线性数学/3.1 激活函数的数学角色.md","exportPath":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","showInTree":true,"treeOrder":10,"backlinks":["大模型中的数学.html"],"type":"markdown","data":null},"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html":{"createdTime":1767062570860,"modifiedTime":1768209060257,"sourceSize":35297,"sourcePath":"第3章 激活函数与非线性数学/3.2 导数推导与梯度特性.md","exportPath":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","showInTree":true,"treeOrder":11,"backlinks":["大模型中的数学.html"],"type":"markdown","data":null},"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html":{"createdTime":1767062570855,"modifiedTime":1768211183928,"sourceSize":34219,"sourcePath":"第3章 激活函数与非线性数学/3.3 梯度饱和与梯度爆炸的数学根源.md","exportPath":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","showInTree":true,"treeOrder":12,"backlinks":["大模型中的数学.html"],"type":"markdown","data":null},"大模型中的数学.html":{"createdTime":1767691816397,"modifiedTime":1768211282688,"sourceSize":696,"sourcePath":"大模型中的数学.md","exportPath":"大模型中的数学.html","showInTree":true,"treeOrder":13,"backlinks":[],"type":"markdown","data":null},"graph/backward.drawio.png":{"createdTime":1768202040796,"modifiedTime":1768202041006,"sourceSize":126631,"sourcePath":"graph/backward.drawio.png","exportPath":"graph/backward.drawio.png","showInTree":false,"treeOrder":0,"backlinks":[],"type":"media","data":null}},"sourceToTarget":{"第1章 数学基础/1.1 线性代数与张量运算.md":"第1章-数学基础/1.1-线性代数与张量运算.html","第1章 数学基础/1.2 概率论与统计.md":"第1章-数学基础/1.2-概率论与统计.html","第1章 数学基础/1.3 微积分与优化基础.md":"第1章-数学基础/1.3-微积分与优化基础.html","第2章 前馈网络数学/2.1 神经元的数学模型.md":"第2章-前馈网络数学/2.1-神经元的数学模型.html","index.md":"index.html","":"site-lib/rss.xml","graph/XORScene_ManimCE_v0.19.1.png":"graph/xorscene_manimce_v0.19.1.png","graph/LinearInseparableScene_ManimCE_v0.19.1.png":"graph/linearinseparablescene_manimce_v0.19.1.png","graph/backprop.drawio.png":"graph/backprop.drawio.png","graph/CombinedScene_ManimCE_v0.19.1.png":"graph/combinedscene_manimce_v0.19.1.png","graph/grad.png":"graph/grad.png","graph/dist_relation.drawio.png":"graph/dist_relation.drawio.png","graph/gauss_dist.drawio.png":"graph/gauss_dist.drawio.png","graph/msg_relation.drawio.png":"graph/msg_relation.drawio.png","graph/llm_flow.drawio.png":"graph/llm_flow.drawio.png","graph/pca.drawio.png":"graph/pca.drawio.png","第2章 前馈网络数学/2.2 神经网络的矩阵形式.md":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","第2章 前馈网络数学/2.3 前向传播的数学本质.md":"第2章-前馈网络数学/2.3-前向传播的数学本质.html","graph/learn_msg.drawio.png":"graph/learn_msg.drawio.png","graph/comp_graph.drawio.png":"graph/comp_graph.drawio.png","第2章 前馈网络数学/2.4 反向传播梯度推导.md":"第2章-前馈网络数学/2.4-反向传播梯度推导.html","第3章 激活函数与非线性数学/3.1 激活函数的数学角色.md":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","第3章 激活函数与非线性数学/3.2 导数推导与梯度特性.md":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","第3章 激活函数与非线性数学/3.3 梯度饱和与梯度爆炸的数学根源.md":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","大模型中的数学.md":"大模型中的数学.html","graph/backward.drawio.png":"graph/backward.drawio.png"},"featureOptions":{"backlinks":{"featureId":"backlinks","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".footer","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Backlinks","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"tags":{"featureId":"tags","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showInlineTags":true,"showFrontmatterTags":true,"info_showInlineTags":{"show":true,"name":"","description":"Show tags defined inside the document at the top of the page.","placeholder":""},"info_showFrontmatterTags":{"show":true,"name":"","description":"Show tags defined in the frontmatter of the document at the top of the page.","placeholder":""}},"alias":{"featureId":"aliases","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header .data-bar","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Aliases","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"properties":{"featureId":"properties","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":".header","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Properties","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"info_hideProperties":{"show":true,"name":"","description":"A list of properties to hide from the properties view","placeholder":""}},"fileNavigation":{"featureId":"file-navigation","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"showCustomIcons":false,"showDefaultFolderIcons":false,"showDefaultFileIcons":false,"defaultFolderIcon":"lucide//folder","defaultFileIcon":"lucide//file","defaultMediaIcon":"lucide//file-image","exposeStartingPath":true,"info_showCustomIcons":{"show":true,"name":"","description":"Show custom icons for files and folders","placeholder":""},"info_showDefaultFolderIcons":{"show":true,"name":"","description":"Show a default icon of a folder for every folder in the tree","placeholder":""},"info_showDefaultFileIcons":{"show":true,"name":"","description":"Show a default icon of a file for every file in the tree","placeholder":""},"info_defaultFolderIcon":{"show":true,"name":"","description":"The icon to use for folders. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultFileIcon":{"show":true,"name":"","description":"The icon to use for files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_defaultMediaIcon":{"show":true,"name":"","description":"The icon to use for media files. Prefix with 'lucide//' to use a Lucide icon","placeholder":""},"info_exposeStartingPath":{"show":true,"name":"","description":"Whether or not to show the current file in the file tree when the page is first loaded","placeholder":""},"includePath":"site-lib/html/file-tree.html"},"search":{"featureId":"search","enabled":false,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#left-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Search...","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"outline":{"featureId":"outline","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Outline","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"startCollapsed":false,"minCollapseDepth":0,"info_startCollapsed":{"show":true,"name":"","description":"Should the outline start collapsed?","placeholder":""},"info_minCollapseDepth":{"show":true,"name":"","description":"Only allow outline items to be collapsed if they are at least this many levels deep in the tree.","placeholder":"","dropdownOptions":{"1":1,"2":2,"No Collapse":100}}},"themeToggle":{"featureId":"theme-toggle","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar .topbar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""}},"graphView":{"featureId":"graph-view","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"#right-sidebar-content","type":"start","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"displayTitle":"Graph View","info_displayTitle":{"show":true,"name":"","description":"Descriptive title to show above the feature","placeholder":""},"showOrphanNodes":true,"showAttachments":false,"allowGlobalGraph":true,"allowExpand":true,"attractionForce":1,"linkLength":15,"repulsionForce":80,"centralForce":2,"edgePruning":100,"minNodeRadius":3,"maxNodeRadius":7,"info_showOrphanNodes":{"show":true,"name":"","description":"Show nodes that are not connected to any other nodes.","placeholder":""},"info_showAttachments":{"show":true,"name":"","description":"Show attachments like images and PDFs as nodes in the graph.","placeholder":""},"info_allowGlobalGraph":{"show":true,"name":"","description":"Allow the user to view the global graph of all nodes.","placeholder":""},"info_allowExpand":{"show":true,"name":"","description":"Allow the user to pop-out the graph view to take up the whole screen","placeholder":""},"info_attractionForce":{"show":true,"name":"","description":"How much should linked nodes attract each other? This will make the graph appear more clustered.","placeholder":""},"info_linkLength":{"show":true,"name":"","description":"How long should the links between nodes be? The shorter the links the more connected nodes will cluster together.","placeholder":""},"info_repulsionForce":{"show":true,"name":"","description":"How much should nodes repel each other? This will make disconnected parts more spread out.","placeholder":""},"info_centralForce":{"show":true,"name":"","description":"How much should nodes be attracted to the center? This will make the graph appear more dense and circular.","placeholder":""},"info_edgePruning":{"show":true,"name":"","description":"Edges with a length above this threshold will not be rendered, however they will still contribute to the simulation. This can help large tangled graphs look more organised. Hovering over a node will still display these links.","placeholder":""},"info_minNodeRadius":{"show":true,"name":"","description":"How small should the smallest nodes be? The smaller a node is the less it will attract other nodes.","placeholder":""},"info_maxNodeRadius":{"show":true,"name":"","description":"How large should the largest nodes be? Nodes are sized by how many links they have. The larger a node is the more it will attract other nodes. This can be used to create a good grouping around the most important nodes.","placeholder":""}},"sidebar":{"featureId":"sidebar","enabled":false,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"allowResizing":true,"allowCollapsing":true,"rightDefaultWidth":"20em","leftDefaultWidth":"20em","info_allowResizing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be resized","placeholder":""},"info_allowCollapsing":{"show":true,"name":"","description":"Whether or not to allow the sidebars to be collapsed","placeholder":""},"info_rightDefaultWidth":{"show":true,"name":"","description":"The default width of the right sidebar","placeholder":""},"info_leftDefaultWidth":{"show":true,"name":"","description":"The default width of the left sidebar","placeholder":""}},"customHead":{"featureId":"custom-head","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"featurePlacement":{"selector":"head","type":"end","info_selector":{"show":true,"name":"","description":"CSS selector for an element. The feature will be placed relative to this element.","placeholder":""},"info_type":{"show":true,"name":"","description":"Will this feature be placed before, after, or inside (at the beggining or end).","placeholder":"","dropdownOptions":{"Before":"before","After":"after","Start":"start","End":"end"}}},"info_featurePlacement":{"show":true,"name":"","description":"Where to place this feature on the page. (Relative to the selector)","placeholder":""},"info_includePath":{"show":false,"name":"","description":"","placeholder":""},"sourcePath":"","info_sourcePath":{"show":true,"name":"","description":"The local path to the source .html file which will be included.","placeholder":"","fileInputOptions":{"makeRelativeToVault":true,"browseButton":true}},"includePath":"site-lib/html/custom-head.html"},"document":{"featureId":"obsidian-document","enabled":true,"unavailable":false,"alwaysEnabled":true,"hideSettingsButton":false,"allowFoldingLists":true,"allowFoldingHeadings":true,"documentWidth":"40em","info_allowFoldingLists":{"show":true,"name":"","description":"Whether or not to allow lists to be folded","placeholder":""},"info_allowFoldingHeadings":{"show":true,"name":"","description":"Whether or not to allow headings to be folded","placeholder":""},"info_documentWidth":{"show":true,"name":"","description":"The width of the document","placeholder":""}},"rss":{"featureId":"rss","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":false,"siteUrl":"","authorName":"","info_siteUrl":{"show":true,"name":"","description":"The url that this site will be hosted at","placeholder":"https://example.com/mysite"},"info_authorName":{"show":true,"name":"","description":"The name of the author of the site","placeholder":""}},"linkPreview":{"featureId":"link-preview","enabled":true,"unavailable":false,"alwaysEnabled":false,"hideSettingsButton":true}},"modifiedTime":1768211296196,"siteName":"math_of_llm","vaultName":"math_of_llm","exportRoot":"","baseURL":"","pluginVersion":"1.9.2","themeName":"","bodyClasses":"publish css-settings-manager styled-scrollbars show-inline-title show-ribbon is-focused","hasFavicon":false}