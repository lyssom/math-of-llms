{"documentCount":28,"nextId":102,"documentIds":{"21":"大模型中的数学.html","41":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","42":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","44":"第2章-前馈网络数学/2.4-反向传播梯度推导.html","45":"第2章-前馈网络数学/2.3-前向传播的数学本质.html","46":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","47":"第2章-前馈网络数学/2.1-神经元的数学模型.html","48":"第1章-数学基础/1.3-微积分与优化基础.html","50":"第1章-数学基础/1.1-线性代数与张量运算.html","51":"第4章-损失函数数学/4.1-均方误差（mse）的数学基础与几何解释.html","61":"第4章-损失函数数学/4.2-交叉熵的概率论推导.html","69":"第4章-损失函数数学/4.4-infonce与注意力的softmax统一.html","70":"第4章-损失函数数学/4.3-损失函数数学结构对比.html","71":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","85":"index.html","86":"第5章-注意力机制数学/5.6-注意力机制的变体.html","87":"第5章-注意力机制数学/5.5-注意力矩阵的谱性质与低秩结构.html","88":"第6章-位置编码数学/6.4-rotary-position-embedding（rope）的复数、群论完整推导.html","89":"第6章-位置编码数学/6.3-可学习位置编码的矩阵性质.html","90":"第6章-位置编码数学/6.2-频率空间的数学分析.html","91":"第6章-位置编码数学/6.1-正弦余弦位置编码的数学定义与推导.html","92":"第5章-注意力机制数学/5.4-注意力如何建模长程依赖.html","93":"第5章-注意力机制数学/5.3-多头注意力的矩阵推导与表达能力分析.html","94":"第5章-注意力机制数学/5.2-query、key、value的矩阵表示与变换.html","95":"第5章-注意力机制数学/5.1-scaled-dot-product-attention-的数学公式.html","96":"第1章-数学基础/1.2-概率论与统计.html","97":"第4章-损失函数数学/4.6-损失函数的优化性质.html","101":"第4章-损失函数数学/4.5-大语言模型中的损失函数.html"},"fieldIds":{"title":0,"aliases":1,"headers":2,"tags":3,"path":4,"content":5},"fieldLength":{"21":[1,1,17,1,2,64],"41":[2,1,35,1,5,900],"42":[3,1,14,1,6,433],"44":[3,1,19,1,6,513],"45":[3,1,15,1,6,611],"46":[2,1,7,1,5,305],"47":[3,1,14,1,6,542],"48":[3,1,6,1,6,500],"50":[2,1,26,1,5,871],"51":[5,1,32,1,8,665],"61":[3,1,44,1,6,872],"69":[2,1,21,1,5,731],"70":[3,1,27,1,6,808],"71":[3,1,28,1,6,858],"85":[1,1,23,1,2,137],"86":[3,1,27,1,6,541],"87":[2,1,48,1,5,910],"88":[8,1,22,1,11,920],"89":[3,1,14,1,6,725],"90":[3,1,13,1,6,694],"91":[3,1,12,1,6,707],"92":[3,1,50,1,6,1075],"93":[3,1,38,1,6,812],"94":[5,1,26,1,8,437],"95":[7,1,28,1,10,573],"96":[3,1,17,1,6,990],"97":[3,1,20,1,6,540],"101":[3,1,37,1,6,693]},"averageFieldLength":[3.142857142857143,1,24.285714285714285,1,6,658.1071428571428],"storedFields":{"21":{"title":"大模型中的数学","aliases":[],"headers":["Wir müssen wissen, wir werden wissen --David Hilbert","我们必须知道，我们终将知道 --大卫·希尔伯特","第一章 线性代数基础","第二章 前馈网络数学","第三章 激活函数与非线性数学"],"tags":[],"path":"大模型中的数学.html"},"41":{"title":"3.3 梯度饱和与梯度爆炸的数学根源","aliases":[],"headers":["3.3.1 梯度饱和的数学机制","饱和现象的数学定义","Sigmoid函数的饱和区域分析","Tanh函数的饱和特性","饱和效应的累积放大机制","3.3.2 梯度爆炸的数学机制","梯度爆炸的现象描述","简单RNN的梯度分析","Transformer中的梯度爆炸机制","残差连接的数学作用","梯度爆炸的阈值与裁剪策略","3.3.3 梯度流与信息保持","有效秩与信息传递","奇异值与梯度范数分析","初始化与梯度流","与注意力机制的梯度流","3.3.4 归一化技术与梯度稳定","批归一化的数学原理","层归一化的数学原理","层归一化与Transformer的配合","RMSNorm与计算优化","归一化技术的梯度稳定机制","3.3.5 激活函数设计的数学原则","避免梯度饱和区域","保持梯度流稳定性","提供足够的非线性表达能力","计算效率","与下游任务的契合","与注意力机制的设计协同","3.3.6本节小结"],"tags":[],"path":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html"},"42":{"title":"3.2 导数推导与梯度特性","aliases":[],"headers":["3.2.1 Sigmoid函数的导数与性质","Sigmoid函数的数学定义","导数推导与自化性质","高阶导数与泰勒展开","几何解释与梯度特性","3.2.2 Tanh函数的导数与性质","Tanh函数的数学定义与性质","与Sigmoid的数学关系","导数推导与性质分析","3.2.3 ReLU函数族的导数与性质","ReLU函数的定义与导数"],"tags":[],"path":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html"},"44":{"title":"2.4 反向传播梯度推导","aliases":[],"headers":["2.4.1 引言：为什么需要反向传播","2.4.2 链式法则的矩阵形式","2.4.3 误差信号的传播","2.4.4 参数梯度的完整推导","2.4.5 批量处理的梯度计算","2.4.6 计算复杂度分析","2.4.7 反向传播与自动微分的关系","2.4.8 数值稳定性与实现细节","2.4.9 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.4-反向传播梯度推导.html"},"45":{"title":"2.3 前向传播的数学本质","aliases":[],"headers":["2.3.1 引言：从计算到函数逼近","2.3.2 复合函数视角下的前向传播","2.3.3 信息流动的几何描述","2.3.4 计算图的表示与实现","2.3.5 前向传播的计算复杂度","2.3.6 前向传播的并行性","2.3.7 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.3-前向传播的数学本质.html"},"46":{"title":"2.2 神经网络的矩阵形式","aliases":[],"headers":["2.2.1 引言：从神经元到层","2.2.2 单层网络的矩阵表示","2.2.3 多层网络的矩阵形式"],"tags":[],"path":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html"},"47":{"title":"2.1 神经元的数学模型","aliases":[],"headers":["2.1.1 从生物神经元到数学抽象","2.1.2 感知机与仿射变换","2.1.3 激活函数的数学角色","2.1.4 神经元的几何解释","2.1.5 概率视角下的神经元模型","2.1.6 神经元模型的矩阵表示","2.1.7 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.1-神经元的数学模型.html"},"48":{"title":"1.3 微积分与优化基础","aliases":[],"headers":["1.3.1 偏导数与链式法则","1.3.2 多元函数梯度与Hessian","1.3.3 梯度下降与凸优化基础"],"tags":[],"path":"第1章-数学基础/1.3-微积分与优化基础.html"},"50":{"title":"1.1 线性代数与张量运算","aliases":[],"headers":["1.1.1 线性代数的核心地位","1.1.2 基本运算与运算规则","1.1.3 张量的表示与运算","1.1.4 特征空间的几何直觉","1.1.5 特征值与特征向量","1.1.6 奇异值与低秩近似","1.1.7 正交性与正交矩阵","1.1.8 矩阵范数与度量","1.1.9 矩阵分解的深化理解","1.1.10 Kronecker积与向量化","1.1.11 张量网络与高维运算","1.1.12 批处理与并行计算的张量视图","1.1.13 计算复杂度与内存效率"],"tags":[],"path":"第1章-数学基础/1.1-线性代数与张量运算.html"},"51":{"title":"4.1 均方误差（MSE）的数学基础与几何解释","aliases":[],"headers":["4.1.1 基本定义与符号约定","4.1.2 加权均方误差的推广","4.1.3 总体MSE与样本MSE","4.1.4 条件期望与最优预测","4.1.5 高斯噪声假设与最大似然估计","4.1.6 MSE与其他损失函数的联系","4.1.7 向量空间中的MSE","4.1.8 正交投影与最优预测","4.1.9 几何图示与直观理解","4.1.10 期望MSE的分解","4.1.11 各分解项的数学意义","4.1.12 偏差-方差权衡的可视化","4.1.13 凸性与优化特性","4.1.14 对异常值的敏感性","4.1.15 MSE的梯度特性","本节小结"],"tags":[],"path":"第4章-损失函数数学/4.1-均方误差（mse）的数学基础与几何解释.html"},"61":{"title":"4.2 交叉熵的概率论推导","aliases":[],"headers":["4.2.1 信息论基础：熵与散度的数学定义","自信息的概率论定义","香农熵：概率分布的信息期望","Kullback-Leibler散度：分布间差异的信息度量","4.2.2 交叉熵与KL散度的数学关系","交叉熵的定义与推导","交叉熵与KL散度的等价性分析","交叉熵的数学性质","4.2.3 交叉熵在分类任务中的完整推导","二分类任务的交叉熵损失","多分类任务与Softmax函数","多分类交叉熵损失的完整推导","从Logits到概率的完整流程","4.2.4 Softmax与交叉熵的梯度计算","交叉熵梯度的链式法则","雅可比矩阵的结构分析","4.2.5 数值稳定性问题与LogSumExp技巧","Softmax数值不稳定的根源","Log-Subtraction问题 除了Softmax","LogSumExp技巧的数学原理","融合计算：Softmax-CrossEntropy联合优化","融合梯度计算","4.2.6 交叉熵与最大似然估计的统一视角","最大似然估计的基本原理","分类任务的最大似然推导","从KL散度视角的再解释","概率解释的完整性","泛化性讨论","4.2.7 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.2-交叉熵的概率论推导.html"},"69":{"title":"4.4 InfoNCE与注意力的Softmax统一","aliases":[],"headers":["4.4.1 InfoNCE损失函数的数学基础","从NCE到InfoNCE的理论演进","InfoNCE的数学定义与推导","InfoNCE的互信息下界性质","4.4.2 InfoNCE的梯度结构与优化动力学","梯度的显式计算","正负样本对比机制的几何解释","温度参数的作用与影响","4.4.3 Softmax：统一的数学框架","Softmax操作的几何与概率本质","Softmax在三大场景中的统一性","从Softmax统一性看Transformer的设计哲学","4.4.4 从InfoNCE到注意力的数学桥梁","注意力作为广义对比学习","互信息最大化的视角","多头注意力的数学结构"],"tags":[],"path":"第4章-损失函数数学/4.4-infonce与注意力的softmax统一.html"},"70":{"title":"4.3 损失函数数学结构对比","aliases":[],"headers":["4.3.1 均方误差与交叉熵的数学结构对比","结构定义与基本形式","梯度结构的对比分析","Hessian矩阵与曲率分析","4.3.2 损失函数数学性质的系统性对比","凸性分析","梯度饱和特性","边界行为分析","4.3.3 几何视角下的损失函数分析","概率单纯形上的几何结构","信息几何视角","黎曼流形优化","4.3.4 任务适配性与损失函数选择","回归任务与分类任务的本质差异","损失函数选择的原则","多任务学习中的损失函数组合","4.3.5 与注意力机制的数学联系","Softmax结构的统一性","注意力权重与概率分布的类比","位置编码与特征工程的对应","4.3.6 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.3-损失函数数学结构对比.html"},"71":{"title":"3.1 激活函数的数学角色","aliases":[],"headers":["3.1.1 非线性的数学必要性","线性映射的数学结构","线性模型表达能力的根本局限","非线性变换打破表达瓶颈","3.1.2 激活函数作为函数逼近的基石","通用逼近定理的数学表述","局部基函数与函数重构","深度与宽度的数学权衡","3.1.3 概率视角下的激活函数","Sigmoid函数与伯努利分布","Softmax函数与多项式分布","Gumbel-Softmax与离散分布采样","3.1.4 信息论视角与激活函数","激活函数的信息压缩作用","激活函数与互信息最大化","激活函数的信息几何视角","3.1.5 激活函数与Transformer架构的数学协同","前馈网络中的激活函数选择","注意力机制与激活函数的协同","激活函数与位置编码的配合","3.1.6 本节小结"],"tags":[],"path":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html"},"85":{"title":"大模型中的数学","aliases":[],"headers":["Wir müssen wissen, wir werden wissen --David Hilbert","我们必须知道，我们终将知道 --大卫·希尔伯特","第一章 线性代数基础","第二章 前馈网络数学","第三章 激活函数与非线性数学","第四章 损失函数数学","第五章 注意力机制数学","第六章 位置编码数学"],"tags":[],"path":"index.html"},"86":{"title":"5.6 注意力机制的变体","aliases":[],"headers":["5.6.1 稀疏注意力的数学框架","5.6.2 稀疏模式的学习与优化","5.6.3 稀疏注意力的复杂度分析","5.6.4 线性注意力的数学定义","5.6.5 特征映射函数的设计","5.6.6 线性注意力的表达能力分析","5.6.7 递归形式的线性注意力","5.6.8 Flash注意力的数学原理","5.6.9 Flash注意力的复杂度与性能分析","5.6.10 Flash注意力-2与Flash注意力-3","5.6.11 多查询注意力的数学定义","5.6.12 分组查询注意力的平衡设计","5.6.15 门控与注意力的交互分析"],"tags":[],"path":"第5章-注意力机制数学/5.6-注意力机制的变体.html"},"87":{"title":"5.5 注意力矩阵的谱性质与低秩结构","aliases":[],"headers":["5.5.1 注意力矩阵的数学定义","5.5.2 注意力矩阵与Gram矩阵的关系","5.5.3 注意力矩阵的范数性质","5.5.4 注意力矩阵的特征值分析","5.5.5 谱半径与收敛性","5.5.6 特征向量的数学结构","5.5.7 条件数与数值稳定性","5.5.8 低秩结构的定义与直观解释","5.5.9 Softmax变换对秩的影响","5.5.10 实证分析：有效秩与衰减奇异值","5.5.11 低秩分解与计算优化","5.5.12 低秩结构与信息瓶颈","5.5.13 与卷积矩阵的比较","5.5.14 与Gram矩阵的比较","5.5.15 与核矩阵的比较","5.5.16 表达能力的理论界限","5.5.7 泛化能力的理论分析","5.5.18 计算效率的理论分析","5.5.19 奇异值分布的实证分析","5.5.20 注意力矩阵的病态性问题","5.5.21 线性注意力的谱分析","5.5.22 稀疏注意力的谱性质","5.5.23 注意力矩阵的低秩近似与模型压缩","5.5.24 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.5-注意力矩阵的谱性质与低秩结构.html"},"88":{"title":"6.4 Rotary Position Embedding（RoPE）的复数、群论完整推导","aliases":[],"headers":["6.4.1 位置编码的本质问题","6.4.2 RoPE的核心思想：从幅度到相位的范式转换","6.4.3 RoPE的数学形式化","6.4.4 群论视角：RoPE的深层结构","6.4.5 RoPE与正弦位置编码的对比分析","6.4.6 RoPE的工程实现","6.4.7 RoPE的变体与扩展","6.4.8 RoPE在大语言模型中的应用","6.4.9 位置编码的更广阔图景","6.4.10 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.4-rotary-position-embedding（rope）的复数、群论完整推导.html"},"89":{"title":"6.3 可学习位置编码的矩阵性质","aliases":[],"headers":["6.3.1 可学习位置编码的定义与参数化","6.3.2 表达能力分析","6.3.3 矩阵秩与奇异值分析","6.3.4 初始化策略与训练动态","6.3.5 与正弦编码的比较分析","6.3.6 参数效率与实践考量","6.3.7 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.3-可学习位置编码的矩阵性质.html"},"90":{"title":"6.2 频率空间的数学分析","aliases":[],"headers":["6.2.1傅里叶分析基础","6.2.2 正弦余弦编码的频谱结构","6.2.3 频率选择与编码分辨率","6.2.4 相对位置的频率表示","6.2.5 复数形式的统一分析","6.2.6 频率空间与位置空间的对偶性","6.2.7 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.2-频率空间的数学分析.html"},"91":{"title":"6.1 正弦余弦位置编码的数学定义与推导","aliases":[],"headers":["6.1.1 正弦余弦位置编码的数学定义","6.1.2 编码的唯一性与可辨识性","6.1.3 与注意力分数的交互分析","6.1.4 数值示例与可视化","6.1.5 正弦余弦编码与其他编码的比较","6.1.6 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.1-正弦余弦位置编码的数学定义与推导.html"},"92":{"title":"5.4 注意力如何建模长程依赖","aliases":[],"headers":["5.4.1 循环神经网络的梯度流问题","5.4.2 路径长度与依赖建模","5.4.3 依赖跨度的数学度量","5.4.4 全连接结构与信息直达","5.4.5 注意力权重的归一化与信息聚合","5.4.6 相对位置与内容无关的交互","5.4.7 信息传递路径的数学分析","5.4.8 长序列中的效率优势","5.4.9 内存带宽与计算效率","5.4.10 注意力权重的分布特性","5.4.11 局部注意力与全局注意力","5.4.12 注意力头的专业化分工","5.4.13 层级化的依赖建模","5.4.14 依赖传递的数学模型","5.4.15 残差连接与梯度流动","5.4.16 层级特征与依赖跨度","5.4.17 位置编码的必要性","5.4.18 正弦位置编码的数学性质","5.4.19 旋转位置编码与相对位置建模","5.4.20 远程依赖的边界效应","5.4.21 注意力机制的函数空间","5.4.22 表达能力与计算能力界限","5.4.23 依赖建模的计算复杂度下界","5.4.24 注意力机制的归纳偏置","5.4.25 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.4-注意力如何建模长程依赖.html"},"93":{"title":"5.3 多头注意力的矩阵推导与表达能力分析","aliases":[],"headers":["5.3.1 从单头到多头的动机","5.3.2 多头注意力的数学定义","5.3.3 参数数量与维度配置","5.3.4 投影矩阵的并行计算","5.3.5 多头注意力的张量计算形式","5.3.6 注意力头的重参数化视角","5.3.7 多头机制与子空间学习","5.3.8 与混合专家模型的类比","5.3.9 注意力头的分工与专业化","5.3.10 头数与头维度的权衡","5.3.11 多头注意力的复杂度","5.3.12 并行计算与硬件加速","5.3.13 注意力头的稀疏激活","5.3.14 注意力头的分组与跨头交互","5.3.15 多查询注意力与分组查询注意力","5.3.16 多头注意力的函数逼近视角","5.3.17 头间的信息流动分析","5.3.18 多头注意力与卷积运算的关系","5.3.19 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.3-多头注意力的矩阵推导与表达能力分析.html"},"94":{"title":"5.2 Query、Key、Value的矩阵表示与变换","aliases":[],"headers":["5.2.1 输入嵌入矩阵的数学表示","5.2.2 线性投影的矩阵形式","5.2.3 维度设计与参数数量","5.2.4 投影矩阵的行列式分析","5.2.5 投影矩阵的奇异值分解","5.2.6 正交投影与投影矩阵的初始化","5.2.7 查询空间、键空间与值空间的语义分离","5.2.8 投影的几何视角：空间变换与特征提取","5.2.9 注意力分数的矩阵分解表示","5.2.10 反向传播中的梯度计算","5.2.11 缩放对梯度稳定性的影响","5.2.12 多层堆叠中的表示变换"],"tags":[],"path":"第5章-注意力机制数学/5.2-query、key、value的矩阵表示与变换.html"},"95":{"title":"5.1 Scaled Dot-Product Attention 的数学公式","aliases":[],"headers":["5.1.1 注意力机制的核心思想","5.1.2 缩放点积注意力的数学定义","5.1.3 注意力分数矩阵的几何意义","5.1.4 缩放因子的统计学原理","5.1.5 Softmax函数的饱和行为分析","5.1.6 缩放因子的数学推导与作用","5.1.7 注意力权重的概率解释","5.1.8 Softmax的梯度流分析","5.1.9 注意力权重的温度参数推广","5.1.10 注意力计算的矩阵运算流程","5.1.11 计算复杂度分析","5.1.12 因果掩码的数学形式","5.1.13 填充掩码的实现","5.1.14 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.1-scaled-dot-product-attention-的数学公式.html"},"96":{"title":"1.2 概率论与统计","aliases":[],"headers":["1.2.1 随机变量与概率分布","1.2.2 期望、方差与协方差","1.2.3 高斯分布与多元高斯","1.2.4 KL散度与交叉熵","1.2.5 统计推断与参数估计","1.2.6 大数定律与中心极限定理","1.2.7 假设检验与模型评估","1.2.8 Logits与概率分布的生成"],"tags":[],"path":"第1章-数学基础/1.2-概率论与统计.html"},"97":{"title":"4.6 损失函数的优化性质","aliases":[],"headers":["4.6.1 优化景观的几何分析","凸性与全局最优性","Hessian矩阵与曲率分析","鞍点与局部极小值的景观分析","4.6.2 梯度下降动力学与收敛分析","梯度流与离散动力系统","学习率调度与收敛性","自适应优化算法的收敛性质","梯度裁剪与爆炸抑制","混合精度训练与损失缩放","4.6.5 损失函数优化性质的比较分析","不同损失函数的优化难度比较","学习率敏感性分析","收敛速度与最终性能","4.6.6 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.6-损失函数的优化性质.html"},"101":{"title":"4.5 大语言模型中的损失函数","aliases":[],"headers":["4.5.1 语言建模损失：下一个Token预测的数学框架","自回归语言建模的形式化定义","位置无关性与注意力机制的数学关系","前缀语言建模与填充Token的数学处理","4.5.2 掩码语言建模：完形填空任务的数学分析","掩码语言建模的形式化定义","掩码策略的数学分析","分词器对损失函数的影响","4.5.3 多任务学习中的损失函数组合","多任务学习的数学框架","梯度归一化与帕累托优化","大语言模型中的典型多任务设置","4.5.4 人类反馈强化学习损失（RLHF）","RLHF的数学框架","KL散度约束的数学分析","PPO在RLHF中的应用","4.5.5 直接偏好优化（DPO）","DPO的数学推导","DPO的梯度分析","隐式奖励与对齐机制","4.5.6 损失函数前沿与未来方向","损失函数的理论极限","自适应损失函数","超越Softmax的归一化","损失函数与模型架构的协同设计","4.5.7 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.5-大语言模型中的损失函数.html"}},"dirtCount":3,"index":[["演化的梯度流方程为",{"5":{"97":1}}],["建立了理论收敛速度与实际优化行为之间的联系",{"5":{"97":2}}],["建模这种依赖需要",{"5":{"92":1}}],["建模这种依赖需要步的顺序计算",{"5":{"92":1}}],["建模成本都是固定的",{"5":{"92":2}}],["建模全局长程依赖",{"5":{"92":2}}],["建模是一个核心挑战",{"5":{"92":2}}],["建模能力使其在理解任务上具有优势",{"5":{"99":2,"101":2}}],["静态损失缩放使用固定不变的缩放因子",{"5":{"97":2}}],["帮助模型快速收敛到最优解",{"5":{"97":2}}],["帮助我们理解模型性能评估的可靠性",{"5":{"96":2}}],["初期使用",{"5":{"97":2}}],["初期使用较小的学习率有助于优化器",{"5":{"97":2}}],["初期的小学习率允许模型逐渐学习位置信息的细微模式",{"5":{"97":2}}],["初始的微小差异通过训练过程的放大",{"5":{"93":2}}],["初始编码范数与内容嵌入范数相当是合理的选择",{"5":{"89":2}}],["初始化策略和批量归一化的效果",{"5":{"96":2}}],["初始化策略与训练动态",{"2":{"89":1},"5":{"89":1}}],["初始化的选择会影响训练初期的梯度流动和收敛速度",{"5":{"94":2}}],["初始化的编码矩阵",{"5":{"89":2}}],["初始化尺度的选择影响训练初期的梯度流动",{"5":{"89":2}}],["初始化尺度等",{"5":{"89":2}}],["初始化方法",{"5":{"89":2}}],["初始化应该在",{"5":{"89":2}}],["初始化输出块",{"5":{"86":1}}],["初始化输出块​为零",{"5":{"86":1}}],["初始化行和",{"5":{"86":1}}],["初始化行和​为零",{"5":{"86":1}}],["初始化行最大值",{"5":{"86":1}}],["初始化行最大值​为",{"5":{"86":1}}],["初始化与梯度流",{"2":{"41":1},"5":{"41":1}}],["彩票假设",{"5":{"97":2}}],["找到一个主要的模式并紧密地拟合它",{"5":{"96":1}}],["鼓励",{"5":{"96":1}}],["鼓励权重矩阵的列向量相互正交",{"5":{"50":2}}],["及其概率分布",{"5":{"96":1}}],["划分为两个子向量",{"5":{"96":1}}],["划分为两个区域",{"5":{"45":1}}],["倾向于反向变化",{"5":{"96":1}}],["倾向于同向变化",{"5":{"96":1}}],["倾向于使用纯rope或纯alibi",{"5":{"88":2}}],["置换检验在比较两个模型在特定测试集上的性能时很有用",{"5":{"96":2}}],["置换检验通过随机打乱标签来生成置换样本",{"5":{"96":2}}],["置换检验",{"5":{"96":2}}],["置信区间比p值提供更多信息",{"5":{"96":2}}],["置信度",{"5":{"69":2}}],["报告效应量可以帮助读者理解改进的实际意义",{"5":{"96":2}}],["备择假设是我们想要支持的假设",{"5":{"96":2}}],["验证改进措施的有效性以及检测数据中的系统性偏差",{"5":{"96":2}}],["验证集越大",{"5":{"96":2}}],["验证一下复数乘法与矩阵乘法的对应关系",{"5":{"88":2}}],["弱大数定律保证样本均值以概率收敛于期望",{"5":{"96":2}}],["弱归纳偏置",{"5":{"92":2}}],["贝叶斯方法可用于模型选择",{"5":{"96":2}}],["贝叶斯估计使用后验分布进行预测",{"5":{"96":2}}],["贝叶斯估计是另一种参数估计方法",{"5":{"96":2}}],["贝叶斯估计和期望传播等统计推断方法",{"5":{"96":2}}],["罗下界",{"5":{"96":2}}],["罗森布拉特",{"5":{"47":2}}],["费希尔信息矩阵逆为方差的正态分布",{"5":{"96":2}}],["渐近正态的",{"5":{"96":2}}],["于某个概率分布",{"5":{"96":2}}],["于少数关键位置",{"5":{"92":2}}],["替换为软标签",{"5":{"96":2}}],["替换softmax",{"5":{"87":1}}],["联合熵",{"5":{"96":2}}],["联合训练允许可学习编码调整正弦编码的影响程度",{"5":{"89":2}}],["确定在原假设下观察到该统计量或更极端情况的概率",{"5":{"96":2}}],["确定性高",{"5":{"96":2}}],["确保输出的概率分布满足归一性条件",{"5":{"96":2}}],["确保输出是一个有效的概率分布",{"5":{"61":1}}],["确保比较的公平性",{"5":{"96":2}}],["确保梯度信号可以有效传播",{"5":{"95":2}}],["确保自回归生成时的",{"5":{"91":2}}],["确保位置编码不会过度主导内容信息",{"5":{"88":2}}],["确保位置信息在深层网络中稳定传递",{"5":{"41":2}}],["确保数学上的等价性",{"5":{"86":1}}],["确保误差为非负值",{"5":{"51":2}}],["确保参数更新不会过大",{"5":{"44":2}}],["确保稳定性但可能限制学习速度",{"5":{"41":1}}],["确保了特征的",{"5":{"86":2}}],["确保了主路径",{"5":{"41":2}}],["确保了梯度可以直接通过恒等映射传递",{"5":{"41":2}}],["确保新策略不会偏离旧策略太远",{"5":{"96":2}}],["确保新策略不会与旧策略相差太远",{"5":{"99":2,"101":2}}],["卡尔曼滤波和贝叶斯线性回归中有着核心应用",{"5":{"96":2}}],["稍后详述",{"5":{"96":2}}],["源于多个方面的原因",{"5":{"96":2}}],["源语言句子中的哪些位置",{"5":{"95":2}}],["源语言句子中的相关词语来获取信息",{"5":{"95":2}}],["峰度为3",{"5":{"96":2}}],["峰度",{"5":{"96":2}}],["矩母函数",{"5":{"96":2}}],["矩",{"5":{"96":2}}],["矩阵和鞍点分析",{"5":{"97":2}}],["矩阵和张量运算的计算复杂度和内存效率是核心关注点",{"5":{"50":2}}],["矩阵定义为",{"5":{"97":2}}],["矩阵秩与奇异值分析",{"2":{"89":1},"5":{"89":1}}],["矩阵有",{"5":{"87":1}}],["矩阵补全等",{"5":{"87":2}}],["矩阵接近奇异",{"5":{"87":2}}],["矩阵分解还包括lu分解",{"5":{"50":2}}],["矩阵分解的深化理解",{"2":{"50":1},"5":{"50":1}}],["矩阵范数是分析矩阵性质的强大工具",{"5":{"87":2}}],["矩阵范数是衡量矩阵",{"5":{"50":2}}],["矩阵范数等多个数学工具出发",{"5":{"87":2}}],["矩阵范数的选择取决于具体问题的需求",{"5":{"50":2}}],["矩阵范数将矩阵映射到非负实数",{"5":{"50":2}}],["矩阵范数与度量",{"2":{"50":1},"5":{"50":1}}],["矩阵是query投影和key投影的某种",{"5":{"94":1}}],["矩阵是二维数组",{"5":{"50":2}}],["矩阵是半正定对称矩阵",{"5":{"45":1}}],["矩阵形式的链式法则在深度学习中尤为重要",{"5":{"48":1}}],["矩阵运算一次性处理了所有",{"5":{"47":1}}],["矩阵运算一次性处理了所有个样本",{"5":{"47":1}}],["矩阵运算与批量处理的等价性",{"5":{"47":2}}],["矩阵运算级并行",{"5":{"45":1}}],["矩阵运算级和模型级等多个层次进行并行优化",{"5":{"45":2}}],["矩阵乘法的计算结果是一个的矩阵",{"5":{"95":1}}],["矩阵乘法的计算复杂度是",{"5":{"50":2}}],["矩阵乘法的flops约为",{"5":{"44":1}}],["矩阵乘法是连接两个张量的最基本操作",{"5":{"50":2}}],["矩阵乘法是最重要也是计算量最大的运算",{"5":{"50":2}}],["矩阵乘法满足结合律",{"5":{"50":2}}],["矩阵乘法tile",{"5":{"45":1}}],["矩阵乘法节点",{"5":{"45":2}}],["矩阵乘法",{"5":{"44":3,"50":2,"95":1}}],["矩阵乘法退化为逐元素乘法",{"5":{"44":2}}],["矩阵的特征值对应于数据主成分方向的曲率",{"5":{"97":2}}],["矩阵的特征值分解揭示了损失函数在不同方向上的曲率特性",{"5":{"97":2}}],["矩阵的每一行进行归一化",{"5":{"95":1}}],["矩阵的秩最大为​",{"5":{"94":1}}],["矩阵的大部分能量",{"5":{"87":2}}],["矩阵的条件数",{"5":{"87":2}}],["矩阵的条件数定义为",{"5":{"41":1}}],["矩阵的求逆是找到一个矩阵",{"5":{"50":1}}],["矩阵的求逆是找到一个矩阵使得",{"5":{"50":1}}],["矩阵的转置是将矩阵的行和列互换",{"5":{"50":2}}],["矩阵的形状",{"5":{"50":2}}],["矩阵的谱半径决定了系统的稳定性",{"5":{"41":1}}],["矩阵谱半径与稳定性",{"5":{"41":2}}],["矩阵",{"5":{"41":4,"45":1,"50":2,"61":2,"70":2,"86":1,"87":2,"92":2,"94":4,"97":4}}],["矩阵为",{"5":{"70":2,"97":2}}],["矩阵更为复杂",{"5":{"70":2,"97":2}}],["协方差矩阵在大语言模型中的应用包括",{"5":{"96":2}}],["协方差矩阵的对角线元素是各随机变量的方差",{"5":{"96":2}}],["协方差矩阵的特征值表示编码在不同方向上的分散程度",{"5":{"89":2}}],["协方差矩阵是多维随机变量的二阶中心矩描述",{"5":{"96":2}}],["协方差可以展开为",{"5":{"96":2}}],["协方差定义为",{"5":{"96":2}}],["协方差",{"5":{"96":2}}],["uniform",{"5":{"96":2}}],["unit",{"5":{"42":2,"86":2}}],["落在区间",{"5":{"96":1}}],["落在区间的概率为",{"5":{"96":1}}],["落在饱和区域",{"5":{"42":2}}],["伯努利分布可用于描述二元随机事件",{"5":{"96":2}}],["伯努利分布的期望为",{"5":{"96":2}}],["伯努利分布",{"5":{"96":2}}],["伯努利分布与sigmoid激活",{"5":{"47":2}}],["掷骰子的点数",{"5":{"96":2}}],["涉及",{"5":{"95":2}}],["涉及与的矩阵乘法",{"5":{"95":2}}],["灵活的可扩展性",{"5":{"95":2}}],["去相关",{"5":{"95":2}}],["去除关于输入的冗余信息",{"5":{"71":2}}],["舒适区",{"5":{"95":2}}],["式中",{"5":{"95":2}}],["索引",{"5":{"95":2}}],["索引为",{"5":{"91":4}}],["匹配",{"5":{"95":2}}],["匹配的正样本",{"5":{"69":1}}],["面临着长程依赖难以建模的困境",{"5":{"95":2}}],["填充掩码和因果掩码的组合展示了注意力机制的可扩展性",{"5":{"95":2}}],["填充掩码",{"5":{"95":2}}],["填充掩码的实现",{"2":{"95":1},"5":{"95":1}}],["填充位置不应该参与注意力计算",{"5":{"95":2}}],["填充token的数学处理",{"5":{"99":2,"101":2}}],["回顾",{"5":{"95":2}}],["回顾7",{"5":{"94":2}}],["回归任务",{"5":{"44":2}}],["回归任务与分类任务的本质差异",{"2":{"70":1},"5":{"70":1}}],["回归任务和分类任务在数学本质上的差异决定了它们需要不同类型的损失函数",{"5":{"70":2}}],["回归任务的数学结构",{"5":{"70":2}}],["回归任务选择",{"5":{"70":2}}],["供给",{"5":{"94":2}}],["需求",{"5":{"94":2}}],["需要处理",{"5":{"97":1}}],["需要仔细调参",{"5":{"97":2}}],["需要仔细分析",{"5":{"41":2}}],["需要注意的是",{"5":{"96":2}}],["需要将",{"5":{"93":1}}],["需要将按头进行拆分",{"5":{"93":1}}],["需要读取完整的",{"5":{"92":2}}],["需要读取完整的和矩阵",{"5":{"92":2}}],["需要堆叠约",{"5":{"92":1}}],["需要堆叠约层",{"5":{"92":1}}],["需要两步",{"5":{"92":2}}],["需要分析信息在层间传递的数学机制",{"5":{"92":2}}],["需要步顺序计算",{"5":{"92":1}}],["需要多层累积的",{"5":{"92":2}}],["需要修改注意力计算",{"5":{"91":2}}],["需要特殊处理",{"5":{"91":2}}],["需要特别注意数值稳定性问题",{"5":{"44":2}}],["需要使用适当的多重比较校正方法",{"5":{"96":2}}],["需要使用因果掩码",{"5":{"91":2}}],["需要使用离散傅里叶变换",{"5":{"90":2}}],["需要很多步才能遍历整个圆周",{"5":{"90":2}}],["需要额外的正则化技术",{"5":{"87":2}}],["需要计算和存储完整的",{"5":{"86":1}}],["需要计算和存储完整的注意力矩阵",{"5":{"86":1}}],["需要字节mb的存储",{"5":{"86":1}}],["需要在每个新标记到来时更新注意力输出",{"5":{"86":2}}],["需要在数学特性和计算效率之间进行权衡",{"5":{"41":2}}],["需要经过一系列具体的数学推导",{"5":{"61":2}}],["需要优化的",{"5":{"61":2}}],["需要大量gpu并行计算才能在合理时间内完成",{"5":{"45":2}}],["需要一种中间表示来桥接理论模型和计算程序",{"5":{"45":2}}],["需要一条非线性的决策边界",{"5":{"71":2}}],["需要",{"5":{"41":1,"50":1,"86":1,"92":2}}],["需要​",{"5":{"41":1}}],["需要检测梯度爆炸并采取应对措施",{"5":{"41":2}}],["需要更大的更新来纠正",{"5":{"99":2,"101":2}}],["需要聚合来自左右两侧的token信息",{"5":{"99":1,"101":1}}],["苹果",{"5":{"94":2}}],["肉",{"5":{"94":2}}],["竹子",{"5":{"94":2}}],["文章中各位置的上下文信息将被编码为value向量",{"5":{"94":2}}],["文章中的各个词元",{"5":{"94":2}}],["文本生成本质上是一个随机过程",{"5":{"96":2}}],["文本分类任务",{"5":{"99":2,"101":2}}],["捕捉输入的不同方面特征",{"5":{"94":2}}],["捕获该词元的语义信息",{"5":{"94":2}}],["捕获跨越长距离的语义关联",{"5":{"92":2}}],["捕获句子级别和篇章级别的特征",{"5":{"92":2}}],["捕获词汇级别和短语级别的特征",{"5":{"92":2}}],["捕获比",{"5":{"71":1}}],["捕获比更抽象",{"5":{"71":1}}],["什么样的特征应该作为",{"5":{"94":2}}],["什么内容出现在什么位置",{"5":{"91":2}}],["携带了多少关于输入",{"5":{"93":1}}],["地增加表示的丰富性",{"5":{"93":2}}],["地回传到第一层",{"5":{"92":2}}],["体现在参数层面",{"5":{"93":2}}],["扮演着类似的角色",{"5":{"93":2}}],["小波基",{"5":{"93":2}}],["小的​",{"5":{"90":1}}],["小的",{"5":{"90":5,"91":2}}],["小的奇异值可能对应噪声",{"5":{"89":2}}],["让一个注意力机制学习如何组合不同头的输出",{"5":{"93":2}}],["让我们分析几个代表性大语言模型中rope的具体实现细节",{"5":{"88":2}}],["让我们分析这个选择的数学意义",{"5":{"88":2}}],["让我们具体分析两种编码方式下query",{"5":{"88":2}}],["让我们给出rope平移等变性的严格数学证明",{"5":{"88":2}}],["让我们考虑一个简化的二维例子",{"5":{"88":2}}],["让我们首先建立复数与向量之间的对应关系",{"5":{"88":2}}],["跨头交互的另一种方式是使用",{"5":{"93":2}}],["跨头交互",{"5":{"93":2}}],["跨度为10",{"5":{"92":2}}],["跨度",{"5":{"92":2}}],["活跃",{"5":{"93":2}}],["活跃的神经元可以被解释为检测到特定模式的",{"5":{"71":2}}],["线程块",{"5":{"93":2}}],["线性无关",{"5":{"96":1}}],["线性增长",{"5":{"95":1}}],["线性增加",{"5":{"90":1}}],["线性投影的矩阵形式",{"2":{"94":1},"5":{"94":1}}],["线性偏置的方式可能限制了对局部模式的建模能力",{"5":{"88":2}}],["线性rope在实践中并未展现出显著优势",{"5":{"88":2}}],["线性rope的公式为",{"5":{"88":2}}],["线性rope探索了非均匀旋转的可能性",{"5":{"88":2}}],["线性核",{"5":{"87":2}}],["线性注意力变体的动机所在",{"5":{"95":2}}],["线性注意力避免了",{"5":{"87":1}}],["线性注意力避免了的注意力矩阵计算",{"5":{"87":1}}],["线性注意力可能无法表示某些复杂的注意力模式",{"5":{"87":2}}],["线性注意力通过核函数近似softmax",{"5":{"86":2}}],["线性注意力用核函数近似softmax",{"5":{"86":2}}],["线性注意力",{"5":{"86":4,"87":2}}],["线性注意力的一个常见问题是表达能力受限",{"5":{"87":2}}],["线性注意力的一个重要优势是支持递归",{"5":{"86":2}}],["线性注意力的核矩阵",{"5":{"87":1}}],["线性注意力的核矩阵具有与gram矩阵相似的性质",{"5":{"87":1}}],["线性注意力的核心思想是用核函数",{"5":{"87":1}}],["线性注意力的核心思想是用核函数替换softmax",{"5":{"87":1}}],["线性注意力的谱分析",{"2":{"87":1},"5":{"87":1}}],["线性注意力的递归更新只需要维护累积量",{"5":{"86":1}}],["线性注意力的递归更新只需要维护累积量和",{"5":{"86":1}}],["线性注意力的表达能力受限于所选的核函数",{"5":{"86":2}}],["线性注意力的表达能力分析",{"2":{"86":1},"5":{"86":1}}],["线性注意力的关键在于设计合适的特征映射函数",{"5":{"86":2}}],["线性注意力的数学定义",{"2":{"86":1},"5":{"86":1}}],["线性算子",{"5":{"51":2}}],["线性而非平方增长",{"5":{"51":2}}],["线性回归的闭式解就是通过求解正规方程得到的",{"5":{"50":2}}],["线性变换保持高斯性",{"5":{"96":2}}],["线性变换对特征空间的作用包括旋转",{"5":{"50":2}}],["线性变换是理解特征空间中数据变换的关键概念",{"5":{"50":2}}],["线性变换后的点集满足",{"5":{"47":2}}],["线性变换",{"5":{"45":1}}],["线性变换的奇异值分解",{"5":{"45":2}}],["线性变换的复合仍然是线性变换",{"5":{"71":2}}],["线性变换可以分解为旋转",{"5":{"45":1}}],["线性变换不变性",{"5":{"70":2,"97":2}}],["线性",{"5":{"70":1,"71":6}}],["线性分类器",{"5":{"71":2}}],["线性模型只能学习形如",{"5":{"71":1}}],["线性模型只能学习形如的函数",{"5":{"71":1}}],["线性模型必然产生系统性误差",{"5":{"71":2}}],["线性模型的函数空间是所有仿射函数的集合",{"5":{"71":2}}],["线性模型的学习能力可以用函数空间的维度来刻画",{"5":{"71":2}}],["线性模型虽然具有优美的数学性质",{"5":{"71":2}}],["线性模型表达能力的根本局限",{"2":{"71":1},"5":{"71":1}}],["线性映射描述了输入空间通过旋转",{"5":{"71":2}}],["线性映射可以用矩阵乘法完全描述",{"5":{"71":2}}],["线性映射",{"5":{"71":2}}],["线性映射的数学结构",{"2":{"71":1},"5":{"71":1}}],["线性代数的瑞士军刀",{"5":{"50":2}}],["线性代数的基本运算包括向量加法",{"5":{"50":2}}],["线性代数的核心地位",{"2":{"50":1},"5":{"50":1}}],["线性代数为理解和构建深度学习模型提供了坚实的数学框架",{"5":{"50":2}}],["线性代数作为现代数学的重要分支",{"5":{"50":2}}],["线性代数与张量运算",{"0":{"50":1},"4":{"50":1},"5":{"21":4,"50":1,"85":4}}],["线性代数与张量运算<",{"5":{"21":1,"85":1}}],["线性代数基础",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["线性最小二乘问题",{"5":{"70":1}}],["缓存的空间需求为",{"5":{"93":2}}],["综合来看",{"5":{"93":2}}],["综合以上分析",{"5":{"94":2}}],["综合以上三个阶段",{"5":{"93":2}}],["综合以上结果",{"5":{"44":2}}],["拼接后的输出矩阵",{"5":{"93":2}}],["切片",{"5":{"93":2}}],["切割",{"5":{"45":2}}],["免费",{"5":{"93":4}}],["公式",{"5":{"93":2,"95":4}}],["公式化的",{"5":{"89":2}}],["垫子",{"5":{"93":2}}],["坐",{"5":{"93":4}}],["坐在",{"5":{"93":4}}],["猫",{"5":{"93":6}}],["猫坐在垫子上",{"5":{"93":2}}],["达到百万甚至十亿级别时",{"5":{"92":1}}],["达到最大值",{"5":{"61":2}}],["事先就知道序列中任意两个位置之间是否",{"5":{"92":1}}],["事件与其论元的关联等",{"5":{"92":2}}],["量化地展示了这一优势",{"5":{"92":2}}],["量级",{"5":{"45":1}}],["归纳偏置",{"5":{"92":2}}],["归一化性质",{"5":{"61":1,"70":2}}],["归一化性质确保输出是一个有效的概率分布",{"5":{"61":1}}],["归一化后",{"5":{"51":2}}],["归一化维度",{"5":{"41":1}}],["归一化层",{"5":{"44":1}}],["归一化层会将其缩放",{"5":{"41":2}}],["归一化层的梯度计算涉及除以方差",{"5":{"41":2}}],["归一化",{"5":{"41":2,"69":2}}],["归一化将激活值的分布规范化到固定范围",{"5":{"41":2}}],["归一化确保了不同位置的位置编码具有相似的数值尺度",{"5":{"41":2}}],["归一化的数学效应",{"5":{"41":2}}],["归一化技术",{"5":{"41":2}}],["归一化技术通过多种机制稳定梯度传播",{"5":{"41":2}}],["归一化技术与位置编码在transformer中共同工作",{"5":{"41":2}}],["归一化技术与梯度稳定",{"2":{"41":1},"5":{"41":1}}],["归一化技术是现代深度学习训练的核心组件",{"5":{"41":2}}],["归一化技术的梯度稳定机制",{"2":{"41":1},"5":{"41":1}}],["旨在在保持长程依赖建模能力的同时降低计算复杂度",{"5":{"92":2}}],["必须是对称半正定的",{"5":{"96":2}}],["必须至少检查每一对位置一次",{"5":{"92":2}}],["必然事件的发生几乎不带来任何新信息",{"5":{"61":2}}],["延迟",{"5":{"92":2}}],["邻居",{"5":{"92":2}}],["承担",{"5":{"92":2}}],["承担着特征提取和信息整合的功能",{"5":{"47":1}}],["跳过20个词",{"5":{"92":2}}],["跳过10个词",{"5":{"92":2}}],["顶层",{"5":{"92":2}}],["底层主要进行局部特征提取",{"5":{"92":2}}],["底层",{"5":{"92":2}}],["间接传递到了位置",{"5":{"92":2}}],["间接影响fisher信息度量的几何结构",{"5":{"71":2}}],["桥接",{"5":{"92":2}}],["专门为relu激活函数设计",{"5":{"94":2}}],["专注于q",{"5":{"94":2}}],["专注于学习局部上下文信息",{"5":{"92":2}}],["专业技能",{"5":{"93":2}}],["专业化分工",{"5":{"92":2}}],["专业化",{"5":{"92":2,"93":2}}],["专家",{"5":{"93":4}}],["修改后的公式为",{"5":{"96":2}}],["修饰语",{"5":{"92":2}}],["修正线性单元",{"5":{"42":2,"47":2}}],["名词依赖",{"5":{"92":2}}],["谓语依赖",{"5":{"92":2}}],["谓语关系",{"5":{"92":2,"93":2}}],["往往表现出更多的全局注意力",{"5":{"92":2}}],["往往表现出更多的局部注意力",{"5":{"92":2}}],["浅层",{"5":{"92":2}}],["浅层网络需要指数级数量的神经元才能逼近",{"5":{"71":2}}],["浅层网络的线性组合较为简单",{"5":{"45":2}}],["篇章级别的指代关系等",{"5":{"92":2}}],["聚焦",{"5":{"92":2}}],["聚合的信息",{"5":{"92":1}}],["聚合",{"5":{"44":1,"87":2}}],["聚合信息",{"5":{"69":1}}],["顺序的",{"5":{"92":2}}],["换一种说法",{"5":{"92":2}}],["感受野",{"5":{"92":2}}],["感知机",{"5":{"47":2}}],["感知机的数学定义为",{"5":{"47":2}}],["感知机模型",{"5":{"47":2}}],["感知机通常采用连续可微的激活函数",{"5":{"47":2}}],["感知机是麦肯罗皮层神经元的重要推广",{"5":{"47":2}}],["感知机与仿射变换",{"2":{"47":1},"5":{"47":1}}],["早期层",{"5":{"92":2}}],["扁平化",{"5":{"92":2}}],["甚至更长",{"5":{"92":2}}],["日出",{"5":{"92":2}}],["爬山",{"5":{"92":2}}],["开头的",{"5":{"92":2}}],["开始",{"5":{"41":2,"44":2}}],["张成一个",{"5":{"91":1}}],["张量形式的表示虽然符号上更为复杂",{"5":{"93":2}}],["张量可重排为",{"5":{"50":1}}],["张量可重排为的张量",{"5":{"50":1}}],["张量并行",{"5":{"50":2}}],["张量运算的高效实现是现代深度学习框架的核心能力之一",{"5":{"50":2}}],["张量运算的基本操作包括逐元素运算",{"5":{"50":2}}],["张量网络在模型压缩和高效推理中有着重要的应用价值",{"5":{"50":2}}],["张量网络的思维方式有助于理解复杂的计算结构",{"5":{"50":2}}],["张量网络的核心思想是用多个小张量的乘积之和来表示一个巨大的高维张量",{"5":{"50":2}}],["张量网络是用低维张量通过网络结构表示高维张量的方法",{"5":{"50":2}}],["张量网络与高维运算",{"2":{"50":1},"5":{"50":1}}],["张量分解在模型压缩",{"5":{"50":2}}],["张量分解是处理高维张量的重要技术",{"5":{"50":2}}],["张量收缩指定两个张量的若干维度进行配对相乘并求和",{"5":{"50":2}}],["张量收缩是矩阵乘法在高维张量上的推广",{"5":{"50":2}}],["张量的维度通常具有明确的语义含义",{"5":{"50":2}}],["张量的表示与运算",{"2":{"50":1},"5":{"50":1}}],["张量是多维数组在数学上的抽象表示",{"5":{"50":2}}],["张量是多维数组的自然推广",{"5":{"50":2}}],["张量是最基本的数据结构",{"5":{"50":2}}],["生成任务要求模型在预测位置",{"5":{"95":1}}],["生成任务要求模型在预测位置的输出时只能依赖于位置到的信息",{"5":{"95":1}}],["生成一个独立的编码向量",{"5":{"91":1}}],["生成一个",{"5":{"91":1}}],["生物序列",{"5":{"89":2}}],["生物神经元是一种复杂的细胞系统",{"5":{"47":2}}],["生物神经元在膜电位超过阈值时发放动作电位",{"5":{"71":2}}],["贡献内容",{"5":{"91":2}}],["绘制编码矩阵",{"5":{"91":2}}],["绘制各维度随位置变化的曲线",{"5":{"91":2}}],["观察发现",{"5":{"91":2}}],["看到未来",{"5":{"95":2}}],["看到",{"5":{"91":2,"92":4}}],["矛盾",{"5":{"91":2}}],["既不是局部极小值也不是局部极大值",{"5":{"97":1}}],["既不会太小导致分布过于平坦",{"5":{"95":2}}],["既可以为正也可以为负",{"5":{"96":2}}],["既能覆盖足够多的不同尺度",{"5":{"91":2}}],["既保证了一次更新使用的样本数量",{"5":{"50":2}}],["背后蕴含着深刻的数学直觉和实践经验",{"5":{"91":2}}],["注入",{"5":{"91":2}}],["注意条件协方差不依赖于观测值",{"5":{"96":2}}],["注意这里对每一行独立进行softmax归一化",{"5":{"95":2}}],["注意这里的梯度不依赖于",{"5":{"94":1}}],["注意这里的梯度不依赖于​的具体值",{"5":{"94":1}}],["注意这里矩阵堆叠的方向",{"5":{"93":2}}],["注意这个空间需求与头数成正比",{"5":{"93":2}}],["注意位置1与位置2的点积和位置0与位置1的点积相等",{"5":{"91":2}}],["注意矩阵乘法的方向",{"5":{"44":2}}],["注意力集中于少数位置",{"5":{"95":2}}],["注意力可以通过学习实现",{"5":{"92":2}}],["注意力策略",{"5":{"92":2}}],["注意力具有明显的效率优势",{"5":{"92":2}}],["注意力的张量形式强调了多头的结构",{"5":{"93":2}}],["注意力的",{"5":{"92":1}}],["注意力的是一次性建模所有位置对之间的依赖",{"5":{"92":1}}],["注意力的计算也可以从概率分布的角度理解",{"5":{"61":2}}],["注意力头的注意力",{"5":{"93":2}}],["注意力头的分组与跨头交互",{"2":{"93":1},"5":{"93":1}}],["注意力头的分工与专业化",{"2":{"93":1},"5":{"93":1}}],["注意力头的稀疏激活",{"2":{"93":1},"5":{"93":1}}],["注意力头的重参数化视角",{"2":{"93":1},"5":{"93":1}}],["注意力头的专业化分工",{"2":{"92":1},"5":{"92":1}}],["注意力头数为32",{"5":{"45":2}}],["注意力传递",{"5":{"87":2}}],["注意力矩阵都可以用",{"5":{"87":1}}],["注意力矩阵都可以用个参数完全描述",{"5":{"87":1}}],["注意力矩阵仍然可能出现病态性",{"5":{"87":2}}],["注意力矩阵接近均匀矩阵",{"5":{"87":2}}],["注意力矩阵接近置换矩阵",{"5":{"87":2}}],["注意力矩阵可能在这两种极端情况之间波动",{"5":{"87":2}}],["注意力矩阵可以视为一个核矩阵",{"5":{"87":2}}],["注意力矩阵不一定对称",{"5":{"87":2}}],["注意力矩阵不具有平移不变性",{"5":{"87":2}}],["注意力矩阵经过softmax归一化",{"5":{"87":2}}],["注意力矩阵与核矩阵有以下联系",{"5":{"87":2}}],["注意力矩阵与输入相关",{"5":{"87":2}}],["注意力矩阵与卷积矩阵有几个关键区别",{"5":{"87":2}}],["注意力矩阵与gram矩阵有以下相似之处",{"5":{"87":2}}],["注意力矩阵与gram矩阵有着密切的关系",{"5":{"87":2}}],["注意力矩阵与gram矩阵的关系",{"2":{"87":1},"5":{"87":1}}],["注意力矩阵具有以下几个基本性质",{"5":{"87":1}}],["注意力矩阵是非线性的",{"5":{"87":2}}],["注意力矩阵是稠密的",{"5":{"87":2}}],["注意力矩阵是scaled",{"5":{"87":2}}],["注意力矩阵是query和key矩阵点积后经过softmax归一化得到的",{"5":{"87":2}}],["注意力矩阵的第",{"5":{"87":1}}],["注意力矩阵的第个元素为",{"5":{"87":1}}],["注意力矩阵的病态性是指其条件数过大",{"5":{"87":2}}],["注意力矩阵的病态性问题",{"2":{"87":1},"5":{"87":1}}],["注意力矩阵的秩受query",{"5":{"87":2}}],["注意力矩阵的结构是数据依赖的",{"5":{"87":2}}],["注意力矩阵的奇异值呈现",{"5":{"87":2}}],["注意力矩阵的低秩结构是transformer高效计算的理论基础",{"5":{"87":2}}],["注意力矩阵的低秩结构为模型压缩提供了理论基础",{"5":{"87":2}}],["注意力矩阵的低秩结构为计算优化提供了理论基础",{"5":{"87":2}}],["注意力矩阵的低秩结构对其表达能力有重要影响",{"5":{"87":2}}],["注意力矩阵的低秩结构意味着只使用了的维子空间信息",{"5":{"87":1}}],["注意力矩阵的低秩结构可以解释为一种",{"5":{"87":2}}],["注意力矩阵的低秩结构具有直观的解释",{"5":{"87":2}}],["注意力矩阵的低秩近似与模型压缩",{"2":{"87":1},"5":{"87":1}}],["注意力矩阵的条件数取决于注意力权重的分布",{"5":{"87":2}}],["注意力矩阵的特征向量揭示了其结构特性",{"5":{"87":2}}],["注意力矩阵的特征值具有以下性质",{"5":{"87":2}}],["注意力矩阵的特征值分析",{"2":{"87":1},"5":{"87":1}}],["注意力矩阵的核范数可以用于度量其有效秩",{"5":{"87":2}}],["注意力矩阵的谱可以展开为特征函数的加权和",{"5":{"87":2}}],["注意力矩阵的谱性质与gram矩阵有显著不同",{"5":{"87":2}}],["注意力矩阵的谱性质与低秩结构<",{"5":{"85":1}}],["注意力矩阵的谱性质与低秩结构",{"0":{"87":1},"4":{"87":1},"5":{"85":4,"87":1}}],["注意力矩阵的谱范数定义为",{"5":{"87":2}}],["注意力矩阵的frobenius范数定义为",{"5":{"87":2}}],["注意力矩阵的范数性质",{"2":{"87":1},"5":{"87":1}}],["注意力矩阵的数学定义",{"2":{"87":1},"5":{"87":1}}],["注意力矩阵",{"5":{"86":1,"87":3}}],["注意力窗口",{"5":{"86":2}}],["注意力如何建模长程依赖<",{"5":{"85":1}}],["注意力如何建模长程依赖",{"0":{"92":1},"4":{"92":1},"5":{"85":4,"92":1}}],["注意力层通常被多次堆叠",{"5":{"94":2}}],["注意力层",{"5":{"46":2}}],["注意力层会优先学习对该任务有利的表示",{"5":{"70":2}}],["注意力计算的矩阵运算流程",{"2":{"95":1},"5":{"95":1}}],["注意力计算的雅可比矩阵涉及softmax的梯度",{"5":{"89":2}}],["注意力计算的时间复杂度和空间复杂度均为",{"5":{"86":2}}],["注意力计算",{"5":{"45":2}}],["注意力机制和位置编码则提供了实现这些目标的计算框架",{"5":{"101":2}}],["注意力机制没有",{"5":{"92":4}}],["注意力机制达到了这个任务的下界",{"5":{"92":2}}],["注意力机制具有强大的表达能力",{"5":{"92":2}}],["注意力机制能够表示的函数类与传统的循环网络或卷积网络有显著差异",{"5":{"92":2}}],["注意力机制定义了一类特定的函数",{"5":{"92":2}}],["注意力机制在内存带宽效率方面也具有优势",{"5":{"92":2}}],["注意力机制在scaled",{"5":{"69":1}}],["注意力机制为",{"5":{"92":2}}],["注意力机制通过全连接结构从根本上解决了这一问题",{"5":{"92":2}}],["注意力机制通过softmax归一化将原始的注意力分数转换为有效的权重分布",{"5":{"92":2}}],["注意力机制通过一种革命性的设计解决了这一问题",{"5":{"92":2}}],["注意力机制从根本上改变了这一结构",{"5":{"92":2}}],["注意力机制允许序列中任意两个位置之间直接建立联系",{"5":{"91":2}}],["注意力机制与核方法有着深刻的数学关联",{"5":{"87":2}}],["注意力机制与激活函数的协同",{"2":{"71":1},"5":{"71":1}}],["注意力机制数学",{"2":{"85":1},"4":{"86":1,"87":1,"92":1,"93":1,"94":1,"95":1},"5":{"85":13}}],["注意力机制的",{"5":{"92":1}}],["注意力机制的复杂度在超长序列场景下仍然是瓶颈",{"5":{"92":1}}],["注意力机制的学习目标是学习这个oracle的权重分配",{"5":{"92":2}}],["注意力机制的函数类是",{"5":{"92":2}}],["注意力机制的函数空间",{"2":{"92":1},"5":{"92":1}}],["注意力机制的中间结果",{"5":{"92":2}}],["注意力机制的内存访问模式更加规则",{"5":{"92":2}}],["注意力机制的一个重要特性是它可以同时建模位置相关的依赖和内容相关的依赖",{"5":{"92":2}}],["注意力机制的核心思想可以概括为",{"5":{"95":2}}],["注意力机制的核心思想",{"2":{"95":1},"5":{"95":1}}],["注意力机制的核心特性是",{"5":{"92":1}}],["注意力机制的核心特性是是一个随机矩阵",{"5":{"92":1}}],["注意力机制的核心特性是全连接",{"5":{"92":2}}],["注意力机制的核心是一个",{"5":{"87":1}}],["注意力机制的核心是一个的注意力矩阵",{"5":{"87":1}}],["注意力机制的归纳偏置包括",{"5":{"92":2}}],["注意力机制的归纳偏置",{"2":{"92":1},"5":{"92":1}}],["注意力机制的变体<",{"5":{"85":1}}],["注意力机制的变体",{"0":{"86":1},"4":{"86":1},"5":{"85":4,"86":1}}],["注意力机制的梯度计算可以借鉴分类任务中",{"5":{"61":2}}],["注意力机制的梯度流可以视为一种信息瓶颈",{"5":{"41":2}}],["注意力机制的计算复杂度与序列长度的平方成正比",{"5":{"50":2}}],["注意力机制的计算过程就包含了多个矩阵乘法操作",{"5":{"50":2}}],["注意力机制的批处理实现是理解这一概念的良好例子",{"5":{"50":2}}],["注意力机制中的查询",{"5":{"50":2}}],["注意力机制中的softmax和ffn中的gelu形成了",{"5":{"71":2}}],["注意力机制",{"5":{"69":2,"71":4,"95":2,"101":2}}],["注意力机制使用softmax计算query与各key之间的注意力权重",{"5":{"61":2}}],["注意力机制使用这个分布对value向量进行加权平均",{"5":{"69":2}}],["注意力机制虽然不是直接源于信息论",{"5":{"69":2}}],["注意力机制可以被视为一种广义的对对比学习",{"5":{"69":2}}],["注意力机制提供了一种",{"5":{"92":2}}],["注意力机制提供了动态调整关注焦点的机制",{"5":{"99":2,"101":2}}],["注意力输出的信息量受到其维度​和注意力权重的约束",{"5":{"93":1}}],["注意力输出的第",{"5":{"92":1}}],["注意力输出的第个元素为",{"5":{"92":1}}],["注意力输出可以视为对值向量的",{"5":{"92":1}}],["注意力输出可以理解为value向量在注意力权重分布下的期望",{"5":{"70":1}}],["注意力输出需要经过前馈网络处理",{"5":{"41":2}}],["注意力输出为",{"5":{"41":2,"69":2,"86":2,"87":2,"92":2}}],["注意力输出",{"5":{"70":1,"92":1,"93":1}}],["注意力权重可以被解释为一种条件概率分布",{"5":{"95":2}}],["注意力权重可以满足",{"5":{"86":1}}],["注意力权重可以满足关注",{"5":{"86":1}}],["注意力权重​正确地反映了位置对位置的重要性",{"5":{"92":1}}],["注意力权重​同时依赖于位置和内容​",{"5":{"92":1}}],["注意力权重分布在语义相关的位置上",{"5":{"92":2}}],["注意力权重应该较大",{"5":{"92":4}}],["注意力权重的",{"5":{"97":2}}],["注意力权重的梯度可能具有特殊结构",{"5":{"97":2}}],["注意力权重的温度参数推广",{"2":{"95":1},"5":{"95":1}}],["注意力权重的概率解释",{"2":{"95":1},"5":{"95":1}}],["注意力权重的空间分布",{"5":{"92":2}}],["注意力权重的分布特性",{"2":{"92":1},"5":{"92":1}}],["注意力权重的归一化与信息聚合",{"2":{"92":1},"5":{"92":1}}],["注意力权重的计算结果只依赖于新的顺序",{"5":{"92":2}}],["注意力权重的计算",{"5":{"71":1}}],["注意力权重的计算将query",{"5":{"71":1}}],["注意力权重的计算公式为",{"5":{"61":2,"71":2}}],["注意力权重的谱结构",{"5":{"41":2}}],["注意力权重的稳定性分析可以借鉴",{"5":{"70":2}}],["注意力权重本身是一种概率分布",{"5":{"41":2}}],["注意力权重",{"5":{"61":2,"69":3,"92":2,"96":2}}],["注意力权重矩阵定义了一个有向加权图",{"5":{"92":1}}],["注意力权重矩阵定义了一种依赖关系图",{"5":{"92":1}}],["注意力权重矩阵是softmax作用于",{"5":{"91":1}}],["注意力权重矩阵是softmax作用于的结果",{"5":{"91":1}}],["注意力权重矩阵是非负行随机矩阵",{"5":{"41":1}}],["注意力权重矩阵可以看作是查询张量和键张量的收缩",{"5":{"50":1}}],["注意力权重矩阵",{"5":{"41":2,"50":1,"61":2,"70":2,"92":2}}],["注意力权重矩阵的谱性质决定了梯度如何被",{"5":{"41":1}}],["注意力权重矩阵的谱性质决定了梯度如何在不同位置之间传播",{"5":{"41":2}}],["注意力权重矩阵的计算涉及softmax操作",{"5":{"70":1}}],["注意力权重矩阵具有若干与概率分布类比的性质",{"5":{"70":1}}],["注意力权重与位置距离有明确的关系",{"5":{"92":2}}],["注意力权重与值向量的乘积同样需要",{"5":{"50":2}}],["注意力权重与值矩阵的乘积产生加权输出",{"5":{"50":2}}],["注意力权重与概率分布的类比",{"2":{"70":1},"5":{"70":1}}],["注意力模式",{"5":{"92":2,"99":2,"101":2}}],["注意力分数实际上是在定义的二次型下",{"5":{"94":1}}],["注意力分数特别高",{"5":{"92":2}}],["注意力分数不仅取决于查询和键的内容",{"5":{"92":2}}],["注意力分数只依赖于相对位置",{"5":{"91":2}}],["注意力分数矩阵和注意力权重矩阵都需要的存储空间",{"5":{"95":1}}],["注意力分数矩阵的元素为",{"5":{"95":2}}],["注意力分数矩阵的几何意义",{"2":{"95":1},"5":{"95":1}}],["注意力分数矩阵可以表示为",{"5":{"94":1}}],["注意力分数矩阵可以近似为",{"5":{"87":1}}],["注意力分数矩阵可以近似为​",{"5":{"87":1}}],["注意力分数矩阵为",{"5":{"91":2}}],["注意力分数矩阵",{"5":{"87":1,"92":2,"93":2,"94":1,"95":1}}],["注意力分数矩阵​​可以视为一种核矩阵",{"5":{"87":1}}],["注意力分数的矩阵分解表示",{"2":{"94":1},"5":{"94":1}}],["注意力分数的计算",{"5":{"50":2}}],["注意力分数的计算需要的计算量",{"5":{"50":1}}],["注意力分数的计算在批处理情况下变成的三维张量运算",{"5":{"50":1}}],["注意力分数的计算可以看作是一个张量收缩操作",{"5":{"50":2}}],["注意力分数的方差会很大",{"5":{"41":2}}],["注意力分数",{"5":{"41":2,"94":1}}],["注意力分数被缩放​​",{"5":{"69":1}}],["注意力分数被缩放",{"5":{"69":1}}],["注意力分数或",{"5":{"70":2}}],["注意力分数来自",{"5":{"70":2}}],["注意力分布较为集中",{"5":{"92":2}}],["注意力分布较为均匀",{"5":{"92":2}}],["注意力分布",{"5":{"69":2}}],["注意力作为广义对比学习",{"2":{"69":1},"5":{"69":1}}],["注意力作为软选择的数学表达",{"5":{"69":2}}],["精确率等指标的置信区间",{"5":{"96":2}}],["精确位置",{"5":{"91":2}}],["精度下溢",{"5":{"44":1}}],["角频率",{"5":{"91":1}}],["角频率​随维度呈指数级变化",{"5":{"91":1}}],["角度变化不会过快",{"5":{"88":2}}],["角度可以无限增长",{"5":{"88":2}}],["写成更紧凑的形式",{"5":{"91":2}}],["写成向量形式即",{"5":{"44":2}}],["奇异矩阵",{"5":{"94":2}}],["奇异值的大小决定了各个方向上的",{"5":{"94":2}}],["奇异值的数量",{"5":{"41":2}}],["奇异值衰减的速度与层深",{"5":{"87":2}}],["奇异值分布揭示了编码的结构特性",{"5":{"89":2}}],["奇异值分布",{"5":{"89":2}}],["奇异值分布的分析对于理解模型行为有重要价值",{"5":{"87":2}}],["奇异值分布的实证分析",{"2":{"87":1},"5":{"87":1}}],["奇异值分布更均匀",{"5":{"87":2}}],["奇异值分布通常呈现以下模式",{"5":{"87":2}}],["奇异值分解可以用于理解模型的表达能力",{"5":{"50":2}}],["奇异值分解之所以强大",{"5":{"50":2}}],["奇异值分解与特征值分解有密切关系",{"5":{"50":2}}],["奇异值分解",{"5":{"50":2,"87":2,"94":2}}],["奇异值分解揭示了线性变换的几何本质",{"5":{"45":2}}],["奇异值分解定理",{"5":{"45":2}}],["奇异值与低秩近似",{"2":{"50":1},"5":{"50":1}}],["奇异值与梯度范数分析",{"2":{"41":1},"5":{"41":1}}],["奇异值为",{"5":{"45":1}}],["奇异值为对角线上特征值的平方根",{"5":{"45":1}}],["奇异值",{"5":{"41":1}}],["奇异值和权重初始化的数学原理",{"5":{"41":2}}],["奇异值决定了梯度在对应方向上的缩放因子",{"5":{"41":1}}],["奇异值小于1的方向会导致梯度衰减",{"5":{"41":2}}],["奇异值大于1的方向会导致梯度放大",{"5":{"41":2}}],["奇数维度",{"5":{"91":2}}],["兼容性",{"5":{"91":2}}],["兼容线性代数结构",{"5":{"88":2}}],["纯注意力机制是置换不变的",{"5":{"91":2,"92":2}}],["辐角增加",{"5":{"90":1}}],["辐角增加​",{"5":{"90":1}}],["互不相同",{"5":{"90":1}}],["互信息可用于分析不同位置或不同层之间的信息流动",{"5":{"96":2}}],["互信息始终非负",{"5":{"96":2}}],["互信息在底层较低",{"5":{"92":2}}],["互信息在各层都较高",{"5":{"92":1}}],["互信息逐渐降低",{"5":{"92":2}}],["互信息为1",{"5":{"92":2}}],["互信息衡量的是知道",{"5":{"69":1,"71":1}}],["互信息衡量的是知道后关于的信息量",{"5":{"69":1,"71":1}}],["互信息的下界估计",{"5":{"71":2}}],["互信息",{"5":{"69":2,"71":2,"92":1,"96":2}}],["互信息是非负的",{"5":{"71":2}}],["互信息是信息论中度量两个随机变量之间依赖程度的基本概念",{"5":{"69":2}}],["互信息最大化的视角",{"2":{"69":1},"5":{"69":1}}],["互信息最大化",{"5":{"69":1}}],["呈指数级变化",{"5":{"91":1}}],["呈指数级增长",{"5":{"41":2}}],["呈现出某种",{"5":{"91":2}}],["呈周期性变化",{"5":{"90":1}}],["规律性",{"5":{"90":2}}],["意味着均方误差的曲率在整个参数空间中是不变的",{"5":{"97":2}}],["意味着位置编码的参数是高度结构化的",{"5":{"90":2}}],["意味着所有编码向量线性无关",{"5":{"89":2}}],["起到正则化的作用",{"5":{"90":2}}],["答案取决于具体任务和评估标准",{"5":{"90":2}}],["答案是否定的",{"5":{"51":2}}],["冲激",{"5":{"90":2}}],["虚部包含正弦项",{"5":{"90":2}}],["消除了分别处理正弦和余弦的复杂性",{"5":{"90":2}}],["受最低频率限制",{"5":{"90":2}}],["连续随机变量由概率密度函数",{"5":{"96":2}}],["连续随机变量可以取任意实数值或实数区间内的值",{"5":{"96":2}}],["连续情况",{"5":{"90":2}}],["连续可微",{"5":{"47":2}}],["振幅为1",{"5":{"90":2}}],["符号有所不同",{"5":{"90":2}}],["快速收敛不一定带来更好的最终性能",{"5":{"97":2}}],["快速傅里叶变换",{"5":{"90":2}}],["快速衰减的奇异值分布对应于",{"5":{"89":2}}],["快速衰减分布",{"5":{"89":2}}],["快速衰减",{"5":{"87":4}}],["逆变换为",{"5":{"90":2}}],["逆元",{"5":{"88":2}}],["周期很短",{"5":{"91":2}}],["周期很长",{"5":{"91":2}}],["周期性",{"5":{"90":2}}],["周期函数可以展开为",{"5":{"90":2}}],["周期为10000",{"5":{"90":2}}],["周期为1",{"5":{"90":2}}],["周期为",{"5":{"88":2,"90":2}}],["奠定坚实的理论基础",{"5":{"90":2}}],["奠定了坚实的理论基础",{"5":{"90":2}}],["奠定了坚实的基础",{"5":{"51":2}}],["奠定了理论基础",{"5":{"61":2,"70":2,"71":2}}],["认识到其在灵活性与泛化性之间的权衡",{"5":{"89":2}}],["讨论了训练良好的模型中不同头会发展出",{"5":{"93":2}}],["讨论了混合位置编码的设计",{"5":{"89":2}}],["讨论了数值稳定性问题及其解决方案",{"5":{"61":2}}],["太小的初始化可能限制位置信息的作用",{"5":{"89":2}}],["太大的初始化可能导致训练不稳定",{"5":{"89":2}}],["太远",{"5":{"99":1,"101":1}}],["占比约3",{"5":{"89":2}}],["占总参数量的比例不可忽略",{"5":{"89":2}}],["评估指标为验证集准确率或困惑度",{"5":{"89":2}}],["评分函数可以是神经网络",{"5":{"86":2}}],["干扰内容信息的学习",{"5":{"89":2}}],["想象",{"5":{"89":2}}],["难以表达相对位置信息",{"5":{"91":2}}],["难以泛化到未见过的位置",{"5":{"89":2}}],["难度",{"5":{"51":2}}],["逼近的方向很重要",{"5":{"89":2}}],["逼近误差为零",{"5":{"89":2}}],["逼近目标函数",{"5":{"45":1}}],["满秩",{"5":{"89":2}}],["满秩情况",{"5":{"89":2}}],["满足幂等性",{"5":{"51":1}}],["满足一个关键的性质",{"5":{"51":1}}],["满足非负性",{"5":{"50":2}}],["满足和",{"5":{"50":1,"87":1}}],["满足概率的基本要求",{"5":{"47":2}}],["满足概率分布的所有公理",{"5":{"71":1}}],["满足大规模训练的需求",{"5":{"47":2}}],["满足以下两个条件",{"5":{"71":2}}],["满足",{"5":{"41":1,"42":2,"50":2,"87":5,"88":2}}],["质心位置反映了编码向量的",{"5":{"89":2}}],["采用了rope的变体",{"5":{"88":1}}],["乘法变换",{"5":{"88":1}}],["乘以",{"5":{"88":1}}],["元素的元组",{"5":{"88":1}}],["元素级函数",{"5":{"48":2}}],["绕原点逆时针旋转了",{"5":{"88":1}}],["幅度",{"5":{"88":1}}],["启发式",{"5":{"88":2}}],["启发式设计",{"5":{"88":2}}],["识别句子边界并在这些位置给予更强的位置标记",{"5":{"88":2}}],["识别正样本",{"5":{"69":1}}],["动态损失缩放通常效果更好",{"5":{"97":2}}],["动态位置编码",{"5":{"88":1}}],["动态位置编码探索根据输入内容自适应地调整位置编码",{"5":{"88":1}}],["动量",{"5":{"51":2}}],["动量更新的公式为",{"5":{"48":2}}],["动量方法和噪声梯度有助于算法逃离鞍点",{"5":{"48":2}}],["动量方法可以看作是对梯度进行指数移动平均",{"5":{"48":2}}],["动量方法",{"5":{"48":2}}],["段落结构",{"5":{"88":2}}],["段落的首句往往承担着引言或总结的功能",{"5":{"88":2}}],["技术来处理长上下文场景",{"5":{"88":2}}],["技巧的核心思想是",{"5":{"61":2}}],["技巧",{"5":{"61":4}}],["性能上的竞争力",{"5":{"88":1}}],["性能上的竞争力是第四个因素",{"5":{"88":1}}],["性质及其设计原理",{"5":{"91":2}}],["性质及其在分类任务中的应用",{"5":{"61":4}}],["性质分析",{"5":{"61":2}}],["性质",{"5":{"48":2,"97":2}}],["仍然是一个活跃的研究领域",{"5":{"88":2}}],["仍为超平面",{"5":{"47":2}}],["目前主流的大语言模型",{"5":{"88":2}}],["目标分布",{"5":{"61":2}}],["目标变量",{"5":{"51":1}}],["目标变量服从高斯分布",{"5":{"51":1}}],["目标向量的平方范数等于可解释部分",{"5":{"51":2}}],["目标向量向预测空间",{"5":{"51":2}}],["目标向量对应空间中的一个点",{"5":{"51":1}}],["目标向量",{"5":{"51":3}}],["目标是在",{"5":{"51":1}}],["目标是在中寻找与目标向量距离最小的点",{"5":{"51":1}}],["目标是最小化重构误差",{"5":{"69":2}}],["目标是学习一个从输入到连续输出的映射函数",{"5":{"70":1}}],["目标是学习一个从输入到离散类别标签的映射",{"5":{"70":1}}],["目标是学习一个从输入",{"5":{"70":2}}],["目标",{"5":{"99":2,"101":2}}],["目标则需要基于前缀进行自回归生成",{"5":{"99":2,"101":2}}],["目标函数值与最优值的差距为",{"5":{"48":2}}],["目标函数不会被进一步优化",{"5":{"99":2,"101":2}}],["思路是",{"5":{"88":2}}],["思想可以迁移到注意力机制中",{"5":{"70":2}}],["模长较大的向量更容易获得较大的点积值",{"5":{"95":2}}],["模式",{"5":{"90":2}}],["模糊化",{"5":{"88":2}}],["模型有足够的容量来拟合训练数据",{"5":{"97":2}}],["模型有着有趣的类比",{"5":{"93":2}}],["模型输出logits而非直接预测概率具有深刻的理论依据",{"5":{"96":2}}],["模型输出的未归一化分数",{"5":{"61":2}}],["模型再根据这个分布进行词元的采样或选择",{"5":{"96":2}}],["模型逐渐调整",{"5":{"92":1}}],["模型逐渐调整和​",{"5":{"92":1}}],["模型容量和优化过程",{"5":{"92":2}}],["模型就能够捕获任意位置对之间的依赖关系",{"5":{"92":2}}],["模型可以在不同的子空间中计算其与其他位置的关联强度",{"5":{"93":2}}],["模型可以学习到与距离相关的注意力权重模式",{"5":{"92":2}}],["模型可以学习到",{"5":{"92":2}}],["模型可以学习到当查询位置是代词",{"5":{"92":2}}],["模型可以学习到当查询位置是主语词汇",{"5":{"92":2}}],["模型可能过拟合或学习到不合理的依赖模式",{"5":{"92":2}}],["模型可能无法学习到这些位置的可靠编码",{"5":{"89":2}}],["模型可能无法学习到有区分性的表示",{"5":{"69":2}}],["模型可能需要处理比训练时更长的序列",{"5":{"88":2}}],["模型可能对这些样本产生过度自信的预测",{"5":{"99":2,"101":2}}],["模型必须学习平滑的位置表示",{"5":{"90":2}}],["模型学习到的",{"5":{"94":1}}],["模型学习到的决定了什么样的输入模式会产生较高的注意力分数",{"5":{"94":1}}],["模型学习到的重要特征会对应于较大的奇异值",{"5":{"94":2}}],["模型学习到的是位置的",{"5":{"90":2}}],["模型学习的过程",{"5":{"51":2}}],["模型只能学习到一种类型的关联模式",{"5":{"93":2}}],["模型只能使用插值或外推策略",{"5":{"89":2}}],["模型只能感知相对位置",{"5":{"88":2}}],["模型会学习到最能支持任务目标的位置表示",{"5":{"89":2}}],["模型能够确定应该",{"5":{"95":2}}],["模型能够在底层捕获局部特征",{"5":{"92":2}}],["模型能够高效地并行处理大量数据",{"5":{"50":2}}],["模型能否在推理时处理更长的序列",{"5":{"88":2}}],["模型无法简单地",{"5":{"88":2}}],["模型无法捕捉数据的真实模式",{"5":{"51":2}}],["模型来模仿较大的",{"5":{"87":2}}],["模型需要根据问题",{"5":{"94":2}}],["模型需要",{"5":{"88":2,"89":2}}],["模型需要从复杂的混合信号中",{"5":{"88":2}}],["模型需要更少的训练样本来学习良好的参数",{"5":{"87":2}}],["模型需要基于前缀预测后续token",{"5":{"99":2,"101":2}}],["模型复杂度较低",{"5":{"87":2}}],["模型复杂度可以用多项式的阶数",{"5":{"51":2}}],["模型通过学习这些投影矩阵",{"5":{"94":2}}],["模型通过学习低秩的注意力模式",{"5":{"87":2}}],["模型通过学习",{"5":{"87":2}}],["模型通过参数",{"5":{"51":1}}],["模型通过参数定义了一个预测函数",{"5":{"51":1}}],["模型最后一层",{"5":{"61":2}}],["模型分布由softmax函数给出",{"5":{"61":1}}],["模型分布",{"5":{"61":3}}],["模型",{"5":{"61":2,"95":2,"99":2,"101":2}}],["模型层面",{"5":{"61":2}}],["模型开始过拟合训练数据中的噪声",{"5":{"51":2}}],["模型a优于模型b",{"5":{"96":2}}],["模型为词汇表中的每个词分配一个概率",{"5":{"96":2}}],["模型参数量巨大",{"5":{"48":2}}],["模型克服了这些局限性",{"5":{"47":2}}],["模型级并行",{"5":{"45":1}}],["模型并行化将大模型分布在多个计算设备上",{"5":{"50":2}}],["模型并行",{"5":{"45":2}}],["模型的表示能力可能受到限制",{"5":{"93":2}}],["模型的泛化能力可能提升",{"5":{"93":2}}],["模型的泛化误差与模型的复杂度",{"5":{"87":2}}],["模型的注意力权重分布",{"5":{"87":2}}],["模型的不同层",{"5":{"45":1}}],["模型的输入通常是一个变长的词元序列",{"5":{"94":2}}],["模型的输出分布满足",{"5":{"99":2,"101":2}}],["模型的自注意力机制决定了每个位置关注哪些前面的token",{"5":{"99":2,"101":2}}],["模型预测的概率分布记为",{"5":{"61":2}}],["模型预测的概率分布为",{"5":{"61":2}}],["模型预测",{"5":{"69":2}}],["模型预测为",{"5":{"70":4}}],["模型预测概率分布为",{"5":{"70":2}}],["模型越错误",{"5":{"70":2}}],["模型常常需要同时优化多个目标",{"5":{"70":2}}],["模型中",{"5":{"70":2}}],["模型首先计算一个未归一化的logit向量",{"5":{"99":2,"101":2}}],["模型将退化为简单的加权平均",{"5":{"71":2}}],["模型将退化为",{"5":{"99":2,"101":2}}],["模型被迫学习最相关的特征表示",{"5":{"94":2}}],["模型被训练来根据给定的提示生成内容",{"5":{"99":2,"101":2}}],["模型难以学习有效的表示",{"5":{"99":2,"101":2}}],["模型倾向于选择高概率的词元",{"5":{"96":2}}],["模型倾向于在该任务上表现良好",{"5":{"99":2,"101":2}}],["模型在误差大的样本上接收更大的梯度信号",{"5":{"51":2}}],["模型在该任务上表现不佳",{"5":{"99":2,"101":2}}],["模型更偏好而非",{"5":{"99":1,"101":1}}],["模型更偏好",{"5":{"99":1,"101":1}}],["压扁",{"5":{"94":2}}],["压低",{"5":{"88":2}}],["压缩",{"5":{"87":2}}],["附加信号",{"5":{"88":2}}],["附近",{"5":{"42":1,"69":1}}],["群作用",{"5":{"88":1}}],["群作用描述了如何将群元素",{"5":{"88":2}}],["群论的核心概念是",{"5":{"88":1}}],["群论的核心概念是群作用",{"5":{"88":1}}],["群论视角",{"2":{"88":1},"5":{"88":1}}],["群论完整推导<",{"5":{"85":1}}],["群论完整推导",{"0":{"88":1},"4":{"88":1},"5":{"85":4,"88":1}}],["运算可以利用高度优化的矩阵乘法核心",{"5":{"95":2}}],["运算按分量独立进行",{"5":{"88":2}}],["运算在数学形式上完全一致",{"5":{"61":2}}],["封闭性",{"5":{"88":2}}],["封闭有界集合",{"5":{"71":2}}],["短语边界",{"5":{"88":2}}],["漂移",{"5":{"88":2}}],["添加后",{"5":{"95":1}}],["添加",{"5":{"88":2,"95":1}}],["频域位置编码",{"5":{"88":1}}],["频域位置编码从信号处理的角度重新思考位置编码",{"5":{"88":1}}],["频谱效率高的优点",{"5":{"88":2}}],["频率",{"5":{"90":2}}],["频率域的稀疏性",{"5":{"90":2}}],["频率不变",{"5":{"90":2}}],["频率间隔近似为",{"5":{"90":2}}],["频率成分不变",{"5":{"90":2}}],["频率对应的角频率为",{"5":{"90":1}}],["频率称为第次谐波",{"5":{"90":1}}],["频率空间的视角不仅帮助我们理解正弦余弦编码的数学原理",{"5":{"90":2}}],["频率空间的数学分析<",{"5":{"85":1}}],["频率空间的数学分析",{"0":{"90":1},"4":{"90":1},"5":{"85":4,"90":1}}],["频率空间分析为我们理解位置编码提供了全新的视角",{"5":{"90":2}}],["频率空间与位置空间的对偶性",{"2":{"90":1},"5":{"90":1}}],["频率选择与编码分辨率的关系",{"5":{"90":2}}],["频率选择与编码分辨率",{"2":{"90":1},"5":{"90":1}}],["频率分辨率",{"5":{"90":2}}],["频率分解",{"5":{"89":2}}],["频率分布",{"5":{"70":2,"91":2}}],["频率参数为10000",{"5":{"88":2}}],["频率参数选择10000作为底数",{"5":{"88":2}}],["温和",{"5":{"88":2}}],["温度参数控制着注意力分布的",{"5":{"95":1}}],["温度参数",{"5":{"69":3,"71":1,"95":1}}],["温度参数提供了从",{"5":{"71":1}}],["温度参数是infonce中的一个关键超参数",{"5":{"69":1}}],["温度参数的作用与影响",{"2":{"69":1},"5":{"69":1}}],["温度参数的数学效应",{"5":{"69":2}}],["温度参数对梯度的影响",{"5":{"69":2}}],["温度参数对梯度有双重影响",{"5":{"69":1}}],["温度参数通常设置在较小的值",{"5":{"69":1}}],["弧度",{"5":{"88":2}}],["产生了完全不同的数学性质",{"5":{"88":2}}],["产生较小的梯度",{"5":{"69":2}}],["产生较大的梯度",{"5":{"69":1}}],["共轭先验是使后验分布与先验分布同分布族的先验选择",{"5":{"96":2}}],["共轭梯度法利用hessian的信息来加速收敛",{"5":{"48":2}}],["共享同一个key和value",{"5":{"93":2}}],["共享编码将参数量从",{"5":{"89":1}}],["共享编码将参数量从减少到​",{"5":{"89":1}}],["共享位置编码",{"5":{"89":2}}],["共同构成了大语言模型训练的数学基础",{"5":{"101":2}}],["共同构成完整的变换",{"5":{"88":2}}],["共同建模输入序列中的各种依赖关系",{"5":{"92":2}}],["共同作为注意力层的输入",{"5":{"91":2}}],["共同决定了不同维度上的旋转频率",{"5":{"88":2}}],["出纯粹的位置信息",{"5":{"88":2}}],["出发",{"5":{"61":2}}],["绝对位置信息的丢失",{"5":{"88":1}}],["绝对位置信息的丢失是一个理论上的局限",{"5":{"88":1}}],["绝对位置",{"5":{"88":3,"101":2}}],["绝对位置和分别出现在不同的项中",{"5":{"88":1}}],["绝对位置编码与相对位置编码进行了比较",{"5":{"91":2}}],["绝对位置编码的缺点",{"5":{"91":2}}],["绝对位置编码的优点",{"5":{"91":2}}],["绝对位置编码的数学形式为",{"5":{"91":2}}],["绝对位置编码能力",{"5":{"88":2}}],["绝对位置编码",{"5":{"71":2,"88":2}}],["污染",{"5":{"88":2}}],["句子",{"5":{"88":2}}],["句子开头的词与句子结尾的词之间可能存在语法上的呼应",{"5":{"88":2}}],["句末的标点符号常常标志着句子的结束",{"5":{"88":2}}],["句法关系",{"5":{"71":2}}],["至少有一个被拒绝的概率也会大大增加",{"5":{"96":2}}],["至少有一个等于",{"5":{"61":2}}],["至关重要",{"5":{"88":2}}],["我这里有什么信息",{"5":{"94":2}}],["我需要什么信息",{"5":{"94":2}}],["我喜欢学习",{"5":{"88":2}}],["我们重点分析了缩放因子",{"5":{"95":1}}],["我们重点分析了缩放因子的统计学原理",{"5":{"95":1}}],["我们介绍了scaled",{"5":{"94":2}}],["我们展示了这些损失函数的数学本质",{"5":{"101":2}}],["我们展示了编码向量的数值分布",{"5":{"91":2}}],["我们展示了交叉熵与kl散度的等价关系",{"5":{"61":2}}],["我们进一步分析了注意力权重的分布特性",{"5":{"92":2}}],["我们进一步分析了位置编码如何与注意力计算交互",{"5":{"91":2}}],["我们进一步讨论了梯度流与信息保持的关系",{"5":{"41":2}}],["我们深入分析了多头注意力的参数结构",{"5":{"93":2}}],["我们深入分析了位置编码的唯一性",{"5":{"91":2}}],["我们深入探讨了频率选择与编码分辨率的关系",{"5":{"90":2}}],["我们给出具体的数值示例",{"5":{"91":2}}],["我们证明了位置差",{"5":{"90":1}}],["我们证明了位置差的编码分辨率与频率​成反比",{"5":{"90":1}}],["我们还深入探讨了正则化与泛化的数学联系",{"5":{"97":2}}],["我们还详细分析了多头注意力的计算复杂度",{"5":{"93":2}}],["我们还介绍了复数形式的统一分析",{"5":{"90":2}}],["我们还讨论了注意力机制的矩阵形式",{"5":{"95":2}}],["我们还讨论了低秩结构与信息瓶颈理论的关系",{"5":{"87":2}}],["我们还讨论了条件数对数值稳定性的影响",{"5":{"87":2}}],["我们讨论了数值稳定性问题和实际实现中的关键技术",{"5":{"97":2}}],["我们讨论了频率域与位置域的对偶性",{"5":{"90":2}}],["我们讨论了初始化策略",{"5":{"89":2}}],["我们讨论了前向传播的数学本质",{"5":{"44":2}}],["我们探讨了可学习编码能够表示的函数空间",{"5":{"89":2}}],["我们探讨了激活函数的信息压缩",{"5":{"71":2}}],["我们试图",{"5":{"88":2}}],["我们定义信息传递路径长度的数学概念",{"5":{"92":2}}],["我们定义",{"5":{"88":2}}],["我们定义一个关键中间变量",{"5":{"44":2}}],["我们有",{"5":{"88":4}}],["我们从信息论",{"5":{"97":2}}],["我们从注意力机制的核心思想出发",{"5":{"95":2}}],["我们从子空间学习的角度论证了多头注意力能够表示比单头注意力更丰富的函数类",{"5":{"93":2}}],["我们从多头注意力的设计动机出发",{"5":{"93":2}}],["我们从函数空间和计算复杂度的角度进行了理论分析",{"5":{"92":2}}],["我们从rnn的梯度消失问题出发",{"5":{"92":2}}],["我们从频率域的角度重新分析了相对位置的编码",{"5":{"90":2}}],["我们从未显式地告诉模型",{"5":{"88":2}}],["我们从数学上证明了注意力矩阵的秩最多为",{"5":{"87":1}}],["我们从数学上证明了注意力矩阵的秩最多为​",{"5":{"87":1}}],["我们从计算流程",{"5":{"87":2}}],["我们从统计学角度分析了mse的来源",{"5":{"51":2}}],["我们系统地探讨了标准transformer中注意力机制的数学原理",{"5":{"86":2}}],["我们的优化目标是最小化真实分布",{"5":{"61":2}}],["我们的目标是计算",{"5":{"44":1}}],["我们的目标是计算和对所有层",{"5":{"44":1}}],["我们假设",{"5":{"61":2}}],["我们经常需要度量两个概率分布之间的差异",{"5":{"61":2}}],["我们对所有可能的样本集",{"5":{"51":1}}],["我们对所有可能的样本集取期望",{"5":{"51":1}}],["我们先计算若干个小批次的梯度",{"5":{"50":2}}],["我们得到形状为",{"5":{"50":1}}],["我们得到形状为的张量",{"5":{"50":1}}],["我们得到了一个可以近似离散采样的连续分布",{"5":{"71":2}}],["我们关心如何用少量的奇异值最好地近似原始矩阵",{"5":{"50":2}}],["我们在优化目标中加入正交性惩罚项",{"5":{"50":2}}],["我们使用离散的时间步长",{"5":{"97":2}}],["我们使用迭代算法如幂迭代",{"5":{"50":2}}],["我们使用雅可比矩阵",{"5":{"48":2}}],["我们很少直接计算矩阵的逆",{"5":{"50":2}}],["我们计算两个模型的性能差异",{"5":{"96":2}}],["我们计算每个节点对其直接后继的梯度",{"5":{"45":2}}],["我们根据样本数据计算检验统计量",{"5":{"96":2}}],["我们希望计算",{"5":{"94":1}}],["我们希望计算​和​",{"5":{"94":1}}],["我们希望输出序列",{"5":{"92":1}}],["我们希望输出序列​满足​",{"5":{"92":1}}],["我们希望找到一个预测值",{"5":{"51":1}}],["我们希望找到一个预测值​来最小化期望损失",{"5":{"51":1}}],["我们希望用简单的近似分布",{"5":{"96":1}}],["我们希望用简单的近似分布来逼近复杂的后验分布",{"5":{"96":1}}],["我们希望最大化",{"5":{"71":2}}],["我们希望学习一个参数化的模型分布",{"5":{"69":2}}],["我们同时处理这两类随机变量",{"5":{"96":2}}],["我们会以最快的速度离开当前的等值面",{"5":{"48":2}}],["我们会发现它蕴含着丰富的理论内涵",{"5":{"45":2}}],["我们建立了一个完整的损失函数优化理论框架",{"5":{"97":2}}],["我们建立了仿射变换的数学框架",{"5":{"47":2}}],["我们建立了单个神经元的数学模型",{"5":{"46":2}}],["我们建立了损失函数的数学基础",{"5":{"99":2,"101":2}}],["我们引入齐次坐标",{"5":{"47":2}}],["我们通过信息传递路径分析",{"5":{"92":2}}],["我们通过数值计算来验证这一性质",{"5":{"91":2}}],["我们通过旋转来",{"5":{"88":2}}],["我们通过构造具体的权重和阈值来说明",{"5":{"47":2}}],["我们通常不直接建模随机变量的概率分布",{"5":{"96":2}}],["我们通常有",{"5":{"93":2}}],["我们通常假设",{"5":{"92":1}}],["我们通常假设对于所有",{"5":{"92":1}}],["我们通常最小化负对数似然",{"5":{"61":2}}],["我们通常需要同时计算多个神经元的输出",{"5":{"47":2}}],["我们通常保持权重矩阵和偏置向量的分离形式",{"5":{"46":2}}],["我们通常使用随机梯度下降或其变体",{"5":{"96":2}}],["我们通常使用小批量随机梯度下降",{"5":{"44":2}}],["我们通常使用较窄但较深的网络来达到相同的表达能力",{"5":{"71":2}}],["我们主要关注全连接层",{"5":{"46":2}}],["我们伴随值",{"5":{"45":2}}],["我们论证了激活函数作为通用逼近器核心组件的数学基础",{"5":{"71":2}}],["我们揭示了非线性激活函数打破表达瓶颈的数学必然性",{"5":{"71":2}}],["我们将进一步探讨query",{"5":{"95":2}}],["我们将",{"5":{"93":1}}],["我们将个头的投影矩阵按行堆叠",{"5":{"93":1}}],["我们将所有头的query投影矩阵堆叠成一个大的投影矩阵",{"5":{"93":2}}],["我们将正弦余弦编码与可学习编码",{"5":{"91":2}}],["我们将从谱性质的角度分析注意力矩阵的数学特性",{"5":{"93":2}}],["我们将从谱性质的角度进一步分析注意力矩阵的数学特性",{"5":{"92":2}}],["我们将从频率空间的角度进一步分析位置编码的理论基础",{"5":{"91":2}}],["我们将从群论的角度详细推导旋转位置编码",{"5":{"89":2}}],["我们将从链式法则的矩阵形式出发",{"5":{"44":2}}],["我们将可学习编码与正弦编码进行了比较",{"5":{"89":2}}],["我们将mse置于向量空间的框架下分析",{"5":{"51":2}}],["我们将分析mse的性质",{"5":{"51":2}}],["我们将详细推导mse的偏差",{"5":{"51":2}}],["我们将详细分析损失函数的优化性质",{"5":{"71":2}}],["我们将深入探讨mse在向量空间中的几何解释",{"5":{"51":2}}],["我们将在下一节讨论如何将多个神经元组织成层",{"5":{"47":2}}],["我们将在第2",{"5":{"45":2}}],["我们将看到如何利用链式法则推导出反向传播的梯度计算公式",{"5":{"45":2}}],["我们将复杂的复合函数求导问题分解为误差信号的前向传播和参数梯度的局部计算两个步骤",{"5":{"44":2}}],["我们将讨论反向传播与自动微分的关系",{"5":{"44":2}}],["我们将建立对梯度动力学的系统性理解",{"5":{"41":2}}],["我们需要考察当向量维度",{"5":{"95":1}}],["我们需要考察当向量维度​较大时",{"5":{"95":1}}],["我们需要定义一些数学度量",{"5":{"92":2}}],["我们需要证明",{"5":{"91":2}}],["我们需要理解",{"5":{"89":2}}],["我们需要计算",{"5":{"88":1}}],["我们需要计算旋转矩阵",{"5":{"88":1}}],["我们需要计算每个位置",{"5":{"88":2}}],["我们需要计算梯度",{"5":{"61":2}}],["我们需要构造一个位置感知的表示函数",{"5":{"88":2}}],["我们需要明确哪个分布是",{"5":{"61":2}}],["我们需要首先明确几个基本定义",{"5":{"50":2}}],["我们需要首先理解生物神经元的基本结构及其数学抽象过程",{"5":{"47":2}}],["我们需要首先理解nce的基本思想及其局限性",{"5":{"69":2}}],["我们需要平衡验证集大小和计算成本",{"5":{"96":2}}],["我们需要找到一组模型参数",{"5":{"48":2}}],["我们需要组合多个神经元",{"5":{"46":2}}],["我们需要将链式法则从标量情形推广到向量和矩阵情形",{"5":{"44":2}}],["我们需要从线性映射的局限性出发",{"5":{"71":2}}],["我们需要从个候选样本",{"5":{"69":1}}],["我们需要从",{"5":{"69":1}}],["我们需要显式计算其梯度",{"5":{"69":2}}],["我们需要预测的是这个条件分布本身",{"5":{"70":2}}],["我们总结了激活函数设计的数学原则",{"5":{"41":2}}],["我们首先探讨了优化景观的几何结构",{"5":{"97":2}}],["我们首先需要理解引入多头注意力的动机",{"5":{"93":2}}],["我们首先需要分析传统循环神经网络在长程依赖建模方面的局限性",{"5":{"92":2}}],["我们首先需要建立饱和现象的严格数学定义",{"5":{"41":2}}],["我们首先从位置编码的动机出发",{"5":{"91":2}}],["我们首先回顾了傅里叶分析的基础知识",{"5":{"90":2}}],["我们首先定义了可学习位置编码的参数化形式",{"5":{"89":2}}],["我们首先严格定义了注意力矩阵",{"5":{"87":2}}],["我们首先给出了mse的严格数学定义",{"5":{"51":2}}],["我们首先详细推导了sigmoid和tanh函数的饱和区域和饱和深度",{"5":{"41":2}}],["我们必须知道",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["我们终将知道",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["我们可以得到一个统一的矩阵表达式",{"5":{"94":2}}],["我们可以考虑注意力头的有效容量",{"5":{"93":2}}],["我们可以考虑注意力头的输出表示之间的相关性",{"5":{"93":2}}],["我们可以考虑以下优化策略",{"5":{"89":2}}],["我们可以发现一个有趣的结构",{"5":{"93":2}}],["我们可以证明注意力机制在建模长程依赖方面的效率优势",{"5":{"92":2}}],["我们可以建立一个简化的数学模型",{"5":{"92":2}}],["我们可以通过分析",{"5":{"92":1}}],["我们可以通过分析的模式来推断该头负责的依赖类型",{"5":{"92":1}}],["我们可以通过梯度累积来模拟更大的有效批大小",{"5":{"50":2}}],["我们可以区分",{"5":{"92":2}}],["我们可以理解为什么位置编码能够帮助模型更好地泛化",{"5":{"90":2}}],["我们可以比较两种编码的表达能力",{"5":{"89":2}}],["我们可以分析多头注意力能够表示的函数类",{"5":{"93":2}}],["我们可以分析它能够表示的函数复杂度",{"5":{"92":2}}],["我们可以分析注意力权重的分布特性",{"5":{"92":2}}],["我们可以分析编码向量的以下性质",{"5":{"89":2}}],["我们可以分析位置编码的自由度",{"5":{"89":2}}],["我们可以更清晰地看到两种范式的优势和局限性",{"5":{"89":2}}],["我们可以更深入地理解这一性质的来源",{"5":{"90":2}}],["我们可以更深入地理解",{"5":{"61":2}}],["我们可以根据重要性分数选择top",{"5":{"86":2}}],["我们可以根据样本的难度赋予不同权重",{"5":{"51":2}}],["我们可以给出另一种等价的解释",{"5":{"61":2}}],["我们可以将query生成过程理解为",{"5":{"94":2}}],["我们可以将位置",{"5":{"90":1}}],["我们可以将位置的编码向量视为一个离散序列",{"5":{"90":1}}],["我们可以将目标向量",{"5":{"51":1}}],["我们可以将目标向量和预测向量​视为空间中的两个点",{"5":{"51":1}}],["我们可以将导数表示为",{"5":{"42":2}}],["我们可以对近期数据赋予更高权重以使模型更关注最新的模式",{"5":{"51":2}}],["我们可以对少数类样本赋予更高权重以改善模型性能",{"5":{"51":2}}],["我们可以了解模型使用了多少",{"5":{"50":2}}],["我们可以实现降维同时最小化信息损失",{"5":{"50":2}}],["我们可以构建置信区间来量化估计的不确定性",{"5":{"96":2}}],["我们可以构建任意复杂的非线性函数",{"5":{"46":2}}],["我们可以计算损失函数对各层参数的梯度",{"5":{"44":2}}],["我们可以清楚地看到损失信息如何从输出层反向流动到输入层",{"5":{"44":2}}],["我们可以总结出激活函数设计应遵循的数学原则",{"5":{"41":2}}],["我们分析了梯度下降动力学",{"5":{"97":2}}],["我们分析了梯度爆炸的数学机制",{"5":{"41":2}}],["我们分析了正弦位置编码和旋转位置编码的数学性质",{"5":{"92":2}}],["我们分析了mse的性质",{"5":{"51":2}}],["我们分析了激活函数与transformer架构",{"5":{"71":2}}],["我们分析了sigmoid",{"5":{"71":2}}],["我们分析了多头注意力的矩阵推导",{"5":{"69":2}}],["我们分析了位置编码的数学性质",{"5":{"70":2}}],["我们分别深入探讨了均方误差",{"5":{"69":2,"70":2}}],["我们实际上是在最小化模型分布与真实分布之间的kl散度",{"5":{"61":2}}],["我们实际上是在问",{"5":{"48":2}}],["我们实际上是在寻找一个能够拟合目标函数的数学表达",{"5":{"45":2}}],["我们实际上在优化表示之间的互信息",{"5":{"69":2}}],["我们已经从计算单元和结构组织的角度介绍了神经网络的数学描述",{"5":{"45":2}}],["我们已经介绍了softmax函数的定义和基本性质",{"5":{"69":2}}],["我们现在展示softmax操作如何统一分类损失",{"5":{"69":2}}],["我们现在可以构建从infonce到注意力的直接桥梁",{"5":{"69":2}}],["我们并不显式定义正负样本",{"5":{"69":2}}],["我们不是通过dft从数据中学习频率",{"5":{"90":2}}],["我们不是从离散集合中",{"5":{"69":2}}],["我们不需要任何显式的相对位置计算",{"5":{"88":2}}],["我们不需要存储或重新计算导数",{"5":{"42":2}}],["我们详细探讨了scaled",{"5":{"93":2}}],["我们详细分析了位置编码的频谱结构",{"5":{"90":2}}],["我们详细分析了注意力机制和位置编码的数学基础",{"5":{"70":2}}],["我们详细推导了编码矩阵的秩",{"5":{"89":2}}],["我们详细推导了注意力矩阵的特征值和特征向量结构",{"5":{"87":2}}],["我们详细推导了期望mse的分解公式",{"5":{"51":2}}],["我们详细推导了交叉熵与最大似然估计的等价性",{"5":{"69":2}}],["我们最小化",{"5":{"70":2}}],["我们发现均方误差的梯度与预测误差成正比",{"5":{"69":1}}],["我们发现均方误差的梯度与预测误差",{"5":{"69":1}}],["我们发现",{"5":{"70":2}}],["我们来分析损失函数关于参数的梯度",{"5":{"70":1}}],["我们来分析损失函数关于参数",{"5":{"70":1}}],["狗咬人",{"5":{"88":2,"91":2,"92":2}}],["工程实现及其在现代大语言模型中的核心地位",{"5":{"88":2}}],["教师",{"5":{"87":2}}],["学生模型可以学习到教师模型注意力矩阵的低秩近似",{"5":{"87":2}}],["学生",{"5":{"87":2}}],["学习到对任务最有帮助的位置表示",{"5":{"89":2}}],["学习喜欢我",{"5":{"88":2}}],["学习率过小",{"5":{"97":2}}],["学习率过大",{"5":{"97":2}}],["学习率选择不当可能导致训练不稳定",{"5":{"97":2}}],["学习率保持不变",{"5":{"97":2}}],["学习率被自动减小",{"5":{"97":2}}],["学习率是梯度下降中最重要的超参数",{"5":{"97":2}}],["学习率是一个关键的超参数",{"5":{"48":2}}],["学习率",{"5":{"97":2}}],["学习率敏感性分析",{"2":{"97":1},"5":{"97":1}}],["学习率的选择方面",{"5":{"89":2}}],["学习率调整和梯度裁剪中都有应用",{"5":{"50":2}}],["学习率调度定义了一个函数",{"5":{"97":2}}],["学习率调度策略在训练过程中动态调整学习率",{"5":{"97":2}}],["学习率调度与收敛性",{"2":{"97":1},"5":{"97":1}}],["学习率调度",{"5":{"48":2,"97":4}}],["学习率预热",{"5":{"48":2}}],["学习率按余弦曲线从初始值逐渐减小到零",{"5":{"48":2}}],["学习率按指数函数递减",{"5":{"48":2}}],["学习率太大可能导致跳过最优解甚至发散",{"5":{"48":2}}],["学习率太小会导致收敛缓慢",{"5":{"48":2}}],["学习token之间的注意力模式",{"5":{"71":2}}],["学习效率低下",{"5":{"99":2,"101":2}}],["另一个联系是通过",{"5":{"93":2}}],["另一种常见的初始化是kaiming初始化",{"5":{"94":2}}],["另一种常见配置是令",{"5":{"94":2}}],["另一种多头注意力的变体是",{"5":{"93":2}}],["另一种方法是使用绝对位置编码的扩展",{"5":{"92":2}}],["另一种可视化方法是使用主成分分析",{"5":{"91":2}}],["另一种混合设计是使用可学习编码替代正弦编码的某些频率成分",{"5":{"89":2}}],["另一种压缩技术是知识蒸馏",{"5":{"87":2}}],["另外",{"5":{"50":2}}],["枢纽",{"5":{"87":2}}],["病态性通常发生在注意力权重分布极端不均匀的情况下",{"5":{"87":2}}],["说明底层信息能够有效传递到高层",{"5":{"92":2}}],["说明该位置的注意力是",{"5":{"92":4}}],["说明该头的注意力分布更均匀",{"5":{"87":2}}],["说明该头主要关注单一位置",{"5":{"87":2}}],["说明编码的有效秩远低于满秩",{"5":{"89":2}}],["说明模型的表达能力主要集中在少数方向上",{"5":{"50":2}}],["突破",{"5":{"87":2}}],["突触是神经元之间传递信息的连接点",{"5":{"47":2}}],["堆叠多层注意力可以",{"5":{"87":2}}],["足以唯一标识远超过任何实际序列长度的位置",{"5":{"90":2}}],["足以捕获大部分语义信息",{"5":{"87":2}}],["足够",{"5":{"91":2}}],["足够大",{"5":{"91":2}}],["足够大时",{"5":{"69":1,"90":1}}],["足够宽的单层网络可以逼近任意函数",{"5":{"71":2}}],["版本",{"5":{"87":2}}],["秩为",{"5":{"89":1}}],["秩为的逼近",{"5":{"89":1}}],["秩越高",{"5":{"89":2}}],["秩越低",{"5":{"87":2,"89":2}}],["秩衡量了编码向量的线性无关程度",{"5":{"89":1}}],["秩",{"5":{"87":1,"89":1}}],["秩的低秩矩阵有个自由度",{"5":{"87":1}}],["秩较低的矩阵参数化空间较小",{"5":{"87":2}}],["秩可能略有增加",{"5":{"87":2}}],["紧凑",{"5":{"87":2}}],["头维度128",{"5":{"93":2}}],["头维度",{"5":{"93":2}}],["头",{"5":{"93":4}}],["头数",{"5":{"93":1}}],["头数和头维度",{"5":{"93":1}}],["头数为",{"5":{"93":4}}],["头数与头维度的权衡",{"2":{"93":1},"5":{"93":1}}],["头间的信息流动分析",{"2":{"93":1},"5":{"93":1}}],["头间差异",{"5":{"87":2}}],["头选择",{"5":{"87":2}}],["头部专业化",{"5":{"70":2}}],["交叉协方差",{"5":{"87":2,"94":2}}],["交叉gram矩阵",{"5":{"87":2}}],["交叉熵可以分解为熵与kl散度之和",{"5":{"96":2}}],["交叉熵在这些方面的优势使其成为分类任务的首选",{"5":{"61":2}}],["交叉熵在分类任务中的完整推导",{"2":{"61":1},"5":{"61":1}}],["交叉熵在分类任务中通常比",{"5":{"70":2}}],["交叉熵中的",{"5":{"61":2}}],["交叉熵关于模型分布参数",{"5":{"61":2}}],["交叉熵",{"5":{"61":6,"69":2,"70":4,"97":1}}],["交叉熵的框架之上",{"5":{"101":2}}],["交叉熵的学习率敏感性中等",{"5":{"97":2}}],["交叉熵的定义可以分解为两部分",{"5":{"61":2}}],["交叉熵的定义与推导",{"2":{"61":1},"5":{"61":1}}],["交叉熵的概念便自然浮现",{"5":{"61":2}}],["交叉熵的概率论推导",{"0":{"61":1},"4":{"61":1},"5":{"61":1,"85":4}}],["交叉熵的概率论推导<",{"5":{"85":1}}],["交叉熵的数学性质",{"2":{"61":1},"5":{"61":1}}],["交叉熵的数学结构同样具有鲜明的特征",{"5":{"70":2}}],["交叉熵的梯度形式",{"5":{"97":2}}],["交叉熵的梯度与预测概率与真实标签的差值成正比",{"5":{"69":1}}],["交叉熵的梯度与预测概率与真实标签的差值",{"5":{"69":1}}],["交叉熵的梯度不受",{"5":{"70":2}}],["交叉熵的",{"5":{"70":4,"97":2}}],["交叉熵的变曲率意味着优化空间在不同区域的形状是不同的",{"5":{"70":2}}],["交叉熵的强凸性取决于数据的",{"5":{"70":2,"97":2}}],["交叉熵的这种无上界特性反映了其对错误预测的",{"5":{"70":2}}],["交叉熵的对数增长对应于范数的某种极端形式",{"5":{"70":1}}],["交叉熵的对数增长对应于",{"5":{"70":1}}],["交叉熵的核心结构",{"5":{"99":2,"101":2}}],["交叉熵的理论极限",{"5":{"99":2,"101":2}}],["交叉熵与kl散度密切相关",{"5":{"96":2}}],["交叉熵与kl散度之间存在着简洁而深刻的数学关系",{"5":{"61":2}}],["交叉熵与kl散度的等价性可以形式化地表述",{"5":{"61":2}}],["交叉熵与kl散度的等价性分析",{"2":{"61":1},"5":{"61":1}}],["交叉熵与kl散度的数学关系",{"2":{"61":1},"5":{"61":1}}],["交叉熵与最大似然估计的统一视角",{"2":{"61":1},"5":{"61":1}}],["交叉熵定义为",{"5":{"61":2,"96":2}}],["交叉熵损失虽然也是凸函数",{"5":{"97":2}}],["交叉熵损失在混合精度训练中需要特别处理",{"5":{"97":2}}],["交叉熵损失在概率接近",{"5":{"61":2}}],["交叉熵损失相对于模型输出",{"5":{"96":2}}],["交叉熵损失与最大似然估计的联系为我们提供了一个一致的",{"5":{"61":1}}],["交叉熵损失与最大似然估计的联系为我们提供了一个一致的概率解释框架",{"5":{"61":1}}],["交叉熵损失的优势在于它直接与对数似然相关",{"5":{"96":2}}],["交叉熵损失的",{"5":{"61":1}}],["交叉熵损失的梯度特性使得基于梯度优化的训练过程具有良好的数值性质",{"5":{"61":1}}],["交叉熵损失的经验风险为",{"5":{"61":2}}],["交叉熵损失中的",{"5":{"61":2}}],["交叉熵损失函数的优化过程可以被理解为在信息几何框架下",{"5":{"61":2}}],["交叉熵损失函数源自信息论的基本原理",{"5":{"61":2}}],["交叉熵损失函数是最为广泛采用的优化目标",{"5":{"61":2}}],["交叉熵损失函数关于参数是凸函数",{"5":{"70":1,"97":1}}],["交叉熵损失函数关于参数",{"5":{"70":1,"97":1}}],["交叉熵损失",{"5":{"44":4,"70":1,"71":2}}],["交叉熵损失具有清晰的概率解释",{"5":{"71":2}}],["交叉熵损失以及它们的数学结构对比",{"5":{"69":2}}],["交叉熵损失为",{"5":{"69":2}}],["交叉熵损失关于的梯度是",{"5":{"70":1}}],["交叉熵损失是最常用的损失函数",{"5":{"96":2}}],["交叉熵损失是",{"5":{"70":2}}],["交叉熵损失假设预测分布与真实分布的差异可以通过kl散度度量",{"5":{"99":2,"101":2}}],["交叉熵损失可以写为",{"5":{"61":2}}],["交叉熵损失可能无法捕获数据分布的所有方面",{"5":{"99":2,"101":2}}],["交叉熵损失对于",{"5":{"99":2,"101":2}}],["交叉熵梯度的链式法则",{"2":{"61":1},"5":{"61":1}}],["交叉熵梯度",{"5":{"70":2}}],["交叉熵从",{"5":{"70":2}}],["交叉熵是更好的选择",{"5":{"70":2}}],["交叉熵则源于信息论中的相对熵概念",{"5":{"70":1}}],["交叉熵则源于信息论中的",{"5":{"70":1}}],["交叉熵对应于kl散度",{"5":{"70":1}}],["交叉熵对应于",{"5":{"70":1}}],["始终为正",{"5":{"87":2}}],["始终非负",{"5":{"51":1}}],["泛化性",{"5":{"91":2}}],["泛化性讨论",{"2":{"61":1},"5":{"61":1}}],["泛化误差通常较小",{"5":{"87":2}}],["泛化能力的理论分析",{"2":{"87":1},"5":{"87":1}}],["字节",{"5":{"86":2}}],["溢出",{"5":{"86":1}}],["条对角线上",{"5":{"86":1,"87":1}}],["条件协方差为",{"5":{"96":1}}],["条件协方差为​",{"5":{"96":1}}],["条件熵",{"5":{"96":2}}],["条件分布在高斯过程回归",{"5":{"96":2}}],["条件分布是多元高斯分布最优美也最实用的性质之一",{"5":{"96":2}}],["条件期望用于描述在给定上下文的条件下",{"5":{"96":2}}],["条件期望的一个重要性质是全期望公式",{"5":{"96":2}}],["条件期望的这一性质奠定了mse在回归任务中的理论基础",{"5":{"51":2}}],["条件期望是的函数",{"5":{"96":1}}],["条件期望是在给定某些信息的条件下对随机变量的期望",{"5":{"96":2}}],["条件期望也被称为回归函数",{"5":{"51":2}}],["条件期望与最优预测",{"2":{"51":1},"5":{"51":1}}],["条件期望",{"5":{"51":2,"96":1}}],["条件数",{"5":{"97":2}}],["条件数反映了编码矩阵的数值稳定性",{"5":{"89":2}}],["条件数较大",{"5":{"87":2}}],["条件数较小",{"5":{"87":2}}],["条件数为",{"5":{"89":2}}],["条件数为1",{"5":{"87":2}}],["条件数为无穷大",{"5":{"87":4}}],["条件数对训练稳定性有重要影响",{"5":{"87":2}}],["条件数接近1",{"5":{"87":2}}],["条件数定义为最大奇异值与最小奇异值的比值",{"5":{"87":2}}],["条件数与数值稳定性",{"2":{"87":1},"5":{"87":1}}],["条件数与梯度稳定性",{"5":{"41":2}}],["条件数越大",{"5":{"41":2,"97":4}}],["条件概率密度函数为",{"5":{"51":2}}],["条件概率最大的类别",{"5":{"70":2}}],["条件众数",{"5":{"70":1}}],["异步执行",{"5":{"86":2}}],["异常值的影响是线性的",{"5":{"51":2}}],["异常样本的误差为",{"5":{"51":2}}],["≈",{"5":{"86":2}}],["算法",{"5":{"97":4}}],["算法的正确性依赖于softmax的数学性质",{"5":{"86":2}}],["算法流程如下",{"5":{"86":2}}],["算法2",{"5":{"44":2}}],["造成内存带宽瓶颈",{"5":{"86":2}}],["累积到",{"5":{"94":1}}],["累积量可以写为",{"5":{"86":2}}],["累积衰减模型",{"5":{"41":2}}],["旧信息的能力",{"5":{"86":2}}],["傅里叶系数",{"5":{"90":1}}],["傅里叶系数和​通过以下积分计算",{"5":{"90":1}}],["傅里叶变换和微分方程等运算下保持封闭形式",{"5":{"96":2}}],["傅里叶变换和正交基扩展是信号处理和自然语言处理中的重要工具",{"5":{"50":2}}],["傅里叶变换的核心是对偶性",{"5":{"90":2}}],["傅里叶分析揭示了时间域",{"5":{"90":2}}],["傅里叶级数可以更加简洁地表示",{"5":{"90":2}}],["傅里叶级数的深刻意义在于",{"5":{"90":2}}],["傅里叶级数展开定义为",{"5":{"90":2}}],["傅里叶级数是分析周期函数的经典工具",{"5":{"90":2}}],["傅里叶特征映射",{"5":{"86":2}}],["令",{"5":{"86":2,"94":2}}],["块稀疏将序列划分为固定大小的块",{"5":{"86":2}}],["尤其是transformer架构的核心创新之一",{"5":{"95":2}}],["尤其是gpu",{"5":{"86":2}}],["尤其是在损失函数曲率高度各向异性的情况下",{"5":{"70":2}}],["缺点是训练初期位置信息完全缺失",{"5":{"89":2}}],["缺点是仍然需要计算完整的注意力分数矩阵",{"5":{"86":2}}],["缺乏概率分布差异的语义解释",{"5":{"61":2}}],["膨胀窗口注意力",{"5":{"86":2}}],["滑动窗口注意力类似于带状矩阵",{"5":{"86":2}}],["滑动窗口注意力",{"5":{"86":2}}],["远程依赖的边界效应",{"2":{"92":1},"5":{"92":1}}],["远距离位置的编码向量相距较远",{"5":{"91":2}}],["远距离关系",{"5":{"91":2}}],["远小于原始的",{"5":{"89":1}}],["远小于原始的​",{"5":{"89":1}}],["远小于其维度",{"5":{"87":2}}],["远小于标准注意力的",{"5":{"86":2}}],["远小于标准注意力的100",{"5":{"86":2}}],["远离",{"5":{"70":1}}],["读者将建立起对注意力机制内部运作机制的完整理解",{"5":{"94":2}}],["读者将建立起对注意力变体全景图的理解",{"5":{"86":2}}],["读者将建立起对多头注意力数学本质的完整理解",{"5":{"93":2}}],["读者将建立起对位置编码频率空间本质的深入理解",{"5":{"90":2}}],["读者应该建立起对位置编码频率空间本质的深入理解",{"5":{"90":2}}],["读者应该建立起对mse的全面深入理解",{"5":{"51":2}}],["读者应该能够从数学层面深入理解多头注意力的架构设计",{"5":{"93":2}}],["读者应该能够从数学层面深入理解注意力机制为何能够有效建模长程依赖",{"5":{"92":2}}],["读者应该能够从矩阵分析的角度深入理解可学习位置编码的性质",{"5":{"89":2}}],["读者应该能够从线性代数和数值分析的角度深入理解注意力矩阵的数学本质",{"5":{"87":2}}],["读者应能够理解反向传播的工作原理",{"5":{"44":2}}],["探索",{"5":{"92":2}}],["探索根据输入内容自适应地调整位置编码",{"5":{"88":1}}],["探讨其低秩结构及其对模型行为的影响",{"5":{"93":2}}],["探讨其建模长程依赖的数学原理",{"5":{"92":2}}],["探讨它们的数学原理",{"5":{"86":2}}],["探测器",{"5":{"71":2}}],["门控softmax",{"5":{"101":2}}],["门控交叉注意力",{"5":{"86":2}}],["门控机制与注意力计算的交互可以增强模型对不同输入模式的适应性",{"5":{"86":2}}],["门控线性注意力",{"5":{"86":2}}],["门控注意力等",{"5":{"86":2}}],["门控与注意力的交互分析",{"2":{"86":1},"5":{"86":1}}],["长序列是检验长程依赖建模能力的关键场景",{"5":{"92":2}}],["长序列中的效率优势",{"2":{"92":1},"5":{"92":1}}],["长程依赖的建模存在",{"5":{"92":2}}],["长程依赖",{"5":{"92":2}}],["长程依赖建模和位置编码等多个角度深入分析了注意力机制的数学原理",{"5":{"87":2}}],["长程依赖建模机制",{"5":{"86":2}}],["长度外推",{"5":{"91":2,"92":2}}],["长距离衰减问题",{"5":{"88":1}}],["长距离衰减问题是一个实践中的挑战",{"5":{"88":1}}],["长距离回指",{"5":{"88":2}}],["长对话建模",{"5":{"95":2}}],["长对话",{"5":{"86":2}}],["递归函数",{"5":{"92":2}}],["递归形式的另一个优势是支持",{"5":{"86":2}}],["递归形式的线性注意力",{"2":{"86":1},"5":{"86":1}}],["递推地",{"5":{"45":2}}],["属于组",{"5":{"86":1}}],["属于正类",{"5":{"61":1}}],["属于类别1",{"5":{"47":1}}],["属于类别0",{"5":{"47":1}}],["哪个分布是",{"5":{"61":2}}],["固定+可学习",{"5":{"89":2}}],["固定",{"5":{"89":2}}],["固定稀疏模式",{"5":{"86":2}}],["固定的形式",{"5":{"91":2}}],["固定的",{"5":{"61":2}}],["固有随机性",{"5":{"51":2}}],["得分",{"5":{"61":2}}],["得到词汇表上各词元的生成概率分布",{"5":{"96":2}}],["得到最终的输出矩阵",{"5":{"95":2}}],["得到最终的聚合结果",{"5":{"95":2}}],["得到注意力权重矩阵",{"5":{"95":2}}],["得到生成当前词所需的信息",{"5":{"95":2}}],["得到查询向量",{"5":{"94":1}}],["得到查询向量​",{"5":{"94":1}}],["得到query矩阵",{"5":{"94":2}}],["得到多头注意力的最终输出",{"5":{"93":2}}],["得到各自的输出",{"5":{"93":2}}],["得到两个",{"5":{"88":1}}],["得到两个维的向量3",{"5":{"88":1}}],["得到的注意力矩阵",{"5":{"87":1}}],["得到的注意力矩阵定义为",{"5":{"87":1}}],["得到",{"5":{"87":2,"88":1,"90":1}}],["得到奇异值序列",{"5":{"87":2}}],["得到期望mse",{"5":{"51":2}}],["得到样本属于正类的概率估计",{"5":{"71":2}}],["机制通过并行运行多个独立的注意力头",{"5":{"93":2}}],["机制",{"5":{"87":2}}],["机制中",{"5":{"61":2}}],["机制可以被理解为一种",{"5":{"61":2}}],["机制的几何效果是",{"5":{"69":2}}],["机制与均方误差中的梯度裁剪有类似的功能",{"5":{"99":2,"101":2}}],["关键在于",{"5":{"94":2}}],["关键在于模型是否能够学习到正确的注意力权重模式",{"5":{"92":2}}],["关键的区别在于卷积的局部性和注意力的全局性",{"5":{"93":2}}],["关键的超参数包括",{"5":{"89":2}}],["关键的理解在于",{"5":{"88":2}}],["关键发现",{"5":{"70":2}}],["关注输入的局部窗口",{"5":{"92":2}}],["关注位置",{"5":{"91":1,"92":2}}],["关注的是位置之间的相对关系",{"5":{"88":2}}],["关注的循环依赖",{"5":{"86":1}}],["关注",{"5":{"86":4,"89":2,"95":2}}],["关注程度",{"5":{"61":2,"69":2}}],["关于特征值1的特征向量",{"5":{"87":1}}],["关于参数",{"5":{"51":1}}],["关于参数和的梯度为",{"5":{"51":1}}],["关于",{"5":{"48":1,"51":1,"70":2,"92":2,"96":1,"97":1}}],["关于变量",{"5":{"48":1}}],["关于为什么深度神经网络能够有效优化",{"5":{"48":2}}],["关于输入",{"5":{"71":1,"87":2}}],["关于第一层权重",{"5":{"41":1}}],["关于查询向量的梯度",{"5":{"69":1}}],["关于查询向量",{"5":{"69":1}}],["关于键向量的梯度",{"5":{"69":1}}],["关于键向量",{"5":{"69":1}}],["关于负样本键的梯度",{"5":{"69":1}}],["关于负样本键",{"5":{"69":1}}],["寻找最优的编码方案",{"5":{"61":2}}],["尽可能接近真实数据分布",{"5":{"61":2}}],["尽管多头注意力有",{"5":{"93":2}}],["尽管多头注意力有个注意力头",{"5":{"93":1}}],["尽管多头注意力有个头",{"5":{"93":1}}],["尽管",{"5":{"92":2}}],["尽管昨天下了很大的雨",{"5":{"92":2}}],["尽管编码矩阵有",{"5":{"89":1}}],["尽管编码矩阵有个参数",{"5":{"89":1}}],["尽管rope具有诸多优势",{"5":{"88":2}}],["尽管超长上下文",{"5":{"88":2}}],["尽管adam在实践中表现优异",{"5":{"48":2}}],["尽管单个神经元结构极其简单",{"5":{"47":2}}],["尽管整体上是非线性的",{"5":{"45":2}}],["尽管softmax是当前损失函数设计的主流选择",{"5":{"99":2,"101":2}}],["经验证据表明",{"5":{"87":2}}],["经验风险的最小化在一定条件下",{"5":{"61":2}}],["经验分布收敛于真实数据分布",{"5":{"61":2}}],["经验上",{"5":{"69":2}}],["经过缩放后",{"5":{"95":2}}],["经过投影矩阵",{"5":{"94":1}}],["经过投影矩阵​变换后得到query向量​",{"5":{"94":1}}],["经过第一层注意力后得到",{"5":{"94":2}}],["经过步传递后",{"5":{"92":1}}],["经过步传播后",{"5":{"87":1}}],["经过rope编码后的query",{"5":{"88":2}}],["经过rope编码后变为",{"5":{"88":2}}],["经过足够多次的",{"5":{"87":2}}],["经过softmax变换后",{"5":{"87":2}}],["经过softmax归一化后",{"5":{"87":2}}],["经过sigmoid变换后",{"5":{"71":2}}],["经过注意力计算后的输出张量会与输入张量形状相同",{"5":{"50":2}}],["经过线性投影并拆分注意力头后",{"5":{"50":2}}],["经过线性变换得到的",{"5":{"87":1}}],["经过线性变换",{"5":{"47":1}}],["经过线性变换后",{"5":{"47":1}}],["经过激活函数的输出为",{"5":{"71":2}}],["经过",{"5":{"41":1,"42":1,"61":2,"87":2,"92":1}}],["经过层后",{"5":{"41":1,"42":1}}],["经过交叉熵的概率论推导",{"5":{"99":2,"101":2}}],["下计算",{"5":{"97":2}}],["下进行大部分计算来加速训练",{"5":{"97":2}}],["下降",{"5":{"97":2}}],["下界性质",{"5":{"61":1}}],["下界性质表明",{"5":{"61":1}}],["下观察到该样本集的概率",{"5":{"61":2}}],["下一步无法开始",{"5":{"92":2}}],["下一层的输入分布会发生偏移",{"5":{"42":2}}],["下一节",{"5":{"89":2,"91":2,"92":2,"93":2,"95":2}}],["下一节我们将讨论损失函数与优化目标",{"5":{"45":2}}],["下一节将从",{"5":{"70":2}}],["下一个词的条件概率分布的期望特性",{"5":{"96":2}}],["下一个词的选择遵循某种概率分布",{"5":{"96":2}}],["下一个token预测的数学框架",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["下",{"5":{"69":2,"93":2,"97":2}}],["下的期望",{"5":{"70":1}}],["编码点沿螺旋轨迹移动",{"5":{"91":2}}],["编码接近",{"5":{"91":2}}],["编码位置之间的几何关系",{"5":{"91":2}}],["编码语义相似性",{"5":{"91":2}}],["编码应当能够处理训练时未见过的序列长度",{"5":{"91":2}}],["编码就能够为每个位置生成唯一的表示",{"5":{"90":2}}],["编码差可以表示为",{"5":{"90":2}}],["编码公式中的正弦和余弦函数不是随意选择的",{"5":{"90":2}}],["编码充分利用了",{"5":{"89":1}}],["编码充分利用了​维空间",{"5":{"89":1}}],["编码维度",{"5":{"89":1}}],["编码维度​是表达能力的硬上限",{"5":{"89":1}}],["编码可以预先计算并缓存",{"5":{"89":2}}],["编码主要捕获位置的主要变化模式",{"5":{"89":2}}],["编码容量不足",{"5":{"89":2}}],["编码有足够的容量表示所有位置",{"5":{"89":2}}],["编码的生成和计算应当高效",{"5":{"91":2}}],["编码的唯一性与可辨识性",{"2":{"91":1},"5":{"91":1}}],["编码的唯一性由各频率成分的线性无关性保证",{"5":{"90":2}}],["编码的表达能力与泛化能力之间存在权衡",{"5":{"89":2}}],["编码的有效表达能力受到训练数据分布的影响",{"5":{"89":2}}],["编码被限制在某个低维子空间中",{"5":{"89":2}}],["编码向量集合",{"5":{"91":1}}],["编码向量集合张成一个维的子空间",{"5":{"91":1}}],["编码向量在二维投影空间中形成螺旋或圆形分布",{"5":{"91":2}}],["编码向量在球面上均匀分布",{"5":{"91":2}}],["编码向量可以视为高维空间中的点",{"5":{"91":2}}],["编码向量可以张成整个",{"5":{"89":1}}],["编码向量可以张成整个​空间",{"5":{"89":1}}],["编码向量序列是线性无关的",{"5":{"90":2}}],["编码向量越",{"5":{"89":4}}],["编码向量的分布具有某种",{"5":{"91":2}}],["编码向量的分散程度可以用方差矩阵来度量",{"5":{"89":2}}],["编码向量的偶数维度",{"5":{"91":2}}],["编码向量的维度决定了它最多能够区分",{"5":{"89":1}}],["编码向量的维度决定了它最多能够区分个位置",{"5":{"89":1}}],["编码向量之间的角度关系可以通过gram矩阵分析",{"5":{"89":2}}],["编码向量应该形成一个能够唯一标识每个位置的配置",{"5":{"89":2}}],["编码矩阵",{"5":{"89":3}}],["编码矩阵接近奇异",{"5":{"89":2}}],["编码矩阵秩的上界为",{"5":{"89":2}}],["编码矩阵通常可以达到满秩",{"5":{"89":2}}],["编码矩阵的低秩近似是分析和压缩可学习位置编码的重要工具",{"5":{"89":2}}],["编码矩阵的奇异值分布揭示了编码的结构特性",{"5":{"89":2}}],["编码矩阵的秩与长度外推能力之间存在重要的关系",{"5":{"89":2}}],["编码矩阵的秩会逐渐增加",{"5":{"89":2}}],["编码矩阵的秩",{"5":{"89":1}}],["编码矩阵的具体数值完全由训练数据决定",{"5":{"89":1}}],["编码矩阵中的所有元素都是可学习的参数",{"5":{"89":1}}],["编码成",{"5":{"88":2}}],["编码后计算attention",{"5":{"88":2}}],["编码来自",{"5":{"61":2}}],["编码",{"5":{"61":2,"90":2,"91":2}}],["编码函数定义了相似度函数",{"5":{"69":1}}],["编码函数",{"5":{"69":1}}],["整体位置",{"5":{"91":2}}],["整体缩放梯度的大小",{"5":{"69":2}}],["整数",{"5":{"61":2}}],["整个序列或批次的损失是这些位置损失的均值或和",{"5":{"96":2}}],["整个序列的信息",{"5":{"92":2}}],["整个计算可以视为对输入",{"5":{"93":1}}],["整个计算可以视为对输入进行一系列线性变换和非线性变换的组合",{"5":{"93":1}}],["整个编码向量是不同频率复指数序列的实部和虚部的组合",{"5":{"90":2}}],["整个数据集的似然函数为",{"5":{"61":2}}],["整个网络仍然等价于一个简单的线性变换",{"5":{"71":2}}],["框架会自动处理",{"5":{"61":2}}],["默认实现已经采用了融合计算策略",{"5":{"61":2}}],["展开后得到四项",{"5":{"91":2}}],["展现了深度表示学习的数学基础",{"5":{"45":2}}],["展示了它们如何支持相对位置相关的注意力模式",{"5":{"92":2}}],["展示了如何使用复指数函数简洁地表示位置编码",{"5":{"90":2}}],["展示了",{"5":{"61":2}}],["展示了损失函数理论在大模型架构设计中的核心地位",{"5":{"70":2}}],["展示这些看似不同的数学结构如何殊途同归",{"5":{"69":2}}],["展示它与注意力机制中",{"5":{"70":2}}],["$dhd",{"5":{"86":1}}],["$",{"5":{"61":2,"86":1,"97":2}}],["$$",{"5":{"46":2,"61":8,"86":2}}],["简称",{"5":{"61":2}}],["简称kl散度",{"5":{"61":2}}],["简称nce",{"5":{"69":2}}],["简称llms",{"5":{"99":2,"101":2}}],["简称mlm",{"5":{"99":2,"101":2}}],["简称rlhf",{"5":{"99":2,"101":2}}],["简称ppo",{"5":{"99":2,"101":2}}],["简称dpo",{"5":{"99":2,"101":2}}],["简单的模型通常具有高偏差但低方差",{"5":{"51":2}}],["简单的模型",{"5":{"51":2}}],["简单性有时比数学上的",{"5":{"71":2}}],["简单性与表达能力之间存在权衡",{"5":{"41":2}}],["简单rnn的梯度分析",{"2":{"41":1},"5":{"41":1}}],["简化形式",{"5":{"99":2,"101":2}}],["越接近",{"5":{"61":3}}],["越确定",{"5":{"61":2}}],["越不确定",{"5":{"61":2}}],["稀有事件",{"5":{"61":2}}],["稀疏激活引入了一种非线性",{"5":{"93":2}}],["稀疏激活",{"5":{"93":6}}],["稀疏矩阵运算在现代硬件",{"5":{"86":2}}],["稀疏模式的选择对谱性质有重要影响",{"5":{"87":2}}],["稀疏模式的不规则内存访问可能导致缓存命中率降低",{"5":{"86":2}}],["稀疏模式的学习与优化",{"2":{"86":1},"5":{"86":1}}],["稀疏模式可以分为以下几类",{"5":{"86":2}}],["稀疏注意力矩阵的谱性质取决于稀疏模式",{"5":{"87":2}}],["稀疏注意力矩阵的非零元素比例为",{"5":{"86":2}}],["稀疏注意力也引入了新的计算挑战",{"5":{"86":2}}],["稀疏注意力显著降低了计算复杂度",{"5":{"86":2}}],["稀疏注意力",{"5":{"86":2,"87":2}}],["稀疏注意力的谱性质",{"2":{"87":1},"5":{"87":1}}],["稀疏注意力的关键是设计合适的稀疏模式",{"5":{"86":1}}],["稀疏注意力的关键是设计合适的稀疏模式​",{"5":{"86":1}}],["稀疏注意力的复杂度分析",{"2":{"86":1},"5":{"86":1}}],["稀疏注意力的数学框架",{"2":{"86":1},"5":{"86":1}}],["稀疏注意力和分层注意力",{"5":{"50":2}}],["稀疏表示提供了更好的可解释性",{"5":{"71":2}}],["稀疏表示对噪声更加鲁棒",{"5":{"71":2}}],["稀疏表示在信息论上具有几个优势",{"5":{"71":2}}],["稀疏编码通常更高效地利用参数",{"5":{"71":2}}],["稀疏softmax",{"5":{"69":2,"99":2,"101":2}}],["惊喜",{"5":{"61":2}}],["香农",{"5":{"61":2}}],["香农熵",{"2":{"61":1},"5":{"61":4}}],["克劳德",{"5":{"61":2}}],["克罗内克函数",{"5":{"44":2}}],["融合实现",{"5":{"97":2}}],["融合",{"5":{"61":1,"91":2}}],["融合梯度计算",{"2":{"61":1},"5":{"61":1}}],["融合计算不仅适用于前向传播",{"5":{"61":2}}],["融合计算的核心思想是直接计算损失值",{"5":{"61":2}}],["融合计算",{"2":{"61":1},"5":{"61":2}}],["好的设计不是添加复杂性",{"5":{"88":2}}],["好",{"5":{"51":2,"97":2,"99":2,"101":2}}],["良好的理论性质和天然的长度外推能力",{"5":{"91":2}}],["良好的表示应该在压缩输入信息的同时保留与任务相关的信息",{"5":{"87":2}}],["良好的表示应该最大化",{"5":{"71":1}}],["良好的表示",{"5":{"71":1}}],["良好",{"5":{"51":2,"97":1}}],["测试序列长度显著超过训练序列长度",{"5":{"89":2}}],["测试mse",{"5":{"51":2}}],["测试mse达到最小值",{"5":{"51":2}}],["测量两个向量方向的相似程度",{"5":{"50":1}}],["测量两个向量之间的直线距离",{"5":{"50":1}}],["复内积",{"5":{"90":2}}],["复数编码提供了理解位置变换的另一个视角",{"5":{"90":2}}],["复数编码的内积具有简洁的形式",{"5":{"90":2}}],["复数形式的优雅之处在于",{"5":{"90":2}}],["复数形式的优点在于",{"5":{"90":2}}],["复数形式的统一分析",{"2":{"90":1},"5":{"90":1}}],["复数系数",{"5":{"90":2}}],["复数",{"5":{"88":1}}],["复数乘以得到",{"5":{"88":1}}],["复数乘法可以自然地表示为二维旋转矩阵的乘法",{"5":{"88":2}}],["复数的实部和虚部可以看作是在二维平面上的坐标",{"5":{"88":2}}],["复杂",{"5":{"97":1}}],["复杂度为",{"5":{"93":2}}],["复杂度的局限性及相应的优化方向",{"5":{"92":1}}],["复杂度在超长序列场景下仍然是瓶颈",{"5":{"92":1}}],["复杂度只能建模",{"5":{"92":1}}],["复杂度",{"5":{"86":2}}],["复杂的模型通常具有低偏差但高方差",{"5":{"51":2}}],["复杂的模型",{"5":{"51":2}}],["复合函数的偏导数法则与单变量情况类似",{"5":{"48":2}}],["复合函数的雅可比矩阵",{"5":{"45":2}}],["复合函数的梯度",{"5":{"45":2}}],["复合函数",{"5":{"45":2}}],["复合函数视角下的前向传播",{"2":{"45":1},"5":{"45":1}}],["复合后的结果仍然是线性变换",{"5":{"47":2}}],["复合后仍保持凸性",{"5":{"70":2}}],["垂线的长度",{"5":{"51":2}}],["垂足就是最优预测向量",{"5":{"51":2}}],["唯一性的另一个角度是相位唯一性",{"5":{"90":2}}],["唯一性",{"5":{"51":2,"91":2,"92":2}}],["降低",{"5":{"93":2}}],["降低对异常值的敏感性",{"5":{"51":2}}],["降低到",{"5":{"50":1,"86":1,"87":2,"90":1,"93":1}}],["给予位置编码稍高的学习率可以加速其学习",{"5":{"89":2}}],["给出的点积决定",{"5":{"91":2}}],["给出的最优损失函数就是mse",{"5":{"51":2}}],["给出",{"5":{"87":1}}],["给定前文的条件下",{"5":{"96":2}}],["给定查询",{"5":{"95":2}}],["给定查询向量",{"5":{"69":2}}],["给定序列",{"5":{"92":1}}],["给定序列计算所有位置对的某种关联度量",{"5":{"92":1}}],["给定序列长度",{"5":{"88":1}}],["给定序列长度和模型维度",{"5":{"88":1}}],["给定编码向量",{"5":{"91":2}}],["给定的序列长度",{"5":{"90":1}}],["给定的序列长度限制了可分析的频率范围和频率分辨率",{"5":{"90":1}}],["给定两个点在各自平面上的位置",{"5":{"90":2}}],["给定两个概率分布",{"5":{"61":2}}],["给定足够多的频率成分",{"5":{"90":2}}],["给定秩为",{"5":{"89":1}}],["给定秩为的编码矩阵",{"5":{"89":1}}],["给定训练时见过的序列长度",{"5":{"88":2}}],["给定训练数据集",{"5":{"51":2}}],["给定query向量",{"5":{"88":2}}],["给定角度",{"5":{"88":2}}],["给定独立同分布的样本集",{"5":{"51":1}}],["给定独立同分布的样本集​",{"5":{"51":1}}],["给定网络结构和输入",{"5":{"45":2}}],["给定输入的最优预测是关于的条件期望",{"5":{"51":1}}],["给定输入​",{"5":{"45":1}}],["给定输入",{"5":{"45":1,"51":1,"71":1}}],["给定输入和其上下文",{"5":{"71":1}}],["给定输入序列",{"5":{"99":2,"101":2}}],["给定一个方阵",{"5":{"50":2}}],["给定一个数据分布",{"5":{"69":2}}],["给定一个样本",{"5":{"69":2}}],["给定一个正样本对",{"5":{"69":2}}],["给定一个包含个序列的训练集",{"5":{"99":1,"101":1}}],["给定一个包含",{"5":{"99":1,"101":1}}],["给定偏好对",{"5":{"99":2,"101":2}}],["欧几里得范数",{"5":{"51":2}}],["欧几里得距离",{"5":{"50":1,"70":1}}],["欧几里得距离测量两个向量之间的直线距离",{"5":{"50":1}}],["取特定值",{"5":{"96":1}}],["取决于投影矩阵的学习结果",{"5":{"93":2}}],["取决于具体实现",{"5":{"91":2}}],["取范数",{"5":{"87":2}}],["取期望",{"5":{"51":1}}],["取对数后得到对数似然函数",{"5":{"51":2}}],["取平方得到",{"5":{"51":2}}],["却在数学上严格保证了平移等变性",{"5":{"88":2}}],["却与自注意力的核心运算",{"5":{"88":2}}],["却因主客关系的颠倒而意义迥异",{"5":{"88":2}}],["却表达着截然相反的事实",{"5":{"88":2}}],["却蕴含着深刻的统计学内涵",{"5":{"51":2}}],["却抓住了生物神经元信息处理的核心特征",{"5":{"47":2}}],["各位置的注意力权重趋于均匀",{"5":{"95":2}}],["各元素的概率值保持在合理的范围内",{"5":{"95":2}}],["各行之间的",{"5":{"94":2}}],["各头独立计算后简单拼接",{"5":{"93":2}}],["各头的输出被简单地拼接起来",{"5":{"93":2}}],["各项的物理意义如下",{"5":{"91":2}}],["各项",{"5":{"90":1}}],["各项正是这个函数的傅里叶基函数",{"5":{"90":1}}],["各频率成分在对数尺度上是均匀分布的",{"5":{"90":2}}],["各种稀疏注意力算法",{"5":{"87":2}}],["各种稀疏注意力和线性注意力算法都利用了注意力矩阵的低秩结构",{"5":{"87":2}}],["各分解项的数学意义",{"2":{"51":1},"5":{"51":1}}],["各层参数的梯度",{"5":{"44":1}}],["各层参数的梯度​",{"5":{"44":1}}],["流式多处理器",{"5":{"93":2}}],["流形等",{"5":{"51":2}}],["流水线并行",{"5":{"50":2}}],["流多处理器",{"5":{"45":1}}],["代词可能距离指代对象很远",{"5":{"92":2}}],["代词可能回指句子开头提到的实体",{"5":{"88":2}}],["代词与其指代对象的关联",{"5":{"92":2}}],["代表模型对第",{"5":{"96":1}}],["代表了源语言各个词语的",{"5":{"95":2}}],["代表了当前生成位置对信息的需求",{"5":{"95":2}}],["代表平移",{"5":{"88":2}}],["代表向量的方向",{"5":{"88":2}}],["代表向量的长度",{"5":{"88":2}}],["代价是增加额外的计算量",{"5":{"50":2}}],["代入rope编码后的向量",{"5":{"88":2}}],["代入",{"5":{"44":1}}],["代入并化简",{"5":{"44":1}}],["代入得",{"5":{"44":2}}],["代入对数几率的表达式",{"5":{"71":1}}],["又不会过于密集或稀疏",{"5":{"91":2}}],["又不受单批次大小的限制",{"5":{"50":2}}],["又称为张量点积或广义矩阵乘法",{"5":{"50":2}}],["×",{"5":{"50":2}}],["卷积可以写为矩阵乘法形式",{"5":{"93":2}}],["卷积核的感受野是固定的",{"5":{"93":2}}],["卷积核定义了一组可学习的滤波器",{"5":{"93":2}}],["卷积核固定",{"5":{"87":2}}],["卷积网络的函数类是",{"5":{"92":2}}],["卷积的路径长度与层数成正比",{"5":{"92":2}}],["卷积结构",{"5":{"88":2}}],["卷积矩阵可以视为具有特殊结构",{"5":{"93":2}}],["卷积矩阵的结构是规则的",{"5":{"87":2}}],["卷积矩阵具有平移不变性",{"5":{"87":2}}],["卷积矩阵与输入无关",{"5":{"87":2}}],["卷积矩阵是稀疏的",{"5":{"87":2}}],["卷积矩阵",{"5":{"87":2}}],["卷积运算和批处理操作",{"5":{"50":2}}],["卷积层",{"5":{"46":2}}],["查询空间",{"2":{"94":1},"5":{"94":1}}],["查询向量",{"5":{"92":1,"95":2}}],["查询向量​和键向量​既编码了位置信息",{"5":{"92":1}}],["查询",{"5":{"50":2,"69":4,"94":4,"95":6}}],["查询矩阵与键矩阵的乘积产生注意力分数",{"5":{"50":2}}],["组内所有头的注意力输出并拼接",{"5":{"93":1}}],["组内进行全连接交互",{"5":{"93":2}}],["组间进行独立的计算",{"5":{"93":2}}],["组织",{"5":{"90":1}}],["组",{"5":{"86":1,"93":1}}],["组成部分",{"5":{"51":4}}],["组成",{"5":{"50":2}}],["组合后达到更高的有效秩",{"5":{"87":2}}],["组合后达到更高的有效表达能力",{"5":{"87":2}}],["组合的梯度计算",{"5":{"61":2}}],["组合在梯度计算上的优美对称性",{"5":{"61":2}}],["组合",{"5":{"71":1}}],["才能进行这种分解",{"5":{"50":2}}],["∞",{"5":{"50":2}}],["典型的衰减曲线类似于幂律分布或指数分布",{"5":{"87":2}}],["典型的低秩适配方法如lora",{"5":{"50":2}}],["典型的操作包括矩阵乘法",{"5":{"48":2}}],["被分为",{"5":{"93":1}}],["被分解为多个低阶张量的组合",{"5":{"50":2}}],["被正确分类",{"5":{"61":2}}],["被誉为",{"5":{"50":2}}],["被推离负样本的加权中心",{"5":{"69":2}}],["被拉向",{"5":{"69":2}}],["被",{"5":{"70":1}}],["被掩码的位置的位置编码被设为0或忽略",{"5":{"91":2}}],["被掩码的概率分布",{"5":{"99":1,"101":1}}],["被掩码位置的双向表示需要聚合来自左右两侧的token信息",{"5":{"99":1,"101":1}}],["被掩码位置的双向表示",{"5":{"99":1,"101":1}}],["被独立地选为掩码位置",{"5":{"99":1,"101":1}}],["被人类偏好于",{"5":{"99":1,"101":1}}],["谱正则化鼓励编码矩阵具有更好的条件数",{"5":{"89":2}}],["谱正则化惩罚小的奇异值或大的条件数",{"5":{"89":2}}],["谱成分",{"5":{"87":2}}],["谱归一化",{"5":{"50":4}}],["谱范数为",{"5":{"89":2}}],["谱范数为1",{"5":{"87":2}}],["谱范数不超过行数的平方根",{"5":{"87":2}}],["谱范数越接近行数",{"5":{"87":2}}],["谱范数越小",{"5":{"87":2}}],["谱范数的上界取决于注意力权重的分布",{"5":{"87":2}}],["谱范数最重要",{"5":{"50":2}}],["谱范数是核心概念",{"5":{"50":2}}],["谱范数在深度学习中有重要应用",{"5":{"50":2}}],["谱范数",{"5":{"50":6}}],["谱半径",{"5":{"87":2}}],["谱半径与矩阵幂的收敛性密切相关",{"5":{"87":2}}],["谱半径与收敛性",{"2":{"87":1},"5":{"87":1}}],["谱半径与特征值的关系",{"5":{"41":2}}],["谱半径可能大于或小于最大奇异值",{"5":{"41":2}}],["谱半径等于最大奇异值",{"5":{"41":2}}],["谱半径等于1是临界状态",{"5":{"41":2}}],["谱半径是特征值模的最大值",{"5":{"41":2}}],["遗忘机制确保了远处的旧信息逐渐衰减",{"5":{"86":2}}],["遗忘",{"5":{"50":2,"86":2}}],["子空间是特征空间中的重要结构",{"5":{"50":2}}],["剪切则是一种保持体积但改变形状的变换",{"5":{"50":2}}],["投影到一个",{"5":{"94":1}}],["投影到低维空间",{"5":{"87":1}}],["投影的几何视角",{"2":{"94":1},"5":{"94":1}}],["投影权重张量",{"5":{"93":2}}],["投影",{"5":{"61":2,"91":2,"95":2}}],["投影矩阵通常通过适当的初始化",{"5":{"94":2}}],["投影矩阵是可逆的当且仅当其行列式",{"5":{"94":1}}],["投影矩阵是方阵还是长矩阵取决于维度的选择",{"5":{"94":1}}],["投影矩阵的学习率选择",{"5":{"97":2}}],["投影矩阵的列向量是通过数据驱动的学习得到的",{"5":{"94":2}}],["投影矩阵的列向量逐渐学习到输入数据的重要特征方向",{"5":{"94":2}}],["投影矩阵的初始化策略对模型的训练动态有重要影响",{"5":{"94":2}}],["投影矩阵的奇异值分解",{"2":{"94":1},"5":{"94":1}}],["投影矩阵的行列式分析",{"2":{"94":1},"5":{"94":1}}],["投影矩阵的并行计算",{"2":{"93":1},"5":{"93":1}}],["投影矩阵为",{"5":{"87":1}}],["投影矩阵为和​",{"5":{"87":1}}],["投影矩阵",{"5":{"51":1,"94":2}}],["投影矩阵满足幂等性",{"5":{"51":1}}],["投影部分",{"5":{"51":2}}],["投影点就是最优预测",{"5":{"51":2}}],["投影将向量映射到低维子空间",{"5":{"50":2}}],["投影和剪切等",{"5":{"50":2}}],["余弦相似度",{"5":{"50":1}}],["余弦相似度是最常用的相似度度量",{"5":{"50":2}}],["余弦相似度测量两个向量方向的相似程度",{"5":{"50":1}}],["余弦退火",{"5":{"48":2,"97":2}}],["过拟合的风险降低",{"5":{"93":2}}],["过滤掉噪声和不重要信息",{"5":{"94":2}}],["过滤",{"5":{"86":2}}],["过高的维度则可能导致过拟合和计算效率的下降",{"5":{"50":2}}],["过低的维度可能导致表达能力的损失",{"5":{"50":2}}],["过于确定的初始化",{"5":{"89":2}}],["过于随机的初始化可能引入过多噪声",{"5":{"89":2}}],["过于复杂的激活函数可能难以训练或过拟合",{"5":{"41":2}}],["过于简单的激活函数",{"5":{"41":2}}],["过于简单的激活函数可能限制网络的表达能力",{"5":{"41":2}}],["过大的权重差异可能导致某些任务的梯度主导",{"5":{"99":2,"101":2}}],["嵌入之间的互信息",{"5":{"92":1}}],["嵌入维度为",{"5":{"89":1,"91":1}}],["嵌入维度为​",{"5":{"89":1,"91":1}}],["嵌入维度",{"5":{"50":2}}],["嵌入空间的维度通常远小于词表大小",{"5":{"50":2}}],["嵌入空间通常是密集的",{"5":{"50":2}}],["嵌入空间中引入了一个额外的几何结构",{"5":{"70":2}}],["嵌入层将词汇映射为向量",{"5":{"50":2}}],["嵌入在空间中",{"5":{"70":1}}],["嵌入在",{"5":{"70":1}}],["嵌入",{"5":{"99":2,"101":2}}],["零初始化的优点是简单",{"5":{"89":2}}],["零初始化",{"5":{"89":2}}],["零角度旋转",{"5":{"88":1}}],["零角度旋转是单位元",{"5":{"88":1}}],["零空间",{"5":{"50":2}}],["零阶张量",{"5":{"50":2}}],["零中心版本",{"5":{"42":2}}],["零中心输出使得各层的输入分布更加稳定",{"5":{"42":2}}],["零中心输出减少了梯度偏移问题",{"5":{"42":2}}],["记忆",{"5":{"90":2}}],["记录训练mse",{"5":{"51":2}}],["记作",{"5":{"50":1}}],["记作或",{"5":{"50":1}}],["记为​",{"5":{"96":1}}],["记为或",{"5":{"50":1}}],["记为",{"5":{"46":2,"50":1,"96":7}}],["转速由频率决定",{"5":{"90":2}}],["转置运算满足",{"5":{"50":1}}],["转置运算满足和",{"5":{"50":1}}],["转置性质",{"5":{"50":2}}],["转置",{"5":{"50":2}}],["转置操作也经常用于梯度的反向传播计算",{"5":{"50":2}}],["转置和求逆等",{"5":{"50":2}}],["转换为标准高斯随机变量",{"5":{"96":1}}],["转换为有效的类别概率分布",{"5":{"96":2}}],["转换为有效的概率分布",{"5":{"69":2}}],["转换为有效概率分布的函数就是",{"5":{"61":2}}],["转换为概率",{"5":{"71":1}}],["转换为显式",{"5":{"71":2}}],["转换为后验概率的过程",{"5":{"71":2}}],["转换为类别概率分布",{"5":{"70":2}}],["键的点积相似度度量了信息需求与信息标识之间的匹配程度",{"5":{"95":2}}],["键位置对之间的相似度",{"5":{"95":2}}],["键位置是其指代对象时",{"5":{"92":2}}],["键位置是谓语词汇时",{"5":{"92":2}}],["键相似度计算",{"5":{"95":4}}],["键​作为相关信息来源的概率",{"5":{"95":1}}],["键向量",{"5":{"95":2}}],["键空间与值空间的语义分离",{"2":{"94":1},"5":{"94":1}}],["键和值张量",{"5":{"50":2}}],["键",{"5":{"50":4,"69":2,"94":4,"95":1}}],["依概率收敛于",{"5":{"96":1}}],["依赖实现",{"5":{"97":1}}],["依赖关系的强度由内容相似度决定",{"5":{"92":2}}],["依赖关系的最大跨度为",{"5":{"92":2}}],["依赖跨度较大",{"5":{"92":2}}],["依赖跨度较小",{"5":{"92":2}}],["依赖跨度的数学度量",{"2":{"92":1},"5":{"92":1}}],["依赖的",{"5":{"92":2}}],["依赖",{"5":{"92":4}}],["依赖建模的计算复杂度下界",{"2":{"92":1},"5":{"92":1}}],["依赖传递的数学模型",{"2":{"92":1},"5":{"92":1}}],["依赖于query和key",{"5":{"87":2}}],["依赖于当前的预测概率",{"5":{"70":2,"97":2}}],["依赖于预测值",{"5":{"70":2}}],["依此类推",{"5":{"45":2}}],["覆盖",{"5":{"87":2,"96":1}}],["维随机向量",{"5":{"96":2}}],["维空间",{"5":{"93":1}}],["维度设计与参数数量",{"2":{"94":1},"5":{"94":1}}],["维度为",{"5":{"93":2}}],["维度为列",{"5":{"91":2}}],["维度​",{"5":{"93":1}}],["维度配对",{"5":{"91":2}}],["维度配对的设计",{"5":{"91":2}}],["维度通常较大",{"5":{"91":2}}],["维度较小",{"5":{"91":2}}],["维度",{"5":{"90":1,"93":1}}],["维度对应不同的频率成分",{"5":{"90":1}}],["维度的选择是一个重要的超参数",{"5":{"50":2}}],["维子空间信息",{"5":{"87":1}}],["维欧几里得空间",{"5":{"51":1}}],["维向量",{"5":{"47":1,"50":1}}],["维向量映射到概率单纯形",{"5":{"71":1}}],["维的概率向量",{"5":{"96":1}}],["维的值向量",{"5":{"95":1}}],["维的键向量",{"5":{"95":1}}],["维的子空间",{"5":{"91":1}}],["维的子空间中",{"5":{"87":1}}],["维的编码向量投影到二维平面进行可视化",{"5":{"91":1}}],["维的编码向量",{"5":{"91":1}}],["维的",{"5":{"88":1}}],["维的向量",{"5":{"88":1}}],["维的紧致流形",{"5":{"70":1}}],["还能有效避免深度神经网络中常见的梯度消失问题",{"5":{"96":2}}],["还为所有类别分配非零的概率值",{"5":{"96":2}}],["还给出了差异大小的可能范围",{"5":{"96":2}}],["还容易引发梯度消失或梯度爆炸问题",{"5":{"95":2}}],["还容易引发梯度消失问题",{"5":{"92":2}}],["还有一些头专注于学习语义相似性",{"5":{"93":2}}],["还有一种可视化方法是热力图展示",{"5":{"91":2}}],["还有某些头可能专注于学习位置相近词语之间的局部关联",{"5":{"93":2}}],["还明确地包含查询和键之间的相对位置信息",{"5":{"92":2}}],["还引入了位置插值",{"5":{"88":2}}],["还与激活值的存储需求有关",{"5":{"50":2}}],["还可以从概率论的角度进行分析",{"5":{"47":2}}],["还是来自分布",{"5":{"69":2}}],["独立时互信息为0",{"5":{"96":1}}],["独立时互信息为零",{"5":{"71":1}}],["独立于样本量",{"5":{"96":2}}],["独立于无关选择的特性",{"5":{"69":1}}],["独立",{"5":{"95":1,"96":1}}],["独立地进行scaled",{"5":{"93":1}}],["独立地进行注意力计算",{"5":{"93":3}}],["独立处理输入",{"5":{"93":2}}],["趋向于正态分布",{"5":{"96":2}}],["趋向于0",{"5":{"94":1}}],["趋向于尖锐分布",{"5":{"94":2}}],["趋向无穷时",{"5":{"51":1}}],["趋近于其渐近值",{"5":{"41":1}}],["指出",{"5":{"96":2}}],["指出rope的旋转操作可以在attention计算前完成",{"5":{"88":2}}],["指的是序列中相距较远的位置之间存在的语义关联或结构关系",{"5":{"92":2}}],["指的是矩阵的秩",{"5":{"87":2}}],["指数函数",{"5":{"96":1}}],["指数函数的非负性保证了输出概率的非负性",{"5":{"96":1}}],["指数频率分布",{"5":{"91":1}}],["指数频率分布​的几何直觉是",{"5":{"91":1}}],["指数频率分布对应于某种特定的先验分布",{"5":{"90":2}}],["指数分布在语言模型中的应用包括描述某些随机过程的到达时间间隔",{"5":{"96":2}}],["指数分布的期望为",{"5":{"96":1}}],["指数分布的期望为​",{"5":{"96":1}}],["指数分布的频率确保了从低频到高频的广泛覆盖",{"5":{"90":2}}],["指数分布是否是绝对最优的",{"5":{"90":2}}],["指数分布提供了对数尺度的均匀分辨率",{"5":{"90":2}}],["指数分布",{"5":{"90":2,"96":2}}],["指数化",{"5":{"87":2}}],["指数核的数值稳定性较差",{"5":{"86":2}}],["指数特征映射",{"5":{"86":2}}],["指数衰减",{"5":{"41":1,"48":2}}],["指数增长",{"5":{"41":1}}],["指向某个方向",{"5":{"69":1}}],["指令遵循任务",{"5":{"99":2,"101":2}}],["重要",{"5":{"96":2}}],["重要的是",{"5":{"93":2}}],["重复多次",{"5":{"96":2}}],["重新发现",{"5":{"92":2}}],["重新浮现",{"5":{"92":2}}],["重新分布",{"5":{"87":2}}],["重写损失函数为",{"5":{"61":2}}],["重塑操作改变张量的形状但不改变其包含的数据元素",{"5":{"50":2}}],["重塑",{"5":{"50":2}}],["服从一维高斯分布",{"5":{"96":1}}],["服从多元高斯分布",{"5":{"96":1}}],["服从以真实参数为均值",{"5":{"96":2}}],["服从高斯分布",{"5":{"51":1}}],["服从伯努利分布",{"5":{"47":1}}],["选择哪种kl形式取决于我们希望对近似分布施加什么样的约束",{"5":{"96":2}}],["选择权重最大的",{"5":{"93":1}}],["选择权重最大的个头参与计算",{"5":{"93":1}}],["选择逐元素相加而非拼接",{"5":{"91":2}}],["选择合适的",{"5":{"89":1}}],["选择合适的使得初始编码向量具有适当的范数",{"5":{"89":1}}],["选择取决于具体任务需求和约束条件",{"5":{"89":2}}],["选择取决于具体任务和需求",{"5":{"89":2}}],["选择",{"5":{"87":2,"95":2}}],["选择和互信息最大化作用",{"5":{"71":2}}],["选择正样本",{"5":{"69":2}}],["选择概率",{"5":{"69":1}}],["尖锐性",{"5":{"92":2}}],["尖锐",{"5":{"69":4,"87":2,"89":2,"95":2,"96":2}}],["某个",{"5":{"96":1}}],["某个的概率为1",{"5":{"96":1}}],["某一概率为1",{"5":{"61":2}}],["某些语言模型变体使用高斯混合模型来表示输出分布",{"5":{"96":2}}],["某些语言依赖具有指数衰减的距离敏感性",{"5":{"88":2}}],["某些头专注于学习语法依存关系",{"5":{"93":2}}],["某些头专门建模",{"5":{"92":6}}],["某些头具有较低的熵",{"5":{"92":2}}],["某些头具有较高的熵",{"5":{"92":2}}],["某些头可能专注于学习语义角色",{"5":{"93":2}}],["某些头可能专注于学习语法结构",{"5":{"93":2}}],["某些头可能专注于学习远距离的依赖",{"5":{"92":2}}],["某些头可能专注于学习近距离的依赖",{"5":{"92":2}}],["某些头的衰减较慢",{"5":{"87":2}}],["某些头的注意力矩阵有更多的非平凡奇异值",{"5":{"87":2}}],["某些头的注意力矩阵几乎是秩1的",{"5":{"87":4}}],["某些任务可能需要更精细的高频分辨率",{"5":{"90":2}}],["某些任务可能需要更精细的低频分辨率",{"5":{"90":2}}],["某些实践表明",{"5":{"89":2}}],["某些会减小",{"5":{"89":2}}],["某些奇异值会增大",{"5":{"89":2}}],["某些模型在不同层使用不同的位置编码策略",{"5":{"88":2}}],["某些模型探索了将绝对位置编码",{"5":{"88":2}}],["某些特定方向",{"5":{"50":2}}],["许多自然现象和测量误差都近似服从高斯分布",{"5":{"96":2}}],["许多激活函数",{"5":{"71":2}}],["许多激活函数可以解释为某种概率分布的参数变换",{"5":{"71":2}}],["许多损失函数都可以被视为某种形式的",{"5":{"69":2}}],["便于解释和比较",{"5":{"96":2}}],["便于进行理论分析和与贝叶斯方法的比较",{"5":{"95":2}}],["便于进行理论分析和硬件优化",{"5":{"93":2}}],["便于进行理论分析",{"5":{"94":2}}],["便于进行残差连接和层归一化操作",{"5":{"50":2}}],["便于后续处理",{"5":{"61":2}}],["困难",{"5":{"41":2}}],["困难负样本",{"5":{"69":2}}],["困难样本",{"5":{"70":2}}],["困惑度",{"5":{"96":2}}],["困惑",{"5":{"70":2}}],["词汇表上的概率分布就是典型的pmf",{"5":{"96":2}}],["词汇表中的词索引等",{"5":{"96":2}}],["词索引是离散的",{"5":{"96":2}}],["词序承载着重要的语法和语义信息",{"5":{"92":2}}],["词序信息等局部依赖",{"5":{"92":2}}],["词→短语→句子→篇章",{"5":{"92":2}}],["词等不同层次",{"5":{"88":2}}],["词语的顺序承载着至关重要的语义信息",{"5":{"88":2}}],["词嵌入维度为",{"5":{"94":1}}],["词嵌入维度为​",{"5":{"94":1}}],["词嵌入的降维可视化",{"5":{"50":2}}],["词嵌入空间具有一些重要的几何性质",{"5":{"50":2}}],["词嵌入空间就是一个典型的高维特征空间",{"5":{"50":2}}],["词向量就是典型的向量表示",{"5":{"50":2}}],["词袋",{"5":{"99":2,"101":2}}],["期望有限",{"5":{"96":2}}],["期望传播提供了有用的近似工具",{"5":{"96":2}}],["期望传播在某些情况下比变分推断更准确",{"5":{"96":2}}],["期望传播",{"5":{"96":2}}],["期望具有线性性质",{"5":{"96":2}}],["期望定义为",{"5":{"96":2}}],["期望",{"2":{"96":1},"5":{"96":6}}],["期望范数为",{"5":{"89":2}}],["期望获得的信息量",{"5":{"61":2}}],["期望mse的分解",{"2":{"51":1},"5":{"51":1}}],["沿",{"5":{"48":1}}],["沿计算图的边反向传播梯度",{"5":{"44":2}}],["沿计算图的反向方向累积计算导数",{"5":{"44":2}}],["沿计算图的前向方向累积计算导数",{"5":{"44":2}}],["衡量差异的实际大小",{"5":{"96":2}}],["衡量两个随机变量之间的信息共享程度",{"5":{"96":2}}],["衡量两个随机变量之间的线性相关程度",{"5":{"96":2}}],["衡量随机变量取值的离散程度",{"5":{"96":2}}],["衡量了编码向量的线性无关程度",{"5":{"89":1}}],["衡量模型预测对训练数据的敏感程度",{"5":{"51":2}}],["衡量模型预测的平均值与真实函数之间的差异",{"5":{"51":2}}],["衡量预测值与真实值之间差异的平方的平均值",{"5":{"51":2}}],["衡量输出",{"5":{"48":1}}],["衡量函数沿特定方向的变化率",{"5":{"48":2}}],["衡量和之间的依赖程度",{"5":{"69":1}}],["衡量",{"5":{"69":1}}],["衡量位置包含多少关于序列其他部分的信息",{"5":{"69":1}}],["衡量位置",{"5":{"69":1}}],["传递",{"5":{"92":2}}],["传导的",{"5":{"48":1}}],["传统的循环神经网络",{"5":{"92":2,"95":2}}],["传统的带动量sgd可能优于adam",{"5":{"48":2}}],["传统的最大似然估计需要计算归一化常数",{"5":{"69":2}}],["商的偏导数",{"5":{"48":1}}],["商的偏导数​​",{"5":{"48":1}}],["穿行",{"5":{"48":2}}],["允许模型学习到位置",{"5":{"92":2}}],["允许信息直接从较浅的层流向较深的层",{"5":{"92":2}}],["允许每个位置与远距离的随机位置交互",{"5":{"86":2}}],["允许梯度下降在不同局部最小值之间",{"5":{"48":2}}],["允许快速学习但可能容忍不稳定",{"5":{"41":1}}],["允许关注前缀内的所有位置",{"5":{"99":2,"101":2}}],["此外",{"5":{"48":2,"50":2,"86":4,"88":2,"92":2,"93":4,"95":2,"96":2,"97":2,"99":2,"101":2}}],["此时缩放后点积的方差为1",{"5":{"95":2}}],["此时对角线元素",{"5":{"94":1}}],["此时对角线元素趋向于0",{"5":{"94":1}}],["此时是置换矩阵",{"5":{"87":1}}],["此时mse达到理论最小值",{"5":{"51":2}}],["此时网络的输出为",{"5":{"71":2}}],["此时",{"5":{"42":4,"51":4,"87":3,"91":2}}],["此时语言建模损失无法学习到序列中的位置模式",{"5":{"99":2,"101":2}}],["鞍点附近的曲率特性决定了优化算法逃离鞍点的速度",{"5":{"97":2}}],["鞍点的存在是普遍的",{"5":{"97":2}}],["鞍点",{"5":{"97":2}}],["鞍点与局部极小值的景观分析",{"2":{"97":1},"5":{"97":1}}],["鞍点比局部最小值更为常见",{"5":{"48":2}}],["鞍点逃离是大规模非凸优化中的一个重要问题",{"5":{"48":2}}],["鞍点和平坦区域",{"5":{"48":2}}],["伪凸",{"5":{"48":2}}],["利普希茨连续的损失函数",{"5":{"97":1}}],["利普希茨连续梯度的凸函数",{"5":{"48":2}}],["利用权衡很有价值",{"5":{"96":2}}],["利用三角恒等式简化",{"5":{"92":2}}],["利用三角恒等式",{"5":{"91":2}}],["利用旋转矩阵的性质直接计算",{"5":{"88":2}}],["利用旋转矩阵的正交性",{"5":{"88":2}}],["利用低秩结构",{"5":{"87":2}}],["利用gpu的异步执行能力隐藏内存访问延迟",{"5":{"86":2}}],["利用gpu的并行计算能力",{"5":{"86":2}}],["利用代数规则求导",{"5":{"45":2}}],["利用有限差分近似",{"5":{"45":2}}],["利用微分中值定理",{"5":{"45":2}}],["利用",{"5":{"42":1}}],["利用平方差公式",{"5":{"42":2}}],["利用​",{"5":{"42":1}}],["利用了损失函数在参数空间中的几何结构",{"5":{"70":2}}],["要覆盖长度为",{"5":{"92":1}}],["要覆盖长度为的序列",{"5":{"92":1}}],["要么完全跳过",{"5":{"86":2}}],["要求上述不等式在",{"5":{"48":1}}],["要求上述不等式在和时取严格小于号",{"5":{"48":1}}],["要将这两个类别分开",{"5":{"71":2}}],["凸",{"5":{"97":2}}],["凸函数的极值性质保证了",{"5":{"51":2}}],["凸函数是一类特殊的函数",{"5":{"48":2}}],["凸函数",{"5":{"70":4,"97":2}}],["凸性",{"5":{"97":1}}],["凸性与全局最优性",{"2":{"97":1},"5":{"97":1}}],["凸性与优化特性",{"2":{"51":1},"5":{"51":1}}],["凸性的充要条件是hessian矩阵半正定",{"5":{"48":2}}],["凸性分析",{"2":{"70":1},"5":{"70":3}}],["凸性是损失函数最重要的性质之一",{"5":{"51":2}}],["凸性是优化理论中最重要的概念之一",{"5":{"70":2}}],["凸性保证了全局最小值的存在",{"5":{"70":2}}],["凸优化是研究凸函数在凸集上最小化问题的数学分支",{"5":{"48":2}}],["阶中心矩定义为",{"5":{"96":2}}],["阶原点矩定义为",{"5":{"96":2}}],["阶梯分布",{"5":{"89":2}}],["阶梯衰减",{"5":{"48":2,"97":2}}],["阶张量有",{"5":{"50":1}}],["阶跃激活函数的数学定义为",{"5":{"47":2}}],["步使用的学习率",{"5":{"97":1}}],["步",{"5":{"92":1}}],["步顺序计算",{"5":{"92":1}}],["步传递后",{"5":{"92":1}}],["步传播后",{"5":{"87":1}}],["步的顺序计算",{"5":{"92":2}}],["步的传递才能到达",{"5":{"92":1}}],["步的参数",{"5":{"48":1}}],["步间接传播的比例",{"5":{"87":1}}],["步长",{"5":{"48":2}}],["步骤",{"5":{"44":2}}],["基函数",{"5":{"93":2}}],["基函数展开的理论保证告诉我们",{"5":{"93":2}}],["基函数展开",{"5":{"93":2}}],["基准频率",{"5":{"91":3}}],["基准频率为10000",{"5":{"91":2}}],["基准频率的选择是一个关键的设计决策",{"5":{"91":1}}],["基频为",{"5":{"90":2}}],["基础",{"5":{"89":2}}],["基因组序列等",{"5":{"86":2}}],["基于重要性的稀疏化是最常见的自适应策略",{"5":{"86":2}}],["基于",{"5":{"61":2}}],["基于这一直觉",{"5":{"61":2}}],["基于前文的分析",{"5":{"41":2}}],["基于上述分析",{"5":{"70":2}}],["基本思想是从原始数据中有放回地抽取与原数据等大的样本",{"5":{"96":2}}],["基本定义与符号约定",{"2":{"51":1},"5":{"51":1}}],["基本运算与运算规则",{"2":{"50":1},"5":{"50":1}}],["基本梯度下降算法的更新规则为",{"5":{"48":2}}],["牛顿矩阵省略了二阶导数项",{"5":{"48":2}}],["牛顿矩阵为",{"5":{"48":2}}],["牛顿矩阵是hessian的一个常用近似",{"5":{"48":2}}],["显式编码相对位置",{"5":{"91":2}}],["显式计算hessian是不现实的",{"5":{"48":2}}],["显著性检验的解读需要谨慎",{"5":{"96":2}}],["显著降低了参数量和内存需求",{"5":{"93":2}}],["显著小于",{"5":{"86":2}}],["显著提升了长序列注意力的计算效率",{"5":{"93":2}}],["显著提升了表示的丰富性和任务的性能",{"5":{"93":2}}],["显著提升了深层模型的可训练性",{"5":{"50":2}}],["显著提高了训练效率",{"5":{"47":2}}],["显著",{"5":{"41":2}}],["少数位置权重较大",{"5":{"89":2}}],["少数几个特征值很大",{"5":{"48":2}}],["少量的活跃神经元可以编码大量的信息",{"5":{"71":2}}],["凹凸程度",{"5":{"48":2}}],["负责组间的信息混合",{"5":{"93":2}}],["负责处理对应的一对维度",{"5":{"88":2}}],["负的最大值",{"5":{"48":2}}],["负梯度方向则是函数下降最快的方向",{"5":{"48":2}}],["负对数似然",{"5":{"70":2,"99":1,"101":1}}],["负对数似然正是交叉熵损失",{"5":{"99":1,"101":1}}],["负样本对",{"5":{"69":2}}],["负样本",{"5":{"69":2,"101":2}}],["极大值或鞍点",{"5":{"48":2}}],["极大地促进了离散分布表示学习的发展",{"5":{"71":2}}],["驻点",{"5":{"48":2}}],["范数为1",{"5":{"87":1}}],["范数​",{"5":{"50":1}}],["范数分别是列范数和行范数",{"5":{"50":2}}],["范数和∞",{"5":{"50":2}}],["范数",{"5":{"48":2,"50":5,"87":2}}],["范数的平方",{"5":{"70":1}}],["范数的某种极端形式",{"5":{"70":1}}],["范围内",{"5":{"44":2,"61":2}}],["接下来",{"5":{"93":2}}],["接力",{"5":{"92":2}}],["接着计算",{"5":{"48":1}}],["接着计算​",{"5":{"48":1}}],["接近序列开头",{"5":{"92":1}}],["接近​的某些维度",{"5":{"89":1}}],["接近​",{"5":{"89":1}}],["接近置换矩阵",{"5":{"87":2}}],["接近one",{"5":{"69":2,"71":2,"94":2}}],["接近均匀分布",{"5":{"71":2}}],["接近均匀混合",{"5":{"41":1}}],["接近",{"5":{"42":2,"70":3,"89":2,"91":4}}],["接近秩",{"5":{"41":2}}],["接近饱和区域",{"5":{"41":2}}],["接近1时",{"5":{"41":1}}],["接近1",{"5":{"69":2,"91":2}}],["接近100",{"5":{"99":2,"101":2}}],["接近0或显著小于邻居位置的权重",{"5":{"92":2}}],["接近0或1",{"5":{"42":2}}],["接近0或1时",{"5":{"41":1,"70":1}}],["接近0",{"5":{"91":4,"95":1,"99":2,"101":2}}],["情况稍微复杂一些",{"5":{"48":2}}],["偏度",{"5":{"96":2}}],["偏差的平方",{"5":{"51":1}}],["偏差的平方始终非负",{"5":{"51":1}}],["偏差平方和方差随模型复杂度的变化",{"5":{"51":2}}],["偏差和方差达到较好的平衡",{"5":{"51":2}}],["偏差主导了mse",{"5":{"51":2}}],["偏差反映了模型假设的局限性",{"5":{"51":2}}],["偏差",{"2":{"51":1},"5":{"51":17}}],["偏导数可以理解为多元函数图像与包含坐标轴",{"5":{"48":1}}],["偏导数可以理解为多元函数图像与包含坐标轴的平面相交得到的一元函数曲线的切线斜率",{"5":{"48":1}}],["偏导数的计算遵循与单变量导数相同的规则",{"5":{"48":2}}],["偏导数是多元函数对单个变量的导数",{"5":{"48":2}}],["偏导数与链式法则",{"2":{"48":1},"5":{"48":1}}],["偏置向量为",{"5":{"47":2}}],["偏置向量通常有特殊的初始化策略",{"5":{"46":2}}],["偏置",{"5":{"45":1,"46":6}}],["偏置加法",{"5":{"45":2}}],["偏置的全连接层",{"5":{"45":1}}],["偏置节点",{"5":{"45":2}}],["偏置项提供灵活的阈值调节",{"5":{"47":2}}],["偏置项则提供了灵活的阈值调节能力",{"5":{"47":1}}],["偏置项",{"5":{"44":1,"47":1}}],["偏置项直接加到净输入上",{"5":{"44":1}}],["偏置梯度为",{"5":{"44":2}}],["偏置梯度",{"5":{"44":2}}],["偏好程度",{"5":{"61":2}}],["偏好",{"5":{"99":2,"101":2}}],["偏离1太远时",{"5":{"99":1,"101":1}}],["点是函数的鞍点",{"5":{"97":1}}],["点积运算隐含地测量了两个向量在各个维度上的",{"5":{"95":2}}],["点积分布的范围将变得非常宽",{"5":{"95":2}}],["点积结果需要通过softmax函数转换为概率分布",{"5":{"95":2}}],["点积的量级与",{"5":{"95":1}}],["点积的量级与​​成正比",{"5":{"95":1}}],["点积的分布大致集中在",{"5":{"95":1}}],["点积的分布大致集中在区间内",{"5":{"95":1}}],["点积的分布特性",{"5":{"95":1}}],["点积的方差会以",{"5":{"95":1}}],["点积的方差会以的速度线性增长",{"5":{"95":1}}],["点积的方差为",{"5":{"95":2}}],["点积是各分量乘积之和",{"5":{"95":2}}],["点积是相对位置",{"5":{"90":1}}],["点积是相对位置的函数",{"5":{"90":1}}],["点积还包含了向量模长的影响",{"5":{"95":2}}],["点积等于它们之间夹角的余弦值",{"5":{"95":2}}],["点积作为相似度度量具有清晰的几何解释",{"5":{"95":2}}],["点积",{"5":{"94":2,"95":1}}],["点积与位置差的关系",{"5":{"91":2}}],["点积呈现下降趋势",{"5":{"91":2}}],["点积只依赖于相对位置",{"5":{"91":2}}],["点积总体上随位置距离增加而减小",{"5":{"91":2}}],["点积随",{"5":{"90":1}}],["点积随呈周期性变化",{"5":{"90":1}}],["点积可以解释为各频率成分相位差余弦的叠加",{"5":{"90":2}}],["点积不依赖于绝对位置",{"5":{"90":2}}],["点运动缓慢",{"5":{"90":2}}],["点迅速遍历整个圆周",{"5":{"90":2}}],["点",{"5":{"47":2,"97":1}}],["点和属于类别1",{"5":{"47":1}}],["点和属于类别0",{"5":{"47":1}}],["扩展为齐次坐标",{"5":{"47":1}}],["变分推断被用于vae的训练",{"5":{"96":2}}],["变分推断假设后验分布可以用某个简单的分布族",{"5":{"96":2}}],["变分推断是一类近似贝叶斯推断的方法",{"5":{"96":2}}],["变分dropout",{"5":{"96":2}}],["变大",{"5":{"92":1}}],["变化",{"5":{"88":1}}],["变化是对称的",{"5":{"70":1}}],["变化是不对称且加速的",{"5":{"70":1}}],["变化是",{"5":{"70":2}}],["变换到频域",{"5":{"90":2}}],["变换到输出空间的过程",{"5":{"71":2}}],["变换得到类别概率",{"5":{"61":2}}],["变换映射到概率单纯形上",{"5":{"70":2}}],["变换",{"5":{"70":2,"88":2}}],["变换的结构高度相似",{"5":{"99":2,"101":2}}],["变异性",{"5":{"51":2}}],["变为",{"5":{"47":1,"88":3,"90":1}}],["变得很小",{"5":{"70":1}}],["后期使用学习率衰减进行精细调优",{"5":{"97":2}}],["后的复合函数可能产生非凸的损失景观",{"5":{"97":2}}],["后的频率",{"5":{"88":2}}],["后者就是编码器分布与标准高斯先验之间的kl散度",{"5":{"96":2}}],["后者为负或零",{"5":{"70":2}}],["后全局",{"5":{"92":2}}],["后个维度几乎不携带信息",{"5":{"89":1}}],["后续的模型如bert使用了类似的配置",{"5":{"93":2}}],["后续的研究也提出了稀疏激活的多头注意力变体",{"5":{"93":2}}],["后续的研究表明",{"5":{"91":2}}],["后续的研究者开始探索乘法式的位置编码方式",{"5":{"88":2}}],["后续奇异值迅速减小",{"5":{"87":4}}],["后",{"5":{"47":1,"89":1,"95":1,"96":1}}],["后关于",{"5":{"69":1,"71":1}}],["赋予神经网络万能逼近能力",{"5":{"47":2}}],["逻辑回归模型正是使用sigmoid神经元进行二分类",{"5":{"47":2}}],["逻辑或",{"5":{"47":2}}],["没有明显的局部偏向",{"5":{"92":2}}],["没有明显的主导方向",{"5":{"89":2}}],["没有连续的分量",{"5":{"90":2}}],["没有可训练参数",{"5":{"89":2}}],["没有破坏注意力机制的代数结构",{"5":{"88":2}}],["没有归一化",{"5":{"87":2}}],["没有激活函数的神经网络等价于单层线性变换",{"5":{"47":2}}],["没有激活函数的神经网络无论多深",{"5":{"47":2}}],["没有softmax的线性归一化将无法实现注意力的",{"5":{"71":2}}],["结合前述的点积方差分析",{"5":{"95":2}}],["结合",{"5":{"91":2}}],["结合了绝对和相对位置的优点",{"5":{"88":2}}],["结合了ntk",{"5":{"88":2}}],["结合方式通常是在rope编码的基础上添加一个可学习的绝对位置嵌入",{"5":{"88":2}}],["结合性",{"5":{"88":2}}],["结合全局位置和局部位置",{"5":{"86":2}}],["结果是一个",{"5":{"95":1}}],["结果是一个的注意力分数矩阵",{"5":{"95":1}}],["结果是将向量的每个分量都乘以该标量",{"5":{"50":2}}],["结果",{"5":{"93":1}}],["结果包含了所有头的query向量",{"5":{"93":1}}],["结果等价于逐样本计算后按行堆叠",{"5":{"47":2}}],["结论成立",{"5":{"47":2}}],["结论显然成立",{"5":{"47":2}}],["结构包含类似",{"5":{"97":1}}],["结构包含类似的项",{"5":{"97":1}}],["结构高度相似",{"5":{"97":2}}],["结构",{"5":{"61":3,"71":2,"89":4,"92":2}}],["结构意味着激活值在0",{"5":{"42":2}}],["结构定义与基本形式",{"2":{"70":1},"5":{"70":1}}],["打破了神经网络的线性瓶颈",{"5":{"47":2}}],["旋转位置编码与相对位置建模",{"2":{"92":1},"5":{"92":1}}],["旋转位置编码",{"5":{"91":2,"92":2}}],["旋转矩阵",{"5":{"88":1}}],["旋转矩阵仍然有定义",{"5":{"88":2}}],["旋转向量",{"5":{"88":2}}],["旋转向量的直接实现需要遍历每个位置和每个维度",{"5":{"88":2}}],["旋转角度由相对位置和频率共同决定",{"5":{"90":2}}],["旋转角度较小",{"5":{"90":2}}],["旋转角度较大",{"5":{"90":2}}],["旋转角度",{"5":{"88":3}}],["旋转角度为",{"5":{"88":2}}],["旋转角度也只是落在实数轴的某个位置上",{"5":{"88":1}}],["旋转操作",{"5":{"88":1}}],["旋转操作是线性变换的一种",{"5":{"88":2}}],["旋转操作不改变向量的长度",{"5":{"88":2}}],["旋转后的向量为",{"5":{"88":2}}],["旋转保持向量的模长不变",{"5":{"50":2}}],["旋转",{"5":{"47":2,"88":6,"94":2}}],["彭罗斯广义逆",{"5":{"47":2}}],["提高泛化能力",{"5":{"96":2}}],["提出了一种更高效的隐式实现方式",{"5":{"88":2}}],["提出了各种改进",{"5":{"86":2}}],["提出的感知机",{"5":{"47":2}}],["提供稳定的全局位置信息",{"5":{"89":2}}],["提供了有力的数学工具",{"5":{"90":2}}],["提供了从",{"5":{"71":1}}],["提供了自适应的激活特性",{"5":{"41":1}}],["提供足够的非线性表达能力",{"2":{"41":1},"5":{"41":1}}],["弗兰克",{"5":{"47":2}}],["阈值",{"5":{"47":6}}],["阈值化",{"5":{"71":2}}],["假设检验的基本框架包括原假设",{"5":{"96":2}}],["假设检验被用于比较不同模型的性能",{"5":{"96":2}}],["假设检验是统计推断的重要工具",{"5":{"96":2}}],["假设检验与模型评估",{"2":{"96":1},"5":{"96":1}}],["假设网络中有大量独立的噪声源",{"5":{"96":2}}],["假设这些数据独立同分布",{"5":{"96":2}}],["假设查询向量",{"5":{"95":1}}],["假设查询向量和键向量的分量都是独立同分布的随机变量",{"5":{"95":1}}],["假设均值为0",{"5":{"95":2}}],["假设有",{"5":{"93":1}}],["假设有个流式多处理器",{"5":{"93":1}}],["假设模型隐藏维度为",{"5":{"93":2}}],["假设模型需要处理一个包含",{"5":{"93":2}}],["假设模型是一个简单的线性函数",{"5":{"51":2}}],["假设​",{"5":{"91":1}}],["假设存在非零系数",{"5":{"90":1}}],["假设存在非零系数​使得",{"5":{"90":1}}],["假设每个维度携带独立的信息",{"5":{"93":2}}],["假设每个频率成分独立",{"5":{"90":2}}],["假设每次计算需要",{"5":{"92":1}}],["假设每次计算需要步",{"5":{"92":1}}],["假设每层的参数可以独立地近似其hessian块",{"5":{"48":2}}],["假设总参数量为",{"5":{"89":2}}],["假设d为偶数",{"5":{"88":2}}],["假设我们正在处理一个阅读理解任务",{"5":{"94":2}}],["假设我们需要位置",{"5":{"93":1}}],["假设我们需要位置和位置之间的关联强度依赖于某种复杂的非线性函数",{"5":{"93":1}}],["假设我们有一个信息源",{"5":{"95":2}}],["假设我们有一个",{"5":{"92":2}}],["假设我们有一个二维向量",{"5":{"88":2}}],["假设我们只关心某个特定的长程依赖关系",{"5":{"92":2}}],["假设我们使用均方误差作为损失函数",{"5":{"51":2}}],["假设注意力权重与位置距离成反比",{"5":{"87":2}}],["假设依赖关系主要存在于邻近位置之间",{"5":{"86":2}}],["假设样本是独立同分布的",{"5":{"61":2}}],["假设样本之间相互独立",{"5":{"61":2}}],["假设",{"5":{"51":1,"91":1,"94":2,"95":2}}],["假设在给定输入",{"5":{"51":1}}],["假设在给定输入的条件下",{"5":{"51":1}}],["假设个样本的误差约为1",{"5":{"51":1}}],["假设一个",{"5":{"50":1}}],["假设一个的权重矩阵被近似为​",{"5":{"50":1}}],["假设预训练模型的权重更新可以表示为低秩矩阵",{"5":{"50":2}}],["假设给定输入",{"5":{"47":1}}],["假设给定输入时",{"5":{"47":1}}],["假设对于",{"5":{"47":1}}],["假设对于层网络",{"5":{"47":1}}],["假设输入为",{"5":{"47":2}}],["假设数据生成分布为",{"5":{"69":2}}],["假设数据服从一个加性噪声模型",{"5":{"70":2}}],["假设数据服从一个条件概率分布",{"5":{"70":2}}],["细胞体对这些输入信号进行整合处理",{"5":{"47":2}}],["等位置编码的数学性质",{"5":{"97":2}}],["等效的高斯dropout在权重上引入方差为",{"5":{"96":1}}],["等效的高斯dropout在权重上引入方差为的高斯噪声",{"5":{"96":1}}],["等算法中非常重要",{"5":{"96":2}}],["等的表示",{"5":{"92":2}}],["等模式化的注意力策略",{"5":{"92":2}}],["等式不可能对所有",{"5":{"91":1}}],["等式不可能对所有同时成立",{"5":{"91":1}}],["等式右边只依赖于相对位置",{"5":{"88":2}}],["等式右边的第二和第三项是内容与位置的交叉项",{"5":{"88":2}}],["等价但不同",{"5":{"89":2}}],["等价于最大化数据的似然函数",{"5":{"61":2}}],["等价于最小化",{"5":{"61":2}}],["等变的",{"5":{"88":1}}],["等变",{"5":{"88":2}}],["等号成立当且仅当该行是one",{"5":{"87":2}}],["等于矩阵",{"5":{"50":1}}],["等于矩阵的最大特征值的平方根",{"5":{"50":1}}],["等现代深度学习框架均支持广播机制",{"5":{"50":2}}],["等",{"5":{"50":2,"93":2,"94":2}}],["等部分组成",{"5":{"47":2}}],["轴突",{"5":{"47":2}}],["树突负责接收来自其他神经元的信号",{"5":{"47":2}}],["树突",{"5":{"47":2}}],["首次提出了人工神经元的数学模型",{"5":{"47":2}}],["首先在输入空间中进行旋转和反射",{"5":{"94":2}}],["首先重塑为",{"5":{"93":1}}],["首先重塑为的三维张量",{"5":{"93":1}}],["首先是周期性对偶",{"5":{"90":2}}],["首先分析frobenius范数",{"5":{"87":2}}],["首先需要根据查询与各个信息片段的匹配程度计算权重",{"5":{"95":2}}],["首先需要回溯其理论基础",{"5":{"61":2}}],["首先需要判断是偏差问题还是方差问题",{"5":{"51":2}}],["首先计算",{"5":{"48":1,"61":2}}],["首先计算​",{"5":{"48":1}}],["首先计算单个样本的hessian",{"5":{"70":2}}],["首先通过仿射变换",{"5":{"47":1}}],["首先通过仿射变换进行线性投影",{"5":{"47":1}}],["首先",{"5":{"41":6,"42":2,"48":4,"50":10,"61":4,"69":2,"70":2,"71":2,"86":2,"87":2,"88":2,"89":2,"90":2,"91":2,"93":6,"95":2,"96":4}}],["发现",{"5":{"97":2}}],["发现性能更优的激活函数",{"5":{"41":2}}],["发表了开创性的论文",{"5":{"47":2}}],["皮茨",{"5":{"47":2}}],["麦肯罗皮层神经元的表达能力",{"5":{"47":2}}],["麦肯罗皮层神经元实际上是一个线性分类器",{"5":{"47":2}}],["麦肯罗皮层神经元",{"5":{"47":2}}],["麦肯罗皮层神经元模型存在明显的局限性",{"5":{"47":2}}],["麦肯罗皮层神经元模型虽然简单",{"5":{"47":2}}],["麦肯罗皮层神经元模型",{"5":{"47":2}}],["麦肯罗皮层",{"5":{"47":2}}],["齐次坐标被广泛用于处理平移",{"5":{"47":2}}],["齐次坐标表示的优势在于它将仿射变换转化为单一的矩阵乘法运算",{"5":{"47":2}}],["齐次坐标表示",{"5":{"47":2}}],["齐次坐标表示在理论上具有优雅性",{"5":{"46":2}}],["齐次坐标形式的单层网络",{"5":{"46":2}}],["齐次性和三角不等式等性质",{"5":{"50":2}}],["齐次性",{"5":{"71":2}}],["具体选择取决于变分推断的目标",{"5":{"96":2}}],["具体步骤如下",{"5":{"88":2}}],["具体推导略",{"5":{"51":2}}],["具体的",{"5":{"50":2}}],["具体做法是如果梯度的范数超过阈值",{"5":{"50":2}}],["具体来说",{"5":{"50":6,"51":2,"96":4}}],["具体而言",{"5":{"41":2,"42":2,"71":2,"88":7,"93":2,"95":4,"96":4,"99":2,"101":2}}],["具有良好的对应关系",{"5":{"92":2}}],["具有天然的",{"5":{"91":2}}],["具有优雅的数学形式和良好的理论性质",{"5":{"91":2}}],["具有优美的理论性质和高效的算法",{"5":{"48":2}}],["具有与gram矩阵相似的性质",{"5":{"87":1}}],["具有以下几个基本性质",{"5":{"87":1}}],["具有低秩结构的模型",{"5":{"87":2}}],["具有了对数几率",{"5":{"61":2}}],["具有完整特征向量基",{"5":{"50":2}}],["具有旋转不变性",{"5":{"50":2}}],["具有重要的几何意义和计算优势",{"5":{"50":2}}],["具有记忆功能",{"5":{"46":2}}],["具有相同的量纲",{"5":{"96":1}}],["具有相同的形式",{"5":{"70":1}}],["具有相同的函数形式",{"5":{"99":1,"101":1}}],["具有封闭形式的解",{"5":{"70":2}}],["具有若干重要性质",{"5":{"61":2}}],["具有若干与概率分布类比的性质",{"5":{"70":1}}],["就近似服从正态分布",{"5":{"96":2}}],["就像一个旋转的指针",{"5":{"90":2}}],["就是scaled",{"5":{"95":2}}],["就是在保持自注意力并行计算优势的同时",{"5":{"88":2}}],["就是在这个预测空间中寻找最接近目标向量",{"5":{"51":1}}],["就是在这个预测空间中寻找最接近目标向量的点",{"5":{"51":1}}],["就是mse的平方根",{"5":{"51":2}}],["就是典型的逐元素运算",{"5":{"50":1}}],["就是保留前k个最大的奇异值及其对应的奇异向量",{"5":{"50":2}}],["就是",{"5":{"69":1}}],["就可以实现对大型语言模型的有效微调",{"5":{"50":2}}],["就可以实现异或逻辑",{"5":{"46":2}}],["就依赖于这种思想",{"5":{"50":2}}],["就将梯度向量乘以一个缩放因子使其范数等于阈值",{"5":{"50":2}}],["就具有通用逼近能力",{"5":{"71":2}}],["就能同时支持分类损失和注意力机制",{"5":{"69":2}}],["右特征向量",{"5":{"87":1}}],["右特征向量满足",{"5":{"87":1}}],["右上",{"5":{"46":2}}],["右下",{"5":{"46":2}}],["左边可以是任意小的数",{"5":{"91":2}}],["左特征向量",{"5":{"87":1}}],["左特征向量和右特征向量之间满足正交关系",{"5":{"87":2}}],["左特征向量​满足",{"5":{"87":1}}],["左下",{"5":{"46":2}}],["左上",{"5":{"46":2}}],["寄存器",{"5":{"45":1}}],["列元素为",{"5":{"89":2}}],["列空间",{"5":{"50":2}}],["列",{"5":{"47":1,"89":2}}],["列索引",{"5":{"46":1}}],["列索引对应输入特征的索引",{"5":{"46":1}}],["列和",{"5":{"45":1}}],["列仅依赖于",{"5":{"45":1}}],["行对应序列中第",{"5":{"94":1}}],["行第",{"5":{"89":2}}],["行",{"5":{"87":2,"89":1,"92":1,"95":3}}],["行随机性",{"5":{"87":2}}],["行归一化",{"5":{"87":2}}],["行列式为0",{"5":{"87":2}}],["行列式非零",{"5":{"50":2}}],["行的范数满足",{"5":{"87":2}}],["行数和列数",{"5":{"50":2}}],["行和为1",{"5":{"92":2,"93":2}}],["行和等于一",{"5":{"87":2}}],["行和",{"5":{"45":1}}],["行仅依赖于",{"5":{"45":1}}],["行为样本",{"5":{"44":1}}],["行为",{"5":{"42":2,"47":2}}],["次顺序计算",{"5":{"92":1}}],["次矩阵乘法的累积",{"5":{"92":1}}],["次后的结果",{"5":{"90":1}}],["次变换的结果",{"5":{"90":1}}],["次谐波的角频率为",{"5":{"90":1}}],["次谐波",{"5":{"90":1}}],["次直积",{"5":{"88":1}}],["次要",{"5":{"87":2}}],["次",{"5":{"87":2}}],["次标量乘法和加法",{"5":{"50":1}}],["次迭代后",{"5":{"48":1}}],["次加法",{"5":{"45":1}}],["次乘法和",{"5":{"45":2}}],["次非线性变换后到达输出层",{"5":{"45":1}}],["之间存在某种关联",{"5":{"93":2}}],["之间取得平衡",{"5":{"89":2}}],["之间",{"5":{"45":1}}],["之间的关联强度依赖于某种复杂的非线性函数",{"5":{"93":1}}],["之间的关联",{"5":{"92":2}}],["之间的距离为",{"5":{"92":1}}],["之间的相似度只取决于它们的距离",{"5":{"91":1}}],["之间的相似度分数",{"5":{"69":2}}],["之间的余弦相似度",{"5":{"87":1}}],["之间的",{"5":{"61":2}}],["之间的统一性提供了必要的背景知识",{"5":{"61":2}}],["之间的深刻联系",{"5":{"61":2}}],["之间的夹角",{"5":{"48":1}}],["之间的依赖程度",{"5":{"69":1,"71":1}}],["之间的互信息定义为",{"5":{"69":1,"71":1}}],["之间的乘积交互",{"5":{"71":1}}],["之间的数学协同关系",{"5":{"71":2}}],["之间的交叉熵",{"5":{"69":4}}],["之间的差异",{"5":{"48":1,"61":1}}],["之间的差值成正比",{"5":{"69":2}}],["介于",{"5":{"45":1}}],["总的投影参数数量为",{"5":{"94":2}}],["总空间为",{"5":{"93":2}}],["总计算量保持不变",{"5":{"93":2}}],["总计约",{"5":{"45":1}}],["总计约​次flops",{"5":{"45":1}}],["总时间复杂度为",{"5":{"92":2,"95":2}}],["总是从主语指向谓语",{"5":{"92":2}}],["总复杂度为",{"5":{"92":2}}],["总能在至少一个维度上区分任意两个不同的位置",{"5":{"91":2}}],["总参数量为",{"5":{"86":2}}],["总hbm访问量约为",{"5":{"86":4}}],["总体mse是模型在",{"5":{"51":2}}],["总体mse",{"5":{"51":2}}],["总体mse与样本mse",{"2":{"51":1},"5":{"51":1}}],["总flops为各层flops之和",{"5":{"45":2}}],["总",{"5":{"70":2}}],["总结",{"5":{"70":2}}],["浮点运算数",{"5":{"45":2}}],["浮点运算数flops",{"5":{"45":2}}],["拓扑排序保证每个节点在其所有前驱节点之后",{"5":{"45":2}}],["遍历",{"5":{"45":2}}],["例2",{"5":{"45":4}}],["例如神经网络输出的概率分布",{"5":{"61":2}}],["例如将一个",{"5":{"50":1}}],["例如将一个的二维张量重塑为的三维张量",{"5":{"50":1}}],["例如",{"5":{"46":2,"50":20,"51":2,"69":2,"70":8,"71":1,"86":2,"87":4,"88":6,"89":6,"92":8,"93":4,"96":4,"97":2,"99":2,"101":2}}],["例如​",{"5":{"71":1}}],["例如在sigmoid函数的中间区域",{"5":{"41":2}}],["边缘分布仍然是高斯分布",{"5":{"96":2}}],["边的权重为注意力分数",{"5":{"92":2}}],["边表示依赖关系",{"5":{"48":2}}],["边",{"5":{"45":2}}],["边界位置需要",{"5":{"92":2}}],["边界效应在注意力权重的分布上有所体现",{"5":{"92":2}}],["边界效应",{"5":{"92":2}}],["边界是由数据决定的",{"5":{"45":2}}],["边界行为分析",{"2":{"70":1},"5":{"70":1}}],["边界行为由二次函数的性质决定",{"5":{"70":2}}],["边界行为由对数函数的性质决定",{"5":{"70":2}}],["边际分布为",{"5":{"69":2}}],["边际化解释",{"5":{"70":2}}],["边际化",{"5":{"70":2}}],["靠近输出的层",{"5":{"92":2}}],["靠近输出",{"5":{"45":2,"87":2,"92":2}}],["靠近输入的层",{"5":{"92":2}}],["靠近输入",{"5":{"45":2,"87":2,"92":2}}],["纹理",{"5":{"45":2}}],["低",{"5":{"97":1}}],["低层使用细粒度编码",{"5":{"89":2}}],["低维度区域呈现高频波动",{"5":{"91":2}}],["低维度捕获位置的粗粒度变化",{"5":{"91":2}}],["低维度对位置变化敏感",{"5":{"90":2}}],["低维度",{"5":{"88":2,"90":2,"91":2}}],["低秩参数化",{"5":{"89":2}}],["低秩矩阵的运算更快",{"5":{"89":2}}],["低秩矩阵可以用少数几个外积的和来近似表示",{"5":{"87":2}}],["低秩分解",{"5":{"87":2}}],["低秩分解与计算优化",{"2":{"87":1},"5":{"87":1}}],["低秩",{"5":{"87":2,"89":2}}],["低秩结构为注意力计算的理论加速提供了依据",{"5":{"87":2}}],["低秩结构对模型的泛化能力有积极影响",{"5":{"87":2}}],["低秩结构对应于第二项的最小化",{"5":{"87":2}}],["低秩结构是注意力矩阵最重要的数学特性之一",{"5":{"87":2}}],["低秩结构与信息瓶颈",{"2":{"87":1},"5":{"87":1}}],["低秩结构的定义与直观解释",{"2":{"87":1},"5":{"87":1}}],["低秩更新是一种核心技术",{"5":{"50":2}}],["低秩近似",{"5":{"93":2}}],["低秩近似等性质",{"5":{"89":2}}],["低秩近似可以去除这些噪声",{"5":{"89":2}}],["低秩近似的应用包括",{"5":{"89":2}}],["低秩近似技术有着广泛的应用",{"5":{"50":2}}],["低秩近似是奇异值分解最重要的应用之一",{"5":{"50":2}}],["低级特征",{"5":{"45":2}}],["低频端可分辨的最小频率间隔为",{"5":{"90":2}}],["低频成分的相位差仍然较小",{"5":{"91":2}}],["低频成分的间隔较大",{"5":{"90":2}}],["低频成分捕获长距离的位置关系",{"5":{"90":2}}],["低频成分捕获位置的整体结构和长距离关系",{"5":{"90":2}}],["低频成分覆盖长距离的位置关系",{"5":{"90":2}}],["低频成分",{"5":{"90":6,"91":2}}],["低频成分描述函数的整体趋势",{"5":{"90":2}}],["低频成分使用正弦编码",{"5":{"89":2}}],["低频成分对应长程位置依赖",{"5":{"88":2}}],["低频成分编码远程依赖",{"5":{"88":2}}],["低频成分编码粗粒度的位置关系",{"5":{"71":2}}],["低频",{"5":{"71":2}}],["低温度会给予这个负样本更高的概率",{"5":{"69":2}}],["低温度使得模型对正负样本的区别更加敏感",{"5":{"69":2}}],["主题模型的推断和某些贝叶斯神经网络方法",{"5":{"96":2}}],["主导项是的二次复杂度",{"5":{"95":1}}],["主导项是",{"5":{"93":4,"95":1}}],["主语和谓语通常相距不太远",{"5":{"92":2}}],["主语",{"5":{"92":2}}],["主流实现通常将d设为偶数",{"5":{"88":2}}],["主成分分析和谱聚类中都有应用",{"5":{"50":2}}],["主轴方向",{"5":{"45":2}}],["主要捕获句子级别和篇章级别的特征",{"5":{"92":2}}],["主要捕获词汇级别和短语级别的特征",{"5":{"92":2}}],["主要",{"5":{"87":2}}],["主要区别在于归一化和非线性",{"5":{"87":2}}],["主要操作",{"5":{"44":1}}],["主要使用语言建模损失",{"5":{"99":2,"101":2}}],["先局部",{"5":{"92":2}}],["先验知识",{"5":{"91":2}}],["先预训练正弦编码",{"5":{"89":2}}],["先学习简单样本再学习困难样本",{"5":{"51":2}}],["先通过pca进行初步降维",{"5":{"50":2}}],["先通过前向传播保存必要的中间值",{"5":{"44":2}}],["先将输入向量旋转到",{"5":{"45":2}}],["执行另一个旋转",{"5":{"45":2}}],["执行缩放",{"5":{"45":2}}],["执行旋转",{"5":{"45":2}}],["执行完整的前向传播",{"5":{"44":2}}],["仿射变换进行线性特征组合",{"5":{"47":2}}],["仿射变换本身是线性变换",{"5":{"47":1}}],["仿射变换本质上是对输入的线性投影",{"5":{"45":1}}],["仿射变换承担着特征提取和信息整合的功能",{"5":{"47":1}}],["仿射变换将其映射为另一个超平面",{"5":{"47":2}}],["仿射变换在几何上等价于先进行线性变换",{"5":{"47":1}}],["仿射变换在几何上等价于线性变换",{"5":{"45":1}}],["仿射变换退化为线性变换",{"5":{"47":2}}],["仿射变换是线性变换的推广",{"5":{"47":2}}],["仿射变换",{"5":{"45":2,"47":5}}],["仿射变换负责旋转",{"5":{"45":2}}],["仿射变换的几何意义",{"5":{"45":2,"47":2}}],["空间变换与特征提取",{"2":{"94":1},"5":{"94":1}}],["空间变换等多个数学视角",{"5":{"45":2}}],["空间需求",{"5":{"93":2}}],["空间需求会增加",{"5":{"93":2}}],["空间为",{"5":{"93":2}}],["空间关系",{"5":{"93":2}}],["空间",{"5":{"92":2}}],["空间按",{"5":{"45":1}}],["空间复杂度方面",{"5":{"95":2}}],["空间复杂度与单头注意力相当",{"5":{"93":2}}],["空间复杂度为",{"5":{"86":4}}],["空间复杂度同样降低到",{"5":{"86":2}}],["空间复杂度",{"5":{"44":1,"45":1}}],["空间中的向量映射到概率单纯形",{"5":{"96":1}}],["空间中的一个点",{"5":{"51":1}}],["空间中的一个子流形",{"5":{"51":1}}],["空间中的一条直线",{"5":{"51":1}}],["空间中的每个点代表一个数据样本在该空间中的表示",{"5":{"50":2}}],["空间中",{"5":{"70":1}}],["空间中由坐标轴截距为",{"5":{"70":1}}],["链式法则给出了多个随机变量的联合熵分解",{"5":{"96":2}}],["链式法则的应用更加直观和系统",{"5":{"48":2}}],["链式法则的应用更加系统化",{"5":{"48":2}}],["链式法则的基本形式可以这样表述",{"5":{"48":2}}],["链式法则的矩阵形式",{"2":{"44":1},"5":{"44":1}}],["链式法则使得我们能够计算损失函数相对于任意深层参数的梯度",{"5":{"48":2}}],["链式法则将在下文详细讨论",{"5":{"48":2}}],["链式法则是分析这一复合过程的核心数学工具",{"5":{"45":2}}],["链式法则在反向传播算法中扮演着核心角色",{"5":{"45":2}}],["链式法则",{"5":{"45":4,"48":4}}],["适当标准化后",{"5":{"96":2}}],["适当的权重初始化",{"5":{"44":1}}],["适合区分较远的位置",{"5":{"90":2}}],["适合区分很近的位置",{"5":{"90":2}}],["适合捕获整体位置信息",{"5":{"90":2}}],["适应任务的局部位置模式",{"5":{"89":2}}],["适用场景",{"5":{"44":1,"45":1}}],["适用于transformer",{"5":{"41":1}}],["适用于rnn",{"5":{"41":1}}],["存储位置编码",{"5":{"89":2}}],["存储",{"5":{"89":2}}],["存储优化方面",{"5":{"89":2}}],["存储在sram",{"5":{"86":1}}],["存储输出",{"5":{"86":2}}],["存储和加载输出矩阵",{"5":{"86":2}}],["存储和加载注意力矩阵",{"5":{"86":2}}],["存储中间值",{"5":{"44":1,"45":1}}],["存在常数",{"5":{"92":2}}],["存在直接边",{"5":{"92":2}}],["存在潜在的语义关联",{"5":{"92":2}}],["存在某个维度",{"5":{"91":1}}],["存在某个维度使得相位差​不等于",{"5":{"91":1}}],["存在权衡关系",{"5":{"90":2}}],["存在结构性冲突",{"5":{"88":2}}],["存在一个更为微妙的问题",{"5":{"61":2}}],["存在一个单隐藏层神经网络",{"5":{"71":2}}],["存在多个局部最优解",{"5":{"51":2,"89":2}}],["存在正交矩阵",{"5":{"50":2}}],["存在大量的局部最小值",{"5":{"48":2}}],["存在",{"5":{"45":1}}],["存在介于和之间",{"5":{"45":1}}],["额外",{"5":{"44":1,"45":1}}],["掌握本节的数学基础后",{"5":{"44":2,"47":2}}],["再通过重参数化技巧进行采样",{"5":{"96":2}}],["再通过输入嵌入矩阵",{"5":{"94":1}}],["再通过输入嵌入矩阵累积到​",{"5":{"94":1}}],["再经过第二层注意力得到",{"5":{"94":2}}],["再微调可学习残差",{"5":{"89":2}}],["再到对齐阶段的rlhf和dpo损失",{"5":{"101":2}}],["再到位置差2时的约2",{"5":{"91":2}}],["再到更长",{"5":{"88":2}}],["再到实际应用中的性质和注意事项",{"5":{"51":2}}],["再次产生",{"5":{"61":2}}],["再用非线性方法进行二维或三维嵌入",{"5":{"50":2}}],["再进行平移变换",{"5":{"47":2}}],["再加上参数更新的少量计算",{"5":{"44":2}}],["一维高斯分布的概率密度函数为",{"5":{"96":2}}],["一是梯度可以流回所有位置",{"5":{"92":2}}],["一步到位",{"5":{"92":2}}],["一种缓解边界效应的方法是使用循环位置编码",{"5":{"92":2}}],["一种常用的可视化方法是将编码向量视为时间序列",{"5":{"91":2}}],["一种常用的压缩技术是权重分解",{"5":{"87":2}}],["一种常见的设计是将正弦编码和可学习编码相加",{"5":{"89":2}}],["一种常见的优化方法是",{"5":{"88":1}}],["一种常见的优化方法是逐位置计算角度的正弦和余弦",{"5":{"88":1}}],["一种简单但有效的自适应策略是基于各任务梯度的范数调整权重",{"5":{"99":2,"101":2}}],["一般来说",{"5":{"61":2}}],["一般情况下​",{"5":{"87":1}}],["一般情况下",{"5":{"87":1,"96":2}}],["一组向量如果两两正交且都是单位向量",{"5":{"50":2}}],["一阶原点矩是期望",{"5":{"96":2}}],["一阶张量",{"5":{"50":2}}],["一阶张量是向量",{"5":{"50":2}}],["一阶矩估计",{"5":{"48":2}}],["一层中的",{"5":{"46":1}}],["一层中的个神经元实际上定义了个超平面",{"5":{"46":1}}],["一次性加载",{"5":{"92":2}}],["一次",{"5":{"87":2}}],["一次乘法或一次加法计为一次flops",{"5":{"45":2}}],["一次前向传播加上一次反向传播",{"5":{"44":2}}],["一个凸损失函数具有这样的性质",{"5":{"97":2}}],["一个凸函数具有这样的性质",{"5":{"70":2}}],["一个很大的模型可能在某些指标上有统计显著的改进",{"5":{"96":2}}],["一个词是否被mask掉",{"5":{"96":2}}],["一个常见的初始化方案是xavier初始化",{"5":{"94":2}}],["一个简单的例子是",{"5":{"93":2}}],["一个全知的函数",{"5":{"92":2}}],["一个关键问题是",{"5":{"92":2}}],["一个关键的优化技巧是",{"5":{"61":1}}],["一个关键的优化技巧是融合",{"5":{"61":1}}],["一个函数",{"5":{"51":1}}],["一个函数是凸函数",{"5":{"51":1}}],["一个",{"5":{"50":1,"70":1,"97":1}}],["一个张量网络由若干个节点",{"5":{"50":2}}],["一个线性变换",{"5":{"50":1}}],["一个线性变换可以用一个矩阵",{"5":{"50":1}}],["一个线性",{"5":{"50":2}}],["一个k维子空间是特征空间的一个k维线性子集",{"5":{"50":2}}],["一个n维特征空间可以理解为一个n维欧几里得空间",{"5":{"50":2}}],["一个阶张量有个索引",{"5":{"50":1}}],["一个300维的词向量可以看作是一个从自然语言到连续向量空间的映射",{"5":{"50":2}}],["一个仿射变换",{"5":{"45":1}}],["一个仿射变换可以理解为",{"5":{"45":1}}],["一个映射",{"5":{"71":1}}],["一个映射是线性的",{"5":{"71":1}}],["一个强凸函数满足",{"5":{"70":1,"97":1}}],["一个核心发现是",{"5":{"70":2}}],["一个解",{"5":{"99":2,"101":2}}],["若服从多元高斯分布",{"5":{"96":1}}],["若和独立",{"5":{"96":1}}],["若干次",{"5":{"86":4}}],["若两个张量的维度数不同",{"5":{"50":2}}],["若​",{"5":{"47":1}}],["若",{"5":{"44":6,"47":1,"96":6}}],["若目标函数具有",{"5":{"71":1}}],["若目标函数具有组合",{"5":{"71":1}}],["若真实标签",{"5":{"70":2}}],["根据历史梯度的平方和调整学习率",{"5":{"97":2}}],["根据中心极限定理",{"5":{"96":2}}],["根据期望的线性性质",{"5":{"96":2}}],["根据取值方式的不同",{"5":{"96":2}}],["根据3σ原则",{"5":{"95":2}}],["根据独立随机变量之和的方差公式",{"5":{"95":2}}],["根据注意力权重对值进行加权求和",{"5":{"95":2}}],["根据标准配置",{"5":{"93":2}}],["根据公式",{"5":{"91":2}}],["根据5",{"5":{"89":2}}],["根据旋转矩阵的性质",{"5":{"88":2}}],["根据矩阵范数的相容性",{"5":{"87":2}}],["根据矩阵微积分的链式法则",{"5":{"45":2}}],["根据cauchy",{"5":{"87":2}}],["根据硬件特性自适应调整",{"5":{"86":2}}],["根据统计学习理论",{"5":{"61":2}}],["根据正交性",{"5":{"51":2}}],["根据eckart",{"5":{"50":2}}],["根据大数定律",{"5":{"51":2,"96":4}}],["根据链式法则",{"5":{"45":1,"89":2,"92":2,"94":2}}],["根据链式法则​",{"5":{"45":1}}],["根据层映射的定义",{"5":{"45":2}}],["根据定理2",{"5":{"44":2}}],["根据误差信号的定义和链式法则",{"5":{"44":2}}],["根据信息论中的性质",{"5":{"70":2,"97":2}}],["根据任务重要性赋值",{"5":{"70":2}}],["根据概率论的知识",{"5":{"95":2}}],["根据概率论的链式法则",{"5":{"99":2,"101":2}}],["根据4",{"5":{"99":2,"101":2}}],["根据训练动态自动调整各损失项的权重",{"5":{"99":2,"101":2}}],["常数项",{"5":{"51":1}}],["常数项不影响优化结果",{"5":{"51":1}}],["常数因子的偏导数等于该因子乘以函数的偏导数",{"5":{"48":2}}],["常数的偏导数为零",{"5":{"48":2}}],["常数矩阵",{"5":{"70":1}}],["常用于多模态模型",{"5":{"86":2}}],["常用于矩阵补全",{"5":{"50":2}}],["常用的效应量包括cohen",{"5":{"96":2}}],["常用的方法是通过张量重塑",{"5":{"93":2}}],["常用的选择是",{"5":{"89":2}}],["常用的选择是或",{"5":{"89":1}}],["常用的选择是或​​",{"5":{"89":1}}],["常用的正则化方法包括l2正则化",{"5":{"89":2}}],["常用的初始化方法包括均匀初始化",{"5":{"89":2}}],["常用的矩阵范数包括frobenius范数",{"5":{"50":2}}],["常用的激活函数如sigmoid和tanh被选择是因为它们具有良好的数学性质",{"5":{"71":2}}],["常用的策略包括",{"5":{"70":2}}],["常用激活函数",{"5":{"47":2}}],["常见的学习率调度策略包括",{"5":{"97":2}}],["常见的学习率调度方法包括",{"5":{"48":2}}],["常见的策略包括",{"5":{"87":2}}],["常见的张量分解方法包括",{"5":{"50":2}}],["常见的数值稳定性问题与解决方案",{"5":{"44":1}}],["常见的数值稳定性问题与解决方案本节系统阐述了反向传播算法的数学理论基础",{"5":{"44":1}}],["常见的裁剪方式包括",{"5":{"44":2}}],["常见的任务包括",{"5":{"99":2,"101":2}}],["常见输出层误差信号",{"5":{"44":2}}],["称为马氏距离",{"5":{"96":2}}],["称为精度矩阵",{"5":{"96":2}}],["称为第",{"5":{"90":1}}],["称为特殊正交群",{"5":{"88":2}}],["称为欠拟合",{"5":{"50":2}}],["称为净输入",{"5":{"47":2}}],["称为奇异值",{"5":{"45":2,"50":2}}],["称为",{"5":{"44":2,"96":1}}],["称为logits",{"5":{"96":1}}],["称为logit",{"5":{"71":2}}],["故也是的线性函数",{"5":{"45":1}}],["故的维度为​",{"5":{"44":1}}],["故",{"5":{"44":7,"45":5,"47":2}}],["标识",{"5":{"95":4}}],["标志着位置编码研究的成熟",{"5":{"88":2}}],["标志着计算神经科学和人工智能的诞生",{"5":{"47":2}}],["标记",{"5":{"88":2}}],["标准梯度下降使用全局统一的学习率",{"5":{"97":2}}],["标准梯度下降在逃离鞍点时可能很慢",{"5":{"97":2}}],["标准梯度下降以",{"5":{"48":1}}],["标准梯度下降以的速度收敛",{"5":{"48":1}}],["标准化均值差异",{"5":{"96":2}}],["标准vae假设潜在变量服从标准高斯分布",{"5":{"96":2}}],["标准高斯分布",{"5":{"96":2}}],["标准差将是",{"5":{"95":1}}],["标准差将是时的2倍",{"5":{"95":1}}],["标准差也为1",{"5":{"95":2}}],["标准差为",{"5":{"95":4}}],["标准rope的频率参数为",{"5":{"88":2}}],["标准rope的一个潜在问题是",{"5":{"88":2}}],["标准实现需要将数据从hbm",{"5":{"86":2}}],["标准注意力计算需要将完整的注意力矩阵",{"5":{"86":1}}],["标准注意力计算需要将完整的注意力矩阵存储在sram",{"5":{"86":1}}],["标准注意力的hbm访问量约为",{"5":{"86":2}}],["标准注意力的内存访问包括",{"5":{"86":2}}],["标准注意力的递归更新需要维护完整的注意力矩阵",{"5":{"86":2}}],["标准注意力的计算瓶颈在于softmax操作",{"5":{"86":2}}],["标准注意力机制的一些设计选择",{"5":{"86":2}}],["标准注意力机制存在一个根本性的计算瓶颈",{"5":{"86":2}}],["标准优化算法的内存开销巨大",{"5":{"48":2}}],["标准nce存在几个局限性",{"5":{"69":2}}],["标准",{"5":{"70":2}}],["标准的多头注意力在所有头上都进行完整的计算",{"5":{"93":2}}],["标准的bert使用随机均匀掩码",{"5":{"99":2,"101":2}}],["标签平滑相当于在训练时使用真实分布与均匀分布的混合作为目标分布",{"5":{"96":2}}],["标签平滑可以防止模型对训练数据过度自信",{"5":{"96":2}}],["标签平滑",{"5":{"96":2}}],["标签应该使用类别索引",{"5":{"61":2}}],["标签",{"5":{"47":1}}],["标签服从伯努利分布",{"5":{"47":1}}],["标签为1",{"5":{"99":2,"101":2}}],["标量乘法是将一个向量与一个标量",{"5":{"50":2}}],["标量乘法",{"5":{"50":2}}],["标量",{"5":{"45":2}}],["标量损失函数的梯度",{"5":{"44":1}}],["标量对矩阵的链式法则",{"5":{"44":2}}],["标量链式法则",{"5":{"44":2}}],["批处理相当于将多个独立的矩阵运算堆叠成一个大的张量运算",{"5":{"50":2}}],["批处理的数学优势来自于矩阵运算的并行性",{"5":{"50":2}}],["批处理是在现有维度之外增加一个新的批处理维度",{"5":{"50":2}}],["批处理",{"5":{"50":2,"51":2}}],["批处理与并行计算的张量视图",{"2":{"50":1},"5":{"50":1}}],["批量归一化对每一层的输入进行标准化",{"5":{"96":2}}],["批量归一化",{"5":{"96":2}}],["批量计算",{"5":{"92":2}}],["批量处理还提供了隐式的正则化效果",{"5":{"47":2}}],["批量处理",{"5":{"47":2}}],["批量处理的梯度计算",{"2":{"44":1},"5":{"44":1}}],["批量标签",{"5":{"44":1}}],["批量标签​",{"5":{"44":1}}],["批量偏置梯度",{"5":{"44":2}}],["批量情况下",{"5":{"44":4}}],["批量权重梯度",{"5":{"44":2}}],["批量损失对净输入的梯度为",{"5":{"44":2}}],["批量损失通常定义为各样本损失的平均值",{"5":{"44":2}}],["批量误差信号",{"5":{"44":2}}],["批量前向传播",{"5":{"44":2}}],["批归一化在这些场景中会遇到统计量估计不稳定的问题",{"5":{"41":2}}],["批归一化在卷积网络中的应用",{"5":{"41":2}}],["批归一化对每个通道独立计算统计量",{"5":{"41":2}}],["批归一化通常应用于通道维度",{"5":{"41":2}}],["批归一化将激活值的分布规范化到均值为0",{"5":{"41":2}}],["批归一化定义为",{"5":{"41":2}}],["批归一化的反向传播需要计算四个梯度",{"5":{"41":2}}],["批归一化的梯度计算",{"5":{"41":2}}],["批归一化的核心思想是规范化每层的输入分布",{"5":{"41":2}}],["批归一化的数学原理",{"2":{"41":1},"5":{"41":1}}],["批归一化",{"5":{"41":7,"44":1}}],["引入可学习的门控机制来调整softmax的行为",{"5":{"101":2}}],["引入缩放因子后",{"5":{"94":2}}],["引入复指数函数",{"5":{"90":2}}],["引入门控机制来调节信息流动",{"5":{"86":2}}],["引入了一个额外的非线性变换层",{"5":{"93":2}}],["引入了一定的噪声",{"5":{"41":2}}],["引入了可学习的参数和连续可微的激活函数",{"5":{"47":2}}],["引入了",{"5":{"69":2}}],["引言",{"2":{"44":1,"45":1,"46":1},"5":{"44":1,"45":1,"46":1}}],["权重衰减和标签平滑的作用机制",{"5":{"97":2}}],["权重初始化的尺度选择",{"5":{"96":2}}],["权重初始化的一个核心目标是确保各层的前向激活和反向梯度都具有稳定的方差",{"5":{"41":2}}],["权重依赖于输入数据",{"5":{"93":2}}],["权重会集中到这些位置上",{"5":{"92":2}}],["权重由注意力分数隐式决定",{"5":{"92":2}}],["权重序列",{"5":{"90":1}}],["权重序列​与正弦序列和余弦序列正交",{"5":{"90":1}}],["权重分布更均匀",{"5":{"87":2}}],["权重和阈值需要人工设定",{"5":{"47":2}}],["权重矩阵与",{"5":{"50":1}}],["权重矩阵的低秩近似可以显著减少参数量和计算量",{"5":{"50":2}}],["权重矩阵的列空间决定了该层能够表示的特征空间的范围",{"5":{"50":2}}],["权重矩阵的元素​表示第个输入特征与第个输出神经元之间的连接强度",{"5":{"46":1}}],["权重矩阵的元素含义",{"5":{"46":2}}],["权重矩阵无处不在",{"5":{"50":2}}],["权重矩阵决定了不同特征之间的线性组合方式",{"5":{"47":1}}],["权重矩阵",{"5":{"46":1,"47":1}}],["权重",{"5":{"45":2,"46":6}}],["权重节点",{"5":{"45":2}}],["权重梯度为",{"5":{"44":2}}],["权重梯度",{"5":{"44":2}}],["权重的乘积",{"5":{"71":1}}],["权重的选择是一个开放问题",{"5":{"70":2}}],["权重较高的任务主导学习过程",{"5":{"99":2,"101":2}}],["权重较低的任务可能被",{"5":{"99":2,"101":2}}],["权重较小",{"5":{"99":2,"101":2}}],["权重较大",{"5":{"99":2,"101":2}}],["描述分布的尾部厚度",{"5":{"96":2}}],["描述分布的对称性",{"5":{"96":2}}],["描述词嵌入向量的分布特性",{"5":{"96":2}}],["描述了随机变量的",{"5":{"96":2}}],["描述了数据点之间的相似性结构",{"5":{"87":2}}],["描述独立事件发生的时间间隔",{"5":{"96":2}}],["描述单次二元试验的成功概率",{"5":{"96":2}}],["描述的是在数据分布层面上的期望损失",{"5":{"51":2}}],["描述",{"5":{"45":2,"71":2,"96":2}}],["该指标是总体性能的一个估计",{"5":{"96":2}}],["该批量内样本的均值和方差是整体数据均值和方差的良好估计",{"5":{"96":2}}],["该位置的交叉熵损失为",{"5":{"96":2}}],["该向量的各维度由不同频率的正弦和余弦函数值组成",{"5":{"91":2}}],["该先验倾向于低频的位置变化",{"5":{"90":2}}],["该定理表明",{"5":{"90":2}}],["该点在圆周上旋转的角度为",{"5":{"90":2}}],["该表达式可以直接从定义",{"5":{"61":2}}],["该表示保留了位置信息",{"5":{"71":2}}],["该层定义的映射",{"5":{"45":1}}],["该层定义的映射​为",{"5":{"45":1}}],["该隐藏层有",{"5":{"71":1}}],["该隐藏层有个神经元",{"5":{"71":1}}],["该分布度量查询与每个候选的匹配程度",{"5":{"69":2}}],["该分布度量当前位置对每个位置的",{"5":{"69":1}}],["该分布度量当前位置",{"5":{"69":1}}],["缩放后点积的方差保持为常数",{"5":{"95":2}}],["缩放对梯度稳定性的影响",{"2":{"94":1},"5":{"94":1}}],["缩放频率参数等价于对位置信号进行了某种",{"5":{"88":2}}],["缩放的倍数就是特征值",{"5":{"50":2}}],["缩放改变向量的模长",{"5":{"50":2}}],["缩放",{"5":{"50":2,"88":2}}],["缩放等仿射变换",{"5":{"47":2}}],["缩放和平移",{"5":{"45":2}}],["缩放和剪切等基本变换的组合",{"5":{"45":2}}],["缩放和反射等基本操作的组合",{"5":{"94":2}}],["缩放和反射",{"5":{"71":2}}],["缩放点积注意力的数学定义",{"2":{"95":1},"5":{"95":1}}],["缩放点积注意力",{"5":{"71":2,"95":2}}],["缩放因子​​可以被理解为一个温度参数",{"5":{"95":1}}],["缩放因子​​的设计正是为了抵消维度增长对点积量级的影响",{"5":{"95":1}}],["缩放因子​​的引入部分解决了这个问题",{"5":{"87":1}}],["缩放因子​​的引入部分原因就是为了控制注意力矩阵的条件数",{"5":{"87":1}}],["缩放因子的引入极大地改善了梯度流的质量",{"5":{"95":2}}],["缩放因子的引入还可以被视为一种",{"5":{"95":2}}],["缩放因子的数学推导与作用",{"2":{"95":1},"5":{"95":1}}],["缩放因子的统计学原理",{"2":{"95":1},"5":{"95":1}}],["缩放因子的这一作用与批归一化",{"5":{"94":2}}],["缩放因子防止softmax进入极端区域",{"5":{"87":2}}],["缩放因子有效地",{"5":{"69":1}}],["缩放因子",{"5":{"69":1,"70":1,"87":2,"95":2}}],["缩放因子实际上就是一种温度调节",{"5":{"70":1}}],["陡峭",{"5":{"48":2,"71":2}}],["陡峭程度",{"5":{"70":2}}],["高",{"5":{"97":1}}],["高效的矩阵并行计算",{"5":{"95":2}}],["高于rnn的",{"5":{"92":1}}],["高层在局部特征的基础上进行全局整合",{"5":{"92":2}}],["高层使用粗粒度编码",{"5":{"89":2}}],["高速公路",{"5":{"92":2}}],["高速缓存",{"5":{"86":2}}],["高度表达的位置编码可能对训练位置过拟合",{"5":{"89":2}}],["高维度区域呈现低频波动",{"5":{"91":2}}],["高维度捕获位置的精细变化",{"5":{"91":2}}],["高维度能够编码位置的细粒度信息",{"5":{"91":2}}],["高维度使用高频的设计能够提供多尺度的位置表示",{"5":{"90":2}}],["高维度对位置变化不敏感",{"5":{"90":2}}],["高维度",{"5":{"88":2,"90":2,"91":2}}],["高维情况下为抛物柱面",{"5":{"70":2}}],["高带宽内存",{"5":{"86":2}}],["高斯过程的一个优势是它提供了预测的不确定性估计",{"5":{"96":2}}],["高斯过程回归在超参数优化",{"5":{"96":2}}],["高斯过程是函数的先验分布",{"5":{"96":2}}],["高斯输出分布用于回归任务",{"5":{"96":2}}],["高斯分布通常作为潜在空间的先验分布",{"5":{"96":2}}],["高斯分布可用于建模响应的不确定性",{"5":{"96":2}}],["高斯分布扮演着多种重要角色",{"5":{"96":2}}],["高斯分布具有最大的熵",{"5":{"96":2}}],["高斯分布的共轭先验还是高斯分布",{"5":{"96":2}}],["高斯分布的熵",{"5":{"96":2}}],["高斯分布的偏度为0",{"5":{"96":2}}],["高斯分布的众数",{"5":{"96":2}}],["高斯分布的期望为",{"5":{"96":2}}],["高斯分布在卷积",{"5":{"96":2}}],["高斯分布之所以如此重要",{"5":{"96":2}}],["高斯分布",{"5":{"96":2}}],["高斯分布与多元高斯",{"2":{"96":1},"5":{"96":1}}],["高斯假设下的mle问题简化为最小化平方误差之和",{"5":{"51":2}}],["高斯噪声假设与最大似然估计",{"2":{"51":1},"5":{"51":1}}],["高斯",{"5":{"48":6}}],["高级语义特征",{"5":{"45":2}}],["高频端可表示的最高频率受奈奎斯特采样定理限制",{"5":{"90":2}}],["高频成分捕获短距离的位置变化",{"5":{"90":2}}],["高频成分捕获位置的精细变化和短距离差异",{"5":{"90":2}}],["高频成分不携带信息",{"5":{"90":2}}],["高频成分的间隔较小",{"5":{"90":2}}],["高频成分覆盖短距离的位置关系",{"5":{"90":2}}],["高频成分描述函数的细节变化",{"5":{"90":2}}],["高频成分使用可学习编码",{"5":{"89":2}}],["高频成分",{"5":{"88":2,"90":6,"91":2}}],["高频成分对应短程位置依赖",{"5":{"88":2}}],["高频成分编码局部细节",{"5":{"88":2}}],["高频成分编码细粒度的位置关系",{"5":{"71":2}}],["高频",{"5":{"71":2}}],["高阶导数的复杂结构反映了sigmoid非线性变换的深层特性",{"5":{"42":2}}],["高阶导数与泰勒展开",{"2":{"42":1},"5":{"42":1}}],["操作对输入尺度敏感",{"5":{"97":2}}],["操作是一种典型的长程依赖",{"5":{"92":2}}],["操作与第五章注意力机制中的",{"5":{"61":2}}],["操作来将相似度分数转换为概率分布",{"5":{"61":2}}],["操作",{"5":{"45":2,"69":2,"71":2,"95":2}}],["操作贯穿了整个",{"5":{"70":2}}],["操作的数学等价性",{"5":{"70":2}}],["操作作为统一的主题贯穿两者",{"5":{"70":2}}],["操作在两种场景下的统一性",{"5":{"70":2}}],["两层网络的异或解",{"5":{"46":2}}],["两者共同决定了大语言模型的性能和能力边界",{"5":{"101":2}}],["两者之间也存在重要的区别",{"5":{"93":2}}],["两者通过逐元素相加",{"5":{"91":2}}],["两者成对出现",{"5":{"91":2}}],["两者相加得到最终表示",{"5":{"88":2}}],["两者相乘得到",{"5":{"70":2}}],["两者完全一致",{"5":{"88":2}}],["两者本质上都是在将一个未归一化的得分向量转换为有效的概率分布",{"5":{"61":2}}],["两者在优化意义上是等价的",{"5":{"61":2}}],["两者的输出再通过某种方式组合",{"5":{"46":2}}],["两者的组合使得网络能够学习复杂的非线性决策边界",{"5":{"45":2}}],["两者都能得到有效的更新",{"5":{"91":2}}],["两者都可以视为某种相似度矩阵",{"5":{"87":2}}],["两者都描述了数据点之间的成对关系",{"5":{"87":2}}],["两者都涉及概率加权的信号传递",{"5":{"71":2}}],["两者都存在梯度饱和问题",{"5":{"42":2}}],["两者都是对称矩阵",{"5":{"87":2}}],["两者都是函数值与其",{"5":{"42":2}}],["两者都是",{"5":{"99":2,"101":2}}],["两种编码各有优劣",{"5":{"89":2}}],["两种",{"5":{"45":2}}],["两种最常用的损失函数",{"5":{"70":2}}],["两种损失函数的梯度结构反映了它们在优化动态上的本质差异",{"5":{"70":2}}],["两种应用在数学形式上完全相同",{"5":{"70":2}}],["两种应用可以共享许多数学分析工具",{"5":{"70":2}}],["两个模型没有差异",{"5":{"96":2}}],["两个独立同分布随机变量乘积的期望和方差可以计算如下",{"5":{"95":2}}],["两个位置的相位差为",{"5":{"90":1}}],["两个位置的相位差为​",{"5":{"90":1}}],["两个位置的编码之间的关系可以通过相对旋转来描述",{"5":{"90":2}}],["两个旋转的复合仍是一个旋转",{"5":{"88":2}}],["两个向量",{"5":{"50":1}}],["两个向量和如果满足",{"5":{"50":1}}],["两个三阶张量",{"5":{"50":1}}],["两个三阶张量和在第二和第三维度上的收缩产生一个的矩阵",{"5":{"50":1}}],["两个矩阵",{"5":{"50":1}}],["两个矩阵和的乘积",{"5":{"50":1}}],["两个二阶张量",{"5":{"50":2}}],["两个维度相同的向量可以逐分量相加",{"5":{"50":2}}],["两个神经元的组合可以解决异或问题",{"5":{"46":2}}],["两个随机变量",{"5":{"69":1,"71":1}}],["两个随机变量和之间的互信息定义为",{"5":{"69":1,"71":1}}],["两个概率分布之间的",{"5":{"70":2,"71":2}}],["到监督微调阶段的多任务损失",{"5":{"101":2}}],["到查询空间的新坐标系中",{"5":{"94":2}}],["到时刻",{"5":{"92":1}}],["到内容嵌入中",{"5":{"91":2}}],["到2021年前后rope",{"5":{"88":2}}],["到位置差1时的约3",{"5":{"91":2}}],["到位置",{"5":{"87":1,"92":4}}],["到低维空间",{"5":{"87":2}}],["到真实分布所在的低维流形上",{"5":{"61":2}}],["到几何解释",{"5":{"51":2}}],["到对应的输入维度",{"5":{"44":2}}],["到",{"5":{"46":1,"47":1,"50":1,"69":2,"70":4,"71":2,"87":1,"88":6,"92":4,"95":2,"99":4,"101":4}}],["到概率单纯形的投影",{"5":{"69":2}}],["到连续输出",{"5":{"70":1}}],["到离散类别标签",{"5":{"70":1}}],["到损失函数结构的系统性对比",{"5":{"99":2,"101":2}}],["到是生成目标",{"5":{"99":1,"101":1}}],["完整地描述了多头注意力的计算过程",{"5":{"93":2}}],["完整的位置角度矩阵为",{"5":{"88":2}}],["完成了从输入空间到输出空间的非线性变换",{"5":{"47":2}}],["完成了从线性预测到概率预测的关键一步",{"5":{"71":2}}],["完全对称",{"5":{"96":2}}],["完全相关",{"5":{"92":2}}],["完全相同",{"5":{"46":1,"93":2}}],["完全忽略绝对位置会限制模型对这类信息的捕捉能力",{"5":{"88":2}}],["完全确定时熵为零",{"5":{"61":2}}],["完全不确定时熵最大",{"5":{"61":2}}],["完全无法捕获",{"5":{"71":1}}],["完全无法捕获和之间的乘积交互",{"5":{"71":1}}],["完全抑制负信息",{"5":{"71":2}}],["完全一致",{"5":{"70":1}}],["完形填空任务的数学分析",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["用",{"5":{"89":1}}],["用​替代",{"5":{"89":1}}],["用群作用把",{"5":{"88":2}}],["用户可能需要模型处理比训练时更长的上下文",{"5":{"88":2}}],["用户在使用时应注意",{"5":{"61":2}}],["用矩阵语言表达就是",{"5":{"88":2}}],["用矩阵形式表示为",{"5":{"88":2}}],["用不同复杂度的模型进行训练",{"5":{"51":2}}],["用秩不超过k的矩阵对",{"5":{"50":1}}],["用秩不超过k的矩阵对的最佳近似",{"5":{"50":1}}],["用于比较两个版本",{"5":{"96":2}}],["用于根据样本数据判断关于总体的假设是否成立",{"5":{"96":2}}],["用于建模连续的随机过程",{"5":{"96":2}}],["用于描述卷积操作的线性变换",{"5":{"87":2}}],["用于处理超大序列或有限gpu内存的情况",{"5":{"50":2}}],["用于处理序列数据",{"5":{"46":2}}],["用于将模型的logits输出转换为概率分布",{"5":{"47":2}}],["用于模拟独立随机变量序列的最大值分布",{"5":{"71":2}}],["用于对异常值鲁棒的场景",{"5":{"70":2}}],["用logits表示",{"5":{"71":2}}],["用途",{"5":{"69":1}}],["形容词",{"5":{"92":2}}],["形状相同的三阶张量",{"5":{"48":1}}],["形成了紧密的呼应",{"5":{"101":2}}],["形成了明显的分工模式",{"5":{"93":2}}],["形成一个维度为",{"5":{"93":1}}],["形成一个维度为的矩阵",{"5":{"93":1}}],["形成一个完全连通图",{"5":{"92":2}}],["形成对比",{"5":{"92":2}}],["形成转折关系",{"5":{"92":2}}],["形成维的",{"5":{"88":1}}],["形成",{"5":{"51":1,"88":1}}],["形成多层网络结构",{"5":{"46":2}}],["形成更复杂的token交互模式",{"5":{"71":2}}],["形式在数学上非常优美",{"5":{"42":2}}],["语法关系",{"5":{"93":2}}],["语法依赖",{"5":{"88":2,"92":2}}],["语义空间",{"5":{"94":2}}],["语义角色关系",{"5":{"93":2}}],["语义头往往表现出全局注意力模式",{"5":{"92":2}}],["语义依赖",{"5":{"92":2}}],["语义不同的词则距离较远",{"5":{"50":2}}],["语义关系",{"5":{"71":2}}],["语义相近的词在空间中距离较近",{"5":{"50":2}}],["语义相关",{"5":{"69":1}}],["语句含义",{"5":{"45":2}}],["语言具有强烈的顺序性",{"5":{"92":2}}],["语言中存在许多非线性位置依赖",{"5":{"88":2}}],["语言中的某些结构",{"5":{"88":2}}],["语言模型预测下一个词的概率分布为",{"5":{"96":2}}],["语言模型的训练目标是最大化训练语料库的对数似然",{"5":{"96":2}}],["语言模型的训练目标就是学习这个条件概率分布",{"5":{"96":2}}],["语言模型的输出层正是类别分布的一个典型应用",{"5":{"96":2}}],["语言模型",{"5":{"86":2}}],["语言",{"5":{"71":2,"94":2}}],["语言建模损失使用下一个token作为正样本",{"5":{"101":2}}],["语言建模损失",{"2":{"99":1,"101":1},"5":{"99":3,"101":3}}],["语言建模损失本质上是交叉熵损失在序列预测任务中的应用",{"5":{"99":2,"101":2}}],["语言建模的负对数似然损失为",{"5":{"99":2,"101":2}}],["语音",{"5":{"71":2}}],["任务完成率等指标",{"5":{"96":2}}],["任务中",{"5":{"95":2}}],["任务相关的相似度函数",{"5":{"94":2}}],["任务类型等因素有关",{"5":{"87":2}}],["任务契合",{"5":{"41":2}}],["任务适配性与损失函数选择",{"2":{"70":1},"5":{"70":1}}],["任务的损失函数为",{"5":{"99":1,"101":1}}],["任务",{"5":{"99":1,"101":1}}],["任务权重的影响",{"5":{"99":2,"101":2}}],["任务权重的选择直接影响优化轨迹和最终性能",{"5":{"99":1,"101":1}}],["任务权重",{"5":{"99":1,"101":1}}],["任何高斯随机变量都可以通过标准化变换",{"5":{"96":1}}],["任何高斯随机变量都可以通过标准化变换转换为标准高斯随机变量",{"5":{"96":1}}],["任何局部最小值都是全局最小值",{"5":{"51":2,"97":2}}],["任何局部最优解都是全局最优解",{"5":{"48":2}}],["任何其他投影点都会产生更大的误差",{"5":{"51":2}}],["任何回归模型的最终目标",{"5":{"51":2}}],["任何超平面",{"5":{"47":1}}],["任何超平面都只能将平面划分为两个半平面",{"5":{"47":1}}],["任意连续函数都可以用这些基函数的线性组合来逼近",{"5":{"93":2}}],["任意两个位置之间可以直接建立联系",{"5":{"92":2}}],["任意两个选项的相对选择概率仅取决于它们各自的",{"5":{"69":2}}],["任意位置之间可以存在依赖关系",{"5":{"92":2}}],["任意位置都可以被唯一标识",{"5":{"89":2}}],["任意矩阵都可以分解为",{"5":{"94":1}}],["任意矩阵",{"5":{"45":1,"94":1}}],["任意矩阵可以分解为",{"5":{"45":1}}],["任意非常数有界连续函数",{"5":{"71":2}}],["方差接近1",{"5":{"96":2}}],["方差有限",{"5":{"96":1}}],["方差达到克拉美",{"5":{"96":2}}],["方差用于分析梯度的波动",{"5":{"96":2}}],["方差的性质包括",{"5":{"96":2}}],["方差的平方根称为标准差",{"5":{"96":2}}],["方差可以展开为",{"5":{"96":2}}],["方差和协方差是概率论中最重要的一阶和二阶统计量",{"5":{"96":2}}],["方差和不可减少误差三个部分",{"5":{"51":2}}],["方差与协方差",{"2":{"96":1},"5":{"96":1}}],["方差为的高斯分布",{"5":{"96":1}}],["方差为​",{"5":{"96":2}}],["方差为",{"5":{"95":2,"96":9}}],["方差为1的特殊高斯分布",{"5":{"96":2}}],["方差为1的标准正态分布附近",{"5":{"41":2}}],["方差为1",{"5":{"41":2}}],["方差主导了mse",{"5":{"51":2}}],["方差权衡的数学本质和实践意义",{"5":{"51":2}}],["方差权衡的数学表达可以通过模拟实验来验证",{"5":{"51":2}}],["方差权衡的可视化",{"2":{"51":1},"5":{"51":1}}],["方差权衡可以通过学习曲线来可视化",{"5":{"51":2}}],["方差权衡",{"5":{"51":2}}],["方差始终非负",{"5":{"51":2}}],["方差反映了模型对训练数据中随机变化的响应程度",{"5":{"51":2}}],["方差分解是理解机器学习泛化误差的核心理论工具",{"5":{"51":2}}],["方差分解对于实际模型选择和调优至关重要",{"5":{"51":2}}],["方差分解中的三个项各有明确的数学意义",{"5":{"51":2}}],["方差分解",{"5":{"51":6}}],["方差",{"5":{"51":4,"96":3}}],["方向",{"5":{"48":4}}],["方向导数在梯度方向",{"5":{"48":2}}],["方向导数",{"5":{"48":2}}],["方向和幅度来构造任意复杂的曲面",{"5":{"71":2}}],["决定",{"5":{"61":2,"88":3,"97":2}}],["决定了什么样的输入模式会产生较高的注意力分数",{"5":{"94":1}}],["决定了信息的计量单位",{"5":{"61":1}}],["决定了它所代表的线性变换的类型",{"5":{"50":2}}],["决定了不同特征之间的线性组合方式",{"5":{"47":1}}],["决定了梯度在对应方向上的缩放因子",{"5":{"41":1}}],["决定了系统的稳定性",{"5":{"41":1}}],["决定了各任务在优化过程中的相对重要性",{"5":{"99":2,"101":2}}],["决策树的深度",{"5":{"51":2}}],["决策边界",{"5":{"47":4,"71":2}}],["都离不开这些基本概念",{"5":{"96":2}}],["都需要",{"5":{"95":1}}],["都对多头注意力的并行实现进行了高度优化",{"5":{"93":2}}],["都对这种张量运算进行了高度优化",{"5":{"93":2}}],["都对应或",{"5":{"70":1}}],["都对应",{"5":{"70":1}}],["都存在一定比例的注意力权重分配给距离为",{"5":{"92":1}}],["都存在一定比例的注意力权重分配给距离为的位置对",{"5":{"92":1}}],["都有指向所有其他节点的边",{"5":{"92":2}}],["都接近1",{"5":{"91":2}}],["都利用了注意力矩阵的低秩或稀疏结构",{"5":{"87":2}}],["都可能很大",{"5":{"61":2}}],["都可以分解为",{"5":{"94":1}}],["都可以理解为学习一个能够逼近真实条件期望函数的映射",{"5":{"51":2}}],["都可以作为激活函数",{"5":{"71":2}}],["都只能将平面划分为两个半平面",{"5":{"47":1}}],["都无法达到良好的分类效果",{"5":{"71":2}}],["都使用相同形式的梯度计算",{"5":{"69":2}}],["都与",{"5":{"70":2}}],["都是鞍点或局部极小值",{"5":{"97":2}}],["都是",{"5":{"93":2}}],["都是振幅为1的正弦波",{"5":{"90":1}}],["都是简单的查表",{"5":{"89":2}}],["都是在softmax框架下比较查询与候选集的匹配程度",{"5":{"69":2}}],["都是研究特定数学结构的几何性质",{"5":{"70":2}}],["都是将一个向量",{"5":{"70":2}}],["都用于防止优化过程中的过大更新",{"5":{"99":2,"101":2}}],["平滑",{"5":{"89":2,"91":2}}],["平滑的",{"5":{"88":2}}],["平方操作使得这个异常样本的误差平方为100",{"5":{"51":2}}],["平方操作确保了误差的正定性",{"5":{"51":2}}],["平移等变性的群论定义如下",{"5":{"88":2}}],["平移等变性",{"5":{"88":3}}],["平移不变",{"5":{"87":2}}],["平移不变性",{"5":{"47":2,"90":2}}],["平移变换",{"5":{"47":1}}],["平移变换则将整个超平面平移",{"5":{"47":1}}],["平坦",{"5":{"48":2,"69":2,"71":2,"87":2,"89":2,"95":2,"96":2}}],["平行线保持平行",{"5":{"71":2}}],["平均所需的信息量",{"5":{"61":2}}],["平均绝对误差",{"5":{"51":2,"70":2}}],["平均",{"5":{"41":2}}],["上是单射的",{"5":{"91":1}}],["上是相近的",{"5":{"70":2}}],["上的任务",{"5":{"93":2}}],["上的平移操作",{"5":{"90":2}}],["上的效率不如密集矩阵运算",{"5":{"86":2}}],["上的一个严格凹函数",{"5":{"61":2}}],["上为零",{"5":{"48":2}}],["上取得最小值",{"5":{"48":2}}],["上取得最大值",{"5":{"48":2}}],["上述矩阵运算等价于对每个样本单独进行神经元计算后的结果堆叠",{"5":{"47":2}}],["上述表达式可以简化为",{"5":{"71":2}}],["上述梯度公式揭示了infonce优化过程中的核心机制",{"5":{"69":2}}],["上表现为线性函数",{"5":{"45":2}}],["上",{"5":{"69":1,"70":1,"71":1,"88":2,"96":1}}],["上定义一个注意力分布",{"5":{"69":2}}],["上下文",{"5":{"99":2,"101":2}}],["逐渐收敛到不同的局部最优解",{"5":{"93":2}}],["逐渐增加直到找到稳定的最大允许值",{"5":{"41":2}}],["逐位置计算角度的正弦和余弦",{"5":{"88":1}}],["逐块加载",{"5":{"86":2}}],["逐块计算注意力输出",{"5":{"86":2}}],["逐层传播到输入层",{"5":{"44":1}}],["逐层计算误差信号和参数梯度",{"5":{"44":2}}],["逐步扩展",{"5":{"92":2}}],["逐步推广到现代深度学习中常用的连续激活神经元",{"5":{"47":2}}],["逐步推导出完整的梯度计算公式",{"5":{"44":2}}],["逐步推导出各层参数的梯度计算公式",{"5":{"44":2}}],["逐步揭示非线性变换的必要性和数学价值",{"5":{"71":2}}],["逐元素运算对张量中的每个独立元素应用相同的函数",{"5":{"50":2}}],["逐元素乘法",{"5":{"44":1}}],["逐元素乘法约为",{"5":{"44":1}}],["逐元素乘法约为​",{"5":{"44":1}}],["逐元素应用于向量",{"5":{"71":1}}],["逐元素非线性变换",{"5":{"71":2}}],["网络处理输入的不同部分",{"5":{"93":2}}],["网络输出",{"5":{"45":1}}],["网络输出是输入的线性函数",{"5":{"45":1}}],["网络通过前向传播计算输出",{"5":{"45":2}}],["网络参数",{"5":{"44":1}}],["网络参数​",{"5":{"44":1}}],["网络现在是",{"5":{"71":1}}],["网络现在是分段线性或非线性的",{"5":{"71":1}}],["网络的学习目标",{"5":{"45":2}}],["网络的原始输出",{"5":{"71":2}}],["网络的表达能力始终局限于输入的线性变换",{"5":{"71":2}}],["网络包含一个隐藏层",{"5":{"71":2}}],["网络架构设计之间的深层联系",{"5":{"41":2}}],["深层",{"5":{"87":2,"92":4}}],["深层注意力矩阵通常比浅层注意力矩阵有更快的奇异值衰减",{"5":{"87":2}}],["深层网络因此陷入",{"5":{"41":2}}],["深入剖析缩放因子的统计学原理",{"5":{"95":2}}],["深入剖析前向传播的内在机制",{"5":{"45":2}}],["深入理解这种差异",{"5":{"88":2}}],["深入分析其参数结构和计算复杂度",{"5":{"93":2}}],["深入分析单层神经元的数学结构",{"5":{"47":2}}],["深入分析激活函数的数学角色",{"5":{"71":2}}],["深度神经网络的损失函数通常是非凸的",{"5":{"48":2}}],["深度神经网络的训练涉及大量的梯度计算和参数更新",{"5":{"44":2}}],["深度学习中的mse损失通常仍然可以优化到较好的解",{"5":{"51":2}}],["深度学习框架也提供了高效的函数来计算矩阵的特征值分解和奇异值分解",{"5":{"50":2}}],["深度学习框架会在不显式复制数据的情况下",{"5":{"50":2}}],["深度学习框架",{"5":{"47":2}}],["深度学习框架只需要实现softmax的梯度",{"5":{"69":2}}],["深度学习应用",{"5":{"44":1,"45":1}}],["深度",{"5":{"46":2}}],["深度为",{"5":{"71":1}}],["深度为的网络可以等效于宽度为指数级别的浅层网络",{"5":{"71":1}}],["深度指数优势",{"5":{"71":2}}],["深度网络往往收敛到具有相似损失值的局部最小值",{"5":{"48":2}}],["深度网络的表示空间具有层次化的结构",{"5":{"45":2}}],["深度网络的特征空间层次",{"5":{"45":2}}],["深度网络用指数级更少的参数可以达到与浅层网络相同的表达能力",{"5":{"71":2}}],["深度网络能够表示的函数空间远大于同等参数量的浅层网络",{"5":{"71":2}}],["深度网络通过逐层的非线性变换",{"5":{"71":2}}],["深度优先",{"5":{"71":2}}],["深度与宽度的数学权衡",{"2":{"71":1},"5":{"71":1}}],["处求值",{"5":{"96":1}}],["处词元的嵌入向量",{"5":{"94":1}}],["处理",{"5":{"88":2}}],["处理编码向量",{"5":{"71":2}}],["处的相位",{"5":{"90":1}}],["处的梯度定义为",{"5":{"48":1}}],["处的偏导数定义为",{"5":{"48":1}}],["处有",{"5":{"42":1}}],["处有拐点",{"5":{"42":1}}],["处取得",{"5":{"42":2}}],["处",{"5":{"42":3}}],["处于充分激活状态",{"5":{"41":2}}],["处于深度饱和状态",{"5":{"41":2}}],["处于激活区域",{"5":{"41":2}}],["|f",{"5":{"46":1}}],["|w|",{"5":{"42":2}}],["|",{"5":{"42":4,"46":3}}],["you",{"5":{"91":2}}],["young",{"5":{"50":2}}],["yet",{"5":{"88":2}}],["yarn",{"5":{"88":2}}],["yarn优化",{"5":{"88":2}}],["yi",{"5":{"42":1}}],["yk",{"5":{"42":1}}],["y",{"5":{"42":24,"46":4,"61":20}}],["xl的相对位置编码",{"5":{"88":2}}],["xw",{"5":{"86":6}}],["xorscene",{"5":{"47":4}}],["xor",{"5":{"47":2}}],["x^2",{"5":{"46":2}}],["xi",{"5":{"46":4}}],["x",{"5":{"42":72,"46":14,"69":7}}],["xavier方差",{"5":{"41":1}}],["xavier初始化和he初始化都假设权重服从某种高斯分布",{"5":{"96":2}}],["xavier初始化和he初始化正是为了避免这个问题而设计的",{"5":{"41":2}}],["xavier初始化的方差为",{"5":{"94":2}}],["xavier初始化的方差推导",{"5":{"41":2}}],["xavier初始化取两者的几何平均",{"5":{"41":1}}],["xavier初始化取两者的几何平均​",{"5":{"41":1}}],["xavier初始化将权重方差设置为",{"5":{"41":1}}],["xavier初始化将权重方差设置为​",{"5":{"41":1}}],["xavier初始化将权重初始化为",{"5":{"41":2}}],["xavier初始化",{"5":{"41":2}}],["xm",{"5":{"69":1}}],["v变换的数学本质",{"5":{"94":2}}],["vllm",{"5":{"93":2}}],["vc维",{"5":{"87":2}}],["vc维控制",{"5":{"61":2}}],["vectorization",{"5":{"50":2}}],["v0",{"5":{"47":8,"48":4}}],["vaswani等人提出的正弦位置编码",{"5":{"88":2}}],["value空间可以与query",{"5":{"94":2}}],["value不需要参与匹配计算",{"5":{"94":2}}],["value向量承载的是信息的内容本身",{"5":{"94":2}}],["value向量的期望",{"5":{"69":2}}],["value通过三个独立的投影矩阵被映射到不同的空间",{"5":{"94":2}}],["value通过三个独立的线性变换从输入嵌入中生成",{"5":{"94":2}}],["value都将从这个基础表示中通过线性变换生成",{"5":{"94":2}}],["value是如何从输入嵌入表示中生成的",{"5":{"94":2}}],["value张量切片",{"5":{"93":1}}],["value张量切片独立地进行scaled",{"5":{"93":1}}],["value进行注意力计算",{"5":{"93":2}}],["value块",{"5":{"86":2}}],["value矩阵划分为小块",{"5":{"86":2}}],["value的维度设计是一个重要的设计决策",{"5":{"94":2}}],["value的投影矩阵",{"5":{"94":2}}],["value的投影结果",{"5":{"93":2}}],["value的投影阶段",{"5":{"93":2}}],["value的投影计算是线性的",{"5":{"71":2}}],["value的计算分别为",{"5":{"91":2}}],["value的矩阵推导",{"5":{"95":2}}],["value的矩阵乘法可以在半精度下进行",{"5":{"93":2}}],["value的矩阵变换是掌握注意力机制的关键一步",{"5":{"94":2}}],["value的矩阵变换",{"5":{"86":2,"93":2}}],["value的矩阵表示与变换<",{"5":{"85":1}}],["value的矩阵表示与变换",{"0":{"94":1},"4":{"94":1},"5":{"85":4,"94":1}}],["value",{"5":{"45":2,"50":2,"94":2,"95":4,"97":2}}],["value投影都是独立的可学习参数",{"5":{"94":2}}],["value投影定义了从输入空间到某个",{"5":{"93":1}}],["value投影定义了从输入空间到某个​维子空间的映射",{"5":{"93":1}}],["value投影矩阵的梯度对于分析模型的训练动态至关重要",{"5":{"94":2}}],["value投影矩阵",{"5":{"93":4}}],["value投影会同时考虑内容信息和位置信息",{"5":{"91":2}}],["value投影",{"5":{"69":2}}],["variational",{"5":{"96":2}}],["variance",{"5":{"51":8,"96":2}}],["var",{"5":{"42":2,"46":13}}],["v^2",{"5":{"42":2}}],["v^t",{"5":{"42":4}}],["v^",{"5":{"69":4}}],["v",{"5":{"42":12,"86":15,"97":10}}],["vs",{"5":{"42":2}}],["加上残差连接和层归一化后",{"5":{"92":2}}],["加上正样本共个样本",{"5":{"69":1}}],["加上正样本共",{"5":{"69":1}}],["加性注入",{"5":{"88":1}}],["加性位置编码无法产生真正具有相对位置感知的注意力模式",{"5":{"88":2}}],["加载",{"5":{"86":1}}],["加载和存储中间状态",{"5":{"86":2}}],["加载矩阵",{"5":{"86":1}}],["加权聚合",{"5":{"95":2}}],["加权形式",{"5":{"51":2}}],["加权mse在实际应用中有广泛的用途",{"5":{"51":2}}],["加权mse的矩阵形式可以通过对角权重矩阵来表示",{"5":{"51":2}}],["加权均方误差",{"5":{"51":2}}],["加权均方误差的推广",{"2":{"51":1},"5":{"51":1}}],["加权和至少为1",{"5":{"47":2}}],["加权和为",{"5":{"47":6}}],["加权求和信息整合与阈值触发机制",{"5":{"47":2}}],["加权求和结果",{"5":{"42":2}}],["加权求和法",{"5":{"70":2}}],["加权平均中的权重正是softmax概率",{"5":{"69":2}}],["加权",{"5":{"69":1}}],["加法操作破坏了内容向量的原有结构",{"5":{"88":2}}],["加法节点",{"5":{"45":2}}],["加法",{"5":{"45":2,"48":2,"50":2}}],["加速了训练收敛",{"5":{"42":2}}],["二项分布的共轭先验是beta分布",{"5":{"96":2}}],["二项分布的期望为",{"5":{"96":2}}],["二项分布在语言模型中的应用包括描述一批样本中被mask的词的数量",{"5":{"96":2}}],["二项分布",{"5":{"96":2}}],["二是保留了信息的冗余性",{"5":{"92":2}}],["二元交叉熵损失",{"5":{"61":1}}],["二元交叉熵损失为",{"5":{"61":2}}],["二分类问题是分类任务中最简单但最具代表性的情形",{"5":{"61":2}}],["二分类任务的交叉熵损失",{"2":{"61":1},"5":{"61":1}}],["二分类任务",{"5":{"44":2}}],["二次型",{"5":{"96":2}}],["二次函数",{"5":{"51":2,"70":1}}],["二次增长",{"5":{"70":2}}],["二次损失",{"5":{"70":1}}],["二阶中心矩是方差",{"5":{"96":2}}],["二阶张量",{"5":{"50":2}}],["二阶张量是矩阵",{"5":{"50":2}}],["二阶矩估计",{"5":{"48":2}}],["二阶导数为负",{"5":{"42":2}}],["二阶导数为正",{"5":{"42":2}}],["二阶导数的几何意义表示sigmoid曲线的曲率变化",{"5":{"42":2}}],["二字的含义所在",{"5":{"46":2}}],["微批处理",{"5":{"50":2}}],["微积分是研究变化率和累积效应的数学分支",{"5":{"48":2}}],["微积分与优化基础",{"0":{"48":1},"4":{"48":1},"5":{"21":4,"48":1,"85":4}}],["微积分与优化基础<",{"5":{"21":1,"85":1}}],["微分关系导致了导数的简洁表示",{"5":{"42":2}}],["将对应维度的分量相乘后累加",{"5":{"95":2}}],["将相似度矩阵转换为概率分布",{"5":{"95":2}}],["将建立起对注意力机制数学本质的深刻理解",{"5":{"95":2}}],["将每个输入嵌入向量",{"5":{"94":1}}],["将每个输入嵌入向量投影到一个​维的查询空间",{"5":{"94":1}}],["将被编码为key向量",{"5":{"94":2}}],["将被编码为query向量",{"5":{"94":2}}],["将高维输入压缩到低维空间",{"5":{"94":2}}],["将拼接结果映射回",{"5":{"93":1}}],["将拼接后的多头输出映射回原始模型维度",{"5":{"93":2}}],["将编码向量投影到前两个主成分",{"5":{"91":2}}],["将编码矩阵",{"5":{"89":1}}],["将编码矩阵参数化为低秩分解",{"5":{"89":1}}],["将公式",{"5":{"91":2}}],["将其按维度配对",{"5":{"90":1}}],["将其按维度配对组织",{"5":{"90":1}}],["将复杂度从",{"5":{"90":1}}],["将复杂度从降低到",{"5":{"90":1}}],["将​按频率成分展开",{"5":{"90":1}}],["将点积公式重写为",{"5":{"90":2}}],["将帮助我们更好地改进和发展下一代位置编码技术",{"5":{"88":2}}],["将变为",{"5":{"88":1}}],["将位置索引映射到编码向量",{"5":{"89":2}}],["将位置编码视为可学习的参数",{"5":{"89":2}}],["将位置编码问题置于复数框架下后",{"5":{"88":2}}],["将位置视为时间信号",{"5":{"88":2}}],["将位置信息嵌入到token嵌入中",{"5":{"41":2}}],["将序列的顺序信息注入到模型之中",{"5":{"88":2}}],["将投影矩阵",{"5":{"87":1}}],["将投影矩阵​",{"5":{"87":1}}],["将或投影到低维空间",{"5":{"87":1}}],["将注意力计算的复杂度从",{"5":{"86":1}}],["将注意力计算的复杂度从降低到",{"5":{"86":1}}],["将完整的注意力计算分解为块级别的计算",{"5":{"86":2}}],["将logits转化为满足概率公理的分布",{"5":{"61":1}}],["将真实分布和模型分布代入交叉熵定义",{"5":{"61":2}}],["将在本节最后部分详细讨论",{"5":{"61":2}}],["将在第三章进行详细的数学分析",{"5":{"47":2}}],["将交叉熵从信息论的概念转化为可计算的损失函数",{"5":{"61":2}}],["将模型分布",{"5":{"61":2}}],["将这一分解应用于投影矩阵",{"5":{"94":1}}],["将这一分解应用于投影矩阵​",{"5":{"94":1}}],["将这两个向量复制或交错排列",{"5":{"88":2}}],["将这个分解代入mse并展开",{"5":{"51":2}}],["将这些概念应用于激活函数分析",{"5":{"71":2}}],["将形状较小的张量在某些维度上",{"5":{"50":1}}],["将形状较小的张量在某些维度上视为被重复",{"5":{"50":1}}],["将它们累加起来",{"5":{"50":2}}],["将离散的符号表示转化为连续的数值表示",{"5":{"50":2}}],["将神经元的净输入",{"5":{"47":1}}],["将到的等式依次代入",{"5":{"46":1}}],["将权重矩阵和偏置合并为扩展权重矩阵",{"5":{"46":2,"47":2}}],["将矩阵乘法展开为元素形式",{"5":{"46":2}}],["将按列分割",{"5":{"45":1}}],["将结果旋转到输出空间",{"5":{"45":2}}],["将求和写为矩阵形式",{"5":{"44":2}}],["将输入的高维信息压缩到低维流形上进行计算",{"5":{"87":2}}],["将输入的尺度归一化",{"5":{"42":2}}],["将输入空间划分为两个区域",{"5":{"47":2}}],["将输入空间按超平面",{"5":{"45":1}}],["将输入空间按超平面划分为两个区域",{"5":{"45":1}}],["将输入向量",{"5":{"47":1}}],["将输入向量扩展为齐次坐标",{"5":{"47":1}}],["将输入向量旋转到奇异向量方向",{"5":{"45":2}}],["将输入扩展为齐次坐标",{"5":{"46":2}}],["将输出层的损失信息传递到输入层",{"5":{"44":2}}],["将梯度计算的时间复杂度从指数级降低到线性级",{"5":{"44":2}}],["将query和key的生成过程代入注意力分数的计算中",{"5":{"94":2}}],["将query",{"5":{"71":1}}],["将",{"5":{"45":1,"46":1,"61":2,"70":4,"71":1,"87":1,"88":1,"90":1}}],["将实数轴压缩到有限的区间",{"5":{"71":1}}],["将实数轴压缩到有限的区间或",{"5":{"71":1}}],["将实数域映射到",{"5":{"71":1}}],["将前一层的表示转换为新的表示",{"5":{"71":1}}],["将代入对数几率的表达式",{"5":{"71":1}}],["将空间分为",{"5":{"71":2}}],["将sigmoid函数重写为",{"5":{"42":2}}],["将所有编码向量初始化为零",{"5":{"89":2}}],["将所有元素写为矩阵形式",{"5":{"44":2}}],["将所有序列填充到统一长度",{"5":{"99":2,"101":2}}],["将正样本对的表示拉近",{"5":{"69":2}}],["将任意实数向量映射到概率单纯形上",{"5":{"61":2}}],["将任意实数向量",{"5":{"69":2}}],["将各头的输出投影到一个新的空间",{"5":{"93":2}}],["将各层求和",{"5":{"44":2}}],["将各种问题统一为",{"5":{"69":2}}],["将文档分解为段落",{"5":{"88":2}}],["将文本切分为token序列",{"5":{"99":2,"101":2}}],["值聚合操作则根据注意力权重进行信息加权求和",{"5":{"95":2}}],["值聚合",{"5":{"95":2}}],["值越大表示方向越接近",{"5":{"95":2}}],["值空间的设计同样具有深意",{"5":{"94":2}}],["值得在实际应用中仔细考量",{"5":{"94":2}}],["值得注意的是",{"5":{"47":2,"51":2,"61":2,"88":4,"91":2,"92":2,"93":4,"94":2,"96":2}}],["值",{"5":{"94":4}}],["值向量",{"5":{"92":1,"95":2}}],["值向量​是​的线性变换",{"5":{"92":1}}],["值为1",{"5":{"87":2}}],["值的交互",{"5":{"50":2}}],["值投影都是通过矩阵乘法实现的",{"5":{"50":2}}],["值即可",{"5":{"42":1}}],["值域为",{"5":{"42":4}}],["值表示激活函数的导数接近1",{"5":{"41":1}}],["值表示激活函数具有更好的抗饱和性能",{"5":{"41":1}}],["几个值得关注的方向包括",{"5":{"88":2}}],["几乎秩1",{"5":{"87":2}}],["几乎所有局部极小值都是",{"5":{"97":2}}],["几乎所有临界点",{"5":{"97":2}}],["几乎所有主流的大语言模型都采用了rope或其变体",{"5":{"88":2}}],["几乎所有权重集中在一个位置",{"5":{"87":2}}],["几乎所有的计算都在张量之上进行",{"5":{"50":2}}],["几乎处处相等",{"5":{"61":2}}],["几何解释",{"5":{"61":2,"70":2}}],["几何解释与梯度特性",{"2":{"42":1},"5":{"42":1}}],["几何解释等",{"5":{"70":2}}],["几何图示与直观理解",{"2":{"51":1},"5":{"51":1}}],["几何上",{"5":{"48":2}}],["几何可视化",{"5":{"69":2}}],["几何本质",{"5":{"69":2}}],["几何视角下的损失函数分析",{"2":{"70":1},"5":{"70":1}}],["几何差异的可视化",{"5":{"70":2}}],["几何结构",{"5":{"70":1}}],["同质化",{"5":{"87":2}}],["同一套计算范式可以通过不同的掩码模式适应不同的任务需求",{"5":{"95":2}}],["同一组内的query头共享同一个key和value",{"5":{"86":2}}],["同一batch内",{"5":{"45":2}}],["同上",{"5":{"41":1}}],["同时存在方向",{"5":{"97":1}}],["同时存在方向使得在时减少",{"5":{"97":1}}],["同时理解评估结果的统计显著性",{"5":{"96":2}}],["同时满足因果约束和填充处理的要求",{"5":{"95":2}}],["同时也指出了其",{"5":{"92":1}}],["同时也指出了其复杂度的局限性及相应的优化方向",{"5":{"92":1}}],["同时也使得位置信息能够在后续的线性变换中被有效利用",{"5":{"91":2}}],["同时成立",{"5":{"91":1}}],["同时具有更好的外推性能",{"5":{"88":2}}],["同时改变了向量的方向",{"5":{"88":2}}],["同时又足够小",{"5":{"88":2}}],["同时去除",{"5":{"87":2}}],["同时通过累积因子",{"5":{"86":1}}],["同时通过累积因子确保数学上的等价性",{"5":{"86":1}}],["同时通过非线性组合创造了位置",{"5":{"71":2}}],["同时保持数值稳定性",{"5":{"97":2}}],["同时保持了对所有头的访问能力",{"5":{"93":2}}],["同时保持或接近原始模型的表达能力",{"5":{"87":2}}],["同时保持大部分计算节省",{"5":{"86":2}}],["同时保留了输入的相对顺序信息",{"5":{"42":2}}],["同时对较大的误差给予更大的惩罚",{"5":{"51":2}}],["同时对每个维度施加了非线性变换",{"5":{"71":2}}],["同时相邻位置之间具有平滑的插值性质",{"5":{"71":2}}],["同时最小化",{"5":{"71":2}}],["同时使得注意力机制能够学习非线性的token交互模式",{"5":{"71":2}}],["同时引入有益的非线性",{"5":{"41":2}}],["同时归一化稳定了分支路径的激活分布",{"5":{"41":2}}],["同时实现了通道间的归一化",{"5":{"41":2}}],["同时",{"5":{"47":2,"61":2,"71":2,"96":2}}],["同时被拉向正样本",{"5":{"69":2}}],["同时将所有负样本键的加权平均推离查询向量",{"5":{"69":2}}],["同时将负样本对的表示推远",{"5":{"69":2}}],["同时确保输出满足概率分布的约束",{"5":{"69":2}}],["同时进行分类和回归",{"5":{"70":2}}],["同样通过在注意力分数上添加掩码来实现",{"5":{"95":2}}],["同样可以进行批量读取",{"5":{"92":2}}],["同样使用rope",{"5":{"88":1}}],["同样将未归一化的相似度分数转换为归一化的注意力权重",{"5":{"61":2}}],["同样具有最大似然解释",{"5":{"69":2}}],["包含大量的鞍点和局部极小值",{"5":{"97":2}}],["包含若干个信息片段",{"5":{"95":2}}],["包含完全相同的词汇",{"5":{"88":2}}],["包含该位置被允许关注的位置索引",{"5":{"86":2}}],["包含了所有头的query向量",{"5":{"93":1}}],["包含了位置",{"5":{"92":1}}],["包含了关于所有自变量的二阶偏导数信息",{"5":{"48":2}}],["包含了函数关于所有自变量的偏导数信息",{"5":{"48":2}}],["包含",{"5":{"47":1}}],["包含仿射变换和非线性激活两个组成部分",{"5":{"47":2}}],["包含注意力机制和前馈网络",{"5":{"45":2}}],["包含前向传播",{"5":{"44":2}}],["包含所有可以通过原点的k维平面上的点",{"5":{"50":2}}],["包含所有常数函数",{"5":{"71":2}}],["包含所有奇异值",{"5":{"41":1}}],["包含多少关于序列其他部分的信息",{"5":{"69":1}}],["包括学习率调度和自适应优化算法的收敛性质",{"5":{"97":2}}],["包括凸性",{"5":{"97":2}}],["包括优化景观的几何结构",{"5":{"97":2}}],["包括参数估计和假设检验两大类方法",{"5":{"96":2}}],["包括任意跨度的长程依赖",{"5":{"92":2}}],["包括长程依赖",{"5":{"92":2}}],["包括周期函数的傅里叶级数展开",{"5":{"90":2}}],["包括奇数情况",{"5":{"88":2}}],["包括稀疏注意力",{"5":{"86":2}}],["包括scaled",{"5":{"86":2}}],["包括基本形式",{"5":{"51":2}}],["包括其凸性与优化特性",{"5":{"51":2}}],["包括其凸性",{"5":{"51":2}}],["包括矩阵乘法",{"5":{"50":2}}],["包括结合律",{"5":{"50":2}}],["包括gram",{"5":{"50":2}}],["包括损失函数的",{"5":{"48":2}}],["包括逻辑与",{"5":{"47":2}}],["包括leaky",{"5":{"42":2}}],["包括有效秩",{"5":{"41":2}}],["包括rnn的时间反向传播问题和transformer中的梯度挑战",{"5":{"41":2}}],["包括在线学习和自回归模型",{"5":{"41":2}}],["包括正样本和负样本",{"5":{"69":2}}],["包括正弦余弦编码的频率分解",{"5":{"70":2}}],["包括梯度裁剪和混合精度训练",{"5":{"97":2}}],["包括梯度如何衰减",{"5":{"41":2}}],["包括梯度结构",{"5":{"70":2}}],["随时间",{"5":{"97":1}}],["随增长",{"5":{"95":1}}],["随维度索引",{"5":{"88":1}}],["随维度索引变化",{"5":{"88":1}}],["随后的query",{"5":{"91":2}}],["随后",{"5":{"51":2,"97":2}}],["随样本量增加而减小",{"5":{"51":2}}],["随机采样策略的设计",{"5":{"96":2}}],["随机变量分为离散随机变量和连续随机变量",{"5":{"96":2}}],["随机变量是样本空间到实数集的映射",{"5":{"96":2}}],["随机变量与概率分布",{"2":{"96":1},"5":{"96":1}}],["随机梯度的微小差异会逐渐放大",{"5":{"93":2}}],["随机梯度下降的隐式正则化效应",{"5":{"48":2}}],["随机投影",{"5":{"87":2}}],["随机注意力打破了局部性的限制",{"5":{"86":2}}],["随机注意力",{"5":{"86":2}}],["随机性程度",{"5":{"61":2}}],["随机选择一部分位置",{"5":{"99":2,"101":2}}],["随着批量大小的增加",{"5":{"96":2}}],["随着位置",{"5":{"91":1}}],["随着位置增加",{"5":{"91":2}}],["随着位置的增加",{"5":{"91":1}}],["随着训练的进行",{"5":{"89":2}}],["随着上下文长度需求的不断增长",{"5":{"88":2}}],["随着层数增加",{"5":{"45":2}}],["随着阶数增加",{"5":{"42":2}}],["随着接近",{"5":{"70":1}}],["随着",{"5":{"70":1}}],["随",{"5":{"41":2,"95":1}}],["特征函数提供了有用的分析工具",{"5":{"96":2}}],["特征函数是矩母函数的复数形式",{"5":{"96":2}}],["特征的现象",{"5":{"93":2}}],["特征映射函数的设计",{"2":{"86":1},"5":{"86":1}}],["特征向量表示分散的主方向",{"5":{"89":2}}],["特征向量表示这些曲率对应的方向",{"5":{"48":2}}],["特征向量可能更分散",{"5":{"87":2}}],["特征向量可能类似于平滑函数",{"5":{"87":2}}],["特征向量的正交性也是重要考虑",{"5":{"87":2}}],["特征向量的结构取决于注意力权重的分布模式",{"5":{"87":2}}],["特征向量的数学结构",{"2":{"87":1},"5":{"87":1}}],["特征向量",{"5":{"50":2}}],["特征值非负",{"5":{"87":2}}],["特征值为1",{"5":{"87":2}}],["特征值分析是理解矩阵动力学行为的关键",{"5":{"87":2}}],["特征值分解在动力系统分析",{"5":{"50":2}}],["特征值分解总是可行的",{"5":{"50":2}}],["特征值分解将方阵分解为特征向量矩阵和特征值对角矩阵的乘积",{"5":{"50":2}}],["特征值分解和cholesky分解等多种方法",{"5":{"50":2}}],["特征值分解只适用于方阵且要求矩阵可对角化",{"5":{"50":2}}],["特征值分解用于提取数据中方差最大的方向",{"5":{"50":2}}],["特征值是实数",{"5":{"50":2}}],["特征值可以是负数",{"5":{"50":2}}],["特征值与网络稳定性密切相关",{"5":{"50":2}}],["特征值与特征向量",{"2":{"50":1},"5":{"50":1}}],["特征值和特征向量有着多方面的应用",{"5":{"50":2}}],["特征值和特征向量是线性代数中最深刻也最有应用价值的概念之一",{"5":{"50":2}}],["特征值的模为1",{"5":{"87":2}}],["特征值的模不超过1",{"5":{"87":2}}],["特征值的乘积等于行列式",{"5":{"87":2}}],["特征值的计算需要求解特征方程",{"5":{"50":2}}],["特征值的平方根",{"5":{"45":1}}],["特征层次假说",{"5":{"45":2}}],["特征空间是机器学习和深度学习中描述数据表示的核心概念",{"5":{"50":2}}],["特征空间的几何直觉",{"2":{"50":1},"5":{"50":1}}],["特征空间的维度",{"5":{"47":1}}],["特征空间的维度由神经元的数量决定",{"5":{"47":1}}],["特征空间",{"5":{"45":2}}],["特征维度",{"5":{"41":1}}],["特征工程",{"5":{"70":1}}],["特别适合处理稀疏梯度问题",{"5":{"97":2}}],["特别适用于样本量不大或分布未知的情况",{"5":{"96":2}}],["特别值得注意的是",{"5":{"61":2}}],["特别地",{"5":{"61":2,"70":2}}],["特别是在与sigmoid或softmax结合时",{"5":{"97":2}}],["特别是在循环神经网络和transformer中",{"5":{"97":2}}],["特别是当目标分布是高度非高斯的时候",{"5":{"96":2}}],["特别是要保持与矩阵乘法的良好兼容性",{"5":{"88":2}}],["特别是gpu",{"5":{"47":2}}],["特别是第五章注意力机制",{"5":{"71":2}}],["特别是",{"5":{"70":2}}],["特性是注意力机制建模长程依赖的数学基础",{"5":{"92":2}}],["特性",{"5":{"41":1,"44":1,"45":1,"51":2}}],["特性使其具有良好的可扩展性",{"5":{"69":2}}],["特定任务激活函数",{"5":{"41":2}}],["然后检验这些差异的均值是否显著不为零",{"5":{"96":2}}],["然后沿奇异值方向进行缩放",{"5":{"94":2}}],["然后拼接结果",{"5":{"93":2}}],["然后将对应的值进行加权聚合",{"5":{"95":2}}],["然后将这些权重作为系数对信息内容进行加权平均",{"5":{"95":2}}],["然后将这些关联信息进行融合",{"5":{"93":2}}],["然后将输出平移和缩移到",{"5":{"42":1}}],["然后将输出平移和缩移到区间",{"5":{"42":1}}],["然后详细推导了正弦余弦编码的数学公式",{"5":{"91":2}}],["然后利用向量化操作同时处理所有维度",{"5":{"88":2}}],["然后计算",{"5":{"61":2}}],["然后与输入特征相乘",{"5":{"51":2}}],["然后采取相应的策略",{"5":{"51":2}}],["然后再进行衰减",{"5":{"48":2}}],["然后再衰减",{"5":{"48":2}}],["然后应用链式法则累加梯度",{"5":{"48":2}}],["然后使用累加的梯度进行一次参数更新",{"5":{"50":2}}],["然后使用链式法则组合这些梯度",{"5":{"48":2}}],["然后使用激活函数",{"5":{"71":2}}],["然后使用这些估计的分布来表征原始统计量的不确定性",{"5":{"96":2}}],["然后使用这个奖励模型来指导策略优化",{"5":{"99":2,"101":2}}],["然后逐层向后传递",{"5":{"48":2}}],["然后根据梯度方向更新参数",{"5":{"48":2}}],["然后在attention分数上添加一个与相对位置相关的偏置",{"5":{"88":2}}],["然后在各主轴方向上进行缩放和平移",{"5":{"45":2}}],["然后在下一层再次进行线性变换",{"5":{"71":2}}],["然后从输出层开始",{"5":{"44":2}}],["然后通过最小化近似分布与真实后验分布之间的kl散度来找到最好的近似",{"5":{"96":2}}],["然后通过转置将维度排列为",{"5":{"93":1}}],["然后通过转置将维度排列为​",{"5":{"93":1}}],["然后通过激活函数",{"5":{"47":1}}],["然后通过激活函数进行非线性变换",{"5":{"47":1}}],["然后通过激活函数的导数进行缩放",{"5":{"44":2}}],["然后通过平移",{"5":{"71":2}}],["然后通过softmax将logits转换为概率分布",{"5":{"99":2,"101":2}}],["然后经过gelu激活",{"5":{"71":2}}],["然后",{"5":{"41":2,"51":2,"93":2}}],["然而在实际实现中",{"5":{"46":2}}],["然而",{"5":{"41":6,"42":4,"44":2,"45":2,"46":2,"47":2,"48":2,"51":2,"61":2,"69":4,"70":4,"71":6,"86":12,"87":8,"88":14,"89":8,"90":4,"91":2,"92":4,"93":6,"94":2,"96":4,"97":8,"99":4,"101":4}}],["原假设通常是我们想要拒绝的假设",{"5":{"96":2}}],["原因包括",{"5":{"51":2}}],["原因在于io效率的提升",{"5":{"86":2}}],["原因在于",{"5":{"46":2}}],["原点保持不动",{"5":{"71":2}}],["原始输入中的一部分信息必然被丢弃",{"5":{"94":2}}],["原始输入的梯度贡献仍然保持完整",{"5":{"92":2}}],["原始的",{"5":{"87":1}}],["原始的矩阵有个自由度",{"5":{"87":1}}],["原始注意力分数矩阵",{"5":{"87":1}}],["原始注意力分数矩阵的秩最多为",{"5":{"87":1}}],["原始注意力计算的时间复杂度为",{"5":{"87":2}}],["原始梯度信息仍然可以部分保留",{"5":{"41":2}}],["原始序列和打乱序列之间的kl散度为",{"5":{"69":2}}],["原则3",{"5":{"41":10}}],["原则一",{"5":{"70":2}}],["原则二",{"5":{"70":2}}],["原则三",{"5":{"70":2}}],["原则四",{"5":{"70":2}}],["激活值的内存消耗可能超过参数本身",{"5":{"50":2}}],["激活值溢出",{"5":{"44":1}}],["激活节点",{"5":{"45":2}}],["激活区",{"5":{"71":2}}],["激活分布的稳定化",{"5":{"41":2}}],["激活函数将神经元的净输入映射到输出",{"5":{"47":1}}],["激活函数将线性变换的输出通过一个非线性函数进行变换",{"5":{"71":2}}],["激活函数通过引入非线性",{"5":{"47":2}}],["激活函数通过引入非线性打破了这一限制",{"5":{"47":2}}],["激活函数通过影响概率分布的形状",{"5":{"71":2}}],["激活函数为阈值函数",{"5":{"46":6}}],["激活函数负责非线性切割",{"5":{"45":2}}],["激活函数对这种频率分解信息的处理决定了位置信息如何在网络中传递",{"5":{"71":2}}],["激活函数在几何上对仿射变换的结果执行了逐元素的非线性",{"5":{"45":1}}],["激活函数在transformer架构中的应用体现了深度学习组件之间数学协同的设计哲学",{"5":{"71":2}}],["激活函数在这个几何框架中扮演着重要角色",{"5":{"71":2}}],["激活函数在这个过程中扮演关键角色",{"5":{"71":2}}],["激活函数在这一逼近能力中扮演着核心角色",{"5":{"71":2}}],["激活函数可以视为一种信息瓶颈",{"5":{"71":2}}],["激活函数可以被理解为对输入信息的某种非线性编码和变换",{"5":{"71":2}}],["激活函数类型与逼近效率",{"5":{"71":2}}],["激活函数构成的函数族在适当条件下可以满足这些要求",{"5":{"71":1}}],["激活函数是神经网络能够拟合复杂非线性函数的核心数学组件",{"5":{"71":2}}],["激活函数与位置编码的配合",{"2":{"71":1},"5":{"71":1}}],["激活函数与transformer架构的数学协同",{"2":{"71":1},"5":{"71":1}}],["激活函数与互信息最大化",{"2":{"71":1},"5":{"71":1}}],["激活函数与非线性数学",{"2":{"21":1,"85":1},"4":{"41":1,"42":1,"71":1},"5":{"21":7,"85":7}}],["激活函数作为函数逼近的基石",{"2":{"71":1},"5":{"71":1}}],["激活函数",{"5":{"41":1,"44":1,"45":3,"47":3,"71":5}}],["激活函数需要能够表示复杂的非线性映射",{"5":{"41":2}}],["激活函数导数的范数应该接近1",{"5":{"41":2}}],["激活函数的设计需要满足以下几个条件",{"5":{"47":2}}],["激活函数的几何作用",{"5":{"45":2}}],["激活函数的选择影响这个下界的紧致程度",{"5":{"71":2}}],["激活函数的选择与注意力机制的设计紧密相关",{"5":{"41":2}}],["激活函数的非线性变换增加了编码的丰富性",{"5":{"71":2}}],["激活函数的非线性变换正是实现这种信息筛选的机制",{"5":{"71":2}}],["激活函数的概率解释与损失函数的信息论基础共同构成了分类任务训练的数学框架",{"5":{"71":2}}],["激活函数的输出值较高",{"5":{"71":2}}],["激活函数的引入彻底改变了这一局面",{"5":{"71":2}}],["激活函数的引入打破了这种线性局限",{"5":{"71":2}}],["激活函数的信息几何视角",{"2":{"71":1},"5":{"71":1}}],["激活函数的信息压缩作用",{"2":{"71":1},"5":{"71":1}}],["激活函数的导数结构直接决定了梯度如何在不同层之间传播",{"5":{"42":2}}],["激活函数的导数性质是理解神经网络训练动力学的核心数学基础",{"5":{"42":2}}],["激活函数的导数不应该过大",{"5":{"41":2}}],["激活函数的研究仍在继续",{"5":{"41":2}}],["激活函数的计算成本直接影响训练和推理速度",{"5":{"41":2}}],["激活函数的饱和可以从两个层面理解",{"5":{"41":2}}],["激活函数的数学角色<",{"5":{"21":1,"85":1}}],["激活函数的数学角色",{"0":{"71":1},"2":{"47":1},"4":{"71":1},"5":{"21":4,"47":1,"71":1,"85":4}}],["激活函数设计的数学原则",{"2":{"41":1},"5":{"41":1}}],["理想情况下",{"5":{"89":2}}],["理想的情况是各个头学习到互补的信息",{"5":{"93":2}}],["理想的位置编码应当能够自然地外推到任意长度",{"5":{"88":2}}],["理想的稀疏模式应该保留注意力矩阵的",{"5":{"87":2}}],["理想的层次化学习应该满足",{"5":{"71":2}}],["理想的激活函数应该在整个输入范围内都有非零导数",{"5":{"41":2}}],["理论分析表明",{"5":{"97":4}}],["理论证明",{"5":{"88":2}}],["理论上单层注意力能够建模任意",{"5":{"92":2}}],["理论上比正弦编码更灵活",{"5":{"91":2}}],["理论上注意力矩阵",{"5":{"87":1}}],["理论上注意力矩阵的秩最多为​",{"5":{"87":1}}],["理论上",{"5":{"86":2,"87":2,"90":2,"92":2,"93":2}}],["理论上模型可以学习到条件期望的真实形式",{"5":{"51":2}}],["理论上可以拟合任意连续函数",{"5":{"47":2}}],["理论简洁",{"5":{"41":2}}],["理解logits的本质",{"5":{"96":2}}],["理解最大似然估计",{"5":{"96":2}}],["理解随机变量和概率分布的概念",{"5":{"96":2}}],["理解其数学原理对于深入掌握transformer架构",{"5":{"95":2}}],["理解其数学原理对于深入掌握位置编码的本质",{"5":{"91":2}}],["理解注意力机制的计算复杂度对于分析模型的可扩展性至关重要",{"5":{"95":2}}],["理解注意力权重矩阵的分布特性是分析注意力如何建模长程依赖的关键",{"5":{"92":2}}],["理解softmax的梯度特性对于训练深度神经网络至关重要",{"5":{"95":2}}],["理解query",{"5":{"94":4}}],["理解词序承载的信息",{"5":{"92":2}}],["理解多层堆叠如何影响长程依赖建模",{"5":{"92":2}}],["理解rope的数学本质",{"5":{"88":2}}],["理解rope的局限性对于正确使用它至关重要",{"5":{"88":2}}],["理解交叉熵的物理意义",{"5":{"61":2}}],["理解偏差",{"5":{"51":2}}],["理解偏导数",{"5":{"48":2}}],["理解为什么mse的最优解对应于目标向量在预测空间上的正交投影",{"5":{"51":2}}],["理解不同运算的资源需求对于优化模型设计和硬件利用至关重要",{"5":{"50":2}}],["理解张量运算的数学本质有助于编写高效的代码和诊断性能瓶颈",{"5":{"50":2}}],["理解它们的数学性质和计算特性对于优化模型性能和调试模型行为至关重要",{"5":{"50":2}}],["理解矩阵运算与逐样本计算的一致性",{"5":{"47":2}}],["理解仿射变换的几何本质",{"5":{"47":2}}],["理解了仿射变换与激活函数的组合如何实现从输入到输出的非线性映射",{"5":{"46":2}}],["理解前向传播的计算复杂度对于评估网络效率",{"5":{"45":1}}],["理解前向传播的数学本质",{"5":{"45":4}}],["理解反向传播的计算复杂度对于评估训练效率和设计高效算法至关重要",{"5":{"44":2}}],["理解这三个投影矩阵如何从输入嵌入中生成",{"5":{"95":2}}],["理解这两种情况的数学差异对于理解注意力的表达能力至关重要",{"5":{"94":2}}],["理解这种几何结构有助于理解位置编码如何影响注意力机制的行为",{"5":{"91":2}}],["理解这个矩阵的谱性质",{"5":{"87":2}}],["理解这个几何结构对于设计有效的优化算法至关重要",{"5":{"71":2}}],["理解这些优化性质对于设计和改进大语言模型的训练策略具有重要的理论和实践意义",{"5":{"97":2}}],["理解这些参数的选择动机对于深入掌握编码原理至关重要",{"5":{"91":2}}],["理解这些并行策略的数学基础",{"5":{"50":2}}],["理解这些原因对于理解现代大语言模型的架构设计至关重要",{"5":{"71":2}}],["理解这些差异是选择合适损失函数的基础",{"5":{"70":2}}],["理解这些损失函数的数学结构",{"5":{"99":2,"101":2}}],["理解这一关系对于正确使用交叉熵损失函数至关重要",{"5":{"61":2}}],["理解这一关系有助于我们从更高的视角理解梯度的计算原理",{"5":{"44":2}}],["理解这一几何本质对于分析神经网络的能力和局限性至关重要",{"5":{"47":2}}],["理解这一几何本质对于分析神经网络的表达能力和特征学习机制至关重要",{"5":{"45":2}}],["理解梯度流对于设计深层网络架构和训练策略至关重要",{"5":{"41":2}}],["理解梯度饱和的数学机制对于设计有效的激活函数和训练策略至关重要",{"5":{"41":2}}],["来引导模型学习",{"5":{"101":2}}],["来解决这个问题",{"5":{"97":1}}],["来调节softmax的",{"5":{"96":1}}],["来逼近复杂的后验分布",{"5":{"96":1}}],["来编码来自分布",{"5":{"96":1}}],["来控制总体错误率",{"5":{"96":2}}],["来近似后验分布",{"5":{"96":2}}],["来近似",{"5":{"96":2}}],["来近似它",{"5":{"69":2}}],["来代替",{"5":{"95":2}}],["来确保在训练初期具有较好的条件数",{"5":{"94":2}}],["来传递",{"5":{"94":2}}],["来匹配",{"5":{"94":2}}],["来决定各专家的权重",{"5":{"93":2}}],["来建模不同类型的依赖关系",{"5":{"92":2}}],["来建模超长距离的依赖关系",{"5":{"92":2}}],["来防止位置",{"5":{"91":1}}],["来防止位置关注位置的信息",{"5":{"91":1}}],["来隐式地编码位置",{"5":{"88":2}}],["来隐式地实现类似的目标",{"5":{"69":2}}],["来减少内存带宽消耗",{"5":{"86":2}}],["来降低计算复杂度",{"5":{"86":2}}],["来获取每个样本的损失值",{"5":{"61":2}}],["来组织偏导数",{"5":{"48":2}}],["来估计",{"5":{"71":1}}],["来保持激活的稳定性",{"5":{"41":1}}],["来稳定训练",{"5":{"41":2,"87":2}}],["来自矩阵乘法",{"5":{"45":1}}],["来自数据分布",{"5":{"69":2}}],["来自网络的直接输出",{"5":{"70":2}}],["来自左边第个token的信息",{"5":{"99":1,"101":1}}],["来自左边第",{"5":{"99":1,"101":1}}],["来自右边第个token的信息",{"5":{"99":1,"101":1}}],["来自右边第",{"5":{"99":1,"101":1}}],["来度量预测与真实分布的差异",{"5":{"69":2}}],["来理解任务",{"5":{"99":2,"101":2}}],["来预测偏好概率",{"5":{"99":1,"101":1}}],["研究者发现某些头专注于学习局部上下文信息",{"5":{"93":2}}],["研究者发现训练良好的transformer确实存在这种",{"5":{"92":2}}],["研究者们提出了大量的注意力机制变体",{"5":{"86":2}}],["研究其频谱特性与语言规律的关系",{"5":{"88":2}}],["研究发现",{"5":{"48":2}}],["研究表明",{"5":{"41":2,"48":2,"71":2,"97":2}}],["研究的是梯度在深度网络中传播时的行为",{"5":{"41":2}}],["混合专家",{"5":{"93":2}}],["混合编码的秩最多为",{"5":{"89":1}}],["混合编码的秩最多为​",{"5":{"89":1}}],["混合编码的训练策略可以是",{"5":{"89":2}}],["混合编码的函数空间是正弦编码函数空间与可学习编码函数空间的和集",{"5":{"89":2}}],["混合位置编码结合了可学习编码和正弦编码的优点",{"5":{"89":2}}],["混合位置编码",{"5":{"88":2}}],["混合精度计算",{"5":{"93":2}}],["混合精度利用了现代gpu对低精度运算的专门优化",{"5":{"50":2}}],["混合精度训练使用两种数值精度",{"5":{"97":2}}],["混合精度训练使用半精度浮点数进行计算以节省内存和加速",{"5":{"48":2}}],["混合精度训练与损失缩放",{"2":{"97":1},"5":{"97":1}}],["混合精度训练可以将内存消耗减半",{"5":{"50":2}}],["混合精度训练是另一种重要的效率优化技术",{"5":{"50":2}}],["混合精度训练",{"5":{"44":3,"97":4}}],["混合精度训练中的梯度缩放",{"5":{"44":2}}],["混合积性质",{"5":{"50":2}}],["混合",{"5":{"41":2}}],["更在理论上展示了如何通过参数共享和并行计算来实现丰富的表示学习",{"5":{"93":2}}],["更一般地",{"5":{"92":2}}],["更多的信息传递任务",{"5":{"92":2}}],["更多的频率成分",{"5":{"90":2}}],["更简单的不等式分析",{"5":{"91":2}}],["更简单地",{"5":{"87":2}}],["更低的最低频率",{"5":{"90":2}}],["更高效的实现利用了以下观察",{"5":{"88":2}}],["更深层次的问题在于",{"5":{"88":2}}],["更深刻地反映了机器学习问题的数学本质",{"5":{"51":2}}],["更深刻揭示了大语言模型设计的数学根基",{"5":{"69":2}}],["更精细的块大小选择",{"5":{"86":2}}],["更新输出",{"5":{"86":2}}],["更新行和",{"5":{"86":2}}],["更新后的输出为",{"5":{"86":2}}],["更新规则为",{"5":{"48":2}}],["更可靠",{"5":{"51":2}}],["更重要的是可以避免在中间步骤中存储",{"5":{"61":2}}],["更重要的是",{"5":{"47":2,"70":2,"88":2,"91":2}}],["更会错失利用矩阵运算简化计算的机会",{"5":{"46":2}}],["更能为分析网络的表达能力",{"5":{"45":2}}],["更抽象",{"5":{"71":1}}],["更语义化的特征",{"5":{"71":2}}],["更有价值",{"5":{"71":2}}],["更强",{"5":{"42":2}}],["更接近置换或选择矩阵",{"5":{"41":2}}],["更是对位置信息与注意力机制关系的根本性重新思考",{"5":{"88":2}}],["更是一个几何变换过程",{"5":{"45":2}}],["更是在隐式地最大化输入不同部分之间的互信息",{"5":{"69":2}}],["更快的学习",{"5":{"69":2}}],["更稳定但可能收敛慢",{"5":{"69":2}}],["更合适",{"5":{"70":2}}],["更容易训练",{"5":{"70":2}}],["更复杂的网络结构如树状张量网络",{"5":{"50":2}}],["更复杂的掩码策略",{"5":{"99":2,"101":2}}],["所示",{"5":{"93":2,"95":2}}],["所谓长程依赖",{"5":{"92":2}}],["所谓",{"5":{"87":2}}],["所以全1向量",{"5":{"87":1}}],["所以全1向量是关于特征值1的特征向量",{"5":{"87":1}}],["所以1是特征值",{"5":{"87":2}}],["所需的额外比特数",{"5":{"61":2}}],["所界定",{"5":{"47":1}}],["所有的query",{"5":{"94":2}}],["所有头的输出张量",{"5":{"93":1}}],["所有头的输出张量​沿头维度拼接",{"5":{"93":1}}],["所有头共享同一个key和value",{"5":{"93":2}}],["所有频率成分的相位差都较小",{"5":{"91":2}}],["所有权重约",{"5":{"89":2}}],["所有旋转操作的集合关于矩阵乘法构成一个群",{"5":{"88":2}}],["所有奇异值相近",{"5":{"89":2}}],["所有奇异值为1",{"5":{"87":2}}],["所有奇异值约为1",{"5":{"87":2}}],["所有元素都可能非零",{"5":{"87":2}}],["所有元素大于零",{"5":{"87":2}}],["所有",{"5":{"87":2}}],["所有可能的预测向量",{"5":{"51":1}}],["所有可能的预测向量形成",{"5":{"51":1}}],["所有可能的预测向量形成空间中的一条直线",{"5":{"51":1}}],["所有可能的预测向量形成空间中的一个子流形",{"5":{"51":1}}],["所有特征值的模不超过1",{"5":{"87":2}}],["所有特征值大于零",{"5":{"48":2}}],["所有特征值位于单位圆内",{"5":{"41":2}}],["所有参数的梯度",{"5":{"44":2}}],["所有指数项趋向于1",{"5":{"71":2}}],["所有样本的表示趋向于聚集在一起",{"5":{"69":2}}],["所有位置对之间的依赖",{"5":{"92":2}}],["所有位置编码向量的范数都相等",{"5":{"91":2}}],["所有位置的表示变得相似",{"5":{"87":2}}],["所有位置",{"5":{"69":2,"92":2}}],["所有位置都参与注意力计算",{"5":{"69":2}}],["所有键的概率相近实践中温度参数的选择",{"5":{"69":1}}],["所有键的概率相近",{"5":{"69":1}}],["所有这些正交条件意味着",{"5":{"90":1}}],["所有这些正交条件意味着对于所有成立",{"5":{"90":1}}],["所有这些损失函数都共享softmax",{"5":{"99":2,"101":2}}],["所揭示的深刻数学原理",{"5":{"71":2}}],["9之间",{"5":{"95":2}}],["96",{"5":{"46":2}}],["9",{"2":{"44":1,"50":1,"51":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1},"5":{"41":2,"44":3,"45":4,"47":2,"50":1,"51":1,"71":2,"86":1,"87":1,"88":1,"91":6,"92":1,"93":1,"94":1,"95":1,"99":2,"101":2}}],["9的预测误差在",{"5":{"70":2}}],["9更",{"5":{"70":2}}],["8",{"2":{"44":1,"50":1,"51":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"96":1},"5":{"41":2,"44":3,"45":4,"47":2,"50":1,"51":1,"71":2,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"96":1,"99":2,"101":2}}],["google的研究团队发现",{"5":{"88":2}}],["g",{"5":{"86":9,"88":3,"97":6}}],["gv",{"5":{"86":3}}],["gk",{"5":{"86":3}}],["gq",{"5":{"86":3}}],["gqa",{"5":{"86":2}}],["group",{"5":{"88":2,"90":2}}],["grouped",{"5":{"86":2,"93":4}}],["gram矩阵编码了所有位置对之间的相似性信息",{"5":{"89":2}}],["gram矩阵的第",{"5":{"87":2}}],["gram矩阵的第个元素是向量和的内积",{"5":{"87":1}}],["gram矩阵的第个元素就是向量和之间的余弦相似度",{"5":{"87":1}}],["gram矩阵的秩等于输入矩阵的秩",{"5":{"87":2}}],["gram矩阵只是点积的直接计算",{"5":{"87":2}}],["gram矩阵是半正定矩阵",{"5":{"87":2}}],["gram矩阵是半正定的",{"5":{"86":1}}],["gram矩阵是线性的",{"5":{"87":2}}],["gram矩阵是度量学习",{"5":{"87":1}}],["gram矩阵是向量集合中两两点积的矩阵",{"5":{"87":2}}],["gram矩阵",{"5":{"86":1,"87":1}}],["gram",{"5":{"50":2,"87":2}}],["grad",{"5":{"48":4}}],["gradient",{"5":{"41":2,"44":8,"48":2,"50":2,"70":2}}],["graph",{"5":{"44":3,"45":12,"47":8,"48":9,"50":6,"61":3,"92":2,"96":9}}],["global",{"5":{"86":2}}],["gibbs不等式",{"5":{"61":2}}],["gauss",{"5":{"96":4}}],["gaussian",{"5":{"96":2}}],["gating",{"5":{"93":2}}],["gated",{"5":{"86":4}}],["gan",{"5":{"50":2}}],["gpt",{"5":{"88":4}}],["gpt规模模型的flops估算",{"5":{"45":2}}],["gpu对密集矩阵乘法有高度优化的实现",{"5":{"86":2}}],["gpu可以同时计算批次中所有样本的注意力分数",{"5":{"50":2}}],["gpu的并行计算能力得到充分利用",{"5":{"47":2}}],["gpu共享内存",{"5":{"45":1}}],["gpu线程",{"5":{"45":1}}],["gpu线程块",{"5":{"45":1}}],["gumbel分布是极值分布的一种",{"5":{"71":2}}],["gumbel",{"2":{"71":1},"5":{"71":15}}],["g^t",{"5":{"42":2}}],["generating",{"5":{"96":2}}],["gemm",{"5":{"95":2}}],["geometry",{"5":{"61":4,"70":2}}],["geoffrey",{"5":{"44":2}}],["geq",{"5":{"42":4}}],["gelu激活函数",{"5":{"47":2}}],["gelu提供token内部的特征非线性",{"5":{"71":2}}],["gelu",{"5":{"42":6}}],["gelu等激活函数在理论上都具有同等的表达能力",{"5":{"71":2}}],["gelu等",{"5":{"42":2}}],["gelu等变体通过不同的策略处理负区域",{"5":{"41":2}}],["gelu可以平滑地处理这些概率信息",{"5":{"41":2}}],["gelu被选择作为transformer的激活函数",{"5":{"41":2}}],["gelu的定义",{"5":{"71":1}}],["gelu的定义表明它是输入与其通过标准正态累积分布函数权重的乘积",{"5":{"71":1}}],["gelu的非线性变换将这种位置感知的嵌入转换为新的表示",{"5":{"71":2}}],["gelu的导数不会过于接近零",{"5":{"71":2}}],["gelu的导数",{"5":{"71":2}}],["gelu的平滑特性有助于保持相邻位置信息的连续性",{"5":{"71":4}}],["gelu的平滑性和概率解释使其成为自然的选择",{"5":{"41":2}}],["gelu的设计考虑了这一点",{"5":{"41":2}}],["gelu涉及误差函数的计算",{"5":{"41":2}}],["gelu通过引入高斯分布相关的非线性",{"5":{"41":2}}],["gt",{"5":{"42":5}}],["较尖锐的分布",{"5":{"95":2}}],["较平坦的分布",{"5":{"95":2}}],["较少的头数意味着更大的头维度",{"5":{"93":2}}],["较多的头数意味着更多的子空间并行计算",{"5":{"93":2}}],["较大",{"5":{"88":4,"92":2}}],["较大的特征值表示该方向上损失变化剧烈",{"5":{"97":2}}],["较大的应用场景",{"5":{"95":1}}],["较大的奇异值对应的方向会被放大",{"5":{"94":2}}],["较大的奇异值对应于",{"5":{"71":2}}],["较大的​允许处理更长的序列",{"5":{"89":1}}],["较大的",{"5":{"41":1,"69":3,"89":1}}],["较大的允许快速学习但可能容忍不稳定",{"5":{"41":1}}],["较大的使得分布更加",{"5":{"69":1}}],["较大时",{"5":{"41":1,"86":1,"90":1,"91":1,"95":2,"99":1,"101":1}}],["较小",{"5":{"88":2,"91":2,"92":2}}],["较小的特征值表示该方向上损失变化平缓",{"5":{"97":2}}],["较小的奇异值对应的方向会被压缩",{"5":{"94":2}}],["较小的奇异值对应于",{"5":{"71":2}}],["较小的",{"5":{"41":3,"69":2}}],["较小的值表示激活函数的导数接近1",{"5":{"41":1}}],["较小的值表示激活函数具有更好的抗饱和性能",{"5":{"41":1}}],["较小的确保稳定性但可能限制学习速度",{"5":{"41":1}}],["较小的产生较大的梯度",{"5":{"69":1}}],["较小的使得分布更加",{"5":{"69":1}}],["较小时",{"5":{"41":3,"91":1,"95":1}}],["较低层",{"5":{"45":2}}],["较低的",{"5":{"69":2}}],["较低的掩码率",{"5":{"99":2,"101":2}}],["较高层",{"5":{"45":2}}],["较高的掩码率",{"5":{"99":2,"101":2}}],["按32位浮点数计算",{"5":{"95":2}}],["按头进行拆分",{"5":{"93":1}}],["按frobenius范数",{"5":{"89":2}}],["按列优先的顺序排列矩阵元素",{"5":{"50":2}}],["按列分割",{"5":{"45":1}}],["按列",{"5":{"45":1}}],["按后向顺序计算可以保证每次计算时所需的依赖值已经就绪",{"5":{"45":2}}],["按拓扑排序遍历可以保证计算每个节点时",{"5":{"45":2}}],["按从输入到输出的顺序依次计算各节点的值",{"5":{"45":2}}],["按奇异值拉伸或压缩各方向",{"5":{"45":2}}],["按元素应用",{"5":{"45":2}}],["按值裁剪",{"5":{"44":2}}],["按值裁剪将每个梯度元素裁剪到",{"5":{"41":1,"97":2}}],["按值裁剪将每个梯度元素裁剪到区间",{"5":{"41":1}}],["按降序排列",{"5":{"41":2}}],["按范数裁剪",{"5":{"44":2}}],["按范数裁剪更为常用",{"5":{"41":2,"97":2}}],["按范数裁剪保持了梯度的方向信息",{"5":{"41":2,"97":2}}],["按范数裁剪保持梯度的方向",{"5":{"41":2,"97":2}}],["按范数裁剪与按值裁剪",{"5":{"41":2}}],["部分",{"5":{"41":2,"45":1}}],["每组有",{"5":{"93":1}}],["每组有个头",{"5":{"93":1}}],["每层都进行自注意力计算和前馈网络变换",{"5":{"92":2}}],["每层的激活输出为",{"5":{"41":2,"42":2}}],["每行是一个概率分布",{"5":{"93":2}}],["每行是一个位置的编码",{"5":{"89":2}}],["每行是有效的概率分布",{"5":{"87":2}}],["每行只有一个非零元素",{"5":{"87":2}}],["每行的注意力模式可能完全不同",{"5":{"87":2}}],["每行的非零模式相同",{"5":{"87":2}}],["每行向量模长为1",{"5":{"87":2}}],["每步重新加载",{"5":{"92":2}}],["每步进行矩阵乘法",{"5":{"92":2}}],["每步",{"5":{"86":1}}],["每种分解都有其特定的应用场景和数值性质",{"5":{"50":2}}],["每种操作都有预定义的梯度计算规则",{"5":{"48":2}}],["每次只使用一小批样本来估计梯度",{"5":{"96":2}}],["每次只考虑一个正样本和个负样本",{"5":{"69":1}}],["每次只考虑一个正样本和",{"5":{"69":1}}],["每次计算一行都需要遍历整个序列",{"5":{"92":2}}],["每次训练迭代",{"5":{"44":2}}],["每经过若干个epoch将学习率降低一个因子",{"5":{"48":2}}],["每头维度",{"5":{"45":2}}],["每个片段既有其",{"5":{"95":2}}],["每个投影矩阵定义了一个从输入空间到输出空间的线性变换",{"5":{"94":2}}],["每个投影矩阵定义了从输入空间到各自子空间的一个线性映射",{"5":{"94":2}}],["每个词元首先通过词嵌入",{"5":{"94":2}}],["每个词被映射为一个高维实数向量",{"5":{"50":2}}],["每个基函数",{"5":{"93":1}}],["每个基函数​将输入矩阵映射到一个的注意力输出",{"5":{"93":1}}],["每个可以并行执行",{"5":{"93":1}}],["每个可以并行执行个线程束",{"5":{"93":1}}],["每个滤波器负责检测输入中的某种局部模式",{"5":{"93":2}}],["每个输入维度只贡献给特定的头",{"5":{"93":2}}],["每个输出位置可以依赖于整个输入序列",{"5":{"92":2}}],["每个输出位置只依赖于输入的一个局部窗口",{"5":{"92":2}}],["每个输出神经元与所有输入神经元相连",{"5":{"46":2}}],["每个输出元素需要",{"5":{"45":1}}],["每个输出元素需要次乘法和​次加法",{"5":{"45":1}}],["每个核函数关注输入数据的不同方面",{"5":{"93":2}}],["每个注意力头",{"5":{"93":1}}],["每个注意力头的计算可以视为一个独立的",{"5":{"93":2}}],["每个注意力头的计算与单头注意力相同",{"5":{"93":1}}],["每个注意力头都有自己独立的query",{"5":{"93":2}}],["每个注意力层学习token之间的交互模式",{"5":{"71":2}}],["每个频率成分具有相同的能量",{"5":{"90":2}}],["每个频率成分定义了一个独立的复平面",{"5":{"90":2}}],["每个频率成分定义了一个独立的圆群",{"5":{"90":2}}],["每个频率成分对应一个在单位圆上匀速运动的点",{"5":{"90":2}}],["每个是一个复指数序列",{"5":{"90":1}}],["每个是位置的编码向量",{"5":{"89":1}}],["每个配对对应一个特定的频率成分",{"5":{"90":2}}],["每个维度使用",{"5":{"89":1}}],["每个维度使用位精度",{"5":{"89":1}}],["每个",{"5":{"88":1,"89":1,"90":1}}],["每个的旋转块独立运作",{"5":{"88":1}}],["每个块处理序列的一个子集",{"5":{"93":2}}],["每个块对应一个头的投影",{"5":{"93":2}}],["每个块",{"5":{"88":1}}],["每个块是一个的旋转矩阵",{"5":{"88":1}}],["每个块要么完全参与注意力计算",{"5":{"86":2}}],["每个d维的embedding向量被成对地组织为",{"5":{"88":2}}],["每个微批次独立计算",{"5":{"50":2}}],["每个边代表张量的一个维度",{"5":{"50":2}}],["每个节点是一个张量",{"5":{"50":2}}],["每个秩一矩阵对应一个主成分方向",{"5":{"50":2}}],["每个索引对应张量的一个维度",{"5":{"50":2}}],["每个中间变量的贡献是它对",{"5":{"48":1}}],["每个中间变量的贡献是它对的偏导数与它对的偏导数的乘积之和",{"5":{"48":1}}],["每个参数分量根据其在梯度中的值进行调整",{"5":{"48":2}}],["每个样本为",{"5":{"47":1}}],["每个样本为维向量",{"5":{"47":1}}],["每个子矩阵",{"5":{"45":1}}],["每个子矩阵​可以分配给不同的设备",{"5":{"45":1}}],["每个sigmoid神经元",{"5":{"71":1}}],["每个sigmoid神经元在输入空间中形成一个",{"5":{"71":1}}],["每个激活函数",{"5":{"71":1}}],["每个激活函数在输入空间中定义了一个",{"5":{"71":1}}],["每个神经元本质上是在输入空间中划分一个",{"5":{"71":2}}],["每个神经元",{"5":{"71":2}}],["每个位置可以直接与所有其他位置交互",{"5":{"93":2}}],["每个位置可以直接访问序列中的所有其他位置",{"5":{"92":2}}],["每个位置可以与序列中的所有其他位置直接交互",{"5":{"92":2}}],["每个位置只能与一个固定大小窗口内的位置交互",{"5":{"92":2}}],["每个位置只关注邻近位置",{"5":{"87":2}}],["每个位置只关注其左右各",{"5":{"86":1}}],["每个位置只关注其左右各个位置",{"5":{"86":1}}],["每个位置有唯一的编码",{"5":{"91":2}}],["每个位置有一个独立的位置向量",{"5":{"88":2}}],["每个位置对应球面上的一个点",{"5":{"91":2}}],["每个位置对应唯一的相位向量",{"5":{"90":1}}],["每个位置是一个独特的编码向量",{"5":{"90":2}}],["每个位置关注若干随机位置或选定的",{"5":{"87":2}}],["每个位置关注若干随机选择的位置",{"5":{"86":2}}],["每个位置关注所有位置",{"5":{"87":2}}],["每个位置主要关注邻近位置",{"5":{"87":2}}],["每个位置的旋转角度较小",{"5":{"88":2}}],["每个位置的",{"5":{"86":2}}],["每个位置通过softmax聚合其他位置的信息",{"5":{"69":2}}],["每个位置通过编码函数映射到query和key",{"5":{"69":1}}],["每个位置",{"5":{"69":1,"90":1}}],["每个位置以固定概率被独立地选为掩码位置",{"5":{"99":1,"101":1}}],["每个位置以固定概率",{"5":{"99":1,"101":1}}],["每个负样本被推离",{"5":{"69":2}}],["每个头可以学习不同的投影方向",{"5":{"94":2}}],["每个头可以专注于更细粒度的特征",{"5":{"93":2}}],["每个头可以专注于捕获输入数据的不同特征或不同类型的关联模式",{"5":{"93":2}}],["每个头内部的",{"5":{"93":1}}],["每个头内部的计算可以利用tensor",{"5":{"93":1}}],["每个头只使用自己的一组投影参数",{"5":{"93":2}}],["每个头定义了一种特定的关联模式检测器",{"5":{"93":2}}],["每个头独立进行",{"5":{"93":1}}],["每个头独立进行的注意力计算",{"5":{"93":1}}],["每个头独立地对自己的query",{"5":{"93":2}}],["每个头有独立的key和value投影",{"5":{"93":2}}],["每个头有更强的计算能力",{"5":{"93":2}}],["每个头有自己的参数",{"5":{"93":2}}],["每个头有其独立的query",{"5":{"69":1}}],["每个头处理输入的不同",{"5":{"93":2}}],["每个头的key和value可以缓存历史位置的结果",{"5":{"93":2}}],["每个头的注意力分数矩阵维度为",{"5":{"93":2}}],["每个头的query",{"5":{"93":1}}],["每个头的query向量​就位于张量的第层",{"5":{"93":1}}],["每个头的维度为​",{"5":{"93":1}}],["每个头的维度为",{"5":{"93":7,"94":2}}],["每个头的梯度",{"5":{"41":1}}],["每个头的梯度可能具有不同的范数尺度",{"5":{"41":1}}],["每个头学习到一种特定的",{"5":{"92":2}}],["每个头",{"5":{"69":1,"93":2}}],["每一步计算都需要读取前一步的隐藏状态",{"5":{"92":2}}],["每一列对应一个频率维度",{"5":{"88":2}}],["每一行都是一个有效的概率分布",{"5":{"87":4}}],["每一层仅进行仿射变换而不应用激活函数",{"5":{"47":2}}],["每一层的query",{"5":{"94":2}}],["每一层的每个神经元与前一层的所有神经元相连",{"5":{"46":2}}],["每一层的输出作为下一层的输入",{"5":{"94":2}}],["每一层的输出可以视为输入数据在该层定义的",{"5":{"45":2}}],["每一层的输入和输出都需要被保存用于反向传播",{"5":{"50":2}}],["每一层的输入分布更加稳定",{"5":{"41":2}}],["每一层网络执行了一次从输入空间到输出空间的变换",{"5":{"45":2}}],["每一层网络定义了一个从其输入空间到输出空间的映射",{"5":{"45":2}}],["每一层通过权重矩阵的转置将误差",{"5":{"44":2}}],["每一次非线性变换都扩展了网络能够表示的函数空间",{"5":{"71":2}}],["qkv投影的总参数量为",{"5":{"93":1}}],["qkv投影的总参数量为​",{"5":{"93":1}}],["qkv投影",{"5":{"93":2}}],["qk点积的尺度问题",{"5":{"41":2}}],["qr分解将矩阵分解为正交矩阵",{"5":{"50":1}}],["qr分解将矩阵分解为正交矩阵和上三角矩阵的乘积",{"5":{"50":1}}],["qr分解在线性最小二乘问题中有重要应用",{"5":{"50":2}}],["qr分解有多种计算方法",{"5":{"50":2}}],["qr分解",{"5":{"50":2}}],["quad",{"5":{"42":6,"46":4,"86":8,"97":6}}],["quadratic",{"5":{"70":2}}],["query空间和key空间的设计体现了",{"5":{"94":2}}],["query表达了信息需求的抽象表示",{"5":{"94":2}}],["query和key不必在原始嵌入空间中相似",{"5":{"94":2}}],["query和key矩阵",{"5":{"87":1}}],["query和key矩阵的维度为",{"5":{"87":2}}],["query和key矩阵和是输入嵌入经过线性变换得到的",{"5":{"87":1}}],["query和key向量的点积期望方差为",{"5":{"41":2}}],["query头",{"5":{"86":1}}],["query头属于组",{"5":{"86":1}}],["query块",{"5":{"86":2}}],["query",{"0":{"94":1},"4":{"94":1},"5":{"61":2,"69":2,"70":4,"71":2,"85":5,"86":6,"88":6,"91":2,"93":8,"94":7,"95":6,"97":4}}],["q^",{"5":{"69":4}}],["q",{"5":{"69":1,"86":15}}],["无",{"5":{"97":2}}],["无损",{"5":{"92":2}}],["无需改变算法框架",{"5":{"95":2}}],["无需经过中间层的变换",{"5":{"92":2}}],["无需经过中间节点",{"5":{"92":2}}],["无需经过中间节点的传递",{"5":{"92":2}}],["无需额外的投影层来调整维度",{"5":{"91":2}}],["无理数比率",{"5":{"90":2}}],["无界的",{"5":{"88":2}}],["无法从输出端有效传播回输入端",{"5":{"95":2}}],["无法用单头注意力表示",{"5":{"93":2}}],["无法关注位置",{"5":{"92":2}}],["无法并行化",{"5":{"92":2}}],["无法唯一区分所有位置",{"5":{"89":2}}],["无法捕捉更复杂的位置依赖模式",{"5":{"88":2}}],["无法捕获token之间的复杂依赖关系",{"5":{"71":2}}],["无法直接知道一个token在序列中的绝对位置",{"5":{"88":2}}],["无法直接用于优化",{"5":{"70":2}}],["无法表示某些负定或非定模式",{"5":{"86":2}}],["无法拟合复杂的非线性函数",{"5":{"47":2}}],["无法将类别0的点放在一侧而将类别1的点放在另一侧",{"5":{"47":2}}],["无法通过数据自动学习",{"5":{"47":2}}],["无论原始随机变量的分布是什么",{"5":{"96":2}}],["无论设置多少个头",{"5":{"93":2}}],["无论",{"5":{"92":1}}],["无论依赖关系出现在序列的哪个位置",{"5":{"92":2}}],["无论依赖关系的跨度如何",{"5":{"92":2}}],["无论与的距离有多远",{"5":{"92":1}}],["无论序列多长",{"5":{"87":2}}],["无论使用何种模型都无法减少",{"5":{"51":2}}],["无论预测偏高还是偏低",{"5":{"51":2}}],["无论是计算类别概率还是计算注意力权重",{"5":{"61":2}}],["无论是否满秩",{"5":{"50":2}}],["无论是方阵还是矩形阵",{"5":{"50":2}}],["无论是分类任务还是注意力聚合",{"5":{"69":2}}],["无论堆叠多少层",{"5":{"47":2}}],["无论堆叠多少层神经元",{"5":{"71":2}}],["无论权重如何调整",{"5":{"46":2}}],["无论训练数据有多少",{"5":{"71":2}}],["无论训练多长时间",{"5":{"71":2}}],["无衰减地传播",{"5":{"41":2}}],["无输入",{"5":{"41":2}}],["无激活函数",{"5":{"41":2}}],["无限多",{"5":{"51":2}}],["无限负样本",{"5":{"69":2}}],["无关的常数",{"5":{"61":2}}],["无关",{"5":{"70":1,"87":2,"88":2,"90":1,"91":1,"93":1,"97":1}}],["使学生模型能够学习教师模型的知识",{"5":{"96":2}}],["使模型能够区分不同位置的词元",{"5":{"92":2}}],["使模型完全崩溃",{"5":{"41":2}}],["使其在分类任务中比均方误差更容易优化",{"5":{"97":2}}],["使其均值接近0",{"5":{"96":2}}],["使其能够区分不同位置的词元",{"5":{"92":2}}],["使其更适合表示学习任务",{"5":{"69":2}}],["使注意力分数依赖于query和key之间的相对位置",{"5":{"91":2}}],["使注意力机制能够感知序列中各元素的绝对或相对位置",{"5":{"91":2}}],["使attention的内积天然等变于相对位移",{"5":{"88":2}}],["使梯度能够绕过复杂的非线性变换直接传播到较浅层",{"5":{"50":2}}],["使用高精度",{"5":{"97":2}}],["使用低精度",{"5":{"97":2}}],["使用指数移动平均代替历史梯度平方和",{"5":{"97":2}}],["使用指数移动平均来累积梯度平方",{"5":{"48":2}}],["使用余弦函数平滑地降低学习率",{"5":{"97":2}}],["使用的是gelu",{"5":{"94":2}}],["使用​的qkv投影",{"5":{"93":1}}],["使用非对称的频率或相位",{"5":{"91":2}}],["使用复数编码",{"5":{"90":2}}],["使用复数形式可以更优雅地表示正弦余弦位置编码",{"5":{"90":2}}],["使用三角恒等式",{"5":{"90":2}}],["使用欧拉公式",{"5":{"90":2}}],["使用较高的频率",{"5":{"90":2}}],["使用较低的频率",{"5":{"90":2}}],["使用这些向量与query",{"5":{"88":2}}],["使用不同尺度的基函数同时捕获全局和局部特征",{"5":{"90":2}}],["使用不同分辨率的位置编码在不同层共享",{"5":{"89":2}}],["使用不同频率的正弦和余弦函数来编码位置",{"5":{"88":2}}],["使用不确定性加权等",{"5":{"70":2}}],["使用核函数近似softmax变换",{"5":{"87":2}}],["使用随机矩阵近似",{"5":{"87":2}}],["使用了新的异步指令",{"5":{"86":2}}],["使用和",{"5":{"48":1}}],["使用代数规则显式推导导数公式",{"5":{"48":2}}],["使用有限差分近似导数",{"5":{"48":2}}],["使用齐次坐标",{"5":{"46":2}}],["使用两层神经网络可以解决异或问题",{"5":{"46":2}}],["使用莱布尼茨记号表示为",{"5":{"45":2}}],["使用推论2",{"5":{"44":2}}],["使用交叉熵或均方误差损失时",{"5":{"44":2}}],["使用sigmoid函数作为激活函数",{"5":{"71":2}}],["使用",{"5":{"42":1,"48":1,"70":2,"71":2,"93":1}}],["使用商的求导法则对",{"5":{"42":1}}],["使用商的求导法则对求导",{"5":{"42":1}}],["使用乘积法则对",{"5":{"42":1}}],["使用乘积法则对求导",{"5":{"42":1}}],["使用自身表示导数",{"5":{"42":1}}],["使用链式法则求导",{"5":{"42":2}}],["使用类似的推导",{"5":{"69":2}}],["使用注意力作为编码器",{"5":{"69":2}}],["使用标准的分类损失或序列生成损失",{"5":{"99":2,"101":2}}],["使用基于人类反馈的强化学习损失",{"5":{"99":2,"101":2}}],["使用人类对模型输出的偏好判断来训练一个奖励模型",{"5":{"99":2,"101":2}}],["使用人类编写的示范数据训练策略模型",{"5":{"99":1,"101":1}}],["使用人类编写的示范数据",{"5":{"99":1,"101":1}}],["使用奖励模型指导策略优化",{"5":{"99":2,"101":2}}],["使用变分推断",{"5":{"99":2,"101":2}}],["使激活函数能够适应不同的数据分布和任务需求",{"5":{"41":2}}],["使得特定的损失函数能够更好地发挥模型的能力",{"5":{"101":2}}],["使得优化过程更加稳定",{"5":{"97":2}}],["使得优化过程在这个几何框架下具有更清晰的理论性质",{"5":{"61":2}}],["使得生成的文本既流畅又符合语义逻辑",{"5":{"96":2}}],["使得梯度既不会太小",{"5":{"95":2}}],["使得语义相关的query和key在变换后的空间中具有较高的相似度",{"5":{"94":2}}],["使得语义上与位置相关的依赖能够被建模",{"5":{"91":2}}],["使得总的信息量最大化",{"5":{"93":2}}],["使得注意力机制具有很高的通用性",{"5":{"92":2}}],["使得注意力模式可以逐层组合",{"5":{"71":2}}],["使得当",{"5":{"92":2}}],["使得当​是",{"5":{"92":1}}],["使得当时",{"5":{"92":1}}],["使得后续层中",{"5":{"92":1}}],["使得后续层中变大",{"5":{"92":1}}],["使得后续的自注意力运算无法干净地分离位置信息和内容信息",{"5":{"88":2}}],["使得长序列注意力计算在实践中更加高效",{"5":{"92":2}}],["使得相位差",{"5":{"91":1}}],["使得位置和位置成为",{"5":{"92":1}}],["使得位置",{"5":{"91":1,"92":1}}],["使得位置只能",{"5":{"91":1}}],["使得位置信息能够被有效学习和泛化",{"5":{"91":2}}],["使得softmax后的注意力权重为0",{"5":{"91":2}}],["使得sigmoid和tanh激活后的输出方差保持稳定",{"5":{"41":2}}],["使得对于每个位置",{"5":{"92":2}}],["使得对于合理的序列长度",{"5":{"91":2}}],["使得对所有",{"5":{"88":1}}],["使得编码向量能够同时表达多个尺度的位置信息",{"5":{"90":2}}],["使得初始编码向量具有适当的范数",{"5":{"89":1}}],["使得每个位置编码可以表示为",{"5":{"89":2}}],["使得每行成为概率分布",{"5":{"87":2}}],["使得高频细节不再那么敏感于绝对位置值",{"5":{"88":2}}],["使得在语义相关或语法相关的位置上",{"5":{"95":2}}],["使得在给定的位置索引范围内",{"5":{"88":2}}],["使得在所有任务上都不差且至少一个任务上更好",{"5":{"99":1,"101":1}}],["使得处理超长序列成为可能",{"5":{"87":2}}],["使得看似低秩的运算实际上能够表示复杂函数",{"5":{"87":2}}],["使得基于梯度优化的训练过程具有良好的数值性质",{"5":{"61":1}}],["使得训练更加稳定高效",{"5":{"61":2}}],["使得训练深层网络成为可能",{"5":{"44":2}}],["使得信息的",{"5":{"61":2}}],["使得mse主要由异常样本决定",{"5":{"51":2}}],["使得损失函数达到最小值",{"5":{"51":2}}],["使得等效的变换矩阵具有接近1的特征值",{"5":{"50":2}}],["使得算法能够适应非平稳的目标函数",{"5":{"48":2}}],["使得更新方向更加平滑",{"5":{"48":2}}],["使得预测具有不确定性估计",{"5":{"48":2}}],["使得整体损失函数朝着下降的方向移动",{"5":{"48":2}}],["使得神经网络能够拟合任意复杂的非线性函数",{"5":{"47":2}}],["使得神经网络能够表示任意复杂的非线性映射关系",{"5":{"71":2}}],["使得神经元能够从数据中自动学习最优的连接权重",{"5":{"47":2}}],["使得函数族",{"5":{"71":1}}],["使得函数族能够在输入空间中形成",{"5":{"71":1}}],["使得不同变量对之间的相关性可以直接比较",{"5":{"96":2}}],["使得不同的头收敛到不同的解",{"5":{"93":2}}],["使得不同的参数化方式可以产生相同的函数",{"5":{"48":2}}],["使得不同维度之间的频率跨度合理",{"5":{"91":2}}],["使得不同位置的相位在大多数维度上不会过快收敛",{"5":{"88":2}}],["使得不同位置具有可区分的编码",{"5":{"71":2}}],["使得不同特征方向捕捉不同的信息",{"5":{"50":2}}],["使得不同层之间的信息传递更加可控",{"5":{"71":2}}],["使得和能够捕获输入之间的复杂依赖关系",{"5":{"71":1}}],["使得网络具有稀疏表示的能力",{"5":{"71":2}}],["使得它在实践中通常比sigmoid表现更好",{"5":{"42":2}}],["使得参数估计可以在无界的实数空间中进行",{"5":{"42":2}}],["使得",{"5":{"41":1,"45":2,"48":1,"50":3,"69":1,"71":3,"88":2,"92":1,"97":2,"99":4,"101":4}}],["使得模型难以学习到远距离位置之间的关联",{"5":{"92":2}}],["使得模型难以对位置进行过拟合",{"5":{"90":2}}],["使得模型不会过度拟合训练数据",{"5":{"61":2}}],["使得模型分布",{"5":{"61":2}}],["使得模型在训练数据上的损失函数值最小化",{"5":{"48":2}}],["使得模型能够生成多样化的回复",{"5":{"96":2}}],["使得模型能够稳定地训练",{"5":{"92":2}}],["使得模型能够学习多种类型的依赖关系",{"5":{"91":2}}],["使得模型能够学习位置与内容之间的复杂交互关系",{"5":{"91":2}}],["使得模型能够学习内容相关的依赖",{"5":{"91":2}}],["使得模型能够唯一标识每个位置",{"5":{"91":2}}],["使得模型能够区分不同位置的",{"5":{"70":2}}],["使得模型能够区分不同位置的token",{"5":{"99":2,"101":2}}],["使得模型能够区分",{"5":{"99":2,"101":2}}],["使得模型能够为不同任务学习不同的",{"5":{"99":2,"101":2}}],["使得我们可以使用线性代数的标准工具进行分析",{"5":{"47":2}}],["使得我们可以使用信息论的工具",{"5":{"69":2}}],["使得分布更加",{"5":{"69":2}}],["使得输出梯度更加平衡",{"5":{"41":2}}],["使得输出具有概率解释",{"5":{"70":2}}],["使得交叉熵梯度不受饱和影响",{"5":{"70":2}}],["使得正样本对的相似度高于负样本对",{"5":{"70":2}}],["使得真正依赖于位置的具体身份",{"5":{"99":1,"101":1}}],["使得能够更好地捕获位置与其上下文之间的相对关系",{"5":{"99":1,"101":1}}],["使我们能够理解不同损失函数的数学本质",{"5":{"69":2}}],["训练通常需要多个阶段",{"5":{"97":2}}],["训练中",{"5":{"97":2}}],["训练过程就是使用随机梯度下降等优化算法来最大化这个目标函数",{"5":{"96":2}}],["训练过程实际上是在调整",{"5":{"94":1}}],["训练过程实际上是在调整和",{"5":{"94":1}}],["训练过程实际上是在调整这些投影矩阵的奇异值结构",{"5":{"94":2}}],["训练过程会自然地推动各头学习互补的特征",{"5":{"93":2}}],["训练过程中",{"5":{"92":2}}],["训练良好的transformer中不同头的输出表示往往具有较低的相关性",{"5":{"93":2}}],["训练良好的transformer中不同头表现出不同的熵分布模式",{"5":{"92":2}}],["训练良好的transformer中的不同注意力头确实会发展出不同的",{"5":{"93":2}}],["训练良好的编码通常表现出快速衰减或阶梯状的奇异值分布",{"5":{"89":2}}],["训练良好的可学习位置编码通常表现出快速衰减或阶梯状的奇异值分布",{"5":{"89":2}}],["训练和测试序列长度相近",{"5":{"89":2}}],["训练数据被视为从某个未知的数据生成分布",{"5":{"61":2}}],["训练mse低但测试mse高",{"5":{"51":2}}],["训练mse也高",{"5":{"51":2}}],["训练得到的模型为",{"5":{"51":2}}],["训练集上的mse是总体mse的估计",{"5":{"51":2}}],["训练动态和泛化性能奠定理论基础",{"5":{"45":2}}],["训练停滞",{"5":{"44":1}}],["训练迭代的总复杂度",{"5":{"44":2}}],["训练越不稳定",{"5":{"41":2}}],["训练曲线剧烈振荡无法收敛",{"5":{"41":2}}],["训练不稳定甚至发散",{"5":{"41":2}}],["训练策略模型",{"5":{"99":1,"101":1}}],["训练奖励模型来预测偏好概率",{"5":{"99":1,"101":1}}],["训练奖励模型",{"5":{"99":1,"101":1}}],["进一步增加了损失景观的复杂性",{"5":{"97":2}}],["进一步增加模型复杂度会增加测试mse",{"5":{"51":2}}],["进一步将dropout解释为贝叶斯推断",{"5":{"96":2}}],["进一步优化了注意力计算的内存访问模式",{"5":{"92":2}}],["进一步观察",{"5":{"91":2}}],["进一步降低内存占用",{"5":{"86":2}}],["进入函数值更高的区域",{"5":{"48":2}}],["进入深度饱和区域",{"5":{"41":2}}],["进而使",{"5":{"61":2}}],["进而",{"5":{"51":2}}],["进而影响模型的收敛速度",{"5":{"42":2}}],["进而影响语言建模损失的收敛速度和最终性能",{"5":{"99":2,"101":2}}],["进行参数更新",{"5":{"97":2}}],["进行梯度估计时",{"5":{"96":2}}],["进行一系列线性变换和非线性变换的组合",{"5":{"93":1}}],["进行最终的线性变换",{"5":{"93":1}}],["进行某种操作",{"5":{"90":1}}],["进行平移",{"5":{"90":1}}],["进行qr分解",{"5":{"87":1}}],["进行了专门优化",{"5":{"86":2}}],["进行大部分计算",{"5":{"50":2}}],["进行非线性变换",{"5":{"47":1}}],["进行线性投影",{"5":{"47":1}}],["进行高度并行化的矩阵运算",{"5":{"47":2}}],["进行",{"5":{"41":1}}],["进行掩码",{"5":{"99":2,"101":2}}],["避免过度裁剪导致信息丢失",{"5":{"97":2}}],["避免在曲率变化剧烈的区域过早跳跃",{"5":{"97":2}}],["避免在随机初始化阶段受到过大梯度的干扰",{"5":{"48":2}}],["避免softmax函数进入饱和区域",{"5":{"95":2}}],["避免训练不稳定性",{"5":{"95":2}}],["避免极端的奇异性",{"5":{"94":2}}],["避免重复计算",{"5":{"93":2}}],["避免重复从全局内存读取",{"5":{"92":2}}],["避免了梯度饱和问题",{"5":{"97":2}}],["避免了训练过程中的梯度消失或梯度爆炸问题",{"5":{"94":2}}],["避免了分布外的位置编码",{"5":{"88":2}}],["避免了无限增长的累积量",{"5":{"86":2}}],["避免了数值下溢",{"5":{"61":2}}],["避免了数值溢出",{"5":{"61":2}}],["避免了某些位置主导信息流的问题",{"5":{"41":2}}],["避免了某些任务因梯度范数过大而主导训练",{"5":{"99":2,"101":2}}],["避免数值溢出或下溢",{"5":{"47":2}}],["避免存在大范围的饱和区域",{"5":{"41":2}}],["避免饱和现象的发生",{"5":{"95":2}}],["避免饱和",{"5":{"41":2}}],["避免某些路径成为",{"5":{"41":2}}],["避免梯度饱和区域",{"2":{"41":1},"5":{"41":1}}],["避免奖励模型的过拟合导致的策略崩溃",{"5":{"99":2,"101":2}}],["均匀程度",{"5":{"92":2}}],["均匀性",{"5":{"91":2}}],["均匀初始化",{"5":{"89":2}}],["均匀分布可用于权重初始化",{"5":{"96":2}}],["均匀分布的期望为",{"5":{"96":2}}],["均匀分布对应于",{"5":{"89":2}}],["均匀分布时取得最大值",{"5":{"61":1}}],["均匀分布",{"5":{"70":2,"89":2,"96":2}}],["均为",{"5":{"86":2,"93":2}}],["均为可微函数",{"5":{"45":1}}],["均值和对数方差",{"5":{"96":2}}],["均值和协方差相应地分块为",{"5":{"96":2}}],["均值为0",{"5":{"41":2,"95":2}}],["均值信息可能包含重要的信号",{"5":{"41":2}}],["均方误差可以使用较大学习率而不会导致不稳定",{"5":{"97":2}}],["均方误差具有恒定的hessian和良好的凸性",{"5":{"97":2}}],["均方误差定义为预测值与真实值之差的平方的平均值",{"5":{"51":2}}],["均方误差作为回归任务中最常用的损失函数",{"5":{"51":2}}],["均方误差损失",{"5":{"44":2}}],["均方误差",{"0":{"51":1},"4":{"51":1},"5":{"51":3,"70":4,"85":5,"97":1}}],["均方误差与交叉熵的数学结构对比",{"2":{"70":1},"5":{"70":1}}],["均方误差和交叉熵虽然都是衡量模型预测与真实值差异的函数",{"5":{"70":2}}],["均方误差的梯度饱和问题",{"5":{"97":2}}],["均方误差的梯度与误差项和输入特征的乘积成正比",{"5":{"70":2}}],["均方误差的强凸性系数为",{"5":{"97":2}}],["均方误差的数学结构具有几个显著特征",{"5":{"70":2}}],["均方误差的",{"5":{"70":2}}],["均方误差的恒定曲率意味着优化空间在所有方向上具有相同的",{"5":{"70":2}}],["均方误差梯度",{"5":{"70":2}}],["均方误差是凸函数",{"5":{"70":2,"97":2}}],["均方误差是强凸的",{"5":{"70":1}}],["均方误差是与回归任务自然匹配的损失函数",{"5":{"70":2}}],["均方误差是",{"5":{"70":1}}],["均方误差对学习率的敏感性相对较低",{"5":{"97":2}}],["均方误差对极端错误的惩罚是有限的",{"5":{"70":2}}],["均方误差对应于欧几里得距离的平方",{"5":{"70":1}}],["均方误差对应于",{"5":{"70":1}}],["均方误差从0增加到",{"5":{"70":2}}],["均方误差认为0",{"5":{"70":2}}],["均方误差源于统计学中的二次损失",{"5":{"70":1}}],["均方误差源于统计学中的",{"5":{"70":1}}],["映射到一个",{"5":{"93":1}}],["映射到",{"5":{"61":2}}],["映射到新的特征空间",{"5":{"47":1}}],["映射到输出",{"5":{"47":1}}],["映射到概率单纯形",{"5":{"69":1}}],["映射到query",{"5":{"69":1}}],["个词元的原始置信度评分",{"5":{"96":1}}],["个浮点数",{"5":{"95":1}}],["个键向量",{"5":{"95":1}}],["个键向量的点积相似度",{"5":{"95":1}}],["个查询向量",{"5":{"95":1}}],["个查询向量与第",{"5":{"95":1}}],["个子空间",{"5":{"93":1}}],["个线程束",{"5":{"93":1}}],["个流式多处理器",{"5":{"93":1}}],["个头定义的",{"5":{"93":1}}],["个头参与计算",{"5":{"93":1}}],["个头",{"5":{"93":2,"94":1}}],["个头的输出",{"5":{"93":1}}],["个头的输出拼接为",{"5":{"41":1}}],["个头的配置",{"5":{"93":1}}],["个头的投影矩阵按行堆叠",{"5":{"93":1}}],["个头的投影矩阵总参数量为",{"5":{"93":2}}],["个头的query",{"5":{"93":1}}],["个头的query投影矩阵",{"5":{"93":1}}],["个头的query投影",{"5":{"86":2}}],["个头的总空间为",{"5":{"93":2}}],["个头的总复杂度为",{"5":{"93":2}}],["个头的总参数量为",{"5":{"86":2}}],["个query头为一组",{"5":{"93":2}}],["个频率的周期参数",{"5":{"90":1}}],["个频率成分能够编码约",{"5":{"90":1}}],["个频率成分能够编码约​比特的信息",{"5":{"90":1}}],["个频率成分对应的旋转因子",{"5":{"90":1}}],["个频率成分在位置",{"5":{"90":1}}],["个频率成分的频率",{"5":{"90":1}}],["个频率成分的复指数序列",{"5":{"90":1}}],["个频率成分的复振幅",{"5":{"90":1}}],["个基向量的线性组合",{"5":{"89":1}}],["个维度几乎不携带信息",{"5":{"89":1}}],["个维度携带主要信息",{"5":{"89":1}}],["个正交基向量",{"5":{"89":1}}],["个秩",{"5":{"89":1}}],["个秩一张量的和",{"5":{"50":1}}],["个",{"5":{"88":1,"89":1,"91":2}}],["个独立的二维旋转组成",{"5":{"88":2}}],["个独立的子计算",{"5":{"45":1}}],["个复数",{"5":{"88":2}}],["个奇异值和对应的奇异向量来实现",{"5":{"89":1}}],["个奇异值",{"5":{"87":1}}],["个奇异值加上",{"5":{"87":1}}],["个奇异值加上个奇异向量",{"5":{"87":1}}],["个奇异向量",{"5":{"87":1}}],["个自由度",{"5":{"87":2,"93":1}}],["个位置的嵌入表示",{"5":{"94":1}}],["个位置的最小频率间隔",{"5":{"90":1}}],["个位置的输入embedding",{"5":{"88":1}}],["个位置编码约",{"5":{"90":1}}],["个位置",{"5":{"86":2,"89":1}}],["个事件",{"5":{"61":1}}],["个参数完全描述",{"5":{"87":1}}],["个参数",{"5":{"50":1,"89":1}}],["个参数而不是",{"5":{"50":1}}],["个索引",{"5":{"50":1,"86":1}}],["个类别的logit",{"5":{"96":1}}],["个类别上服从类别分布",{"5":{"96":1}}],["个类别上的概率分布",{"5":{"61":1}}],["个类别之一",{"5":{"47":1}}],["个元素表示第",{"5":{"95":1}}],["个元素是向量",{"5":{"87":1}}],["个元素就是向量",{"5":{"87":1}}],["个元素为",{"5":{"87":1,"89":1,"92":1,"96":1}}],["个元素",{"5":{"46":1}}],["个超平面",{"5":{"46":1}}],["个象限",{"5":{"45":1}}],["个坐标轴分割成",{"5":{"45":1}}],["个分量的乘积",{"5":{"95":1}}],["个分量",{"5":{"44":2,"92":1}}],["个神经元的层",{"5":{"47":1}}],["个神经元的输出归一化为概率分布",{"5":{"47":1}}],["个神经元实际上定义了",{"5":{"46":1}}],["个神经元",{"5":{"71":2}}],["个偏置",{"5":{"71":2}}],["个权重加",{"5":{"71":2}}],["个输入位置的",{"5":{"61":2}}],["个输入特征与第",{"5":{"46":1}}],["个输出位置时",{"5":{"61":2}}],["个输出神经元之间的连接强度",{"5":{"46":1}}],["个输出",{"5":{"45":1}}],["个样本属于第",{"5":{"61":2}}],["个样本中",{"5":{"61":2}}],["个样本的真实类别",{"5":{"61":1}}],["个样本的真实类别标签为",{"5":{"61":2}}],["个样本的数据集",{"5":{"61":1}}],["个样本的误差约为1",{"5":{"51":1}}],["个样本的权重",{"5":{"51":1}}],["个样本的预测值",{"5":{"51":1}}],["个样本的计算可以分解为",{"5":{"45":1}}],["个样本的计算可以分解为个独立的子计算",{"5":{"45":1}}],["个样本的第",{"5":{"45":1}}],["个样本",{"5":{"45":2,"47":2,"69":1}}],["个负样本",{"5":{"69":6}}],["个负样本键",{"5":{"69":1}}],["个候选样本",{"5":{"69":1}}],["个注意力头",{"5":{"69":1,"93":1}}],["个序列的训练集",{"5":{"99":1,"101":1}}],["个token的分布仅依赖于其前面的token",{"5":{"99":1,"101":1}}],["个token的信息",{"5":{"99":2,"101":2}}],["个任务的权重",{"5":{"70":1}}],["个任务",{"5":{"99":1,"101":1}}],["最常见的配置是令",{"5":{"94":2}}],["最常用的方法是加权求和",{"5":{"70":2}}],["最远",{"5":{"92":2}}],["最高频率​对应的周期仍然大于序列长度",{"5":{"91":1}}],["最高频率",{"5":{"90":1,"91":1}}],["最高频率对应周期1",{"5":{"90":1}}],["最低频率",{"5":{"90":1}}],["最低频率对应周期10000",{"5":{"90":1}}],["最低频率成分的周期小于序列长度",{"5":{"90":2}}],["最小特征值可能很小",{"5":{"97":2}}],["最小奇异值为0",{"5":{"87":4}}],["最小二乘估计",{"5":{"51":1}}],["最小二乘估计对应的预测为",{"5":{"51":1}}],["最小化上式等价于最小化",{"5":{"61":2}}],["最小化交叉熵等价于最小化kl散度",{"5":{"61":2}}],["最小化交叉熵等价于最大化数据的对数似然",{"5":{"96":2}}],["最小化交叉熵损失等价于最小化模型分布与数据分布之间的",{"5":{"61":2}}],["最小化",{"5":{"61":2}}],["最小化负对数似然等价于最大化对数似然",{"5":{"61":2}}],["最小化infonce损失等价于最大化互信息的下界",{"5":{"69":2,"71":2}}],["最小化nce损失等价于最小化数据分布与模型分布之间的kl散度",{"5":{"69":2}}],["最小化kl散度",{"5":{"69":2}}],["最小化的是",{"5":{"70":2}}],["最优学习率可以通过hessian的特征值精确计算",{"5":{"97":2}}],["最优逼近可以通过保留前",{"5":{"89":1}}],["最优逼近可以通过保留前个奇异值和对应的奇异向量来实现",{"5":{"89":1}}],["最优编码可能更",{"5":{"89":4}}],["最优编码的性质还取决于任务和数据分布",{"5":{"89":2}}],["最优低秩逼近",{"5":{"89":2}}],["最优解处的hessian矩阵应该是半正定的",{"5":{"89":2}}],["最优解满足梯度为零条件",{"5":{"89":2}}],["最优解满足一个关键的性质",{"5":{"51":1}}],["最优解",{"5":{"51":1}}],["最优的损失函数是什么呢",{"5":{"70":2}}],["最优预测向量",{"5":{"51":1}}],["最优预测向量是目标向量在预测空间上的正交投影",{"5":{"51":1}}],["最优预测为条件均值",{"5":{"70":2}}],["最优预测为条件众数",{"5":{"70":1}}],["最优预测为",{"5":{"70":1}}],["最广泛使用的损失函数之一",{"5":{"51":2}}],["最终性能的决定因素",{"5":{"97":2}}],["最终的矩阵乘法同样可以高效地批量处理",{"5":{"95":2}}],["最终学习到不对称的解",{"5":{"93":2}}],["最终通过拼接和投影进行融合",{"5":{"93":2}}],["最终输出是各专家输出的加权组合",{"5":{"93":2}}],["最终我们在山顶欣赏到了美丽的日出",{"5":{"92":2}}],["最终收敛到特定的分布",{"5":{"89":2}}],["最终得到上述三部分之和",{"5":{"51":2}}],["最终达到较好的收敛效果",{"5":{"48":2}}],["最终映射到输出空间",{"5":{"45":4}}],["最终产生网络输出的过程",{"5":{"45":2}}],["最终产生网络输出",{"5":{"44":2}}],["最大序列长度",{"5":{"89":2}}],["最大值在",{"5":{"42":2}}],["最大值在处取得",{"5":{"42":2}}],["最大似然估计的原理是选择使对数似然最大的参数值",{"5":{"61":1}}],["最大似然估计的基本原理",{"2":{"61":1},"5":{"61":1}}],["最大似然估计是统计学中参数估计的核心方法",{"5":{"61":2}}],["最大似然估计等价于最小化mse",{"5":{"51":2}}],["最大似然估计具有一些重要的渐近性质",{"5":{"96":2}}],["最大似然估计就是找到使对数似然最大的参数值",{"5":{"96":2}}],["最大似然估计",{"5":{"51":2,"61":1,"96":2}}],["最大化计算效率",{"5":{"88":2}}],["最大化对数似然等价于最小化",{"5":{"51":2}}],["最大化互信息的下界",{"5":{"69":2}}],["最大化专家行为的似然",{"5":{"99":2,"101":2}}],["最大化期望奖励同时保持与初始策略的接近",{"5":{"99":2,"101":2}}],["最陡点",{"5":{"42":2}}],["最后在输出空间中进行另一次旋转和反射",{"5":{"94":2}}],["最后建立了交叉熵与最大似然估计之间的深刻联系",{"5":{"61":2}}],["最后一层的误差信号为",{"5":{"51":2}}],["最后对所有样本的平方误差求平均",{"5":{"51":2}}],["最后通过与真实标签计算交叉熵得到损失值",{"5":{"61":2}}],["最后通过解码器重建原始输入",{"5":{"96":2}}],["最后通过对比损失",{"5":{"71":2}}],["最后旋转回原坐标系",{"5":{"45":2}}],["最后将梯度用于参数更新",{"5":{"44":2}}],["最后用这个分布对value进行加权平均",{"5":{"71":2}}],["最后",{"5":{"41":2,"44":2,"51":4,"71":2,"86":2,"87":2,"89":2,"90":2,"91":2,"92":2,"93":4,"97":2}}],["最后揭示了infonce与注意力机制中softmax操作的数学统一性",{"5":{"99":2,"101":2}}],["最相关",{"5":{"69":2}}],["最相似的键获得接近1的概率",{"5":{"69":1}}],["最佳均值预测",{"5":{"51":2}}],["最佳线性无偏估计",{"5":{"70":2}}],["样本mse与总体mse之间的关系可以用以下方差分解来表示",{"5":{"51":2}}],["样本mse收敛于总体mse",{"5":{"51":2}}],["样本mse是总体mse的无偏估计",{"5":{"51":2}}],["样本mse",{"5":{"51":2}}],["样本量很小时",{"5":{"96":2}}],["样本量很大时",{"5":{"96":2}}],["样本均值趋向于随机变量的期望",{"5":{"96":2}}],["样本级并行",{"5":{"45":3}}],["样本可以表示为确定函数",{"5":{"71":1}}],["样本",{"5":{"61":2,"71":1}}],["样本的行为是未定义的",{"5":{"99":2,"101":2}}],["类比于人类认知过程中会从多个角度",{"5":{"93":2}}],["类的",{"5":{"61":2,"69":1}}],["类的概率估计",{"5":{"71":1}}],["类",{"5":{"61":4}}],["类似argmax操作",{"5":{"96":2}}],["类似于moe模型",{"5":{"93":2}}],["类似于带状矩阵",{"5":{"87":2}}],["类似的计算对key和value重复进行",{"5":{"93":2}}],["类似的稳定性分析有助于理解和解决梯度消失或梯度爆炸问题",{"5":{"50":2}}],["类似地",{"5":{"41":2,"42":2,"87":2,"93":6,"94":4}}],["类别分布",{"5":{"96":2}}],["类别为1的条件概率",{"5":{"47":2}}],["类别概率",{"5":{"69":1}}],["以加速收敛",{"5":{"97":2}}],["以实现更好的优化效果",{"5":{"97":2}}],["以避免数值计算问题",{"5":{"95":2}}],["以机器翻译任务为例",{"5":{"95":2}}],["以",{"5":{"94":1}}],["以​的梯度为例",{"5":{"94":1}}],["以注入序列顺序信息",{"5":{"94":2}}],["以控制计算成本",{"5":{"93":2}}],["以卷积核大小为",{"5":{"92":1}}],["以卷积核大小为的卷积为例",{"5":{"92":1}}],["以提供一定的余量",{"5":{"89":2}}],["以提高数值稳定性和计算效率",{"5":{"61":2}}],["以提高计算效率",{"5":{"69":2}}],["以便于按位置查询",{"5":{"89":2}}],["以便利用gpu的高效并行计算能力",{"5":{"88":2}}],["以减少内存占用",{"5":{"89":2}}],["以兼取两者的优势",{"5":{"89":2}}],["以平衡表达能力和计算效率",{"5":{"87":1}}],["以2为底时",{"5":{"61":2}}],["以保持数值稳定性",{"5":{"50":2}}],["以匹配较大的张量",{"5":{"50":2}}],["以transformer模型为例",{"5":{"50":2}}],["以捕获更复杂的多模态特性",{"5":{"96":2}}],["以确保信号在各层之间平稳传播",{"5":{"96":2}}],["以确保梯度可以稳定地反向传播",{"5":{"41":2}}],["以计算换内存",{"5":{"48":2}}],["以此类推直到输入层",{"5":{"48":2}}],["以及它们在注意力机制中的数学角色",{"5":{"95":2}}],["以及什么样的内容应该作为",{"5":{"94":2}}],["以及这三个投影矩阵在数学上的性质和作用",{"5":{"94":2}}],["以及这种能力的来源和边界",{"5":{"92":2}}],["以及各种变体设计",{"5":{"93":2}}],["以及不同头的输出之间有多少冗余或互补信息",{"5":{"93":2}}],["以及不同头如何通过",{"5":{"92":2}}],["以及多强地存在依赖关系",{"5":{"92":2}}],["以及优化大语言模型的性能都具有重要意义",{"5":{"91":2}}],["以及编码空间的几何结构",{"5":{"91":2}}],["以及编码向量的点积与相对位置的数学关系",{"5":{"91":2}}],["以及内容与位置的交互",{"5":{"91":2}}],["以及后续学习旋转位置编码",{"5":{"90":2}}],["以及频率选择的最优性和正则化效应",{"5":{"90":2}}],["以及如何从复数旋转的角度理解位置变换",{"5":{"90":2}}],["以及时域与频域之间的对偶性",{"5":{"90":2}}],["以及复数形式的统一分析",{"5":{"90":2}}],["以及参数效率和实践考量",{"5":{"89":2}}],["以及表达能力与泛化能力之间的权衡",{"5":{"89":2}}],["以及纯位置项",{"5":{"88":2}}],["以及开发新算法奠定了坚实的基础",{"5":{"87":2}}],["以及其在计算优化和泛化能力方面的意义",{"5":{"87":2}}],["以及缩放因子在控制条件数中的作用",{"5":{"87":2}}],["以及与gram矩阵和核矩阵的关系",{"5":{"87":2}}],["以及设计更高效的变体都具有重要意义",{"5":{"87":2}}],["以及更好的线程分配策略",{"5":{"86":2}}],["以及注意力矩阵的谱性质与低秩结构",{"5":{"86":2}}],["以及总体mse与样本mse的区别",{"5":{"51":2}}],["以及评估词嵌入中捕获的语义信息量",{"5":{"96":2}}],["以及评估模型对不同类型输入的响应一致性",{"5":{"96":2}}],["以及在实际应用中的优化策略",{"5":{"89":2}}],["以及在不同场景下的适用性",{"5":{"86":2}}],["以及在神经网络中的梯度计算特性",{"5":{"51":2}}],["以及在变分自编码器中定义潜在空间的先验分布",{"5":{"96":2}}],["以及在概率图模型中作为共轭先验使用",{"5":{"96":2}}],["以及模型输出的不确定性估计",{"5":{"96":2}}],["以及某些正则化技术的理论基础",{"5":{"96":2}}],["以及归一性",{"5":{"96":2}}],["以及局部最小值通常具有相似的损失值等",{"5":{"48":2}}],["以及整个神经网络的矩阵表示形式",{"5":{"47":2}}],["以及计算效率",{"5":{"47":2}}],["以及实际实现中的数值稳定性问题",{"5":{"44":2}}],["以及个负样本​",{"5":{"69":1}}],["以及",{"5":{"69":1,"70":2,"93":2}}],["以及位置编码作为特征工程与损失函数优化的相互作用",{"5":{"70":2}}],["以使",{"5":{"45":1}}],["以relu激活函数为例",{"5":{"45":2}}],["以处理批量输入",{"5":{"44":2}}],["以二分类为例",{"5":{"44":2}}],["以类别",{"5":{"71":1}}],["以类别为基准",{"5":{"71":1}}],["以下是深度学习中常用的激活函数及其数学定义",{"5":{"47":2}}],["以下是一些有前景的未来方向",{"5":{"41":2}}],["以下",{"5":{"41":1}}],["以概率",{"5":{"61":2,"71":2}}],["以引入位置信息",{"5":{"69":2}}],["以获得更好的泛化能力",{"5":{"99":2,"101":2}}],["即该方向是",{"5":{"97":2}}],["即三个空间的维度与输入嵌入维度相同",{"5":{"94":2}}],["即输出是输入向左移动10位",{"5":{"92":2}}],["即位置",{"5":{"92":1}}],["即位置的注意力主要集中在哪些位置",{"5":{"92":1}}],["即位置0的编码",{"5":{"91":2}}],["即注意力分数显式依赖于相对位置",{"5":{"91":2}}],["即改变输入序列的顺序不会改变注意力输出的顺序",{"5":{"91":2}}],["即点沿圆周旋转角度",{"5":{"90":2}}],["即区分两个相近位置的能力取决于频率成分的选择",{"5":{"90":2}}],["即两个位置的",{"5":{"88":1}}],["即两个位置的相对距离",{"5":{"88":1}}],["即自注意力中query和key来自相同输入",{"5":{"87":2}}],["即指数核",{"5":{"86":2}}],["即每个位置旋转固定的角度",{"5":{"88":2}}],["即每个位置平均关注",{"5":{"86":1}}],["即每个位置平均关注个位置",{"5":{"86":1}}],["即每个样本只有一个正确的类别",{"5":{"61":2}}],["即是随机采样的子集",{"5":{"86":1}}],["即预测概率集中在真实类别上",{"5":{"61":2}}],["即给定样本",{"5":{"61":2}}],["即给定前文的条件下预测下一个词的对数概率之和",{"5":{"96":2}}],["即所有可能的预测向量",{"5":{"51":1}}],["即所有可能的预测向量的集合",{"5":{"51":1}}],["即最小化mse",{"5":{"51":2}}],["即最大的不确定性",{"5":{"96":2}}],["即对于所有维度",{"5":{"91":2}}],["即对于所有成立",{"5":{"90":1}}],["即对于任意",{"5":{"51":2}}],["即对于每个输入",{"5":{"51":2}}],["即对所有向量成立",{"5":{"50":1}}],["即通常不等于",{"5":{"50":1}}],["即在真实类别上赋予",{"5":{"96":1}}],["即在真实类别上赋予的概率",{"5":{"96":1}}],["即的支持集必须包含的支持集",{"5":{"96":1}}],["即一般情况下",{"5":{"96":2}}],["即经过",{"5":{"48":1}}],["即经过次迭代后",{"5":{"48":1}}],["即未中心化的方差",{"5":{"48":2}}],["即动量",{"5":{"48":2}}],["即误差按几何级数衰减",{"5":{"48":2}}],["即误差信号矩阵在批量维度上的平均值",{"5":{"44":2}}],["即存在方向",{"5":{"97":1}}],["即存在方向使得在时增加",{"5":{"97":1}}],["即存在一对位置",{"5":{"92":1}}],["即存在一对位置使得且它们之间存在依赖",{"5":{"92":1}}],["即存在",{"5":{"48":1}}],["即存在使得",{"5":{"48":1}}],["即存在常数",{"5":{"41":1}}],["即存在常数使得",{"5":{"41":1}}],["即​",{"5":{"45":1,"48":1,"95":1}}],["即为",{"5":{"44":2}}],["即损失函数对该层净输入的梯度",{"5":{"44":2}}],["即交叉熵损失的曲率正好由softmax分布的fisher信息度量给出",{"5":{"71":2}}],["即当",{"5":{"41":1}}],["即当时​",{"5":{"41":1}}],["即具有实质贡献的维度数",{"5":{"41":2}}],["即",{"5":{"41":2,"42":2,"44":2,"45":5,"46":2,"47":4,"48":5,"50":10,"71":4,"86":5,"87":2,"90":3,"91":2,"95":1,"96":3}}],["即进入显著饱和区域",{"5":{"41":2}}],["即使中间层的梯度很小",{"5":{"92":2}}],["即使某些位置的权重很小",{"5":{"92":2}}],["即使某些位置被掩码遮挡",{"5":{"91":2}}],["即使采用lstm或gru等门控机制缓解这一问题",{"5":{"92":2}}],["即使最低频率成分出现折叠",{"5":{"90":2}}],["即使最终能够表示相同的编码向量",{"5":{"89":2}}],["即使位置索引超出了训练范围",{"5":{"88":2}}],["即使没有发生溢出",{"5":{"61":2}}],["即使所有原假设都为真",{"5":{"96":2}}],["即使很大的差异也可能不显著",{"5":{"96":2}}],["即使很小",{"5":{"41":1}}],["即使是微小的差异也可能是统计显著的",{"5":{"96":2}}],["即使这些最小值在参数空间中相距甚远",{"5":{"48":2}}],["即使不能保证达到全局最优",{"5":{"48":2}}],["即使网络包含多个隐藏层",{"5":{"71":2}}],["即使输入梯度很小",{"5":{"41":2}}],["即使的梯度完全消失",{"5":{"41":1}}],["即使",{"5":{"41":2,"61":2,"70":2}}],["即使在概率接近",{"5":{"70":2}}],["即使当前解远离最优解",{"5":{"70":2}}],["即使误差很大",{"5":{"70":1}}],["即使误差",{"5":{"70":1}}],["即不存在另一个解在所有任务上都不差且至少一个任务上更好",{"5":{"70":2}}],["即模型无法区分序列的不同排列顺序",{"5":{"99":2,"101":2}}],["即相对于参考策略",{"5":{"99":2,"101":2}}],["考虑未缩放的注意力分数",{"5":{"94":2}}],["考虑​的第一列",{"5":{"94":1}}],["考虑需要存储的中间结果",{"5":{"93":2}}],["考虑多头注意力中",{"5":{"93":1}}],["考虑多头注意力中个头的query投影矩阵​的集合",{"5":{"93":1}}],["考虑多分类交叉熵损失",{"5":{"61":2}}],["考虑多分类场景",{"5":{"70":2}}],["考虑损失函数",{"5":{"92":2}}],["考虑损失函数关于​的梯度",{"5":{"92":1}}],["考虑损失函数关于​的梯度​",{"5":{"92":1}}],["考虑从时刻",{"5":{"92":1}}],["考虑从时刻到时刻的信息传递",{"5":{"92":1}}],["考虑计算复杂度的对比",{"5":{"92":2}}],["考虑维度",{"5":{"91":1}}],["考虑维度和的配对",{"5":{"91":1}}],["考虑能够唯一表示长度",{"5":{"90":1}}],["考虑能够唯一表示长度序列所需的最低频率",{"5":{"90":1}}],["考虑第个头的输出携带了多少关于输入的信息",{"5":{"93":1}}],["考虑第个频率成分的复指数序列",{"5":{"90":1}}],["考虑第",{"5":{"90":1,"93":1}}],["考虑编码向量",{"5":{"90":1}}],["考虑编码向量序列",{"5":{"90":2}}],["考虑编码向量​的每个维度配对",{"5":{"90":1}}],["考虑编码矩阵的奇异值分解",{"5":{"89":2}}],["考虑频率",{"5":{"90":2}}],["考虑位置",{"5":{"90":3,"91":2}}],["考虑位置变换算子",{"5":{"90":2}}],["考虑位置的复数编码​",{"5":{"90":1}}],["考虑位置的编码向量​",{"5":{"90":1}}],["考虑位置和位置的编码向量​和​",{"5":{"91":1}}],["考虑位置和",{"5":{"90":1}}],["考虑位置编码向量的范数",{"5":{"91":2}}],["考虑位置编码向量集合",{"5":{"89":2}}],["考虑位置编码的优化问题",{"5":{"89":2}}],["考虑以下几种典型的奇异值分布模式",{"5":{"89":2}}],["考虑以下分解",{"5":{"87":2}}],["考虑正弦余弦编码矩阵",{"5":{"89":2}}],["考虑二维平面上的旋转操作",{"5":{"88":2}}],["考虑二分类问题",{"5":{"47":2}}],["考虑二分类场景",{"5":{"70":2}}],["考虑两层堆叠的情况",{"5":{"94":2}}],["考虑两个不同的头",{"5":{"93":1}}],["考虑两个不同的头和",{"5":{"93":1}}],["考虑两个相邻的多头注意力层",{"5":{"93":2}}],["考虑两个应用了rope的向量",{"5":{"92":2}}],["考虑两个复数编码向量",{"5":{"90":1}}],["考虑两个复数编码向量​和​",{"5":{"90":1}}],["考虑两个位置",{"5":{"88":1,"90":1,"92":1}}],["考虑两个位置和的编码向量​和",{"5":{"92":1}}],["考虑两个位置和的编码向量​和​",{"5":{"90":1}}],["考虑两个位置和",{"5":{"88":1}}],["考虑两种极端情况",{"5":{"87":2}}],["考虑注意力输出的损失函数",{"5":{"94":2}}],["考虑注意力层的输出关于位置编码",{"5":{"89":1}}],["考虑注意力层的输出关于位置编码的梯度",{"5":{"89":1}}],["考虑注意力矩阵",{"5":{"87":1}}],["考虑注意力矩阵的参数化",{"5":{"87":2}}],["考虑注意力矩阵的特征值分解",{"5":{"87":1}}],["考虑注意力矩阵的几种常用范数",{"5":{"87":2}}],["考虑注意力计算的输出",{"5":{"87":2}}],["考虑信息在注意力图上的传播",{"5":{"87":2}}],["考虑的任意行的范数为1",{"5":{"87":1}}],["考虑的第个分量​和的第个分量​",{"5":{"44":1}}],["考虑门控注意力的一种特殊形式",{"5":{"86":2}}],["考虑内存访问量",{"5":{"86":2}}],["考虑在线推理场景",{"5":{"86":2}}],["考虑在给定输入",{"5":{"51":1}}],["考虑在给定输入的条件下",{"5":{"51":1}}],["考虑三个位置",{"5":{"86":2}}],["考虑三层网络的简单情况",{"5":{"41":2}}],["考虑目标向量",{"5":{"51":1}}],["考虑目标向量可以被分解为投影部分和垂直部分的和",{"5":{"51":1}}],["考虑模型复杂度从低到高的变化",{"5":{"51":2}}],["考虑预测空间的概念",{"5":{"51":2}}],["考虑将维的编码向量投影到二维平面进行可视化",{"5":{"91":1}}],["考虑将",{"5":{"91":1,"96":1}}],["考虑将划分为两个子向量和​",{"5":{"96":1}}],["考虑将样本的编码",{"5":{"69":1}}],["考虑将样本",{"5":{"69":1}}],["考虑将偏好对视为一个二分类问题",{"5":{"99":1,"101":1}}],["考虑将偏好对",{"5":{"99":1,"101":1}}],["考虑比值",{"5":{"47":2}}],["考虑异或",{"5":{"47":2}}],["考虑使用sigmoid激活函数的神经元",{"5":{"47":2}}],["考虑超平面",{"5":{"47":2}}],["考虑全连接层的计算",{"5":{"45":2}}],["考虑单个输出元素",{"5":{"45":1}}],["考虑单个输出元素​",{"5":{"45":1}}],["考虑单层全连接网络",{"5":{"45":2}}],["考虑函数",{"5":{"45":2,"70":3,"97":4}}],["考虑函数​",{"5":{"70":1}}],["考虑梯度矩阵的元素",{"5":{"44":1}}],["考虑梯度矩阵的元素​",{"5":{"44":1}}],["考虑",{"5":{"44":1,"87":1,"94":1}}],["考虑批量中每个样本的权重梯度",{"5":{"44":2}}],["考虑权重和偏置",{"5":{"71":2}}],["考虑权重归一化后",{"5":{"41":2}}],["考虑因素",{"5":{"41":1}}],["考虑线性层",{"5":{"41":2}}],["考虑残差网络的反向传播",{"5":{"41":2}}],["考虑简化情况",{"5":{"41":2}}],["考虑了relu激活一半神经元输出为零的特性",{"5":{"41":2}}],["考虑一个具体的例子",{"5":{"94":2}}],["考虑一个具有平均稀疏度",{"5":{"86":2}}],["考虑一个单层的多头注意力模块",{"5":{"93":2}}],["考虑一个典型的gpu配置",{"5":{"93":2}}],["考虑一个层的transformer",{"5":{"92":1}}],["考虑一个层的前馈网络",{"5":{"41":1,"42":1}}],["考虑一个两层的情况",{"5":{"92":2}}],["考虑一个两层网络",{"5":{"46":2}}],["考虑一个参数化的模型分布",{"5":{"61":2}}],["考虑一个多分类问题",{"5":{"61":2}}],["考虑一个包含异常值的数据集",{"5":{"51":2}}],["考虑一个特定的样本集",{"5":{"51":2}}],["考虑一个标准的监督学习场景",{"5":{"51":2}}],["考虑一个简化的例子",{"5":{"87":2}}],["考虑一个简化的二维示例",{"5":{"51":2}}],["考虑一个简化的transformer层",{"5":{"45":2}}],["考虑一个简单的1d卷积操作",{"5":{"93":2}}],["考虑一个简单的长程依赖建模任务",{"5":{"92":2}}],["考虑一个简单的rnn单元",{"5":{"92":2}}],["考虑一个简单的单样本二分类场景",{"5":{"61":2}}],["考虑一个简单的单层线性模型",{"5":{"51":2}}],["考虑一个简单的两层神经网络",{"5":{"48":2}}],["考虑一个简单的互信息估计问题",{"5":{"71":2}}],["考虑一个简单的例子",{"5":{"71":2,"92":2,"93":2}}],["考虑一个简单的infonce实例",{"5":{"69":2}}],["考虑一个简单的线性模型",{"5":{"70":2}}],["考虑一个简单的二分类问题",{"5":{"70":2}}],["考虑一个没有激活函数的浅层神经网络",{"5":{"71":2}}],["考虑一个",{"5":{"41":1,"42":1,"92":1}}],["考虑一个样本",{"5":{"69":2}}],["考虑一个二维空间中的分类问题",{"5":{"71":2}}],["考虑一个二维简化情况",{"5":{"69":2}}],["考虑一个序列",{"5":{"69":4}}],["考虑一个正样本对和个负样本",{"5":{"69":1}}],["考虑一个正样本对",{"5":{"69":1}}],["考虑一个输入",{"5":{"99":2,"101":2}}],["考虑softmax函数",{"5":{"69":2}}],["考虑查询向量的梯度",{"5":{"69":1}}],["考虑查询向量",{"5":{"69":1}}],["考虑贝叶斯决策理论",{"5":{"70":2}}],["考虑数据分布特性",{"5":{"70":2}}],["考虑优化难度",{"5":{"70":2}}],["考虑下游应用需求",{"5":{"70":2}}],["通信的数学理论",{"5":{"61":2}}],["通用逼近定理表明",{"5":{"71":2}}],["通用逼近定理指出",{"5":{"71":2}}],["通用逼近定理的核心数学洞见在于",{"5":{"71":2}}],["通用逼近定理的数学表述",{"2":{"71":1},"5":{"71":1}}],["通用逼近定理",{"5":{"71":2}}],["通用逼近器",{"5":{"71":2}}],["通道维度",{"5":{"41":1}}],["通常对不同层使用差异化的学习率",{"5":{"97":2}}],["通常直接计算log",{"5":{"96":2}}],["通常表示序列长度",{"5":{"95":1}}],["通常表现更好",{"5":{"91":2}}],["通常用非常大的负数",{"5":{"95":2}}],["通常远小于",{"5":{"93":1}}],["通常具有更好的外推性能",{"5":{"91":2}}],["通常具有较高的秩",{"5":{"89":2}}],["通常通过点积或余弦相似度度量",{"5":{"91":2}}],["通常以半精度",{"5":{"89":1}}],["通常不是对称矩阵",{"5":{"87":1}}],["通常不等于",{"5":{"50":1}}],["通常越小",{"5":{"87":2}}],["通常与位置或内容的重要程度相关",{"5":{"87":2}}],["通常需要将不规则的稀疏模式转换为规则的块稀疏格式",{"5":{"86":2}}],["通常使用门控机制",{"5":{"93":2}}],["通常使用",{"5":{"61":1}}],["通常使用对数似然函数",{"5":{"61":1}}],["通常使用默认值",{"5":{"48":1}}],["通常使用特殊的填充token",{"5":{"99":2,"101":2}}],["通常在数千量级",{"5":{"50":1}}],["通常能提供更好的收敛性质",{"5":{"48":2}}],["通常保持较高秩",{"5":{"41":2}}],["通常是正定的",{"5":{"48":2}}],["通常是人工选择的噪声分布",{"5":{"69":2}}],["通常是编码向量的余弦相似度或点积",{"5":{"69":2}}],["通常是唯一的预训练任务",{"5":{"99":2,"101":2}}],["通常为",{"5":{"86":1}}],["通常为mb",{"5":{"86":1}}],["通常为15",{"5":{"99":2,"101":2}}],["通常为0",{"5":{"99":2,"101":2}}],["通常",{"5":{"69":2,"89":2}}],["通常设置为1或5",{"5":{"41":2}}],["通常设置在较小的值",{"5":{"69":1}}],["通常假设向量分量的初始化方差使得",{"5":{"95":2}}],["通常假设",{"5":{"70":2}}],["通过不同的方式定义",{"5":{"101":2}}],["通过这些分析",{"5":{"97":2}}],["通过这个关系可以清晰地看到tanh如何从sigmoid演化而来",{"5":{"42":2}}],["通过自动调整学习率来缓解这个问题",{"5":{"97":2}}],["通过动量或二阶信息加速逃离鞍点的过程",{"5":{"97":2}}],["通过输入变换适应不同需求",{"5":{"95":2}}],["通过输出投影矩阵",{"5":{"93":2}}],["通过输出投影矩阵进行最终的线性变换",{"5":{"93":1}}],["通过输出投影矩阵将拼接结果映射回维空间",{"5":{"93":1}}],["通过简单地修改注意力分数矩阵",{"5":{"95":2}}],["通过简单地让不同位置对应不同的旋转",{"5":{"88":2}}],["通过计算查询与各个键的相似度",{"5":{"95":2}}],["通过计算真实排列的统计量在置换分布中的位置",{"5":{"96":2}}],["通过直接计算序列中任意两个位置之间的关联强度",{"5":{"95":2}}],["通过残差连接和层归一化",{"5":{"94":2}}],["通过与混合专家模型的类比",{"5":{"93":2}}],["通过并行计算在多个子空间中提取特征",{"5":{"93":2}}],["通过并行化计算增加了表示的丰富性",{"5":{"93":2}}],["通过门控权重",{"5":{"93":2}}],["通过多层堆叠的层级化结构",{"5":{"92":2}}],["通过链式法则展开",{"5":{"92":2}}],["通过可视化注意力权重矩阵和统计分析注意力权重的分布特性",{"5":{"92":2}}],["通过可学习的参数化方式来自动发现最优的位置编码函数",{"5":{"88":1}}],["通过可学习的参数",{"5":{"41":1}}],["通过嵌入表示",{"5":{"92":2}}],["通过位置编码引入序列结构",{"5":{"101":2}}],["通过位置编码",{"5":{"92":2}}],["通过投影矩阵",{"5":{"91":1}}],["通过具体的数值示例",{"5":{"91":2}}],["通过其递归结构隐式地携带了位置信息",{"5":{"91":2}}],["通过数学推导",{"5":{"90":2}}],["通过数学归纳法证明",{"5":{"47":2}}],["通过调整相位",{"5":{"90":2}}],["通过调节",{"5":{"86":2}}],["通过实证研究",{"5":{"89":2}}],["通过适当的初始化和正则化实现",{"5":{"89":2}}],["通过比较可学习编码与正弦编码的矩阵性质",{"5":{"89":2}}],["通过反向传播优化得到",{"5":{"91":2}}],["通过反向传播自动从数据中学习最优的位置表示",{"5":{"89":2}}],["通过反向传播学习最优的稀疏结构",{"5":{"86":2}}],["通过线性插值来调整位置索引",{"5":{"88":2}}],["通过线性关系连接",{"5":{"51":1}}],["通过旋转操作来标记位置",{"5":{"88":2}}],["通过让不同维度编码不同频率的位置信息",{"5":{"88":2}}],["通过训练一个较小的",{"5":{"87":2}}],["通过限制每个位置只关注少数位置来降低计算复杂度",{"5":{"87":2}}],["通过先计算",{"5":{"87":2}}],["通过控制点积的量级",{"5":{"87":2}}],["通过将输入投影到低维空间",{"5":{"94":2}}],["通过将q和k分别投影到",{"5":{"93":1}}],["通过将q和k分别投影到个子空间",{"5":{"93":1}}],["通过将每两个维度视为复数平面上的一个点",{"5":{"88":2}}],["通过将点积除以",{"5":{"87":1}}],["通过将点积除以​​",{"5":{"87":1}}],["通过将大型权重张量分解为张量网络的形式",{"5":{"50":2}}],["通过将大型权重张量分解为若干低秩张量",{"5":{"50":2}}],["通过梯度下降优化",{"5":{"86":1}}],["通过使用行最大值进行归一化",{"5":{"86":2}}],["通过正确的归一化累积得到最终结果",{"5":{"86":2}}],["通过同时计算前向和后向的信息流来增强上下文建模能力",{"5":{"86":2}}],["通过选择合适的",{"5":{"86":2}}],["通过选择适当的权重和偏置",{"5":{"47":2}}],["通过学习来确定每个位置应该关注哪些位置",{"5":{"86":2}}],["通过理解",{"5":{"61":2}}],["通过理解误差信号的传播机制",{"5":{"44":2}}],["通过本节的学习",{"5":{"51":2,"86":2,"87":2,"89":2,"90":4,"92":2,"93":4,"94":2,"95":2}}],["通过仔细规划张量的内存布局和运算顺序",{"5":{"50":2}}],["通过向输入嵌入添加位置相关的信息",{"5":{"91":2}}],["通过向量加法直接叠加到内容embedding上",{"5":{"88":2}}],["通过向量化操作可以将其转化为线性方程",{"5":{"50":2}}],["通过向logits添加gumbel噪声并进行softmax变换",{"5":{"71":2}}],["通过惩罚大权重来防止过拟合",{"5":{"50":2}}],["通过gram",{"5":{"50":2}}],["通过保留最大的k个特征值对应的特征向量",{"5":{"50":2}}],["通过保持",{"5":{"48":1}}],["通过保持的单调性来保证收敛性",{"5":{"48":1}}],["通过逆运算解线性方程组",{"5":{"50":2}}],["通过张量运算",{"5":{"50":2}}],["通过边际似然",{"5":{"96":2}}],["通过编码器学习将输入映射到该分布的参数",{"5":{"96":2}}],["通过编码向量表示",{"5":{"71":2}}],["通过编码函数",{"5":{"69":1}}],["通过引入遗忘因子",{"5":{"86":2}}],["通过引入样本权重来反映这种重要性的差异",{"5":{"51":2}}],["通过引入一个校正因子来修复adam在训练早期的方差问题",{"5":{"48":2}}],["通过引入非线性变换",{"5":{"47":2}}],["通过hessian矩阵可以判断驻点的性质",{"5":{"48":2}}],["通过一次矩阵运算同时处理多个样本",{"5":{"47":2}}],["通过在低精度",{"5":{"97":2}}],["通过在隐藏层引入非线性激活函数",{"5":{"47":2}}],["通过在反向传播时放大损失值来间接放大梯度",{"5":{"44":2}}],["通过注意力机制聚合信息",{"5":{"46":2}}],["通过卷积核提取局部特征",{"5":{"46":2}}],["通过增加网络的深度",{"5":{"46":2}}],["通过构建计算图",{"5":{"45":2}}],["通过公式",{"5":{"44":1}}],["通过公式逐层传播到输入层",{"5":{"44":1}}],["通过批量处理",{"5":{"44":2}}],["通过定义误差信号",{"5":{"44":2}}],["通过定义搜索空间和评估指标",{"5":{"41":2}}],["通过",{"5":{"70":2,"71":2}}],["通过序列顺序隐含",{"5":{"71":2}}],["通过矩阵乘法的结合律和分配律",{"5":{"71":2}}],["通过规范化每层的输入分布",{"5":{"41":2}}],["通过严格的数学推导和几何直觉",{"5":{"41":2}}],["通过分析注意力权重矩阵的分布和头的输出表示",{"5":{"93":2}}],["通过分析注意力权重的位置分布",{"5":{"92":2}}],["通过分析权重矩阵的奇异值分布",{"5":{"50":2}}],["通过分析与真实互信息之间的关系",{"5":{"69":1}}],["通过分析",{"5":{"69":1}}],["通过对注意力矩阵进行低秩分解和近似",{"5":{"87":2}}],["通过对矩母函数求导并在",{"5":{"96":1}}],["通过对矩母函数求导并在处求值",{"5":{"96":1}}],["通过对比学习",{"5":{"69":2}}],["only特性可能限制了对这类信息的捕捉",{"5":{"88":2}}],["one",{"5":{"61":2,"70":2,"96":4}}],["output",{"5":{"86":2}}],["outer",{"5":{"61":2}}],["outliers",{"5":{"51":2}}],["out",{"5":{"99":2,"101":2}}],["oracle",{"5":{"92":2}}],["orthogonality",{"5":{"51":2}}],["orthant",{"5":{"45":2}}],["or",{"5":{"47":4}}],["order",{"5":{"45":2}}],["odds",{"5":{"61":2,"71":2}}],["odot",{"5":{"42":2,"86":6,"97":4}}],["oord等人于2018年在论文",{"5":{"69":2}}],["o^",{"5":{"69":8}}],["o",{"5":{"69":4,"86":2}}],["operations",{"5":{"45":2}}],["optimization",{"5":{"99":4,"101":4}}],["层转换为一个高维向量",{"5":{"94":2}}],["层位置",{"5":{"92":1}}],["层级",{"5":{"92":2}}],["层级特征与依赖跨度",{"2":{"92":1},"5":{"92":1}}],["层级化的依赖建模",{"2":{"92":1},"5":{"92":1}}],["层次化位置编码",{"5":{"88":1}}],["层次化位置编码针对文档级别的长上下文设计",{"5":{"88":1}}],["层次化特征学习",{"5":{"71":2}}],["层间差异",{"5":{"87":2}}],["层网络的等效仿射变换形式",{"5":{"47":1}}],["层网络",{"5":{"47":1}}],["层网络中",{"5":{"45":1}}],["层操作",{"5":{"47":2}}],["层非线性变换",{"5":{"45":1}}],["层内的输出神经元",{"5":{"45":1}}],["层内并行",{"5":{"45":2}}],["层映射",{"5":{"45":2}}],["层全连接网络",{"5":{"44":1,"45":1}}],["层偏置的梯度为",{"5":{"44":2}}],["层权重的梯度为",{"5":{"44":2}}],["层激活函数的导数",{"5":{"44":1}}],["层误差信号",{"5":{"44":1}}],["层前馈网络的计算可以表示为一系列函数的复合",{"5":{"46":2}}],["层前馈网络定义的函数",{"5":{"45":1}}],["层前馈网络定义的函数​是各层映射的复合",{"5":{"45":1}}],["层前馈网络",{"5":{"44":1}}],["层的残差贡献",{"5":{"92":1}}],["层的transformer",{"5":{"92":1}}],["层的表示",{"5":{"92":1}}],["层的表示向量",{"5":{"71":1}}],["层的感受野大小约为",{"5":{"92":1}}],["层的激活函数",{"5":{"46":1}}],["层的权重矩阵和偏置向量",{"5":{"46":1}}],["层的概念在神经网络架构设计中具有核心地位",{"5":{"46":2}}],["层的输出为",{"5":{"47":1}}],["层的输出",{"5":{"45":1}}],["层的输入为",{"5":{"45":1,"46":1,"92":1}}],["层的输入",{"5":{"41":1}}],["层的前馈神经网络",{"5":{"45":1,"46":1}}],["层的前馈网络",{"5":{"41":1,"42":1,"47":1}}],["层的误差信号矩阵为",{"5":{"44":1}}],["层的误差信号",{"5":{"44":1}}],["层的计算定义为",{"5":{"44":1}}],["层的计算为",{"5":{"41":1}}],["层后",{"5":{"41":1,"42":1}}],["层归一化是标准",{"5":{"41":1}}],["层归一化位于残差分支的输出端",{"5":{"41":2}}],["层归一化位于残差分支的输入端",{"5":{"41":2}}],["层归一化稳定了注意力计算和前馈计算的输入分布",{"5":{"41":2}}],["层归一化通常部署在注意力层和前馈层之间",{"5":{"41":2}}],["层归一化在每个样本内部计算统计量",{"5":{"41":2}}],["层归一化在特征维度上进行归一化",{"5":{"41":2}}],["层归一化的计算特性",{"5":{"41":2}}],["层归一化的关键优势是它不依赖于batch",{"5":{"41":2}}],["层归一化的数学原理",{"2":{"41":1},"5":{"41":1}}],["层归一化定义为",{"5":{"41":2}}],["层归一化",{"5":{"41":7}}],["层归一化与transformer的配合",{"2":{"41":1},"5":{"41":1}}],["层",{"5":{"45":2,"61":2,"89":1,"92":2,"93":1}}],["不能",{"5":{"95":2}}],["不能使用未来信息",{"5":{"41":2}}],["不再依赖于维度",{"5":{"95":2}}],["不再能够简化为",{"5":{"71":1}}],["不再能够简化为的形式",{"5":{"71":1}}],["不一定是正交的",{"5":{"94":2}}],["不可逆的变换",{"5":{"94":2}}],["不可减少误差是mse的理论下界",{"5":{"51":2}}],["不可减少误差",{"5":{"51":2}}],["不进行显式的加权混合",{"5":{"93":2}}],["不擅长学习哪类函数",{"5":{"92":2}}],["不存在",{"5":{"92":2}}],["不存在饱和问题",{"5":{"41":2}}],["不干扰内容信息的表示",{"5":{"92":2}}],["不包含任何位置信息",{"5":{"92":2}}],["不包含位置信息",{"5":{"88":2}}],["不参与梯度更新",{"5":{"91":2}}],["不显著增加模型的整体计算负担",{"5":{"91":2}}],["不显式地使用位置信息",{"5":{"88":2}}],["不显式计算完整的",{"5":{"87":1}}],["不显式计算完整的注意力矩阵",{"5":{"87":1}}],["不超过最低频率成分的周期",{"5":{"90":1}}],["不如让模型自己发现最适合任务的编码方式",{"5":{"89":2}}],["不影响flash",{"5":{"88":2}}],["不影响优化结果",{"5":{"51":1}}],["不依赖于",{"5":{"89":1}}],["不依赖于特定的数据分布或训练技巧",{"5":{"88":2}}],["不依赖于其他样本",{"5":{"41":2}}],["不会丢失信息",{"5":{"94":2}}],["不会感知到原始的顺序信息",{"5":{"92":2}}],["不会出现",{"5":{"91":2}}],["不会出现训练过程中位置编码",{"5":{"88":2}}],["不会过度放大或抑制梯度",{"5":{"41":2}}],["不区分它们的先后顺序",{"5":{"88":2}}],["不改变线性相关性",{"5":{"87":2}}],["不改变其超平面性质",{"5":{"47":2}}],["不随序列长度增长",{"5":{"87":2}}],["不随",{"5":{"86":1}}],["不随增长",{"5":{"86":1}}],["不考虑概率分布的归一化约束",{"5":{"61":2}}],["不是为每个位置分配一个固定的数值",{"5":{"88":2}}],["不是很大",{"5":{"51":1}}],["不是欧几里得距离",{"5":{"70":2,"71":2}}],["不当的学习率调度可能导致收敛振荡",{"5":{"97":2}}],["不当的实现可能导致数值溢出",{"5":{"44":2}}],["不当使用容易引入隐蔽且难以察觉的",{"5":{"50":2}}],["不相关时等号才成立",{"5":{"96":1}}],["不等于",{"5":{"96":2}}],["不仅计算高效",{"5":{"96":2}}],["不仅定义了优化的目标函数",{"5":{"51":2}}],["不仅会使得表达式繁琐冗长",{"5":{"46":2}}],["不仅有助于我们把握神经网络的工作原理",{"5":{"45":2}}],["不仅有助于深入理解llm的训练机制",{"5":{"99":2,"101":2}}],["不应被归一化消除",{"5":{"41":2}}],["不受层归一化的影响",{"5":{"41":2}}],["不同训练阶段的最优学习率可能变化",{"5":{"97":2}}],["不同损失函数对学习率的敏感性不同",{"5":{"97":2}}],["不同损失函数的优化难度比较",{"2":{"97":1},"5":{"97":1}}],["不同层和头表现出不同的注意力模式",{"5":{"92":2}}],["不同层和不同头的有效依赖跨度差异很大",{"5":{"92":2}}],["不同层的注意力可能关注不同类型的关系",{"5":{"71":2}}],["不同维度对应不同",{"5":{"91":2}}],["不同维度使用不同的频率",{"5":{"90":2}}],["不同位置有不同的编码",{"5":{"92":2}}],["不同位置具有不同的编码",{"5":{"90":2}}],["不同位置的编码趋于相似",{"5":{"90":2}}],["不同位置对应不同的数值",{"5":{"88":2}}],["不同频率成分提供不同精度的位置表示",{"5":{"90":2}}],["不同频率的正交基函数为模型提供了不同尺度的位置信息",{"5":{"70":2}}],["不同头之间仍然存在间接的交互",{"5":{"93":2}}],["不同头的学习任务是相对独立的",{"5":{"93":2}}],["不同头的衰减速度不同",{"5":{"87":2}}],["不同头表现出不同的衰减模式",{"5":{"87":2}}],["不同特征维度进行独立的",{"5":{"86":2}}],["不同组的query头使用不同的key和value",{"5":{"86":2}}],["不同样本的重要性可能有所不同",{"5":{"51":2}}],["不同样本之间的计算是相互独立的",{"5":{"45":2}}],["不同列之间的计算不相互依赖",{"5":{"45":2}}],["不同输出神经元",{"5":{"45":2}}],["不同",{"5":{"45":2,"48":2,"92":2,"93":2}}],["不同的损失函数在优化难度上存在显著差异",{"5":{"97":2}}],["不同的损失函数定义了不同的几何结构",{"5":{"70":1}}],["不同的损失函数定义了不同的",{"5":{"70":1}}],["不同的",{"5":{"93":2}}],["不同的架构有不同的归纳偏置",{"5":{"92":2}}],["不同的头学习不同的关联模式",{"5":{"93":2}}],["不同的头从不同的随机起点开始优化",{"5":{"93":2}}],["不同的头对应于不同的投影方向",{"5":{"93":2}}],["不同的头协同工作",{"5":{"92":2}}],["不同的头可以专门负责建模不同类型的依赖关系",{"5":{"92":2}}],["不同的位置编码",{"5":{"101":2}}],["不同的位置编码会导致不同的注意力分布",{"5":{"91":2}}],["不同的位置应当具有不同的编码向量",{"5":{"91":2}}],["不同的频率成分贡献不同的相对位置表示",{"5":{"90":2}}],["不同的频率成分携带函数的不同",{"5":{"90":2}}],["不同的初始化可能收敛到不同的局部最优",{"5":{"89":2}}],["不同的稀疏模式导致不同的谱特性",{"5":{"87":2}}],["不同的稀疏模式对应不同的归纳偏置和计算特性",{"5":{"86":2}}],["不同的特征映射对应不同的核函数",{"5":{"86":2}}],["不同的神经元学习不同的特征表示",{"5":{"47":2}}],["不同的权重矩阵定义了不同的投影方向",{"5":{"45":2}}],["不同的下游任务可能需要不同特性的激活函数",{"5":{"41":2}}],["不同的注意力头可能专门负责处理不同类型的输入或任务",{"5":{"70":2}}],["不同的掩码策略会导致不同的损失函数形式和模型学习行为",{"5":{"99":2,"101":2}}],["不同的掩码策略对应于不同的",{"5":{"99":2,"101":2}}],["不同的任务可能需要模型关注输入的不同部分",{"5":{"99":2,"101":2}}],["不同设备处理不同样本",{"5":{"41":2}}],["不同注意力分布对应截然不同的谱结构",{"5":{"41":2}}],["不同初始化策略的比较",{"5":{"41":2}}],["不同任务共享相同的注意力层",{"5":{"70":2}}],["不确定性",{"5":{"89":2}}],["不确定性度量",{"5":{"61":2}}],["不确定性程度",{"5":{"61":2}}],["不确定性量化和小样本学习",{"5":{"96":2}}],["不确定性高",{"5":{"96":2}}],["不需要训练",{"5":{"91":2}}],["不需要复杂的插值策略",{"5":{"88":2}}],["不需要额外的相对位置偏置",{"5":{"88":2}}],["不需要经过",{"5":{"61":2}}],["不需要大的更新",{"5":{"99":2,"101":2}}],["不需要显式的奖励模型",{"5":{"99":2,"101":2}}],["不具有这种与fisher信息的直接联系",{"5":{"70":2}}],["不对称性",{"5":{"61":3}}],["不对称性是kl散度区别于传统距离的关键特征",{"5":{"61":1}}],["不对称且加速的",{"5":{"70":1}}],["输入序列经过多层transformer架构的变换后",{"5":{"96":2}}],["输入序列通常需要填充到统一的长度以进行批处理",{"5":{"95":2}}],["输入嵌入通常还需要加上位置编码",{"5":{"94":2}}],["输入嵌入矩阵的数学表示",{"2":{"94":1},"5":{"94":1}}],["输入矩阵分别左乘这三个投影矩阵",{"5":{"94":1}}],["输入矩阵",{"5":{"93":2,"94":1}}],["输入矩阵​与堆叠投影矩阵相乘",{"5":{"93":1}}],["输入矩阵与堆叠后的投影矩阵相乘",{"5":{"93":1}}],["输入为",{"5":{"87":2}}],["输入的",{"5":{"61":2}}],["输入的无限精度被截断为有限范围的输出",{"5":{"71":2}}],["输入张量的形状通常是",{"5":{"50":2}}],["输入节点",{"5":{"45":2}}],["输入节点表示网络的输入数据",{"5":{"45":2}}],["输入数据依次通过每一层的仿射变换和非线性激活",{"5":{"45":2}}],["输入数据通过多层非线性变换",{"5":{"44":2}}],["输入",{"5":{"44":2}}],["输入梯度",{"5":{"41":1}}],["输入梯度和输出梯度满足",{"5":{"41":1}}],["输入到激活函数的线性组合",{"5":{"41":1}}],["输入到激活函数的线性组合可能直接落在饱和区域",{"5":{"41":1}}],["输出可能包含非常小的值",{"5":{"97":2}}],["输出投影负责混合信息",{"5":{"93":2}}],["输出投影后的结果",{"5":{"93":2}}],["输出投影矩阵",{"5":{"93":2}}],["输出投影矩阵​隐含地学习了一种加权方案",{"5":{"93":1}}],["输出投影矩阵会将所有头的信息混合起来",{"5":{"93":1}}],["输出通过递归关系定义",{"5":{"92":2}}],["输出表示第个频率成分的复振幅",{"5":{"90":1}}],["输出表示被",{"5":{"87":2}}],["输出值均为正且和为1",{"5":{"47":2}}],["输出值较低",{"5":{"71":2}}],["输出向量为",{"5":{"46":1}}],["输出向量为​",{"5":{"46":1}}],["输出为0",{"5":{"47":2}}],["输出为1",{"5":{"47":6}}],["输出为",{"5":{"45":1,"46":2,"92":2}}],["输出为​",{"5":{"45":1}}],["输出节点表示网络的最终输出",{"5":{"45":2}}],["输出分布",{"5":{"71":1}}],["输出分布关于输入的fisher信息矩阵正好是softmax雅可比矩阵与自身转置的乘积",{"5":{"71":1}}],["输出映射",{"5":{"71":2}}],["输出总是正的",{"5":{"42":2}}],["输出均值为0",{"5":{"42":2}}],["输出饱和是激活函数值域边界的行为",{"5":{"41":2}}],["输出饱和",{"5":{"41":2}}],["输出饱和关注的是激活函数输出值进入平坦区域的物理现象",{"5":{"41":2}}],["输出饱和和梯度饱和",{"5":{"41":2}}],["输出层的预测分布是一个",{"5":{"96":1}}],["输出层的预测分布是一个维的概率向量",{"5":{"96":1}}],["输出层误差简化为预测误差的平均值",{"5":{"44":2}}],["输出层误差为",{"5":{"44":2}}],["输出层误差",{"5":{"44":2}}],["输出层误差信号与预测误差",{"5":{"44":1}}],["输出层误差信号与预测误差成正比",{"5":{"44":1}}],["输出层误差信号",{"5":{"44":2}}],["输出层有",{"5":{"71":1}}],["输出层有个神经元",{"5":{"71":1}}],["输出层",{"5":{"44":2,"61":2}}],["输出层概率",{"5":{"69":2}}],["输出",{"5":{"44":2,"50":2,"61":8,"70":2,"90":1}}],["输出是各头注意力的拼接和投影",{"5":{"93":2}}],["输出是类别的概率",{"5":{"69":2}}],["输出是加权平均的期望值",{"5":{"69":2}}],["输出求导",{"5":{"70":2}}],["输出的组合",{"5":{"70":4}}],["thread",{"5":{"93":2}}],["theorem",{"5":{"96":2}}],["the",{"5":{"47":2}}],["tiles",{"5":{"86":2}}],["tiling",{"5":{"86":2}}],["times",{"5":{"46":14}}],["tuning",{"5":{"50":2}}],["tucker",{"5":{"50":6}}],["training",{"5":{"97":2}}],["tradeoff",{"5":{"51":2}}],["transpose",{"5":{"50":2}}],["transform",{"5":{"90":6,"91":2}}],["transformation",{"5":{"47":2}}],["transformer原始论文提出的正弦位置编码",{"5":{"92":2}}],["transformer的原始论文使用了",{"5":{"93":1}}],["transformer的原始论文使用了个头的配置",{"5":{"93":1}}],["transformer的不同层捕获不同",{"5":{"92":2}}],["transformer的自注意力机制本身不包含显式的激活函数",{"5":{"71":2}}],["transformer架构的出现从根本上改变了这一局面",{"5":{"91":2}}],["transformer架构虽然解决了rnn的长期依赖问题",{"5":{"41":2}}],["transformer在各种任务上的成功部分归功于其注意力矩阵的隐含低秩结构",{"5":{"87":2}}],["transformer采用了深度网络的架构设计",{"5":{"71":2}}],["transformer中的注意力机制可以被看作一个三阶张量",{"5":{"50":2}}],["transformer中的前馈网络",{"5":{"71":2}}],["transformer中的激活函数选择不是随意的",{"5":{"71":2}}],["transformer中的梯度爆炸机制",{"2":{"41":1},"5":{"41":1}}],["transformer中",{"5":{"41":4}}],["transformer",{"5":{"41":6,"50":2,"70":8,"88":2,"97":8}}],["tree",{"5":{"50":2}}],["tr",{"5":{"46":2}}],["top",{"5":{"96":2}}],["topological",{"5":{"45":2}}],["toeplitz矩阵",{"5":{"93":2}}],["to",{"5":{"42":2,"95":2}}],["tokens",{"5":{"88":4}}],["token",{"5":{"70":4,"99":4,"101":4}}],["token的交互特征",{"5":{"71":2}}],["token的注意",{"5":{"99":4,"101":4}}],["token嵌入的集合在置换下保持不变",{"5":{"99":1,"101":1}}],["token嵌入的集合",{"5":{"99":1,"101":1}}],["tensor",{"5":{"50":4,"93":4}}],["tensorflow",{"5":{"47":2,"61":2,"93":2}}],["test",{"5":{"96":2}}],["text",{"5":{"42":58,"46":28,"61":4,"69":10,"86":12}}],["temperature",{"5":{"69":2,"70":2,"95":2}}],["tangent",{"5":{"88":2}}],["tanh输出",{"5":{"42":1}}],["tanh输出区间",{"5":{"42":1}}],["tanh在激活值接近零的区域",{"5":{"42":2}}],["tanh导数同样会迅速衰减至",{"5":{"42":2}}],["tanh导数的取值范围是",{"5":{"42":2}}],["tanh导数的取值范围",{"5":{"42":2}}],["tanh导数",{"5":{"42":4}}],["tanh先对输入进行2倍缩放",{"5":{"42":2}}],["tanh以0为中心",{"5":{"42":2}}],["tanh本质上是sigmoid的",{"5":{"42":2}}],["tanh和sigmoid存在直接的数学关系",{"5":{"42":2}}],["tanh可以视为sigmoid的缩放和平移版本",{"5":{"42":2}}],["tanh与sigmoid的关系",{"5":{"42":2}}],["tanh与sigmoid的饱和比较",{"5":{"41":2}}],["tanh与sigmoid有密切的数学关系",{"5":{"42":2}}],["tanh",{"5":{"41":1,"42":2,"47":2,"71":4}}],["tanh函数可以表示为sigmoid函数的缩放和平移",{"5":{"42":2}}],["tanh函数是奇函数",{"5":{"42":2}}],["tanh函数",{"5":{"41":1,"42":7}}],["tanh函数的导数为",{"5":{"42":1}}],["tanh函数的导数与性质",{"2":{"42":1},"5":{"42":1}}],["tanh函数的数学定义与性质",{"2":{"42":1},"5":{"42":1}}],["tanh函数的梯度饱和区域为",{"5":{"41":2}}],["tanh函数的饱和特性与sigmoid类似",{"5":{"41":1}}],["tanh函数的饱和特性",{"2":{"41":1},"5":{"41":1}}],["tanh有两个显著差异",{"5":{"41":2}}],["tanh的饱和问题依然严重",{"5":{"42":2}}],["tanh的饱和区域",{"5":{"41":2}}],["tanh的梯度",{"5":{"42":2}}],["tanh的梯度信号更强",{"5":{"41":2}}],["tanh的零中心输出消除了这个问题",{"5":{"42":2}}],["tanh的零中心性质缓解了这个问题",{"5":{"42":2}}],["tanh的零中心性质",{"5":{"42":2}}],["tanh的输出以0为中心",{"5":{"42":2}}],["tanh的输出以零为中心",{"5":{"41":2}}],["tanh的导数峰值更大",{"5":{"42":2}}],["tanh的导数峰值是1而非0",{"5":{"41":2}}],["tanh的导数",{"5":{"42":2}}],["tanh的导数为",{"5":{"41":2}}],["tag",{"5":{"42":50,"46":22,"61":18,"69":8,"86":4,"97":8}}],["target=",{"5":{"21":10,"44":1,"45":2,"47":2,"48":3,"50":2,"61":1,"85":26,"96":3}}],["task",{"5":{"70":2,"99":2,"101":2}}],["tbd",{"5":{"85":2}}],["t",{"5":{"69":6,"97":27}}],["t+1",{"5":{"69":2,"97":2}}],["如大语言模型",{"5":{"97":2}}],["如前节分析的结论",{"5":{"94":2}}],["如前文中的相关token",{"5":{"69":2}}],["如xavier初始化或kaiming初始化",{"5":{"94":2}}],["如固定",{"5":{"93":1}}],["如固定​",{"5":{"93":1}}],["如傅里叶基",{"5":{"93":2}}],["如与​可比",{"5":{"93":1}}],["如短序列处理",{"5":{"93":2}}],["如短语结构",{"5":{"88":2}}],["如同义词或上下文中相关的词",{"5":{"93":2}}],["如同一序列中相邻的token",{"5":{"71":2}}],["如同一序列中相邻的位置",{"5":{"69":2}}],["如相邻词之间的关系",{"5":{"93":2}}],["如相对位置表达能力",{"5":{"90":2}}],["如距离矩阵",{"5":{"92":2}}],["如距离越近权重越大",{"5":{"92":2}}],["如位置",{"5":{"92":1}}],["如位置和位置之间的关联",{"5":{"92":1}}],["如位置编码",{"5":{"90":2}}],["如长距离的修饰关系",{"5":{"92":2}}],["如长文本生成",{"5":{"91":2}}],["如长文本建模",{"5":{"41":2}}],["如长文档处理",{"5":{"95":2}}],["如长文档",{"5":{"86":2}}],["如代词与指代对象的关联",{"5":{"92":2}}],["如代码",{"5":{"89":2}}],["如代码分析",{"5":{"89":2}}],["如取最大值",{"5":{"92":2}}],["如最后一层编码器或解码器",{"5":{"92":2}}],["如最大似然",{"5":{"70":2}}],["如第一层编码器",{"5":{"92":2}}],["如截断或微调",{"5":{"91":2}}],["如旋转位置编码",{"5":{"90":2}}],["如使用低秩分解存储",{"5":{"89":1}}],["如使用低秩分解存储和而非完整",{"5":{"89":1}}],["如在可学习编码上叠加正弦编码",{"5":{"89":2}}],["如零初始化",{"5":{"89":2}}],["如某些位置出现频率很低",{"5":{"89":2}}],["如过拟合风险",{"5":{"89":2}}],["如循环结构",{"5":{"88":2}}],["如超过100k",{"5":{"88":2}}],["如句子边界",{"5":{"88":2}}],["如100k+",{"5":{"88":2}}],["如llama",{"5":{"88":2}}],["如longformer",{"5":{"87":2}}],["如层次化的语法结构",{"5":{"88":2}}],["如层归一化",{"5":{"87":2}}],["如修饰关系",{"5":{"88":2}}],["如主语",{"5":{"92":2,"93":2}}],["如主谓一致",{"5":{"88":2}}],["如主要关注少数几个位置",{"5":{"87":2}}],["如512或1024",{"5":{"88":2}}],["如训练早期或特定数据分布",{"5":{"87":2}}],["如均匀关注所有位置",{"5":{"87":2}}],["如低秩近似",{"5":{"87":2}}],["如需要高秩矩阵表示的函数",{"5":{"87":2}}],["如每行都是one",{"5":{"87":2}}],["如视觉",{"5":{"86":2}}],["如或4096",{"5":{"91":1}}],["如或",{"5":{"86":1,"92":2}}],["如h100",{"5":{"86":2}}],["如滑动窗口",{"5":{"86":2}}],["如cls标记或特殊标记",{"5":{"86":2}}],["如交叉熵损失函数",{"5":{"51":2}}],["如mae",{"5":{"51":2}}],["如神经网络的情况",{"5":{"51":2}}],["如神经元排列的不变性",{"5":{"48":2}}],["如测量噪声",{"5":{"51":2}}],["如深层神经网络在有限数据上训练",{"5":{"51":2}}],["如深层神经网络",{"5":{"51":2}}],["如线性模型拟合非线性数据",{"5":{"51":2}}],["如线性模型",{"5":{"51":2}}],["如线性注意力",{"5":{"50":2}}],["如何让模型感知序列中元素的位置信息一直是一个核心问题",{"5":{"91":2}}],["如何设计更好的特征映射",{"5":{"87":1}}],["如何设计更好的特征映射以平衡表达能力和计算效率",{"5":{"87":1}}],["如何自动学习或设计这种稀疏模式",{"5":{"87":2}}],["如何将矩阵运算分解为可以在不同设备上独立执行的子运算",{"5":{"50":2}}],["如何高效地计算这个复合函数的输出",{"5":{"45":2}}],["如bf16",{"5":{"50":2}}],["如bonferroni校正或false",{"5":{"96":2}}],["如blue",{"5":{"70":2}}],["如bpe",{"5":{"99":2,"101":2}}],["如tensorrt",{"5":{"93":2}}],["如tensor",{"5":{"86":2}}],["如t",{"5":{"50":2}}],["如用户满意度",{"5":{"96":2}}],["如两个模型或两种策略",{"5":{"96":2}}],["如随机初始化",{"5":{"89":2,"96":2}}],["如基于贝叶斯方法的超参数优化",{"5":{"96":2}}],["如高斯分布",{"5":{"70":2,"96":2}}],["如结合vae的表示学习",{"5":{"96":2}}],["如分析神经网络输出的分布特性",{"5":{"96":2}}],["如判断一个词是否属于某个类别",{"5":{"96":2}}],["如词语的嵌入向量",{"5":{"96":2}}],["如词汇关系",{"5":{"71":2}}],["如抛硬币的结果",{"5":{"96":2}}],["如pytorch的tensorflow的张量计算",{"5":{"93":2}}],["如pytorch和tensorflow中",{"5":{"50":2}}],["如pytorch",{"5":{"47":2,"93":2}}],["如异或问题",{"5":{"47":2}}],["如adam",{"5":{"46":2}}],["如初始化为零",{"5":{"46":2}}],["如jacobian矩阵计算",{"5":{"45":2}}],["如张量",{"5":{"45":2}}],["如矩阵乘法",{"5":{"45":2}}],["如物体类别",{"5":{"45":2}}],["如边缘",{"5":{"45":2}}],["如gpt",{"5":{"47":2,"71":2,"93":2}}],["如gelu",{"5":{"71":4}}],["如fisher信息",{"5":{"71":2}}],["如infonce",{"5":{"71":2}}],["如变分自编码器的离散潜在变量",{"5":{"71":2}}],["如图像",{"5":{"71":2}}],["如逻辑回归",{"5":{"71":2}}],["如凸优化问题和解析解的存在性",{"5":{"71":2}}],["如rbf核",{"5":{"87":2}}],["如relu激活函数",{"5":{"50":1}}],["如relu激活函数就是典型的逐元素运算",{"5":{"50":1}}],["如relu或gelu",{"5":{"71":2}}],["如relu",{"5":{"41":2,"48":2}}],["如rope",{"5":{"41":2,"91":2}}],["如0",{"5":{"41":2,"95":2,"96":2}}],["如果学习率过大",{"5":{"97":2}}],["如果查询向量",{"5":{"95":1}}],["如果查询向量在某个维度上的分量较大",{"5":{"95":1}}],["如果所有输入",{"5":{"95":1}}],["如果所有输入​都加上一个较大的常数",{"5":{"95":1}}],["如果所有特征值的实部都为负",{"5":{"50":2}}],["如果头数",{"5":{"93":1}}],["如果头数是的约数",{"5":{"93":1}}],["如果高度相关",{"5":{"93":2}}],["如果基函数集合足够丰富",{"5":{"93":2}}],["如果两个头的输出",{"5":{"93":1}}],["如果两个头的输出​和是独立的",{"5":{"93":1}}],["如果两个头学习到完全相同的特征",{"5":{"93":2}}],["如果两个头学习到正交的特征",{"5":{"93":2}}],["如果两个头学习到相似的特征",{"5":{"93":2}}],["如果两种方法计算的梯度足够接近",{"5":{"44":2}}],["如果没有足够的数据或适当的正则化",{"5":{"92":2}}],["如果中间层能够调整注意力权重",{"5":{"92":2}}],["如果模型认为它们不相关",{"5":{"92":2}}],["如果要建模跨越整个序列的长程依赖",{"5":{"92":2}}],["如果​较小",{"5":{"92":1}}],["如果​",{"5":{"91":1}}],["如果注意力权重分散在很远的位置",{"5":{"92":2}}],["如果注意力权重分布集中",{"5":{"89":2}}],["如果注意力权重分布均匀",{"5":{"89":2}}],["如果注意力权重主要集中在邻近位置",{"5":{"92":2}}],["如果注意力权重表现出全局性",{"5":{"87":2}}],["如果注意力权重表现出局部性",{"5":{"87":2}}],["如果训练数据支持这种平滑性假设",{"5":{"89":2}}],["如果训练数据中的位置分布不均衡",{"5":{"89":2}}],["如果编码矩阵",{"5":{"89":1}}],["如果编码矩阵​​的秩较低",{"5":{"89":1}}],["如果编码是平滑的则外推较好",{"5":{"89":2}}],["如果编码向量的均值为零",{"5":{"89":2}}],["如果核函数是",{"5":{"87":2}}],["如果核函数是正定的",{"5":{"86":2}}],["如果秩较高",{"5":{"87":2}}],["如果某个头的注意力矩阵秩很低",{"5":{"87":2}}],["如果某个任务的损失主导",{"5":{"70":2}}],["如果且",{"5":{"87":1}}],["如果将分类问题错误地使用均方误差作为损失函数",{"5":{"61":2}}],["如果在该点的梯度为零",{"5":{"97":2}}],["如果在",{"5":{"61":2}}],["如果与越接近",{"5":{"61":1}}],["如果对",{"5":{"90":2}}],["如果对进行某种操作",{"5":{"90":1}}],["如果对进行平移",{"5":{"90":1}}],["如果对于任意",{"5":{"51":1}}],["如果对于任意​和任意",{"5":{"51":1}}],["如果对应维度相等",{"5":{"50":2}}],["如果是正定的",{"5":{"86":1}}],["如果是常数",{"5":{"86":1}}],["如果是方差问题",{"5":{"51":2}}],["如果是偏差问题",{"5":{"51":2}}],["如果假设噪声服从拉普拉斯分布",{"5":{"51":2}}],["如果权重未归一化",{"5":{"51":2}}],["如果权重归一化为",{"5":{"51":2}}],["如果满足",{"5":{"50":1}}],["如果进行行交换",{"5":{"50":2}}],["如果主元都不为零",{"5":{"50":2}}],["如果奇异值分布比较均匀",{"5":{"50":2}}],["如果只有少数几个奇异值很大",{"5":{"50":2}}],["如果只改变这个参数而保持其他参数不变",{"5":{"48":2}}],["如果有特征值的实部为正",{"5":{"50":2}}],["如果存在群作用",{"5":{"88":1}}],["如果存在群作用使得对所有",{"5":{"88":1}}],["如果存在非零向量",{"5":{"50":1}}],["如果存在非零向量和标量满足",{"5":{"50":1}}],["如果存在输入区域使得",{"5":{"41":3,"42":2}}],["如果存在输入区域使得趋近于其渐近值",{"5":{"41":1}}],["如果同时检验多个假设",{"5":{"96":2}}],["如果我们希望",{"5":{"92":2}}],["如果我们希望​",{"5":{"92":1}}],["如果我们希望​能够感知到​的信息",{"5":{"92":1}}],["如果我们对输入序列进行重新排序",{"5":{"92":2}}],["如果我们对",{"5":{"87":1}}],["如果我们对进行qr分解",{"5":{"87":1}}],["如果我们对每个神经元单独进行数学描述和计算",{"5":{"46":2}}],["如果我们使用相同的测试集评估两个模型",{"5":{"96":2}}],["如果我们定义",{"5":{"69":4}}],["如果我们把视为",{"5":{"69":1}}],["如果我们把",{"5":{"69":1}}],["如果我们把每个位置视为一个潜在的",{"5":{"69":1}}],["如果我们把每个位置",{"5":{"69":1}}],["如果我们随机打乱序列的顺序",{"5":{"69":2}}],["如果我们训练一个自编码器",{"5":{"69":2}}],["如果我们将其视为对某个参数化分布族",{"5":{"70":2}}],["如果p值小于预先设定的显著性水平",{"5":{"96":2}}],["如果函数",{"5":{"48":1}}],["如果函数的二阶偏导数连续",{"5":{"48":1}}],["如果hessian半正定或半负定",{"5":{"48":2}}],["如果hessian既有正特征值又有负特征值",{"5":{"48":2}}],["如果hessian负定",{"5":{"48":2}}],["如果hessian正定",{"5":{"48":2}}],["如果梯度范数为",{"5":{"97":2}}],["如果梯度范数超过阈值",{"5":{"44":2}}],["如果梯度矩阵的范数随层数增加而指数级增长",{"5":{"41":2}}],["如果激活函数的导数接近零",{"5":{"44":2}}],["如果一组函数满足以下条件",{"5":{"71":2}}],["如果为正",{"5":{"42":1}}],["如果神经元的输入",{"5":{"42":2}}],["如果输入梯度的某些维度较大",{"5":{"41":2}}],["如果接近均匀混合",{"5":{"41":1}}],["如果的秩为",{"5":{"89":1}}],["如果的非主特征值很小",{"5":{"41":1}}],["如果的各元素独立同分布且方差为",{"5":{"41":1}}],["如果很大而没有进行适当的缩放",{"5":{"41":1}}],["如果初始权重过大",{"5":{"41":2}}],["如果每一层的激活都处于饱和状态",{"5":{"41":2,"42":2}}],["如果",{"5":{"41":18,"42":1,"48":2,"50":2,"51":1,"61":5,"69":3,"86":2,"87":3,"89":3,"91":3,"92":1,"97":2,"99":2,"101":2}}],["如果已经与很相似",{"5":{"69":1}}],["如果温度过高",{"5":{"69":2}}],["如果交叉熵损失达到最小",{"5":{"70":2}}],["如果数据中的噪声是高斯分布的",{"5":{"70":2}}],["如果噪声是拉普拉斯分布的",{"5":{"70":2}}],["如果类别不平衡",{"5":{"70":2}}],["如果使用张量运算而非循环",{"5":{"50":2}}],["如果使用正弦余弦位置编码",{"5":{"70":2}}],["如果使用",{"5":{"70":2}}],["如果不是很大",{"5":{"51":1}}],["如果不包含位置信息",{"5":{"99":2,"101":2}}],["如果不存在另一个解使得在所有任务上都不差且至少一个任务上更好",{"5":{"99":1,"101":1}}],["如果不存在另一个解",{"5":{"99":1,"101":1}}],["如果偏好数据存在噪声或不一致性",{"5":{"99":2,"101":2}}],["如",{"5":{"50":2,"61":2,"69":2,"70":8,"86":1,"87":2,"88":4,"89":6,"90":4,"91":3,"92":6,"93":1,"94":2,"95":2,"96":4,"97":12}}],["如softmax归一化",{"5":{"86":2}}],["如sigmoid",{"5":{"71":2}}],["如squad的跨度预测",{"5":{"99":2,"101":2}}],["如情感分析",{"5":{"89":2,"99":2,"101":2}}],["如正弦余弦编码",{"5":{"71":2}}],["如正弦余弦编码的频率分解",{"5":{"71":2}}],["如正弦余弦编码或rope",{"5":{"99":2,"101":2}}],["如正交性",{"5":{"70":2}}],["如warmup或更高的学习率",{"5":{"89":2}}],["如warmup",{"5":{"41":2}}],["如whole",{"5":{"99":2,"101":2}}],["如几百到几千",{"5":{"90":2}}],["如几何分布",{"5":{"99":2,"101":2}}],["如去噪自编码",{"5":{"99":2,"101":2}}],["如对比学习损失",{"5":{"101":2}}],["如对底层使用较小学习率",{"5":{"97":2}}],["如对异常值敏感",{"5":{"99":2,"101":2}}],["实体与其属性的关联",{"5":{"92":2}}],["实部序列",{"5":{"90":1}}],["实部序列和虚部序列都是振幅为1的正弦波",{"5":{"90":1}}],["实验结果表明",{"5":{"91":2}}],["实验观察表明",{"5":{"87":2,"89":2,"93":2}}],["实验表明",{"5":{"41":2}}],["实证研究表明",{"5":{"87":2,"92":2,"93":2}}],["实证分析表明",{"5":{"92":2}}],["实证分析注意力矩阵的奇异值分布可以揭示其低秩结构的实际表现",{"5":{"87":2}}],["实证分析",{"2":{"87":1},"5":{"87":1}}],["实现复杂的长程依赖建模",{"5":{"92":2}}],["实现相对复杂",{"5":{"91":2}}],["实现简单",{"5":{"91":2}}],["实现长度外推",{"5":{"91":2}}],["实现上的简洁性",{"5":{"88":1}}],["实现上的简洁性是第二个因素",{"5":{"88":1}}],["实现",{"5":{"87":1,"96":1}}],["实现的计算",{"5":{"87":1}}],["实现了全局信息的并行建模",{"5":{"95":2}}],["实现了相对位置不变性",{"5":{"91":2}}],["实现了真正的并行计算",{"5":{"91":2}}],["实现了有效的正则化",{"5":{"87":2}}],["实现了线性复杂度",{"5":{"86":2}}],["实现建议",{"5":{"61":2}}],["实现模型并行",{"5":{"45":2}}],["实数编码的点积与复数编码的内积直接相关",{"5":{"90":2}}],["实数编码向量可以通过取复数编码的实部和虚部得到",{"5":{"90":2}}],["实数",{"5":{"50":2}}],["实践表明",{"5":{"97":2}}],["实践表明深度学习模型通常能够找到性能不错的解",{"5":{"48":2}}],["实践中通常使用",{"5":{"97":2}}],["实践中通常需要仔细的学习率调度",{"5":{"41":2}}],["实践中由于",{"5":{"89":2}}],["实践中",{"5":{"51":2,"89":6,"90":2,"93":2,"97":2}}],["实践中常用的策略是从较小的值",{"5":{"41":2}}],["实践中温度参数的选择",{"5":{"69":1}}],["实践意义",{"5":{"69":2}}],["实际是",{"5":{"87":1}}],["实际是​",{"5":{"87":1}}],["实际的上界为",{"5":{"89":1}}],["实际的上界为​",{"5":{"89":1}}],["实际的",{"5":{"87":2}}],["实际的神经网络由大量神经元组成",{"5":{"46":2}}],["实际观察到的秩通常仍然远小于",{"5":{"87":2}}],["实际性能测试表明",{"5":{"86":2}}],["实际上定义了输入空间的一种特定的分块对角结构",{"5":{"93":2}}],["实际上有更强的界",{"5":{"87":2}}],["实际上",{"5":{"61":2,"86":2,"89":2}}],["实际上等于",{"5":{"61":2,"89":2}}],["实际上都共享同一个数学骨架",{"5":{"69":2}}],["实际上在最大化序列不同位置之间的互信息",{"5":{"69":2}}],["实际上是在",{"5":{"94":1}}],["实际上是在自适应地构造基函数集合",{"5":{"93":2}}],["实际上是一个巨大的查找矩阵",{"5":{"50":2}}],["实际上是对数",{"5":{"70":2}}],["实际上就是一种温度调节",{"5":{"70":1}}],["实际显著",{"5":{"96":2}}],["实际参与传播的神经元数量约为",{"5":{"41":1}}],["实际参与传播的神经元数量约为​​",{"5":{"41":1}}],["求和简化为对单个非零项的计算",{"5":{"61":2}}],["求和即可",{"5":{"51":2}}],["求偏导",{"5":{"61":2}}],["求导",{"5":{"42":2,"70":2}}],["也直接影响最终模型的性能和泛化能力",{"5":{"97":2}}],["也接近0",{"5":{"95":1}}],["也不会太大",{"5":{"95":2}}],["也不会太大导致饱和",{"5":{"95":2}}],["也不影响最优参数的位置",{"5":{"51":2}}],["也有其",{"5":{"95":2}}],["也有明确的数学动机",{"5":{"91":2}}],["也可能正交",{"5":{"93":2}}],["也可以与内容投影同时计算",{"5":{"91":2}}],["也可以使用不同的学习率",{"5":{"89":2}}],["也可以尝试结合两种编码",{"5":{"89":2}}],["也编码了内容信息",{"5":{"92":2}}],["也只是落在实数轴的某个位置上",{"5":{"88":1}}],["也减少了出错的可能性",{"5":{"88":2}}],["也揭示了rope与其他位置编码方法的本质区别",{"5":{"88":2}}],["也适用于反向传播",{"5":{"61":2}}],["也就是最小化mse",{"5":{"51":2}}],["也决定了它在网络中的具体作用方式",{"5":{"50":2}}],["也指导着我们理解和解释模型训练过程中的各种现象",{"5":{"96":2}}],["也采用了类似的思想",{"5":{"96":2}}],["也是最经典的编码方法之一",{"5":{"91":2}}],["也是其主要挑战的来源",{"5":{"89":2}}],["也是设计新注意力变体的关键洞察",{"5":{"87":2}}],["也是为什么许多研究致力于开发高效的注意力变体",{"5":{"50":2}}],["也是证明神经网络泛化性质的关键工具",{"5":{"50":2}}],["也是理解反向传播算法的必要前提",{"5":{"46":2}}],["也是",{"5":{"45":1,"92":2}}],["也是参数更新的直接输入",{"5":{"44":2}}],["也使得sigmoid在某些特定场景下仍有应用价值",{"5":{"42":2}}],["也非常小",{"5":{"41":1}}],["也为下一节学习可学习位置编码的矩阵性质",{"5":{"90":2}}],["也为设计新的位置编码方案提供了理论基础",{"5":{"89":2}}],["也为设计新的激活函数和理解训练动态提供了理论指导",{"5":{"71":2}}],["也为选择和理解损失函数提供了坚实的统计学基础",{"5":{"61":2}}],["也为理解线性回归的闭式解提供了几何直觉",{"5":{"51":2}}],["也为理解transformer架构中非线性组件的设计提供了数学基础",{"5":{"71":2}}],["也为分析注意力机制如何利用位置信息提供了视觉基础",{"5":{"91":2}}],["也为分析位置编码的性质",{"5":{"90":2}}],["也为分析神经网络的信息流和表示学习提供了数学工具",{"5":{"71":2}}],["也为分析和改进现有优化算法提供了理论工具",{"5":{"42":2}}],["也为未来新型激活函数的发展提供了指导",{"5":{"41":2}}],["也为在实际机器学习项目中做出明智的损失函数选择提供了理论工具",{"5":{"51":2}}],["也为在复杂模型中选择合适的损失函数提供理论指导",{"5":{"70":2}}],["也为深入理解大语言模型的数学基础提供了整体视角",{"5":{"70":2}}],["也为改进模型训练策略提供了理论基础",{"5":{"99":2,"101":2}}],["也影响梯度的传播特性",{"5":{"41":2}}],["也称为相对熵",{"5":{"61":2}}],["也称为信息含量",{"5":{"61":2}}],["也称为正态分布",{"5":{"96":2}}],["也称为多项分布",{"5":{"96":2}}],["也称为隐藏表示或隐藏激活",{"5":{"46":2}}],["也称为逻辑函数",{"5":{"42":2}}],["fft",{"5":{"90":2}}],["ffn",{"5":{"46":2,"71":2}}],["fast",{"5":{"90":2}}],["fac基于层间独立性假设",{"5":{"48":2}}],["factored",{"5":{"48":2}}],["fac",{"5":{"48":2}}],["flash",{"5":{"86":8,"92":2,"93":2}}],["flash注意力约为",{"5":{"86":1}}],["flash注意力约为字节",{"5":{"86":1}}],["flash注意力相比标准实现可以获得2",{"5":{"86":2}}],["flash注意力将query",{"5":{"86":2}}],["flash注意力",{"2":{"86":1},"5":{"86":5}}],["flash注意力的各个版本都遵循相同的核心理论",{"5":{"86":2}}],["flash注意力的后续版本进一步优化了算法实现和硬件利用率",{"5":{"86":2}}],["flash注意力的io优化收益更大",{"5":{"86":2}}],["flash注意力的内存访问包括",{"5":{"86":2}}],["flash注意力的时间复杂度与标准注意力相同",{"5":{"86":2}}],["flash注意力的复杂度与性能分析",{"2":{"86":1},"5":{"86":1}}],["flash注意力的数学原理",{"2":{"86":1},"5":{"86":1}}],["flops",{"5":{"46":4}}],["floating",{"5":{"45":2}}],["flow",{"5":{"41":2,"50":4}}],["fully",{"5":{"92":2}}],["fused",{"5":{"61":4}}],["function",{"5":{"45":4,"47":2,"51":2,"61":2,"93":2,"96":6,"99":2,"101":2}}],["fp32",{"5":{"50":2,"97":4}}],["fp16或bf16",{"5":{"89":2}}],["fp16",{"5":{"50":2,"93":2,"97":10}}],["frequency",{"5":{"90":2,"91":4}}],["frobenius范数因其可导性好而最常用",{"5":{"50":2}}],["frobenius范数常用于权重衰减",{"5":{"50":2}}],["frobenius范数是最直观的矩阵范数",{"5":{"50":2}}],["frank",{"5":{"47":2}}],["frac",{"5":{"42":72,"46":2,"61":48,"69":2,"97":8}}],["field",{"5":{"92":2}}],["fine",{"5":{"50":2}}],["fisher信息矩阵定义了概率流形上的黎曼度量",{"5":{"71":2}}],["fisher信息矩阵定义为",{"5":{"71":2}}],["fisher信息度量",{"5":{"70":2,"71":2}}],["fisher",{"5":{"70":22,"97":2}}],["f^",{"5":{"46":6}}],["f",{"5":{"42":3,"46":1}}],["fourier",{"5":{"90":4}}],["forward",{"5":{"44":2,"45":4,"46":2,"71":2}}],["forall",{"5":{"42":2}}],["focal",{"5":{"70":2}}],["feed",{"5":{"71":2}}],["feedback",{"5":{"99":2,"101":2}}],["feature",{"5":{"70":2,"86":2}}],["对稀疏特征使用较大的学习率",{"5":{"97":2}}],["对键位置",{"5":{"95":1}}],["对矩阵的每一行进行归一化",{"5":{"95":1}}],["对偶性意味着",{"5":{"90":2}}],["对角线元素",{"5":{"95":2}}],["对角线元素保持在较大的值",{"5":{"95":1}}],["对角线元素接近0",{"5":{"95":1}}],["对角线上",{"5":{"45":1}}],["对角元素为各频率成分的旋转因子",{"5":{"90":1}}],["对角元素为各频率成分的旋转因子​",{"5":{"90":1}}],["对输出进行归一化",{"5":{"86":2}}],["对每个",{"5":{"61":2}}],["对每个通道内的所有空间位置和batch应用相同的归一化参数",{"5":{"41":2}}],["对每个位置的query和key向量应用rope旋转",{"5":{"88":2}}],["对每个位置",{"5":{"69":1}}],["对第",{"5":{"61":2}}],["对这个优化问题求导并令导数为零",{"5":{"51":2}}],["对异常值的敏感性等",{"5":{"51":2}}],["对异常值的敏感性",{"2":{"51":1},"5":{"51":3}}],["对任意",{"5":{"96":1}}],["对任意常数和随机变量",{"5":{"96":1}}],["对任意常数",{"5":{"47":2,"96":1}}],["对常数",{"5":{"96":1}}],["对常数有",{"5":{"96":1}}],["对注意力输出进行非线性变换",{"5":{"71":2}}],["对应维度的分量同时变大",{"5":{"95":2}}],["对应高频振动",{"5":{"91":2}}],["对应高频旋转",{"5":{"88":2}}],["对应低频振动",{"5":{"91":2}}],["对应低频旋转",{"5":{"88":2}}],["对应短周期",{"5":{"91":2}}],["对应长周期",{"5":{"91":2}}],["对应周期1",{"5":{"90":1}}],["对应周期10000",{"5":{"90":1}}],["对应唯一的相位向量",{"5":{"90":1}}],["对应圆群上的点",{"5":{"90":1}}],["对应不同的频率成分",{"5":{"90":1}}],["对应于",{"5":{"89":1}}],["对应于不同的编码配置",{"5":{"89":2}}],["对应于的位置",{"5":{"89":1}}],["对应于最大的非平凡特征值",{"5":{"87":2}}],["对应于特征值1",{"5":{"87":2}}],["对应",{"5":{"48":4,"51":1}}],["对应输入特征的索引",{"5":{"46":1}}],["对应输出神经元的索引",{"5":{"46":1}}],["对应hardmax",{"5":{"71":2}}],["对应硬路由和弱混合行为",{"5":{"41":2}}],["对应的编码向量",{"5":{"91":1}}],["对应的编码向量​和​是不同的",{"5":{"91":1}}],["对应的角频率为",{"5":{"90":1}}],["对应的时域序列会受到影响",{"5":{"90":2}}],["对应的旋转角度",{"5":{"88":2}}],["对应的旋转矩阵为",{"5":{"88":2}}],["对应的核函数为",{"5":{"86":6}}],["对应的预测为",{"5":{"51":1}}],["对应的子词索引集合为",{"5":{"99":2,"101":2}}],["对数频率轴上的等间隔对应于几何级数的频率间隔",{"5":{"90":2}}],["对数运算的引入使得kl散度具有概率解释上的优势",{"5":{"61":2}}],["对数几率",{"5":{"71":2,"96":2}}],["对数似然函数",{"5":{"61":1}}],["对数似然函数为",{"5":{"96":2}}],["对数似然可以分解为序列中每个位置的条件对数概率之和",{"5":{"96":2}}],["对数似然为",{"5":{"61":2,"71":2}}],["对数函数",{"5":{"70":1}}],["对加法和乘法封闭",{"5":{"71":2}}],["对",{"5":{"44":3,"45":2,"48":1,"61":2,"69":3,"95":1}}],["对所有位置都保持不变",{"5":{"88":1}}],["对所有可能的训练集",{"5":{"51":2}}],["对所有向量",{"5":{"50":1}}],["对所有层",{"5":{"44":1}}],["对所有成立",{"5":{"71":1}}],["对所有",{"5":{"61":5,"71":1,"96":1,"97":1}}],["对于强凸和利普希茨连续的损失函数",{"5":{"97":1}}],["对于强凸函数",{"5":{"48":2}}],["对于过参数化的神经网络",{"5":{"97":2}}],["对于随机初始化的神经网络",{"5":{"97":2}}],["对于可微函数",{"5":{"97":2}}],["对于掌握语言模型的生成机制和优化原理至关重要",{"5":{"96":2}}],["对于的序列长度",{"5":{"95":1}}],["对于复杂的输入使用更多的头",{"5":{"93":2}}],["对于复合函数",{"5":{"45":2}}],["对于简单的输入使用较少的头",{"5":{"93":2}}],["对于头数较多的模型",{"5":{"93":2}}],["对于长序列场景",{"5":{"93":2}}],["对于长度为的序列",{"5":{"92":1}}],["对于长度为的输入序列",{"5":{"89":1}}],["对于长度为",{"5":{"89":1,"92":1}}],["对于query",{"5":{"93":2,"94":2}}],["对于query投影",{"5":{"93":2}}],["对于远距离的位置对",{"5":{"92":2}}],["对于近距离的位置对",{"5":{"92":2}}],["对于语义依赖",{"5":{"92":2}}],["对于语法依赖",{"5":{"92":2}}],["对于语言模型",{"5":{"41":2}}],["对于需要建模长程依赖的任务",{"5":{"92":2}}],["对于需要长度外推的任务",{"5":{"91":2}}],["对于需要精细位置区分的任务",{"5":{"89":2}}],["对于互不谐波的多个频率",{"5":{"90":2}}],["对于最低频率成分",{"5":{"90":2}}],["对于最高频率成分",{"5":{"90":2}}],["对于最大的​",{"5":{"69":1}}],["对于最大的",{"5":{"69":1}}],["对于低频成分",{"5":{"90":2}}],["对于低维度",{"5":{"90":2,"91":6}}],["对于高频成分",{"5":{"90":2}}],["对于高维度",{"5":{"90":2,"91":6}}],["对于小的旋转角度",{"5":{"90":2}}],["对于维度为",{"5":{"94":1}}],["对于维度为的矩阵",{"5":{"94":1}}],["对于维度配对",{"5":{"90":2}}],["对于维随机向量",{"5":{"96":1}}],["对于​的行",{"5":{"89":1}}],["对于典型的位置编码",{"5":{"89":2}}],["对于典型配置",{"5":{"86":2}}],["对于向量",{"5":{"88":2}}],["对于向量值函数",{"5":{"44":2}}],["对于rope编码",{"5":{"88":2}}],["对于relu激活函数",{"5":{"41":2}}],["对于正弦位置编码",{"5":{"88":2}}],["对于把握位置编码设计的本质至关重要",{"5":{"88":2}}],["对于实际应用中的任意维度d",{"5":{"88":2}}],["对于实对称矩阵",{"5":{"50":2}}],["对于完整的d维向量",{"5":{"88":2}}],["对于位置接近序列开头",{"5":{"92":1}}],["对于位置依赖",{"5":{"92":2}}],["对于位置编码的分析",{"5":{"90":2}}],["对于位置信息不那么重要的任务",{"5":{"89":2}}],["对于位置和维度索引",{"5":{"88":1}}],["对于位置",{"5":{"88":3,"91":2,"92":3}}],["对于秩为",{"5":{"87":1}}],["对于秩为的矩阵",{"5":{"87":1}}],["对于某些复杂函数",{"5":{"87":2}}],["对于某些函数类",{"5":{"71":2}}],["对于更一般的注意力矩阵",{"5":{"87":2}}],["对于注意力计算阶段",{"5":{"93":2}}],["对于注意力矩阵的幂",{"5":{"87":2}}],["对于注意力矩阵",{"5":{"87":4}}],["对于注意力机制",{"5":{"69":2}}],["对于所有",{"5":{"87":3,"90":2,"92":1,"95":1}}],["对于所有特征值成立",{"5":{"87":2}}],["对于所有成立",{"5":{"87":1}}],["对于一般的行随机矩阵",{"5":{"87":2}}],["对于一个批次",{"5":{"50":2}}],["对于列和也为1的双随机矩阵",{"5":{"87":2}}],["对于标准化的输入",{"5":{"87":2}}],["对于从理论层面把握注意力机制的工作原理",{"5":{"87":2}}],["对于包含",{"5":{"61":1}}],["对于包含个样本的数据集",{"5":{"61":1}}],["对于概率空间中发生概率为",{"5":{"61":1}}],["对于概率空间中发生概率为的事件",{"5":{"61":1}}],["对于概率接近1的事件",{"5":{"61":2}}],["对于非单位向量",{"5":{"95":2}}],["对于非方阵投影",{"5":{"94":2}}],["对于非凸的mse损失",{"5":{"51":2}}],["对于非对称矩阵",{"5":{"41":2}}],["对于凸的",{"5":{"51":2}}],["对于新的测试样本",{"5":{"51":2}}],["对于给定的参数",{"5":{"51":2}}],["对于序列长度较大的应用场景",{"5":{"95":1}}],["对于序列长度为",{"5":{"86":2,"95":2}}],["对于序列长度",{"5":{"50":1,"95":1}}],["对于序列长度和注意力维度",{"5":{"50":1}}],["对于设计和实现高效的大规模训练系统至关重要",{"5":{"50":2}}],["对于大型模型",{"5":{"89":2}}],["对于大型矩阵",{"5":{"50":2}}],["对于大语言模型中的全连接层",{"5":{"50":2}}],["对于矩阵",{"5":{"50":1}}],["对于矩阵方程",{"5":{"50":2}}],["对于矩阵和",{"5":{"50":1}}],["对于方阵投影",{"5":{"94":2}}],["对于方阵",{"5":{"50":2}}],["对于任务特定的场景",{"5":{"91":2}}],["对于任何整数",{"5":{"50":2}}],["对于任意距离",{"5":{"92":2}}],["对于任意两个不同的位置",{"5":{"91":1}}],["对于任意两个不同的位置和",{"5":{"91":1}}],["对于任意两个分布",{"5":{"61":1}}],["对于任意两个分布和",{"5":{"61":1}}],["对于任意平移量",{"5":{"88":2}}],["对于任意位置",{"5":{"87":1,"91":2}}],["对于任意位置和",{"5":{"87":1}}],["对于任意概率分布",{"5":{"61":2}}],["对于任意矩阵",{"5":{"50":2}}],["对于任意输入",{"5":{"45":2}}],["对于任意连续函数",{"5":{"71":1}}],["对于任意连续函数和任意",{"5":{"71":1}}],["对于任意",{"5":{"41":2,"91":2}}],["对于任意向量",{"5":{"70":2}}],["对于任意置换",{"5":{"99":2,"101":2}}],["对于词汇表大小为",{"5":{"96":1}}],["对于词汇表大小为的语言模型",{"5":{"96":1}}],["对于深度学习模型",{"5":{"97":2}}],["对于深入理解语言模型的工作原理和改进模型设计都有重要意义",{"5":{"96":2}}],["对于深层网络和长序列",{"5":{"50":2}}],["对于深层网络",{"5":{"41":2}}],["对于两个单位向量",{"5":{"95":2}}],["对于两个分布",{"5":{"96":1}}],["对于两个分布和",{"5":{"96":1}}],["对于两个离散概率分布",{"5":{"96":1}}],["对于两个离散概率分布和",{"5":{"96":1}}],["对于两个随机变量",{"5":{"96":1}}],["对于两个随机变量和",{"5":{"96":1}}],["对于确定性分布",{"5":{"96":2}}],["对于连续随机变量",{"5":{"96":4}}],["对于乘积",{"5":{"48":1}}],["对于乘积​",{"5":{"48":1}}],["对于不同的输入序列",{"5":{"91":2}}],["对于不同的输出层配置",{"5":{"44":2}}],["对于不同的随机初始化",{"5":{"48":2}}],["对于具有l",{"5":{"48":2}}],["对于二分类任务",{"5":{"61":2}}],["对于二分类问题",{"5":{"47":2}}],["对于二分类",{"5":{"70":2}}],["对于二次可微函数",{"5":{"48":2}}],["对于残差形式的问题",{"5":{"48":2}}],["对于残差连接",{"5":{"41":2}}],["对于每一行",{"5":{"87":1}}],["对于每一行成立",{"5":{"87":1}}],["对于每个分量乘积",{"5":{"95":2}}],["对于每个",{"5":{"92":2}}],["对于每个频率",{"5":{"90":1}}],["对于每个频率​",{"5":{"90":1}}],["对于每个key块",{"5":{"86":1}}],["对于每个key块​和value块​",{"5":{"86":1}}],["对于每个样本",{"5":{"51":2}}],["对于每个测试样本",{"5":{"96":2}}],["对于每个位置",{"5":{"88":2,"96":2,"99":2,"101":2}}],["对于每个操作节点",{"5":{"48":2}}],["对于每个query块",{"5":{"86":1}}],["对于每个query块​",{"5":{"86":1}}],["对于每个query",{"5":{"69":2,"70":2}}],["对于每层",{"5":{"44":2}}],["对于函数",{"5":{"48":4}}],["对于多分类任务",{"5":{"61":2,"70":2}}],["对于多分类问题",{"5":{"47":2}}],["对于多元函数的复合",{"5":{"48":2}}],["对于理解深度学习框架的工作原理至关重要",{"5":{"47":2}}],["对于性质",{"5":{"47":6}}],["对于分析神经网络的能力边界和训练动态至关重要",{"5":{"47":2}}],["对于分类任务",{"5":{"41":2,"44":1,"69":2,"70":2}}],["对于超出",{"5":{"91":1}}],["对于超出​的序列",{"5":{"91":1}}],["对于超长序列的位置编码",{"5":{"89":2}}],["对于超长序列",{"5":{"89":2}}],["对于超平面",{"5":{"47":2}}],["对于超大规模模型",{"5":{"45":2}}],["对于逻辑非运算",{"5":{"47":2}}],["对于逻辑或运算",{"5":{"47":2}}],["对于逻辑与运算",{"5":{"47":2}}],["对于异或等线性不可分的模式无能为力",{"5":{"46":2}}],["对于gpt",{"5":{"45":2}}],["对于映射",{"5":{"45":2}}],["对于按范数裁剪",{"5":{"44":2}}],["对于神经网络损失函数",{"5":{"97":2}}],["对于神经网络中的mse梯度",{"5":{"51":2}}],["对于神经网络等非线性模型",{"5":{"51":2}}],["对于神经网络",{"5":{"44":2}}],["对于层全连接网络",{"5":{"44":1,"45":1}}],["对于层归一化",{"5":{"41":2}}],["对于常见的",{"5":{"93":1}}],["对于常见的的场景",{"5":{"93":1}}],["对于常见的均方误差和交叉熵损失",{"5":{"44":2}}],["对于常见的分类和回归任务",{"5":{"44":2}}],["对于批量输入",{"5":{"44":2,"45":2}}],["对于隐藏层",{"5":{"44":2}}],["对于输出投影阶段",{"5":{"93":2}}],["对于输出投影",{"5":{"93":2}}],["对于输出向量的第",{"5":{"46":1}}],["对于输出向量的第个元素",{"5":{"46":1}}],["对于输出层",{"5":{"44":2}}],["对于输入序列中的每个位置",{"5":{"93":2}}],["对于输入",{"5":{"45":1}}],["对于输入​",{"5":{"45":1}}],["对于输入向量",{"5":{"61":2,"69":2,"71":2}}],["对于softmax激活函数",{"5":{"71":2}}],["对于sigmoid和tanh激活函数",{"5":{"41":2}}],["对于sigmoid激活函数",{"5":{"41":2}}],["对于sigmoid",{"5":{"41":2,"42":2}}],["对于sigmoid函数",{"5":{"41":2}}],["对于相同的逼近精度",{"5":{"71":2}}],["对于这种复杂的分布",{"5":{"71":2}}],["对于视觉任务",{"5":{"41":2}}],["对于网络中的一条前向路径",{"5":{"41":2}}],["对于线性回归模型",{"5":{"51":2}}],["对于线性模型",{"5":{"51":4}}],["对于线性不可分的数据集",{"5":{"47":2}}],["对于线性系统",{"5":{"41":1}}],["对于线性系统​",{"5":{"41":1}}],["对于线性层",{"5":{"41":2}}],["对于线性变换",{"5":{"41":2}}],["对于",{"5":{"41":2,"44":1,"45":1,"86":2,"89":1,"90":2,"91":8,"95":1,"96":1,"97":1}}],["对于对称矩阵",{"5":{"41":2,"50":2}}],["对于tanh",{"5":{"41":2,"42":2}}],["对于激活函数",{"5":{"41":4,"42":2}}],["对于固定的查询位置",{"5":{"95":2}}],["对于固定的和",{"5":{"92":1}}],["对于固定的个事件",{"5":{"61":1}}],["对于固定的符号模式",{"5":{"45":4}}],["对于固定的",{"5":{"51":2,"61":1,"92":1}}],["对于其他",{"5":{"69":2}}],["对于回归任务",{"5":{"70":2}}],["对于均方误差",{"5":{"70":12,"97":6}}],["对于均方误差与",{"5":{"70":2}}],["对于交叉熵损失",{"5":{"70":6,"97":4}}],["对于交叉熵",{"5":{"70":8}}],["对于交叉熵与",{"5":{"70":2}}],["对于平方损失",{"5":{"70":2}}],["对于0",{"5":{"70":2}}],["对于校准需求",{"5":{"70":2}}],["对于排名需求",{"5":{"70":2}}],["对于离散随机变量及其概率分布",{"5":{"96":1}}],["对于离散随机变量",{"5":{"96":5}}],["对于离散动作空间",{"5":{"99":2,"101":2}}],["对的总影响是通过所有中间变量传导的",{"5":{"48":1}}],["对的梯度",{"5":{"69":3}}],["对的形式",{"5":{"99":2,"101":2}}],["对比学习",{"5":{"69":3,"70":2,"99":2,"101":2}}],["对比学习和注意力机制的统一数学工具",{"5":{"69":2}}],["对比学习和注意力机制",{"5":{"69":2}}],["对比学习中的infonce损失以及transformer中的注意力机制",{"5":{"69":2}}],["对比学习中的infonce同样基于信息论",{"5":{"69":2}}],["对比学习的理论局限",{"5":{"99":2,"101":2}}],["对该负样本的关注程度成正比",{"5":{"69":1}}],["对梯度向量逐元素限制在",{"5":{"44":1}}],["对梯度向量逐元素限制在范围内",{"5":{"44":1}}],["对梯度有双重影响",{"5":{"69":1}}],["对位置编码的处理会保留或抑制不同频率的成分",{"5":{"71":2}}],["对位置",{"5":{"69":1,"86":1,"92":3}}],["对称性破缺",{"5":{"93":2}}],["对称性",{"5":{"90":2}}],["对称性大大简化了hessian矩阵的分析和计算",{"5":{"48":2}}],["对称的",{"5":{"70":1}}],["对上述最优策略取对数",{"5":{"99":2,"101":2}}],["对齐程度",{"5":{"95":2}}],["对齐机制",{"5":{"99":2,"101":2}}],["直积群的元素是",{"5":{"88":1}}],["直积群的元素是个元素的元组",{"5":{"88":1}}],["直观地讲",{"5":{"88":2}}],["直观上",{"5":{"95":2,"96":2}}],["直觉上",{"5":{"61":2,"88":2}}],["直到",{"5":{"45":2}}],["直接在",{"5":{"97":2}}],["直接参与了位置",{"5":{"92":1}}],["直接连接",{"5":{"92":2}}],["直接继承自kl散度的非负性和熵的非负性",{"5":{"61":1}}],["直接优化kl散度",{"5":{"61":2}}],["直接求解特征方程通常是不切实际的",{"5":{"50":2}}],["直接计算正弦和余弦函数是昂贵的",{"5":{"88":2}}],["直接计算交叉熵",{"5":{"61":2}}],["直接计算",{"5":{"47":2}}],["直接由定义2",{"5":{"46":2}}],["直接由前向传播的计算顺序可得",{"5":{"45":2}}],["直接对复合函数求导会导致计算量爆炸",{"5":{"44":2}}],["直接加到净输入上",{"5":{"44":1}}],["直接加权可能导致某些任务的梯度主导训练",{"5":{"70":2}}],["直接加权可能导致训练不稳定",{"5":{"99":2,"101":2}}],["直接影响模型对序列结构的建模能力",{"5":{"70":2}}],["直接偏好优化",{"2":{"99":1,"101":1},"5":{"99":3,"101":3}}],["直接使用偏好数据优化策略",{"5":{"99":2,"101":2}}],["直线映射为直线",{"5":{"71":2}}],["计算查询与键之间的相似度矩阵",{"5":{"95":2}}],["计算第",{"5":{"93":1}}],["计算第组内所有头的注意力输出并拼接",{"5":{"93":1}}],["计算可以利用tensor",{"5":{"93":1}}],["计算可以重新排列为",{"5":{"86":2}}],["计算它们表示之间的余弦相似度",{"5":{"93":2}}],["计算它们之间的相对旋转角度",{"5":{"90":2}}],["计算所有位置对的某种关联度量",{"5":{"92":1}}],["计算一次性完成所有位置对的点积相似度",{"5":{"92":2}}],["计算一个软分布来表示每个位置的相关性",{"5":{"69":2}}],["计算需要次顺序计算",{"5":{"92":1}}],["计算时",{"5":{"92":2}}],["计算开销可能更大",{"5":{"91":2}}],["计算开销很小",{"5":{"89":2}}],["计算不同位置对的点积",{"5":{"91":2}}],["计算优化方面",{"5":{"89":2}}],["计算加速",{"5":{"89":2}}],["计算attention分数",{"5":{"88":2}}],["计算query和key向量",{"5":{"88":2}}],["计算角度数组",{"5":{"88":2}}],["计算旋转矩阵的四个元素并执行矩阵乘法",{"5":{"88":2}}],["计算转换为",{"5":{"87":1}}],["计算其奇异值分解",{"5":{"87":2}}],["计算其输出关于输入的梯度",{"5":{"48":2}}],["计算和",{"5":{"87":1,"88":1,"92":1}}],["计算指数差值",{"5":{"86":2}}],["计算行最大值",{"5":{"86":2}}],["计算块级别的注意力分数",{"5":{"86":2}}],["计算稀疏注意力分数矩阵的时间复杂度为",{"5":{"86":2}}],["计算损失",{"5":{"61":2,"99":2,"101":2}}],["计算也存在数值不稳定的风险",{"5":{"61":2}}],["计算会产生",{"5":{"61":2}}],["计算预测误差",{"5":{"51":2}}],["计算量与头数成正比",{"5":{"93":4}}],["计算量约为lu分解的一半",{"5":{"50":2}}],["计算量减少",{"5":{"41":2}}],["计算复杂度对比等数学工具",{"5":{"92":2}}],["计算复杂度从",{"5":{"87":1}}],["计算复杂度从降低到",{"5":{"87":1}}],["计算复杂度",{"5":{"86":2,"95":2}}],["计算复杂度分析",{"2":{"44":1,"95":1},"5":{"44":1,"95":1}}],["计算复杂度与内存效率",{"2":{"50":1},"5":{"50":1}}],["计算复杂度与类别数线性相关等",{"5":{"99":2,"101":2}}],["计算分类任务中的正确预测数量等",{"5":{"96":2}}],["计算各层的输出",{"5":{"46":2}}],["计算顺序",{"5":{"45":1}}],["计算​后合并结果",{"5":{"45":1}}],["计算图的前向遍历",{"5":{"45":2}}],["计算图的表示与实现",{"2":{"45":1},"5":{"45":1}}],["计算图是表示数学表达式的有向无环图",{"5":{"48":2}}],["计算图是连接数学理论和工程实现的桥梁",{"5":{"45":2}}],["计算图是有向无环图",{"5":{"45":2}}],["计算图是一个有向无环图",{"5":{"45":2}}],["计算图是前向传播构建的",{"5":{"44":2}}],["计算图",{"5":{"45":4}}],["计算方向",{"5":{"44":1}}],["计算类型",{"5":{"44":1}}],["计算的flops约为​",{"5":{"44":1}}],["计算的是query和key之间的相似度矩阵",{"5":{"71":2}}],["计算前一层误差",{"5":{"44":2}}],["计算偏置梯度约为",{"5":{"44":1}}],["计算偏置梯度约为​",{"5":{"44":1}}],["计算偏置梯度",{"5":{"44":2}}],["计算权重梯度",{"5":{"44":2}}],["计算梯度",{"5":{"44":2,"48":2}}],["计算成本较高",{"5":{"41":2}}],["计算矩阵的谱半径通常需要数值方法",{"5":{"41":2}}],["计算效率极低",{"5":{"92":2}}],["计算效率显著提升",{"5":{"87":2}}],["计算效率的理论分析",{"2":{"87":1},"5":{"87":1}}],["计算效率",{"2":{"41":1},"5":{"41":3,"91":2}}],["计算",{"5":{"44":5,"45":3,"61":8,"86":2,"87":2,"88":1,"92":4}}],["计算每次的统计量估计",{"5":{"96":2}}],["计算每个位置的输出",{"5":{"69":1}}],["本章的内容与第七章",{"5":{"101":2}}],["本章的核心目标是揭示一个更深层次的统一性",{"5":{"69":2}}],["本质上都是在解决一个相同的数学问题",{"5":{"61":2}}],["本质上是各频率成分相位差余弦的叠加",{"5":{"90":2}}],["本质上是一个依赖于输入数据的动态矩阵",{"5":{"87":2}}],["本质上是在寻找一组参数",{"5":{"51":2}}],["本质上是对输入",{"5":{"45":1}}],["本身的分布高度依赖于",{"5":{"94":1}}],["本身的分布高度依赖于的量级",{"5":{"94":1}}],["本身的数值问题",{"5":{"61":2}}],["本身是线性变换",{"5":{"47":1}}],["本身是线性的",{"5":{"45":1}}],["本身",{"5":{"45":2}}],["本节暂时忽略位置编码的影响",{"5":{"94":2}}],["本节深入分析了注意力机制建模长程依赖的数学原理",{"5":{"92":2}}],["本节深入探讨了旋转位置编码",{"5":{"88":2}}],["本节建立的概率解释框架为",{"5":{"61":2}}],["本节的内容可以概括为以下几个核心要点",{"5":{"47":2}}],["本节的核心内容可以概括为以下几点",{"5":{"44":2,"45":2}}],["本节系统性地分析了大语言模型中使用的各种损失函数",{"5":{"101":2}}],["本节系统地探讨了多头注意力的矩阵推导与表达能力分析",{"5":{"93":2}}],["本节系统地介绍了scaled",{"5":{"95":2}}],["本节系统地介绍了正弦余弦位置编码的数学定义",{"5":{"91":2}}],["本节系统地介绍了均方误差",{"5":{"51":2}}],["本节系统地分析了可学习位置编码的矩阵性质",{"5":{"89":2}}],["本节系统地阐述了神经元的数学模型",{"5":{"47":2}}],["本节系统阐述了反向传播算法的数学理论基础",{"5":{"44":1}}],["本节从优化理论的角度系统性地分析了损失函数的优化性质",{"5":{"97":2}}],["本节从频率空间的角度系统分析了正弦余弦位置编码的数学原理",{"5":{"90":2}}],["本节从谱分析和低秩结构的角度深入探讨了注意力矩阵的数学性质",{"5":{"87":2}}],["本节从信息论的基本概念",{"5":{"61":2}}],["本节从数学的角度系统性地分析了梯度饱和与梯度爆炸的根源",{"5":{"41":2}}],["本节从多个数学视角深入分析了前向传播的本质",{"5":{"45":1}}],["本节从多个数学视角系统性地分析了激活函数的角色",{"5":{"71":2}}],["本节从多个维度系统性地对比了均方误差和交叉熵两种主要损失函数的数学结构",{"5":{"70":2}}],["本节将详细推导",{"5":{"61":2}}],["本节将详细展示从二分类到多分类任务的完整推导过程",{"5":{"61":2}}],["本节将揭示交叉熵损失函数与统计学中经典的最大似然估计",{"5":{"61":2}}],["本节将深入分析这些数值稳定性问题",{"5":{"61":2}}],["本节将系统阐述如何将神经网络表示为矩阵运算的形式",{"5":{"46":2}}],["本节将系统性地分析主要激活函数的概率解释",{"5":{"71":2}}],["本节将系统分析激活函数与transformer其他组件",{"5":{"71":2}}],["本节将系统分析各类归一化技术的数学原理及其对梯度稳定性的影响",{"5":{"41":2}}],["本节将系统分析梯度爆炸的数学根源",{"5":{"41":2}}],["本节将从优化理论的角度",{"5":{"97":2}}],["本节将从矩阵变换的角度深入探讨query",{"5":{"94":2}}],["本节将从矩阵推导的角度系统地阐述多头注意力的计算过程",{"5":{"93":2}}],["本节将从傅里叶分析的基本原理出发",{"5":{"90":2}}],["本节将从线性代数和矩阵分析的角度",{"5":{"89":2}}],["本节将从谱分解",{"5":{"87":2}}],["本节将从复合函数",{"5":{"45":2}}],["本节将从数学角度深入分析注意力机制是如何实现这一突破的",{"5":{"92":2}}],["本节将从数学角度系统推导和解释正弦余弦位置编码的定义",{"5":{"91":2}}],["本节将从数学角度系统分析主要的注意力变体",{"5":{"86":2}}],["本节将从数学本质出发",{"5":{"88":2}}],["本节将从数学定义出发",{"5":{"51":2,"95":2}}],["本节将从数学建模的角度",{"5":{"47":2}}],["本节将从数学推导的角度",{"5":{"44":2}}],["本节将从数学的角度系统性地分析激活函数的理论基础",{"5":{"71":2}}],["本节将从数学的角度系统性地推导各类激活函数的导数公式",{"5":{"42":2}}],["本节将从数学的角度分析梯度流的各种机制",{"5":{"41":2}}],["本节将从数学的角度深入分析这两种现象的根源",{"5":{"41":2}}],["本节将从函数逼近论的角度",{"5":{"71":2}}],["本节将从信息论的基石概念出发",{"5":{"61":2}}],["本节将从信息论和概率论的角度",{"5":{"69":2}}],["本节将从更深的层次分析softmax操作",{"5":{"69":2}}],["本节将从多个角度深入分析梯度饱和的根源",{"5":{"41":2}}],["本节将从多个维度对不同损失函数的数学结构进行系统性的对比分析",{"5":{"70":2}}],["本节将把这些理论基础应用于大语言模型",{"5":{"99":2,"101":2}}],["本节小结",{"2":{"44":1,"45":1,"47":1,"51":1,"61":1,"70":1,"71":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"95":1,"97":1,"101":1},"5":{"44":1,"45":1,"47":1,"51":1,"61":1,"70":1,"71":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"95":1,"97":1,"101":1}}],["本节还讨论了损失函数与注意力机制之间的数学联系",{"5":{"70":2}}],["相近位置的编码向量在高维空间中相距较近",{"5":{"91":2}}],["相距很远的位置",{"5":{"91":2}}],["相加操作允许梯度同时流回内容编码和位置编码",{"5":{"91":2}}],["相加是一种线性融合方式",{"5":{"91":2}}],["相加保持了输入维度不变",{"5":{"91":2}}],["相邻位置在低维度区域的热力图模式相似度较低",{"5":{"91":2}}],["相邻位置的点在球面上相邻",{"5":{"91":2}}],["相邻位置的编码变化平滑",{"5":{"90":2}}],["相邻位置的编码向量相似",{"5":{"89":4}}],["相邻位置的编码应该相似",{"5":{"89":2}}],["相邻或相近的位置",{"5":{"91":2}}],["相邻词之间通常存在更强的语义关联",{"5":{"88":2}}],["相位变化很小",{"5":{"91":2}}],["相位",{"5":{"90":1}}],["相位向量的映射是一一对应的",{"5":{"90":2}}],["相位编码对应于圆群",{"5":{"90":2}}],["相位随位置均匀进动",{"5":{"90":2}}],["相位​随位置线性增加",{"5":{"90":1}}],["相位均匀分布",{"5":{"90":2}}],["相位调制",{"5":{"88":1}}],["相位旋转",{"5":{"88":2}}],["相角",{"5":{"88":2}}],["相同位置的位置投影是相同的",{"5":{"91":2}}],["相同的输入嵌入在不同空间中被赋予了不同的角色",{"5":{"94":2}}],["相同的架构从对称的初始状态出发",{"5":{"93":2}}],["相同的内容嵌入",{"5":{"91":2}}],["相同的位置差产生的编码差异较大",{"5":{"90":2}}],["相同的位置差产生的编码差异较小",{"5":{"90":2}}],["相同",{"5":{"87":1}}],["相当于在核空间中引入了可学习的权重",{"5":{"86":2}}],["相比",{"5":{"90":2}}],["相比于均方误差",{"5":{"61":2}}],["相比之下",{"5":{"41":2,"51":2,"70":4}}],["相反地",{"5":{"61":2}}],["相乘",{"5":{"50":2,"93":2}}],["相一致",{"5":{"45":2}}],["相对距离",{"5":{"88":1}}],["相对位置",{"5":{"101":2}}],["相对位置可学习性",{"5":{"92":2}}],["相对位置可表达性",{"5":{"91":2}}],["相对位置与内容无关的交互",{"2":{"92":1},"5":{"92":1}}],["相对位置对应于复平面上的相对旋转",{"5":{"90":2}}],["相对位置对应于圆群上的平移",{"5":{"90":2}}],["相对位置信息被",{"5":{"90":2}}],["相对位置的频率表示具有以下特点",{"5":{"90":2}}],["相对位置的频率表示",{"2":{"90":1},"5":{"90":3}}],["相对位置编码的缺点",{"5":{"91":2}}],["相对位置编码的优点",{"5":{"91":2}}],["相对位置编码的数学形式为",{"5":{"91":2}}],["相对位置编码问题转化为",{"5":{"90":2}}],["相对位置编码",{"5":{"88":2,"91":2,"92":2}}],["相对位置是什么",{"5":{"88":2}}],["相对位置应该决定注意力权重",{"5":{"88":2}}],["相对位置敏感性",{"5":{"70":2,"97":2}}],["相对熵",{"5":{"61":2,"70":3}}],["相对于其他",{"5":{"61":2}}],["相对于标准正态分布",{"5":{"96":2}}],["相对误差小于",{"5":{"44":2}}],["相关系数可用于分析不同词嵌入维度之间的冗余程度",{"5":{"96":2}}],["相关系数消除了量纲的影响",{"5":{"96":2}}],["相关系数的取值范围为",{"5":{"96":2}}],["相关系数",{"5":{"96":2}}],["相关",{"5":{"41":1,"70":2,"87":2,"89":2,"97":2}}],["相似度越高",{"5":{"95":2}}],["相似度",{"5":{"69":5,"94":2}}],["相似度很高时",{"5":{"69":1}}],["可泛化的解",{"5":{"97":2}}],["可泛化",{"5":{"92":2}}],["可逆性意味着变换是对应的",{"5":{"94":2}}],["可逆性",{"5":{"91":2}}],["可作为后续注意力层的输入",{"5":{"91":2}}],["可区分性",{"5":{"91":2}}],["可外推性",{"5":{"88":2}}],["可学习的位置编码在某些场景下能够取得更好的性能",{"5":{"91":2}}],["可学习的稀疏模式将稀疏模式参数化",{"5":{"86":2}}],["可学习编码没有这种先验",{"5":{"91":2}}],["可学习编码需要",{"5":{"91":1}}],["可学习编码需要的参数量",{"5":{"91":1}}],["可学习编码需要存储参数矩阵",{"5":{"89":2}}],["可学习编码只能处理训练时见过的长度",{"5":{"91":2}}],["可学习编码",{"5":{"89":1}}],["可学习编码在此基础上学习",{"5":{"89":2}}],["可学习编码可能取得更好的性能",{"5":{"91":2}}],["可学习编码可能更优",{"5":{"89":2}}],["可学习编码可以学习任意复杂的位置表示模式",{"5":{"91":2}}],["可学习编码可以自动发现领域特定的位置模式",{"5":{"89":2}}],["可学习编码可以通过最小二乘逼近正弦编码",{"5":{"89":1}}],["可学习编码是数据驱动的",{"5":{"89":2}}],["可学习编码学习到的基可能与正弦编码的基完全不同",{"5":{"89":2}}],["可学习编码的秩最多为",{"5":{"89":1}}],["可学习编码的秩最多为​",{"5":{"89":1}}],["可学习编码的外推能力取决于学习到的编码结构",{"5":{"89":2}}],["可学习编码的函数空间是维度不超过",{"5":{"89":1}}],["可学习编码的函数空间是维度不超过的线性子空间上的所有函数",{"5":{"89":1}}],["可学习编码的函数空间更加灵活但也更加模糊",{"5":{"89":2}}],["可学习编码的函数空间由训练数据决定",{"5":{"89":2}}],["可学习编码的表达能力更加灵活",{"5":{"89":2}}],["可学习编码的矩阵具有怎样的秩和奇异值结构",{"5":{"89":2}}],["可学习编码的矩阵性质",{"5":{"70":2}}],["可学习编码能够表示什么样的位置函数",{"5":{"89":2}}],["可学习位置编码将位置编码视为可学习的参数矩阵",{"5":{"91":2}}],["可学习位置编码将位置编码矩阵视为可训练的参数",{"5":{"89":2}}],["可学习位置编码作为一种重要的位置编码范式",{"5":{"89":2}}],["可学习位置编码通常与正弦余弦编码表现相当",{"5":{"89":2}}],["可学习位置编码通常收敛到某种",{"5":{"89":2}}],["可学习位置编码与正弦余弦位置编码在数学性质上有显著差异",{"5":{"89":2}}],["可学习位置编码在外推时面临的挑战是",{"5":{"89":2}}],["可学习位置编码在理论上可以逼近正弦余弦位置编码",{"5":{"89":2}}],["可学习位置编码在嵌入空间中形成特定的几何结构",{"5":{"89":2}}],["可学习位置编码在transformer架构的早期研究和某些特定任务上取得了良好的性能",{"5":{"89":2}}],["可学习位置编码没有固定的数学公式",{"5":{"89":2}}],["可学习位置编码涉及参数化矩阵的性质分析",{"5":{"89":2}}],["可学习位置编码",{"5":{"89":2}}],["可学习位置编码的有效表示可以用",{"5":{"89":1}}],["可学习位置编码的有效表示可以用个秩",{"5":{"89":1}}],["可学习位置编码的性能对超参数选择敏感",{"5":{"89":2}}],["可学习位置编码的存储和计算优化是实际部署中的重要问题",{"5":{"89":2}}],["可学习位置编码的参数量为",{"5":{"89":1}}],["可学习位置编码的参数量为​",{"5":{"89":1}}],["可学习位置编码的参数效率是实际应用中的重要考量",{"5":{"89":2}}],["可学习位置编码的收敛性分析涉及优化理论",{"5":{"89":2}}],["可学习位置编码的梯度流动是理解训练动态的关键",{"5":{"89":2}}],["可学习位置编码的初始化对训练动态有重要影响",{"5":{"89":2}}],["可学习位置编码的表达能力虽然灵活",{"5":{"89":2}}],["可学习位置编码的表达式能力是指它能够表示的位置函数的范围",{"5":{"89":2}}],["可学习位置编码的数学分析不仅有助于深入理解其工作原理",{"5":{"89":2}}],["可学习位置编码的定义与参数化",{"2":{"89":1},"5":{"89":1}}],["可学习位置编码的矩阵性质<",{"5":{"85":1}}],["可学习位置编码的矩阵性质",{"0":{"89":1},"4":{"89":1},"5":{"85":4,"89":1}}],["可达数十亿",{"5":{"48":2}}],["可验证为正交矩阵",{"5":{"45":2}}],["可得",{"5":{"44":2}}],["可微位置编码",{"5":{"88":1}}],["可微位置编码通过可学习的参数化方式来自动发现最优的位置编码函数",{"5":{"88":1}}],["可微性",{"5":{"61":1}}],["可微性保证其可以作为神经网络的输出层进行端到端优化",{"5":{"61":1}}],["可微函数是凸函数的充要条件是其梯度单调不减",{"5":{"48":2}}],["可微",{"5":{"71":2}}],["可加性",{"5":{"71":2}}],["可能会带来性能提升",{"5":{"101":2}}],["可能会超出浮点数的精度范围",{"5":{"88":2}}],["可能",{"5":{"97":1}}],["可能相交",{"5":{"93":1}}],["可能限制了其捕获复杂模式的能力",{"5":{"93":2}}],["可能限制了位置信息的利用",{"5":{"89":2}}],["可能为负或较小",{"5":{"91":2}}],["可能出现",{"5":{"90":2}}],["可能学习到最优的组合",{"5":{"89":2}}],["可能保持较大的谱范数",{"5":{"87":2}}],["可能负责某种",{"5":{"87":4}}],["可能丢失高维信息",{"5":{"87":2}}],["可能略有不同",{"5":{"87":2}}],["可能无法同时捕获足够多样的特征",{"5":{"93":2}}],["可能无法精确表示这类模式",{"5":{"86":2}}],["可能无法捕捉复杂的数据模式",{"5":{"51":2}}],["可能很大",{"5":{"86":2}}],["可能在某些场景下并非最优",{"5":{"86":2}}],["可能下溢为",{"5":{"61":2}}],["可能溢出双精度浮点数的表示范围",{"5":{"61":2}}],["可能对训练数据中的噪声过度敏感",{"5":{"51":2}}],["可能对权重和偏置维护不同的优化状态",{"5":{"46":2}}],["可能是超平面",{"5":{"51":2}}],["可能是曲线",{"5":{"71":2}}],["可能具有不同的范数尺度",{"5":{"41":1}}],["可能直接落在饱和区域",{"5":{"41":1}}],["可能带来性能的提升",{"5":{"41":2}}],["可能捕获更复杂的模式",{"5":{"41":2}}],["可能导致数值问题",{"5":{"97":2}}],["可能导致优化轨迹偏离连续梯度流",{"5":{"97":2}}],["可能导致优化目标与概率解释不一致",{"5":{"61":2}}],["可能导致训练过程不稳定或收敛较慢",{"5":{"96":2}}],["可能导致训练不稳定",{"5":{"42":2}}],["可能导致",{"5":{"41":2}}],["可能放大或缩小梯度",{"5":{"41":2}}],["可能产生范数增长",{"5":{"41":2}}],["可能产生问题",{"5":{"41":2}}],["可能还有value",{"5":{"69":2}}],["可能不符合正弦余弦编码的假设",{"5":{"89":2}}],["可能不遵循线性位置关系",{"5":{"88":2}}],["可能不精确",{"5":{"61":2}}],["可能不足以推动模型从错误中学习",{"5":{"70":2}}],["可能需要使用加权的交叉熵或",{"5":{"70":2}}],["可以根据输入动态调整",{"5":{"101":2}}],["可以根据梯度范数",{"5":{"99":2,"101":2}}],["可以灵活地适应各种序列处理场景",{"5":{"95":2}}],["可以让模型专注于最相关的位置",{"5":{"95":2}}],["可以帮助模型探索不同的注意力模式",{"5":{"95":2}}],["可以帮助我们从另一个角度理解多头注意力的本质",{"5":{"93":2}}],["可以表示为",{"5":{"94":2}}],["可以表示为确定函数",{"5":{"71":1}}],["可以构造",{"5":{"93":1}}],["可以构造和",{"5":{"93":1}}],["可以类比为",{"5":{"93":2}}],["可以直接",{"5":{"92":2}}],["可以保存在寄存器或共享内存中",{"5":{"92":2}}],["可以保证泛化误差的有界性",{"5":{"61":2}}],["可以并行",{"5":{"92":2}}],["可以并行执行",{"5":{"45":2,"92":2}}],["可以处理任意位置的查询",{"5":{"91":2}}],["可以清晰地看到",{"5":{"91":2}}],["可以清晰地看到预期的u型曲线模式",{"5":{"51":2}}],["可以简化为",{"5":{"91":2}}],["可以唯一恢复位置信息",{"5":{"91":2}}],["可以唯一表示该频率分量的任意相位",{"5":{"91":2}}],["可以唯一确定任意位置",{"5":{"90":2}}],["可以获得更深刻的洞察",{"5":{"90":2}}],["可以统一表示",{"5":{"90":2}}],["可以考虑增量计算",{"5":{"89":2}}],["可以考虑压缩存储",{"5":{"89":2}}],["可以减少总参数量同时保持各层所需的信息分辨率",{"5":{"89":2}}],["可以提高外推性能",{"5":{"89":2}}],["可以适应特定任务的分布",{"5":{"89":2}}],["可以无缝集成到各种模型架构中",{"5":{"88":2}}],["可以高效地处理大批量的序列",{"5":{"88":2}}],["可以近似表示为低秩分解",{"5":{"87":1}}],["可以递归更新",{"5":{"87":2}}],["可以避免softmax进入极端区域",{"5":{"87":2}}],["可以避免数值溢出",{"5":{"86":2}}],["可以揭示其数值特性",{"5":{"87":2}}],["可以揭示其信息处理能力的本质",{"5":{"71":2}}],["可以与所有位置交互",{"5":{"86":2}}],["可以设置",{"5":{"61":2}}],["可以稳定计算为",{"5":{"61":2}}],["可以发现其蕴含着深刻的统计学内涵",{"5":{"61":2}}],["可以被分解为投影部分和垂直部分的和",{"5":{"51":1}}],["可以被视为一个",{"5":{"71":2}}],["可以验证",{"5":{"51":2,"90":2}}],["可以推导出mae",{"5":{"51":2}}],["可以推导出sigmoid的三阶导数",{"5":{"42":2}}],["可以看作是查询张量和键张量的收缩",{"5":{"50":1}}],["可以看出",{"5":{"41":1,"69":1}}],["可以一次性完成所有样本的计算",{"5":{"50":2}}],["可以显著减少模型参数和计算量",{"5":{"87":2}}],["可以显著减少不必要的内存分配和数据拷贝",{"5":{"50":2}}],["可以显著减少参数量和计算量",{"5":{"50":2}}],["可以用振幅或方差来度量",{"5":{"90":2}}],["可以用一个标量来精确量化",{"5":{"61":2}}],["可以用一个矩阵",{"5":{"50":1}}],["可以用投影矩阵",{"5":{"51":1}}],["可以用投影矩阵表示",{"5":{"51":1}}],["可以用于表示高阶交互关系",{"5":{"50":2}}],["可以在保持模型性能的同时显著减少参数量和计算开销",{"5":{"50":2}}],["可以在任何情况下使用",{"5":{"41":2}}],["可以得到和",{"5":{"93":1}}],["可以得到不同稀疏程度的注意力矩阵",{"5":{"86":2}}],["可以得到最优解",{"5":{"51":2}}],["可以得到p值",{"5":{"96":2}}],["可以得到各阶矩",{"5":{"96":2}}],["可以得到",{"5":{"69":2,"93":1}}],["可以理解为在键空间中对查询向量进行的一种",{"5":{"95":2}}],["可以理解为一维数组中的有序数集",{"5":{"50":2}}],["可以理解为将输入向量",{"5":{"47":1}}],["可以理解为",{"5":{"45":2}}],["可以理解为寻找能够最好地逼近目标映射的复合函数形式",{"5":{"45":2}}],["可以理解为value向量在注意力权重分布下的期望",{"5":{"70":1}}],["可以分解为",{"5":{"45":1}}],["可以分解为旋转",{"5":{"45":1}}],["可以从样本级",{"5":{"45":2}}],["可以系统地组织前向传播的计算步骤",{"5":{"45":2}}],["可以系统地探索激活函数的参数空间",{"5":{"41":2}}],["可以将每个头分配给一个流式多处理器",{"5":{"93":2}}],["可以将时间复杂度降低到",{"5":{"87":2}}],["可以将批次进一步划分为更小的微批次",{"5":{"50":2}}],["可以将权重矩阵",{"5":{"45":1}}],["可以将权重矩阵按列",{"5":{"45":1}}],["可以将多个样本的梯度计算合并为矩阵运算",{"5":{"44":2}}],["可以完全并行执行",{"5":{"45":2}}],["可以充分利用硬件的并行计算能力",{"5":{"45":2}}],["可以精确地计算任意复杂函数的导数",{"5":{"45":2}}],["可以精确重构任意复杂的连续函数模式",{"5":{"71":2}}],["可以正交对角化为",{"5":{"45":2}}],["可以取任意值",{"5":{"71":2,"91":1}}],["可以学习从低级局部模式到高级抽象语义的层次化表示",{"5":{"71":2}}],["可以通过温度参数",{"5":{"96":1}}],["可以通过温度参数来调节softmax的",{"5":{"96":1}}],["可以通过缓存机制减少空间需求",{"5":{"93":2}}],["可以通过最小二乘逼近正弦编码",{"5":{"89":1}}],["可以通过填充零向量或截断来处理",{"5":{"88":2}}],["可以通过采样",{"5":{"71":1}}],["可以通过采样来估计",{"5":{"71":1}}],["可以通过调整每个曲面的位置",{"5":{"71":2}}],["可以通过反向传播自动学习",{"5":{"41":1}}],["可以形成复杂的决策边界",{"5":{"71":2}}],["可以定义为各层雅可比矩阵范数的乘积",{"5":{"41":2}}],["可以证明",{"5":{"93":2,"99":2,"101":2}}],["可以证明上述下界不等式",{"5":{"69":2}}],["可以证明最优策略为",{"5":{"99":2,"101":2}}],["可以是任意形式的得分函数",{"5":{"69":2}}],["可以是温度或​",{"5":{"69":1}}],["可以是温度",{"5":{"69":1}}],["可以很容易地修改为",{"5":{"69":2}}],["可以借鉴交叉熵优化的数值稳定性处理策略",{"5":{"70":2}}],["可以使用常量内存",{"5":{"89":2}}],["可以使用黎曼梯度下降",{"5":{"70":1}}],["可以使用",{"5":{"70":3}}],["可以确保优化过程收敛到帕累托前沿",{"5":{"70":2}}],["可以视为对值向量",{"5":{"92":1}}],["可以视为绝对位置编码和相对位置编码的桥梁",{"5":{"91":2}}],["可以视为一个傅里叶级数展开",{"5":{"90":2}}],["可以视为一种",{"5":{"69":2,"87":1}}],["可以视为两个分布在fisher信息度量下的",{"5":{"70":1}}],["可以采用稀疏softmax",{"5":{"99":2,"101":2}}],["可扩展性",{"5":{"69":2}}],["可预测的",{"5":{"70":2}}],["非对称的成对交互",{"5":{"93":2}}],["非对角线元素是变量之间的协方差",{"5":{"96":2}}],["非零奇异值的数量等于矩阵的秩",{"5":{"89":1}}],["非零奇异值的数量等于矩阵的秩​",{"5":{"89":1}}],["非零元素集中在主对角线附近的",{"5":{"87":1}}],["非零元素集中在主对角线附近的条对角线上",{"5":{"87":1}}],["非零元素比例为",{"5":{"87":2}}],["非凸",{"5":{"97":2}}],["非凸性意味着存在多个局部极小值",{"5":{"97":2}}],["非凸性意味着梯度下降可能收敛到局部最小值而非全局最小值",{"5":{"48":2}}],["非凸优化是深度学习面临的现实挑战",{"5":{"48":2}}],["非饱和但有界",{"5":{"71":2}}],["非单调性",{"5":{"71":2}}],["非平凡",{"5":{"71":2}}],["非线性依赖的局限",{"5":{"88":1}}],["非线性依赖的局限是另一个理论考虑",{"5":{"88":1}}],["非线性位置变换破坏了旋转群的代数结构",{"5":{"88":2}}],["非线性特征映射通过堆叠多层神经网络来学习复杂的特征映射",{"5":{"86":2}}],["非线性激活使得网络能够学习越来越复杂的特征组合",{"5":{"45":2}}],["非线性激活函数的",{"5":{"71":2}}],["非线性",{"5":{"47":2,"71":5}}],["非线性变换打破表达瓶颈",{"2":{"71":1},"5":{"71":1}}],["非线性的数学必要性",{"2":{"71":1},"5":{"71":1}}],["非线性表达能力",{"5":{"41":2}}],["非常大时",{"5":{"61":2}}],["非常数",{"5":{"71":2}}],["非常接近1",{"5":{"88":2}}],["非常接近",{"5":{"42":2}}],["非常小时",{"5":{"61":2}}],["非常小",{"5":{"41":1}}],["非常高效",{"5":{"41":2}}],["非负性直接继承自kl散度的非负性和熵的非负性",{"5":{"61":1}}],["非负性对所有成立",{"5":{"96":1}}],["非负性",{"5":{"61":4,"87":2,"96":3}}],["非负",{"5":{"69":2}}],["并探讨其在矩阵运算中的几何意义",{"5":{"95":2}}],["并通过实证研究的发现",{"5":{"93":2}}],["并通过实证研究描述了奇异值的快速衰减模式",{"5":{"87":2}}],["并从表示学习的理论视角探讨多头机制如何增强模型的表达能力",{"5":{"93":2}}],["并从几何和概率两个角度分析了神经元的行为",{"5":{"47":2}}],["并在整个训练过程中保持固定",{"5":{"91":2}}],["并建立",{"5":{"61":2}}],["并建立与后续章节的联系",{"5":{"47":2}}],["并建立与注意力机制章节的数学联系",{"5":{"70":2}}],["并介绍业界标准解决方案",{"5":{"61":2}}],["并深入分析其与注意力机制中softmax运算的内在联系",{"5":{"61":2}}],["并深入探讨激活函数导数对神经网络训练的影响",{"5":{"42":2}}],["并分析这一能力的理论极限",{"5":{"92":2}}],["并分析了谱半径与信息传播收敛性的关系",{"5":{"87":2}}],["并分析了偏差",{"5":{"51":2}}],["并分析反向传播的计算复杂度",{"5":{"44":2}}],["并讨论了mse与其他损失函数",{"5":{"51":2}}],["并将计算速度提高近一倍",{"5":{"50":2}}],["并有效缓解梯度消失问题",{"5":{"50":2}}],["并有能力实现自己的深度学习框架或深入理解现有框架的底层机制",{"5":{"44":2}}],["并相应地调整学习率",{"5":{"48":2}}],["并各自产生输出",{"5":{"46":1}}],["并各自产生输出​",{"5":{"46":1}}],["并行计算特性",{"5":{"93":2}}],["并行计算与硬件加速",{"2":{"93":1},"5":{"93":1}}],["并行粒度",{"5":{"45":1}}],["并行层次",{"5":{"45":1}}],["并为反向传播的梯度计算提供数据结构支持",{"5":{"45":2}}],["并为理解网络的表达能力和计算特性提供了理论工具",{"5":{"45":2}}],["并按与前向传播相反的顺序传播这些梯度",{"5":{"45":2}}],["并按与前向传播相同的顺序计算",{"5":{"45":2}}],["并化简",{"5":{"44":1}}],["并与反向传播计算的梯度进行比较",{"5":{"44":2}}],["并归一化后作为加权系数",{"5":{"69":2}}],["并且与后续的内积运算具有良好的兼容性",{"5":{"88":2}}],["并且其结构与输入数据的内在维度密切相关",{"5":{"87":2}}],["并且计算成本更低",{"5":{"48":2}}],["并且具有以下性质",{"5":{"70":2}}],["并非所有权重组合都能产生帕累托最优解",{"5":{"70":2}}],["正态初始化",{"5":{"89":2}}],["正态初始化和零初始化",{"5":{"89":2}}],["正因如此",{"5":{"88":2}}],["正定gram矩阵",{"5":{"89":2}}],["正定核函数对应于特征函数展开",{"5":{"87":2}}],["正定核只能表示",{"5":{"86":2}}],["正定",{"5":{"86":2}}],["正定性约束限制了可表示的注意力模式",{"5":{"86":2}}],["正定hessian",{"5":{"51":2}}],["正值性",{"5":{"86":2}}],["正数很大或负数很负",{"5":{"61":2}}],["正向",{"5":{"96":1}}],["正向鼓励覆盖的所有模式",{"5":{"96":1}}],["正面或反面",{"5":{"96":2}}],["正交投影与投影矩阵的初始化",{"2":{"94":1},"5":{"94":1}}],["正交投影与最优预测",{"2":{"51":1},"5":{"51":1}}],["正交投影的另一个重要几何性质是误差分解",{"5":{"51":2}}],["正交投影的几何意义可以从以下几个方面理解",{"5":{"51":2}}],["正交投影是一个线性算子",{"5":{"51":2}}],["正交投影是唯一的",{"5":{"51":2}}],["正交投影给出了从目标向量到预测空间的最短距离",{"5":{"51":2}}],["正交变换仍然是有价值的分析工具",{"5":{"50":2}}],["正交约束在优化中被用于防止权重矩阵的秩退化",{"5":{"50":2}}],["正交初始化将权重矩阵初始化为正交矩阵",{"5":{"50":2}}],["正交初始化是一种常用的权重初始化方法",{"5":{"50":2}}],["正交矩阵是满足",{"5":{"50":1}}],["正交矩阵是满足的方阵",{"5":{"50":1}}],["正交矩阵的行列式的绝对值为1",{"5":{"50":2}}],["正交矩阵的逆矩阵等于其转置矩阵",{"5":{"50":2}}],["正交矩阵保持向量的范数不变",{"5":{"50":2}}],["正交矩阵有几个重要的性质",{"5":{"50":2}}],["正交性有着多方面的应用",{"5":{"50":2}}],["正交性是线性代数中一个核心概念",{"5":{"50":2}}],["正交性与正交矩阵",{"2":{"50":1},"5":{"50":1}}],["正交",{"5":{"48":2,"51":1,"90":1}}],["正如我们在4",{"5":{"46":2}}],["正弦函数和余弦函数在区间",{"5":{"91":1}}],["正弦函数和余弦函数在区间上是单射的",{"5":{"91":1}}],["正弦和余弦是相位相差",{"5":{"91":1}}],["正弦和余弦是相位相差的同一频率振动",{"5":{"91":1}}],["正弦和余弦曲线随位置变化剧烈",{"5":{"91":2}}],["正弦和余弦的值必然不同",{"5":{"91":2}}],["正弦和余弦的组合使得编码具有",{"5":{"91":2}}],["正弦和余弦构成了一组正交基",{"5":{"91":2}}],["正弦和余弦函数的取值范围是",{"5":{"71":2}}],["正弦与余弦成对出现",{"5":{"91":2}}],["正弦编码或其变体",{"5":{"91":2}}],["正弦编码和可学习编码在不同任务和场景下各有优势",{"5":{"91":2}}],["正弦编码隐含了",{"5":{"91":2}}],["正弦编码没有可训练参数",{"5":{"91":2}}],["正弦编码的优势在于可以处理任意长度的序列",{"5":{"91":2}}],["正弦编码的函数空间具有明确的结构",{"5":{"89":2}}],["正弦编码提供了",{"5":{"89":2}}],["正弦编码在推理时无需额外计算",{"5":{"89":2}}],["正弦编码天然支持长度外推",{"5":{"89":2}}],["正弦位置编码的一个重要数学性质是",{"5":{"92":2}}],["正弦位置编码的数学性质",{"2":{"92":1},"5":{"92":1}}],["正弦位置编码采用的是",{"5":{"88":1}}],["正弦位置编码采用的是加性注入的设计哲学",{"5":{"88":1}}],["正弦位置编码将位置编码为",{"5":{"88":1}}],["正弦位置编码将位置编码为幅度",{"5":{"88":1}}],["正弦位置编码虽然简单",{"5":{"88":2}}],["正弦位置编码和可学习的绝对位置嵌入都属于这一类别",{"5":{"88":2}}],["正弦位置编码和rope代表了两种截然不同的位置编码范式",{"5":{"88":2}}],["正弦位置编码在理论上可以外推到任意长度",{"5":{"88":2}}],["正弦位置编码也采用了类似的思想",{"5":{"88":2}}],["正弦位置编码存在根本性的数学缺陷",{"5":{"88":2}}],["正弦余弦位置编码以其简洁的数学形式",{"5":{"91":2}}],["正弦余弦位置编码与可学习位置编码",{"5":{"91":2}}],["正弦余弦位置编码在高维空间中形成特定的几何结构",{"5":{"91":2}}],["正弦余弦位置编码中的各个参数都经过精心设计",{"5":{"91":2}}],["正弦余弦位置编码",{"5":{"91":2}}],["正弦余弦位置编码采用指数分布的频率",{"5":{"90":2}}],["正弦余弦位置编码可以自然地视为不同频率正弦波的叠加",{"5":{"90":2}}],["正弦余弦位置编码可以视为在频域中",{"5":{"90":2}}],["正弦余弦位置编码实际上是在频率空间中构造位置信息的表示",{"5":{"90":2}}],["正弦余弦位置编码的核心思想是为序列中的每个位置",{"5":{"91":1}}],["正弦余弦位置编码的核心思想是为序列中的每个位置生成一个维的编码向量",{"5":{"91":1}}],["正弦余弦位置编码的高维空间结构可以通过降维可视化来理解",{"5":{"91":2}}],["正弦余弦位置编码的数学定义",{"2":{"91":1},"5":{"91":1}}],["正弦余弦位置编码的数学定义与推导<",{"5":{"85":1}}],["正弦余弦位置编码的数学定义与推导",{"0":{"91":1},"4":{"91":1},"5":{"85":4,"91":1}}],["正弦余弦位置编码的频率选择",{"5":{"90":2}}],["正弦余弦位置编码的精妙之处在于其背后的频率空间",{"5":{"90":2}}],["正弦余弦编码是一种绝对位置编码",{"5":{"91":2}}],["正弦余弦编码是确定性的",{"5":{"89":2,"91":2}}],["正弦余弦编码与其他编码的比较",{"2":{"91":1},"5":{"91":1}}],["正弦余弦编码通常表现更好",{"5":{"89":2}}],["正弦余弦编码的参数选择是通过实验调优确定的",{"5":{"90":2}}],["正弦余弦编码的频谱结构",{"2":{"90":1},"5":{"90":1}}],["正弦余弦编码的频率分解特性意味着不同频率的成分携带不同尺度的位置信息",{"5":{"71":2}}],["正弦余弦编码的固定结构提供了一种隐式正则化",{"5":{"89":2}}],["正弦余弦编码的函数空间是固定的",{"5":{"89":4}}],["正是为了实现这一约束而设计的",{"5":{"95":2}}],["正是为这一目的而设计的度量工具",{"5":{"61":2}}],["正是因为它对任何矩阵都成立",{"5":{"50":2}}],["正是衡量两个概率分布",{"5":{"71":1}}],["正是非线性激活函数赋予了网络这种强大的表达能力",{"5":{"71":2}}],["正是这个函数的傅里叶基函数",{"5":{"90":1}}],["正是这些代数性质保证了平移等变性的成立",{"5":{"88":2}}],["正是这些保持的性质限制了线性映射的表达能力",{"5":{"71":2}}],["正是这种简单性带来了卓越的计算效率和优异的性能",{"5":{"42":2}}],["正是交叉熵损失",{"5":{"99":1,"101":1}}],["正则化效应",{"5":{"97":2}}],["正则化效果",{"5":{"61":2}}],["正则化等",{"5":{"61":2}}],["正则化",{"5":{"50":2}}],["正负样本对比机制的几何解释",{"2":{"69":1},"5":{"69":1}}],["正样本对",{"5":{"69":2}}],["正样本的编码",{"5":{"69":2}}],["正样本键",{"5":{"69":2}}],["正样本位于附近",{"5":{"69":1}}],["正样本",{"5":{"69":5,"101":2}}],["正规方程",{"5":{"70":2}}],["正确答案",{"5":{"96":2}}],["正确选项梯度减去错误选项梯度",{"5":{"99":2,"101":2}}],["但正则化效应使得最终解倾向于简单",{"5":{"97":2}}],["但正弦余弦编码因其简洁性和良好的外推能力仍被广泛使用",{"5":{"91":2}}],["但既不是局部极小值也不是局部极大值",{"5":{"97":1}}],["但同样捕获了输入数据的重要结构",{"5":{"94":2}}],["但同时",{"5":{"93":2}}],["但同时也意味着",{"5":{"92":2}}],["但同时也带来了新的挑战",{"5":{"89":2}}],["但kaiming初始化在实践中也被广泛使用",{"5":{"94":2}}],["但能提升表示的丰富性",{"5":{"93":2}}],["但并行计算的",{"5":{"93":2}}],["但并没有告诉我们如何调整参数以改善输出",{"5":{"44":2}}],["但每个头的参数数量减少",{"5":{"93":2}}],["但每个头的维度变小",{"5":{"93":2}}],["但每个频率成分乘以一个相位因子",{"5":{"90":4}}],["但存在一些函数可以用多头注意力表示",{"5":{"93":2}}],["但总参数量与单头注意力",{"5":{"93":2}}],["但使用各自独立的投影矩阵",{"5":{"93":2}}],["但需要额外的训练策略来确保长度外推的性能",{"5":{"92":2}}],["但实际上",{"5":{"93":2}}],["但实际效果因任务而异",{"5":{"92":2}}],["但实际的神经网络实现必须在实数域中进行",{"5":{"88":2}}],["但当依赖关系跨越整个序列",{"5":{"92":2}}],["但当这种极端分布与后续的线性变换结合时",{"5":{"41":2}}],["但意义截然不同",{"5":{"92":2}}],["但从信息流的角度",{"5":{"92":2}}],["但方向性强",{"5":{"92":2}}],["但注意力的优势在于",{"5":{"92":2}}],["但感受野的扩展是渐进的",{"5":{"92":2}}],["但",{"5":{"92":2,"97":1}}],["但我们仍然决定去爬山",{"5":{"92":2}}],["但我们可以利用三角恒等式来优化",{"5":{"88":2}}],["但高频成分的相位差可能达到或超过",{"5":{"91":2}}],["但通常不用于位置编码的分析",{"5":{"90":2}}],["但周期非常大",{"5":{"90":2}}],["但fft的数学框架为理解位置编码的频谱性质提供了重要的理论基础",{"5":{"90":2}}],["但计算开销相同",{"5":{"89":2}}],["但会在处理更长序列时被激活",{"5":{"89":2}}],["但仍有广阔的改进空间",{"5":{"88":2}}],["但仍需要保持某些计算的精度",{"5":{"48":2}}],["但缺乏严格的理论保证",{"5":{"88":2}}],["但rope的相对位置",{"5":{"88":2}}],["但rope将其置于乘法",{"5":{"88":2}}],["但也为自适应优化算法提供了更多的曲率信息",{"5":{"97":2}}],["但也增加参数量和过拟合风险",{"5":{"89":2}}],["但也增加了模型的复杂性和参数数量",{"5":{"88":2}}],["但也有理论上界",{"5":{"89":2}}],["但也可能结合其他自监督任务",{"5":{"99":2,"101":2}}],["但模型自发地获得了相对位置感知能力",{"5":{"88":2}}],["但某些语言现象确实依赖于绝对位置",{"5":{"88":2}}],["但表达能力受限",{"5":{"86":2}}],["但表达能力可能受限",{"5":{"41":2}}],["但可能不是所有任务的最优选择",{"5":{"101":2}}],["但可能无法适应所有任务的需求",{"5":{"86":2}}],["但可以完全并行化",{"5":{"92":2}}],["但可以从信息论和函数逼近的角度理解",{"5":{"45":2}}],["但数值上更加稳定",{"5":{"61":2}}],["但非零元素数量减少",{"5":{"87":2}}],["但非零",{"5":{"61":2}}],["但与其他损失函数之间存在数学上的联系",{"5":{"51":2}}],["但这些参数的设计遵循着精心的配置",{"5":{"93":2}}],["但这并不意味着信息没有传递",{"5":{"92":2}}],["但这里的区别在于依赖关系的类型",{"5":{"92":2}}],["但这里因为每行和为1",{"5":{"87":2}}],["但这种理解有助于我们分析模型的学习动态和设计改进方案",{"5":{"95":2}}],["但这种并行化的代价是位置信息的丢失",{"5":{"91":2}}],["但这种添加是生硬的",{"5":{"88":2}}],["但这种关系在实际的注意力计算中被内容的干扰所淹没",{"5":{"88":2}}],["但这是实现高效梯度计算的必要代价",{"5":{"44":2}}],["但这个估计本身存在方差",{"5":{"51":2}}],["但这个拉力被概率加权",{"5":{"69":1}}],["但这个拉力被概率",{"5":{"69":1}}],["但这个下界可能非常松散",{"5":{"99":2,"101":2}}],["但奇异值总是非负的",{"5":{"50":2}}],["但又有所不同",{"5":{"50":2}}],["但又足够高以捕获丰富的语义信息",{"5":{"50":2}}],["但矩阵求逆的数学思想",{"5":{"50":2}}],["但改进幅度可能很小以至于在实际应用中可以忽略",{"5":{"96":2}}],["但在测试性能上可能有显著差异",{"5":{"97":2}}],["但在某些配置下",{"5":{"93":2}}],["但在某些场景",{"5":{"45":2}}],["但在高层可能升高",{"5":{"92":2}}],["但在非常长的序列",{"5":{"88":2}}],["但在序列较短时仍能提供有效的位置区分",{"5":{"88":2}}],["但在深度学习中使用较少",{"5":{"50":2}}],["但在深度学习中的直接应用较少",{"5":{"96":2}}],["但在处理序列数据的某些场景下",{"5":{"50":2}}],["但在语言模型中更常用的是分类分布",{"5":{"96":2}}],["但最近的研究表明",{"5":{"48":2}}],["但不直接",{"5":{"91":2}}],["但不满足交换律",{"5":{"50":2}}],["但不要求保持原点不变",{"5":{"47":2}}],["但不同激活函数的逼近效率是不同的",{"5":{"71":2}}],["但不参与损失计算",{"5":{"99":2,"101":2}}],["但网络在局部区域",{"5":{"45":2}}],["但深入理解前向传播的数学本质",{"5":{"45":2}}],["但softmax操作本质上是另一种形式的非线性激活",{"5":{"71":2}}],["但softmax激活函数引入了关键的非线性",{"5":{"71":2}}],["但近年来relu的流行表明",{"5":{"71":2}}],["但由于输入嵌入",{"5":{"93":1}}],["但由于输入嵌入的各维度之间存在相关性",{"5":{"93":1}}],["但由于数值精度和softmax的平滑效应",{"5":{"87":2}}],["但由于softmax是单调变换且保持行和为1",{"5":{"87":2}}],["但由于广播可能在语法上合法而在语义上错误",{"5":{"50":2}}],["但由于tanh的导数峰值是1而sigmoid只有0",{"5":{"42":2}}],["但由于其输出范围是",{"5":{"41":2}}],["但范数被限制在",{"5":{"41":1}}],["但范数被限制在以下",{"5":{"41":1}}],["但现代硬件和库优化使得其计算开销在可接受范围内",{"5":{"41":2}}],["但按值裁剪可能改变梯度的方向",{"5":{"41":2,"97":2}}],["但引入了新的梯度挑战",{"5":{"41":2}}],["但其hessian依赖于预测概率",{"5":{"97":2}}],["但其总参数量与单头注意力完全相同",{"5":{"93":2}}],["但其有效自由度不超过",{"5":{"89":1}}],["但其有效自由度不超过​",{"5":{"89":1}}],["但其列空间和行空间都位于",{"5":{"87":1}}],["但其列空间和行空间都位于的子空间中",{"5":{"87":1}}],["但其梯度的数学结构与上述推导高度相似",{"5":{"61":2}}],["但其核心洞见对连续激活函数同样适用",{"5":{"46":2}}],["但其分段线性的性质使得它在大规模网络中表现优异",{"5":{"71":2}}],["但其表达能力受到根本性的限制",{"5":{"71":2}}],["但其饱和程度较轻",{"5":{"41":2}}],["但其softmax结构与信息论框架高度兼容",{"5":{"69":2}}],["但它是在训练过程中从数据中学习的",{"5":{"93":2}}],["但它是通过直接对融合后的损失函数求导得到的",{"5":{"61":2}}],["但它清晰地展示了多头注意力的并行结构",{"5":{"93":2}}],["但它展示了多层注意力如何通过",{"5":{"92":2}}],["但它并非完美无缺",{"5":{"88":2}}],["但它们都保持着用",{"5":{"42":2}}],["但它们的比值是合理的",{"5":{"61":2}}],["但它们的数学定义源自完全不同的理论框架",{"5":{"70":2}}],["但它们关于模型参数的梯度都具有类似的形式",{"5":{"70":2}}],["但它们在更深层次上具有内在的联系",{"5":{"70":2}}],["但它们在优化梯度上具有相似的简洁形式",{"5":{"70":2}}],["但它在信息论和统计学中具有不可替代的重要性",{"5":{"61":2}}],["但它在理解激活函数导数性质方面具有重要的教学价值",{"5":{"42":2}}],["但它已经具备了一定的逻辑推理能力",{"5":{"47":2}}],["但它最终会反映在模型的注意力模式中",{"5":{"99":2,"101":2}}],["但它存在一些已知的局限性",{"5":{"99":2,"101":2}}],["但是在自注意力中",{"5":{"69":2}}],["但强凸性决定了优化的收敛速度",{"5":{"70":2}}],["但mse作为独立的损失函数",{"5":{"70":2}}],["但如何选择最优的噪声分布是一个开放问题",{"5":{"69":2}}],["但如果损失函数具有特定的黎曼几何结构",{"5":{"70":2}}],["但有一个异常样本的误差为10",{"5":{"51":2}}],["但有不同的任务特定输出头",{"5":{"70":2}}],["但提供不同尺度的位置分辨率",{"5":{"90":2}}],["但提示本身不需要被",{"5":{"99":2,"101":2}}],["但只对目标token计算损失",{"5":{"99":2,"101":2}}],["作用于矩阵而非向量",{"5":{"61":2}}],["作为一种高效",{"5":{"95":2}}],["作为key时",{"5":{"94":2}}],["作为query时",{"5":{"94":2}}],["作为",{"5":{"61":2,"92":1}}],["作为模型参数",{"5":{"51":1}}],["作为近似",{"5":{"41":1}}],["作为分母",{"5":{"69":2}}],["作正交投影",{"5":{"51":2}}],["中期使用大学习率快速收敛",{"5":{"97":2}}],["中​​这一缩放因子是scaled",{"5":{"95":1}}],["中继",{"5":{"92":2}}],["中较为平缓",{"5":{"91":2}}],["中更为明显",{"5":{"91":2}}],["中提出的位置编码方案",{"5":{"91":2}}],["中提出了infonce",{"5":{"69":2}}],["中采样",{"5":{"89":1}}],["中采样每个编码元素",{"5":{"89":1}}],["中得到更为系统的阐述",{"5":{"61":2}}],["中得到了充分的应用",{"5":{"47":2}}],["中满足",{"5":{"61":1}}],["中独立采样的",{"5":{"61":2}}],["中建立了信息量的严格数学描述",{"5":{"61":2}}],["中寻找与目标向量",{"5":{"51":1}}],["中心语依赖等",{"5":{"92":2}}],["中心趋势",{"5":{"89":2}}],["中心极限定理提供了重要的洞见",{"5":{"96":2}}],["中心极限定理的重要性在于它表明正态分布在自然界中无处不在",{"5":{"96":2}}],["中心极限定理",{"5":{"96":2}}],["中心极限定理表明大量独立随机变量之和趋向于高斯分布",{"5":{"96":2}}],["中心",{"5":{"96":2}}],["中任意点",{"5":{"47":1}}],["中任意点变为",{"5":{"47":1}}],["中最常用的输出激活函数",{"5":{"47":2}}],["中最大值的指数增长占主导",{"5":{"71":2}}],["中间变量",{"5":{"45":1}}],["中间变量的伴随值为",{"5":{"45":1}}],["中间值过大或过小",{"5":{"44":1}}],["中有其独特优势",{"5":{"45":2}}],["中的自注意力机制对学习率也有特殊要求",{"5":{"97":2}}],["中的自注意力机制引入了额外的非线性",{"5":{"97":2}}],["中的一条有向路径",{"5":{"92":1}}],["中的所有元素都是可学习的参数",{"5":{"89":1}}],["中的表示",{"5":{"45":2}}],["中的点",{"5":{"71":1}}],["中的生成机制相契合",{"5":{"71":2}}],["中",{"5":{"41":2,"50":6,"51":3,"61":4,"70":2,"86":2,"92":2,"95":1,"97":1,"99":2,"101":2}}],["中识别出与匹配的正样本​",{"5":{"69":1}}],["中识别出与",{"5":{"69":1}}],["中对应方向的曲率变小",{"5":{"70":2}}],["dft将离散序列从时域",{"5":{"90":2}}],["dft",{"5":{"90":2}}],["dct",{"5":{"87":2}}],["dropout等",{"5":{"87":2}}],["dropout噪声等",{"5":{"96":2}}],["dropout",{"5":{"96":2}}],["dropout可以被解释为对神经网络进行高斯近似",{"5":{"96":2}}],["drawio",{"5":{"44":4,"45":8,"48":4,"50":8,"96":12}}],["dilated",{"5":{"86":2}}],["dimension",{"5":{"50":2}}],["discrete",{"5":{"90":2}}],["discovery",{"5":{"96":2}}],["distillation",{"5":{"87":2}}],["distance",{"5":{"88":2,"96":2}}],["dist",{"5":{"96":8}}],["distribution",{"5":{"47":2,"87":2,"96":16,"99":2,"101":2}}],["divergence",{"5":{"96":2}}],["differentiation",{"5":{"44":2,"45":2,"48":2}}],["diag",{"5":{"42":8}}],["directional",{"5":{"48":2}}],["direct",{"5":{"99":2,"101":2}}],["delta",{"5":{"92":2}}],["dependency",{"5":{"92":4}}],["definite",{"5":{"86":2}}],["deviation",{"5":{"96":2}}],["decoder",{"5":{"91":2}}],["decomposition",{"5":{"50":4,"51":2,"94":2}}],["decay",{"5":{"48":4,"50":2,"97":2}}],["decision",{"5":{"47":2}}],["density",{"5":{"96":2}}],["dense",{"5":{"46":2}}],["dendrites",{"5":{"47":2}}],["descent",{"5":{"44":2,"70":2}}],["derivative",{"5":{"42":2,"48":2}}],["dx",{"5":{"42":4}}],["d",{"5":{"42":4,"69":2,"86":4,"96":4}}],["dot",{"0":{"95":1},"4":{"95":1},"5":{"61":6,"69":4,"71":2,"85":5,"86":2,"87":2,"88":2,"93":4,"94":2,"95":17}}],["dag",{"5":{"44":2,"45":2}}],["david",{"2":{"21":1,"85":1},"5":{"21":1,"44":2,"85":1}}],["data",{"5":{"21":10,"50":2,"85":26}}],["dpo直接优化正样本相对于负样本的概率优势",{"5":{"101":2}}],["dpo",{"2":{"99":1,"101":1},"5":{"99":3,"101":3}}],["dpo的数学推导",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["dpo的核心洞察是",{"5":{"99":2,"101":2}}],["dpo的损失函数为",{"5":{"99":2,"101":2}}],["dpo的最优解与rlhf的最优解是等价的",{"5":{"99":2,"101":2}}],["dpo的梯度分析",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["dpo的梯度",{"5":{"99":2,"101":2}}],["dpo的梯度结构与infonce",{"5":{"99":2,"101":2}}],["dpo的一个重要理论贡献是揭示了策略比率与隐式奖励之间的联系",{"5":{"99":2,"101":2}}],["dpo的优化过程实际上是在调整策略",{"5":{"99":2,"101":2}}],["dpo目标",{"5":{"99":2,"101":2}}],["dpo目标关于的梯度与ppo目标的梯度在期望意义下是相近的",{"5":{"99":1,"101":1}}],["dpo目标关于",{"5":{"99":1,"101":1}}],["dpo损失在形式上是二元交叉熵损失的一种变体",{"5":{"99":2,"101":2}}],["dpo损失关于的梯度为",{"5":{"99":1,"101":1}}],["dpo损失关于",{"5":{"99":1,"101":1}}],["dpo可以被视为一种",{"5":{"99":2,"101":2}}],["dpo训练的策略会改变这些注意力模式",{"5":{"99":2,"101":2}}],["dynamical",{"5":{"50":2}}],["dynamic",{"5":{"99":2,"101":2}}],["雅可比矩阵​​的范数决定了梯度在传递过程中的缩放程度",{"5":{"92":1}}],["雅可比矩阵​是对角矩阵",{"5":{"44":1}}],["雅可比矩阵的结构分析",{"2":{"61":1},"5":{"61":1}}],["雅可比矩阵的对角元素非常小",{"5":{"41":1}}],["雅可比矩阵包含所有一阶偏导数",{"5":{"48":2}}],["雅可比矩阵向量积",{"5":{"44":1,"45":1}}],["雅可比矩阵",{"5":{"41":1,"44":1,"45":2,"48":2,"61":2,"92":1}}],["是通用的",{"5":{"101":2}}],["是门控向量",{"5":{"101":2}}],["是总训练步数",{"5":{"97":1}}],["是初始学习率",{"5":{"97":1}}],["是函数",{"5":{"97":1}}],["是衰减周期",{"5":{"97":2}}],["是衰减因子",{"5":{"97":2}}],["是衰减系数",{"5":{"48":1}}],["是现代大规模语言模型能够稳定训练的重要数学基础",{"5":{"96":2}}],["是现代深度学习",{"5":{"95":2}}],["是现代深度学习框架的核心功能",{"5":{"48":2}}],["是值向量",{"5":{"95":1}}],["是值向量的第行",{"5":{"95":1}}],["是可逆的当且仅当其行列式",{"5":{"94":1}}],["是可学习的逐元素缩放和偏移参数",{"5":{"41":1}}],["是可学习的缩放和偏移参数",{"5":{"41":1}}],["是方阵还是长矩阵取决于维度的选择",{"5":{"94":1}}],["是方差参数",{"5":{"96":2}}],["是方差",{"5":{"41":2}}],["是多头注意力中的头数",{"5":{"94":1}}],["是多项式度数",{"5":{"86":2}}],["是三个可学习的投影矩阵",{"5":{"94":2}}],["是三个核心概念",{"5":{"94":2}}],["是卷积矩阵",{"5":{"93":1}}],["是卷积核",{"5":{"93":1}}],["是卷积核大小",{"5":{"87":2}}],["是独立的",{"5":{"93":1}}],["是独立同分布的随机变量",{"5":{"96":2}}],["是独立同分布的均匀随机变量",{"5":{"71":2}}],["是谓语",{"5":{"93":2}}],["是主语",{"5":{"93":2}}],["是理论上的最优复杂度",{"5":{"92":1}}],["是理解机器学习模型泛化误差的核心理论工具",{"5":{"51":2}}],["是理解牛顿法等二阶优化方法的基础",{"5":{"48":2}}],["是理解其他层类型的基础",{"5":{"46":2}}],["是向量的第",{"5":{"92":1}}],["是向量空间",{"5":{"88":2}}],["是指学习算法对解空间的先验假设",{"5":{"92":2}}],["是各频率的周期",{"5":{"92":2}}],["是维度索引",{"5":{"92":2}}],["是纯粹的位置",{"5":{"91":1}}],["是原始内容之间的注意力分数",{"5":{"91":1}}],["是原始的query和key向量",{"5":{"88":1}}],["是两种针对key和value的变体设计",{"5":{"93":2}}],["是两种主要的编码范式",{"5":{"91":2}}],["是两个随机变量的联合分布的熵",{"5":{"96":2}}],["是两个权重矩阵的乘积",{"5":{"71":1}}],["是针对解码器设计的位置编码变体",{"5":{"91":2}}],["是带有位置信息的输入表示",{"5":{"91":2}}],["是克罗内克",{"5":{"90":1}}],["是克罗内克函数",{"5":{"90":1}}],["是周期为",{"5":{"90":1}}],["是足够的",{"5":{"90":2}}],["是否是最优的",{"5":{"90":2}}],["是小的距离差",{"5":{"90":2}}],["是计算dft的高效算法",{"5":{"90":2}}],["是学习到的位置编码矩阵",{"5":{"89":1}}],["是学习率",{"5":{"48":2}}],["是固定正弦编码",{"5":{"89":2}}],["是固定的常数",{"5":{"91":1}}],["是固定的",{"5":{"61":2,"91":1}}],["是损失函数的利普希茨常数",{"5":{"97":1}}],["是损失函数",{"5":{"89":2}}],["是损失函数在当前参数处的梯度",{"5":{"48":2}}],["是注意力权重矩阵",{"5":{"97":1}}],["是注意力输出关于输入的雅可比矩阵",{"5":{"89":2}}],["是注意力头的数量",{"5":{"50":1}}],["是内容与位置之间的交叉项",{"5":{"91":1}}],["是内容嵌入矩阵",{"5":{"89":2}}],["是内层函数关于自变量的导数",{"5":{"48":2}}],["是首要因素",{"5":{"88":1}}],["是单位元",{"5":{"88":1}}],["是单位矩阵",{"5":{"50":1}}],["是未编码位置的原始向量",{"5":{"88":1}}],["是未知参数",{"5":{"96":1}}],["是角度为",{"5":{"88":1}}],["是虚数单位",{"5":{"88":1,"90":2}}],["是阿贝尔群",{"5":{"88":2}}],["是整数加法群",{"5":{"88":2}}],["是query",{"5":{"88":2}}],["是query投影和key投影的某种",{"5":{"94":1}}],["是query投影和key投影的",{"5":{"87":2}}],["是绝对位置",{"5":{"88":2}}],["是某个可学习的距离函数",{"5":{"92":1}}],["是某个特征映射函数",{"5":{"87":2}}],["是某层某头的注意力矩阵",{"5":{"87":1}}],["是核函数",{"5":{"87":1}}],["是核方法中的核心概念",{"5":{"87":2}}],["是自注意力且",{"5":{"87":1}}],["是度量学习",{"5":{"87":1}}],["是行随机矩阵",{"5":{"87":4}}],["是行向量还是列向量需要注意维度匹配",{"5":{"44":1}}],["是数据矩阵",{"5":{"87":1}}],["是数据的真实联合分布",{"5":{"51":2}}],["是分解秩",{"5":{"87":2}}],["是分析梯度饱和的经典案例",{"5":{"41":1}}],["是分析梯度爆炸的经典模型",{"5":{"41":2}}],["是稀疏注意力研究的重要方向",{"5":{"87":2}}],["是列正交矩阵",{"5":{"87":2}}],["是权衡参数",{"5":{"87":2}}],["是权重矩阵",{"5":{"48":2}}],["是权重因子",{"5":{"99":2,"101":2}}],["是互信息",{"5":{"87":2}}],["是容忍阈值",{"5":{"87":2}}],["是满秩的",{"5":{"87":2}}],["是低秩矩阵",{"5":{"87":3}}],["是奇异值",{"5":{"87":2}}],["是秩",{"5":{"87":2}}],["是秩1矩阵",{"5":{"61":2,"87":2}}],["是置换矩阵",{"5":{"87":3}}],["是特征向量矩阵",{"5":{"87":2}}],["是特征值对角矩阵",{"5":{"87":2}}],["是特征映射对应的核函数",{"5":{"86":1}}],["是特征维度",{"5":{"86":1}}],["是频率参数",{"5":{"86":1}}],["是阈值",{"5":{"86":2}}],["是kl散度区别于传统距离的关键特征",{"5":{"61":1}}],["是one",{"5":{"61":3}}],["是与参数",{"5":{"61":2}}],["是定义在同一概率空间上的两个离散概率分布",{"5":{"61":1}}],["是定义损失函数和分析模型行为的重要工具",{"5":{"96":2}}],["是待估计的参数向量",{"5":{"61":2}}],["是半负定的",{"5":{"61":2}}],["是半正定对称矩阵",{"5":{"45":1}}],["是半正定的",{"5":{"70":2,"86":1,"97":2}}],["是以",{"5":{"61":2}}],["是严格的凸函数",{"5":{"51":1}}],["是垂直的",{"5":{"51":1}}],["是投影矩阵",{"5":{"51":1}}],["是设计矩阵",{"5":{"51":1,"70":1,"97":1}}],["是目标向量",{"5":{"51":1}}],["是目前深度学习中最广泛使用的激活函数",{"5":{"42":2}}],["是迭代次数",{"5":{"51":2}}],["是机器学习中最基本的权衡关系之一",{"5":{"51":2}}],["是机器学习中最基础",{"5":{"51":2}}],["是mse的",{"5":{"51":4}}],["是给定",{"5":{"51":1}}],["是给定的最优预测",{"5":{"51":1}}],["是给定一个随机变量时另一个随机变量的熵",{"5":{"96":2}}],["是高斯噪声的方差",{"5":{"51":2}}],["是估计量的方差",{"5":{"51":2}}],["是我们通常在训练中实际计算的损失",{"5":{"51":2}}],["是衡量数值稳定性的关键指标",{"5":{"87":2}}],["是衡量模型真实性能的理论上指标",{"5":{"51":2}}],["是衡量计算复杂度的标准指标",{"5":{"45":2}}],["是连续型目标变量",{"5":{"51":2}}],["是将矩阵转换为列向量的操作",{"5":{"50":2}}],["是将任意矩阵分解为三个矩阵乘积的强大工具",{"5":{"50":2}}],["是将人类偏好融入模型训练的关键技术",{"5":{"99":2,"101":2}}],["是的第行",{"5":{"92":1}}],["是的前列",{"5":{"89":1}}],["是的截断svd",{"5":{"87":1}}],["是的平稳分布",{"5":{"87":1}}],["是的最大奇异值",{"5":{"87":1}}],["是的矩阵",{"5":{"50":1}}],["是的特征值的平方根",{"5":{"50":1}}],["是的伪逆",{"5":{"45":1}}],["是对角非奇异矩阵",{"5":{"87":1}}],["是对角矩阵",{"5":{"45":2,"94":2}}],["是对应的特征向量",{"5":{"87":2}}],["是对应的特征值",{"5":{"50":2}}],["是所有被矩阵映射到零向量的输入向量构成的空间",{"5":{"50":2}}],["是所谓的",{"5":{"42":2}}],["是矩阵",{"5":{"51":1}}],["是矩阵所有奇异值的和",{"5":{"50":2}}],["是矩阵所有列向量的线性组合构成的空间",{"5":{"50":2}}],["是矩阵的最大奇异值",{"5":{"50":4}}],["是矩阵概念在更高维度的延伸",{"5":{"50":2}}],["是序列长度",{"5":{"50":3}}],["是似然函数",{"5":{"96":1}}],["是模型的隐藏维度",{"5":{"95":1}}],["是模型的线性输出",{"5":{"61":2}}],["是模型的预测均值",{"5":{"51":2}}],["是模型输出的概率分布",{"5":{"61":2}}],["是模型预测分布时",{"5":{"96":1}}],["是模型预测的期望",{"5":{"51":1}}],["是模型预测的条件概率分布",{"5":{"99":2,"101":2}}],["是模型对第",{"5":{"51":1}}],["是模型对第个样本的预测值",{"5":{"51":1}}],["是模型参数",{"5":{"99":1,"101":1}}],["是率参数",{"5":{"96":1}}],["是成功概率",{"5":{"96":1}}],["是边际似然",{"5":{"96":2}}],["是常数",{"5":{"70":3,"86":1,"96":2,"97":1}}],["是常数矩阵",{"5":{"70":1}}],["是信息论中的核心概念",{"5":{"96":2}}],["是协方差矩阵的逆矩阵",{"5":{"96":2}}],["是协方差矩阵的行列式",{"5":{"96":2}}],["是协方差矩阵",{"5":{"96":2}}],["是标准多头注意力和多查询注意力的折中方案",{"5":{"86":2}}],["是标准多头注意力的参数共享变体",{"5":{"86":2}}],["是标准正态分布的cdf",{"5":{"96":1}}],["是标准差",{"5":{"96":2}}],["是标准化的协方差",{"5":{"96":2}}],["是概率论中最深刻的定理之一",{"5":{"96":2}}],["是概率论中最重要的连续分布",{"5":{"96":2}}],["是概率单纯形",{"5":{"71":1}}],["是概率单纯形中的点",{"5":{"71":1}}],["是随机采样的子集",{"5":{"86":1}}],["是随机变量幂次期望的统称",{"5":{"96":2}}],["是随机变量取值的加权平均",{"5":{"96":2}}],["是随机噪声源",{"5":{"71":1}}],["是伯努利分布向多值情况的推广",{"5":{"96":2}}],["是n次独立伯努利试验中成功次数的分布",{"5":{"96":2}}],["是掌握语言模型概率本质的必要前提",{"5":{"96":2}}],["是逐元素乘法",{"5":{"87":2}}],["是逐元素乘方",{"5":{"86":1}}],["是逐元素累积的梯度平方",{"5":{"48":1}}],["是逐元素函数",{"5":{"44":1}}],["是速度向量",{"5":{"48":1}}],["是残差函数的雅可比矩阵",{"5":{"48":1}}],["是梯度与方向",{"5":{"48":1}}],["是梯度为零向量的点",{"5":{"48":2}}],["是外层函数关于中间变量的导数",{"5":{"48":1}}],["是最小学习率",{"5":{"97":2}}],["是最早的解决方案",{"5":{"88":2}}],["是最早提出且应用广泛的归一化技术",{"5":{"41":2}}],["是最直接的选择",{"5":{"86":2}}],["是最后一层的线性输出",{"5":{"51":2}}],["是最基本也是最重要的参数估计方法",{"5":{"96":2}}],["是最简单的一类",{"5":{"88":2}}],["是最简单的连续分布",{"5":{"96":2}}],["是最简单的离散分布",{"5":{"96":2}}],["是最流行的自适应优化算法",{"5":{"48":2}}],["是最优的",{"5":{"70":2}}],["是动量系数",{"5":{"48":2}}],["是加速梯度下降的重要技术",{"5":{"48":2}}],["是在训练过程中动态调整学习率的策略",{"5":{"48":2}}],["是训练超大规模语言模型的必要技术",{"5":{"50":2}}],["是训练神经网络的关键技术基础",{"5":{"48":2}}],["是训练这些模型的主要挑战之一",{"5":{"41":2}}],["是一次性建模所有位置对之间的依赖",{"5":{"92":1}}],["是一类通过替换softmax来降低计算复杂度的注意力变体",{"5":{"87":2}}],["是一个半负定矩阵",{"5":{"97":2}}],["是一个小型多层感知机",{"5":{"93":2}}],["是一个有向图",{"5":{"92":1}}],["是一个等范数向量集合",{"5":{"91":2}}],["是一个经验性选择的常数基准",{"5":{"91":2}}],["是一个离散序列",{"5":{"90":1}}],["是一个复指数序列",{"5":{"90":1}}],["是一个实践中的挑战",{"5":{"88":1}}],["是一个理论上的局限",{"5":{"88":1}}],["是一个缩放因子",{"5":{"88":1}}],["是一个非线性函数",{"5":{"88":1}}],["是一个映射",{"5":{"88":2}}],["是一个集合",{"5":{"88":2}}],["是一个群",{"5":{"88":2}}],["是一个带状矩阵",{"5":{"87":1}}],["是一个随机矩阵",{"5":{"87":1,"92":1}}],["是一个特征映射函数",{"5":{"86":2}}],["是一个可学习的线性变换",{"5":{"93":2}}],["是一个可学习的参数",{"5":{"88":1}}],["是一个可学习的评分函数",{"5":{"86":2}}],["是一个可微函数",{"5":{"48":1}}],["是一个one",{"5":{"61":1}}],["是一个单位向量",{"5":{"48":1}}],["是一个",{"5":{"48":1,"50":2,"88":1,"95":2,"96":2}}],["是一个与",{"5":{"48":1}}],["是一个多元函数",{"5":{"48":1}}],["是一个的矩阵",{"5":{"48":1,"50":1}}],["是一个关键的数学组件",{"5":{"99":1,"101":1}}],["是一种典型的变体",{"5":{"93":2}}],["是一种io感知的注意力计算算法",{"5":{"86":2}}],["是一种重要的权重归一化技术",{"5":{"50":2}}],["是一种常用的内存优化技术",{"5":{"50":2}}],["是一种常用的权重归一化技术",{"5":{"50":2}}],["是一种常用的正则化技术",{"5":{"96":2}}],["是一种非参数方法",{"5":{"96":2}}],["是一种通过重采样来估计统计量方差的非参数方法",{"5":{"96":2}}],["是一种通过调整权重来稳定训练的方法",{"5":{"41":2}}],["是一种用于深度学习的hessian近似方法",{"5":{"48":2}}],["是一种二值神经元模型",{"5":{"47":2}}],["是一种利用计算图高效计算导数的技术",{"5":{"45":2}}],["是一种防止梯度爆炸的技术",{"5":{"44":2}}],["是一种验证反向传播实现正确性的数值方法",{"5":{"44":2}}],["是微积分中最重要的法则之一",{"5":{"48":2}}],["是深入掌握深度学习优化算法的必要基础",{"5":{"48":2}}],["是深度学习训练和推理的基本范式",{"5":{"50":2}}],["是深度学习训练的标准范式",{"5":{"47":2}}],["是深度学习中最重要的技术之一",{"5":{"96":2}}],["是深度学习中最核心的算法之一",{"5":{"44":2}}],["是深度学习优化中最重要的进展之一",{"5":{"48":2}}],["是全1向量",{"5":{"47":2,"87":4,"89":1}}],["是仿射变换算子",{"5":{"47":1}}],["是线性注意力研究的核心问题",{"5":{"87":2}}],["是线性代数中最重要的研究对象之一",{"5":{"50":2}}],["是线性变换与平移的组合",{"5":{"47":1}}],["是线性的",{"5":{"70":2,"71":1}}],["是净输入矩阵",{"5":{"47":2}}],["是由训练数据经验估计的",{"5":{"61":2}}],["是由数据本身的固有噪声决定的",{"5":{"51":2}}],["是由网络参数定义的函数映射",{"5":{"45":1}}],["是由gutmann和hyvarinen于2010年提出的一种参数密度估计方法",{"5":{"69":2}}],["是由softmax定义的概率分布",{"5":{"69":1}}],["是这种中间表示的标准形式",{"5":{"45":2}}],["是神经网络信息流动的基本方式",{"5":{"45":2}}],["是缩放后注意力分数矩阵",{"5":{"94":1}}],["是缩放后损失的梯度",{"5":{"44":2}}],["是缩放因子",{"5":{"44":1,"51":1}}],["是网络预测",{"5":{"44":1}}],["是网络的原始输出",{"5":{"71":1}}],["是网络对第",{"5":{"69":1}}],["是真实分布而",{"5":{"96":1}}],["是真实词元",{"5":{"96":2}}],["是真实标签",{"5":{"44":2}}],["是真实的数据分布",{"5":{"61":2}}],["是真实的目标函数",{"5":{"70":1}}],["是激活函数",{"5":{"48":1,"71":1}}],["是负样本的数量",{"5":{"69":1,"71":1}}],["是gumbel噪声",{"5":{"71":1}}],["是第个头定义的",{"5":{"93":1}}],["是第个头的query",{"5":{"93":1}}],["是第个频率成分对应的旋转因子",{"5":{"90":1}}],["是第个样本的真实类别",{"5":{"61":1}}],["是第四个因素",{"5":{"88":1}}],["是第三个因素",{"5":{"88":1}}],["是第二个因素",{"5":{"88":1}}],["是第",{"5":{"48":1,"61":1,"70":1,"71":1,"88":1,"90":1,"93":2,"96":1}}],["是非线性函数",{"5":{"71":1}}],["是非线性激活函数",{"5":{"71":1}}],["是非负行随机矩阵",{"5":{"41":1}}],["是变换矩阵",{"5":{"71":1}}],["是transformer架构可训练性的关键保障",{"5":{"95":2}}],["是transformer架构成功的关键因素之一",{"5":{"95":2}}],["是transformer原始论文",{"5":{"91":2}}],["是transformer中使用的标准归一化技术",{"5":{"41":2}}],["是transformer中缓解梯度问题的重要技术",{"5":{"41":2}}],["是token嵌入矩阵",{"5":{"71":2}}],["是温度参数",{"5":{"69":2,"71":2,"86":2,"99":2,"101":2}}],["是偏置向量",{"5":{"48":2}}],["是偏置",{"5":{"71":2}}],["是紧集",{"5":{"71":2}}],["是因为在适当条件下",{"5":{"71":2}}],["是因为它与注意力的概率输出特性相契合",{"5":{"41":2}}],["是平移向量",{"5":{"71":2}}],["是组合偏置向量",{"5":{"71":2}}],["是输入序列",{"5":{"93":2}}],["是输入依赖的注意力矩阵",{"5":{"92":2}}],["是输入嵌入",{"5":{"87":1}}],["是输入特征",{"5":{"51":1}}],["是输入神经元的数量",{"5":{"96":1}}],["是输入",{"5":{"45":1,"48":2}}],["是输入权重向量",{"5":{"71":1}}],["是输入层到隐藏层的权重矩阵",{"5":{"71":1}}],["是输入分布的支持域",{"5":{"41":1}}],["是输出投影矩阵",{"5":{"93":2}}],["是输出",{"5":{"48":2}}],["是输出矩阵",{"5":{"47":2}}],["是输出权重",{"5":{"71":2}}],["是输出层的偏置向量",{"5":{"71":2}}],["是隐藏维度",{"5":{"50":2}}],["是隐藏层到输出层的权重矩阵",{"5":{"71":2}}],["是隐藏状态向量",{"5":{"41":1}}],["是隐藏状态",{"5":{"41":1}}],["是相应的偏置向量",{"5":{"71":2}}],["是双曲正割函数",{"5":{"42":1}}],["是另一个理论考虑",{"5":{"88":1}}],["是另一个重要的矩阵结构",{"5":{"87":2}}],["是另一个经典的激活函数",{"5":{"42":2}}],["是另一种基于对比学习的损失函数",{"5":{"61":2}}],["是另一种近似推断方法",{"5":{"96":2}}],["是均值向量",{"5":{"96":1}}],["是均值参数",{"5":{"96":1}}],["是均值为0",{"5":{"96":2}}],["是均值",{"5":{"41":1}}],["是批大小",{"5":{"50":1}}],["是批处理的一个变体",{"5":{"50":2}}],["是批量大小",{"5":{"50":1}}],["是批量隐藏激活矩阵",{"5":{"44":1}}],["是批量预测矩阵",{"5":{"44":1}}],["是批均值",{"5":{"41":1}}],["是批方差",{"5":{"41":2}}],["是饱和阈值",{"5":{"41":2}}],["是层归一化的一种简化变体",{"5":{"41":2}}],["是防止除零的小常数",{"5":{"41":2}}],["是",{"5":{"41":3,"42":4,"45":1,"47":1,"50":3,"61":2,"69":1,"70":5,"71":1,"87":5,"88":1,"89":1,"92":3,"93":5,"96":3}}],["是位置索引",{"5":{"92":2}}],["是位置到向量的映射",{"5":{"91":1}}],["是位置与内容之间的交叉项",{"5":{"91":1}}],["是位置编码能够有效支持位置相关建模的关键",{"5":{"90":2,"91":2}}],["是位置编码研究中的重要范式之一",{"5":{"89":2}}],["是位置编码矩阵的前",{"5":{"89":1}}],["是位置编码矩阵的前行",{"5":{"89":1}}],["是位置编码矩阵",{"5":{"71":1,"89":2,"92":2}}],["是位置的值向量",{"5":{"92":1}}],["是位置的内容key",{"5":{"91":1}}],["是位置的内容query",{"5":{"91":1}}],["是位置的位置query",{"5":{"91":1}}],["是位置的邻域集合",{"5":{"86":1}}],["是位置的双向上下文表示",{"5":{"99":1,"101":1}}],["是位置",{"5":{"69":1,"86":1,"89":1,"91":3,"92":1,"99":4,"101":4}}],["是正定的",{"5":{"86":1}}],["是正交投影的核心定义",{"5":{"51":2}}],["是正交矩阵",{"5":{"45":1,"94":1}}],["是正样本对之间的相似度分数",{"5":{"69":2}}],["是样本级别的交叉熵损失",{"5":{"61":2}}],["是样本与所有其他样本",{"5":{"69":1}}],["是样本",{"5":{"69":1}}],["是每个batch中样本的数量",{"5":{"69":2}}],["是infonce中的一个关键超参数",{"5":{"69":1}}],["是查询向量",{"5":{"69":1}}],["是查询",{"5":{"69":1}}],["是描述损失函数曲率的标准工具",{"5":{"97":2}}],["是描述离散随机变量概率分布的函数",{"5":{"96":2}}],["是描述曲率的标准工具",{"5":{"70":2}}],["是参数向量",{"5":{"51":2}}],["是参数依赖的",{"5":{"70":1,"97":2}}],["是研究概率分布空间几何性质的理论框架",{"5":{"70":2}}],["是噪声样本与正样本的比例",{"5":{"69":1}}],["是噪声项",{"5":{"70":2}}],["是预测空间",{"5":{"51":1}}],["是预测误差",{"5":{"70":1}}],["是凸函数吗",{"5":{"51":1}}],["是凸函数",{"5":{"51":1,"70":2,"97":1}}],["是凸函数的充要条件是对于任意",{"5":{"48":1}}],["是凸的",{"5":{"70":2,"97":3}}],["是黎曼度量张量",{"5":{"70":1}}],["是bert等模型采用的预训练任务",{"5":{"99":2,"101":2}}],["是帕累托最优的",{"5":{"99":2,"101":2}}],["是rope的典型使用者",{"5":{"88":1}}],["是rope的主要竞争者之一",{"5":{"88":2}}],["是rlhf中最常用的策略优化算法",{"5":{"99":2,"101":2}}],["是重要性采样比率",{"5":{"99":2,"101":2}}],["是优势函数",{"5":{"99":2,"101":2}}],["是裁剪阈值",{"5":{"41":1}}],["是裁剪参数",{"5":{"99":2,"101":2}}],["是近年来广泛使用的位置编码方案",{"5":{"92":2}}],["是近年来提出的一种简化的对齐方法",{"5":{"99":2,"101":2}}],["是来自词汇表",{"5":{"99":1,"101":1}}],["是前缀",{"5":{"99":1,"101":1}}],["是生成目标",{"5":{"99":1,"101":1}}],["是有效秩",{"5":{"87":1}}],["是有效token",{"5":{"99":1,"101":1}}],["是填充token",{"5":{"99":1,"101":1}}],["是被选中的样本索引",{"5":{"69":2}}],["是被掩码的span集合",{"5":{"99":1,"101":1}}],["是子词单元",{"5":{"99":1,"101":1}}],["是共享参数",{"5":{"99":1,"101":1}}],["是任意非常数的有界连续函数",{"5":{"71":1}}],["是任务",{"5":{"99":2,"101":2}}],["是span的长度",{"5":{"99":2,"101":2}}],["是sigmoid函数",{"5":{"99":2,"101":2}}],["是归一化常数",{"5":{"99":1,"101":1}}],["是增加似然的梯度",{"5":{"99":1,"101":1}}],["是增加",{"5":{"99":1,"101":1}}],["是减少似然的梯度",{"5":{"99":1,"101":1}}],["是减少",{"5":{"99":1,"101":1}}],["是时间",{"5":{"99":1,"101":1}}],["+",{"5":{"42":26,"46":20,"61":14,"69":2,"97":12,"99":2,"101":4}}],["^",{"5":{"46":30}}],["^t",{"5":{"42":4,"46":6}}],["^c",{"5":{"42":10,"61":10}}],["^2",{"5":{"42":8,"46":2,"61":2}}],["rwkv",{"5":{"88":2}}],["r为矩阵的秩",{"5":{"50":2}}],["rademacher复杂度等",{"5":{"87":2}}],["radius",{"5":{"87":2}}],["radam",{"5":{"48":2}}],["range",{"5":{"92":2}}],["random",{"5":{"86":2}}],["rank",{"5":{"50":2,"87":6}}],["rate控制",{"5":{"96":2}}],["rate",{"5":{"48":2}}],["rule",{"5":{"48":2}}],["rumelhart",{"5":{"44":2}}],["rotary",{"0":{"88":1},"4":{"88":1},"5":{"85":5,"88":3,"92":2}}],["rosenblatt",{"5":{"47":2}}],["ronald",{"5":{"44":2}}],["root",{"5":{"41":2}}],["rope正是将这种几何直觉推广到高维向量空间",{"5":{"90":2}}],["rope采用的是",{"5":{"88":1}}],["rope采用的是乘法变换的设计哲学",{"5":{"88":1}}],["rope中使用的群结构是",{"5":{"88":1}}],["rope中使用的群结构是的次直积",{"5":{"88":1}}],["rope及其变体为应对这些挑战提供了有力的工具",{"5":{"88":2}}],["rope则建立在坚实的群论基础上",{"5":{"88":2}}],["rope则提出了一个根本不同的视角",{"5":{"88":2}}],["rope代表了位置编码设计从",{"5":{"88":2}}],["rope可以缓解这一问题",{"5":{"88":2}}],["rope在各种语言建模任务上与或超越其他位置编码方法",{"5":{"88":2}}],["rope在大语言模型中的应用",{"2":{"88":1},"5":{"88":1}}],["rope不需要额外的可学习参数",{"5":{"88":2}}],["rope不仅仅是一种新的位置编码技术",{"5":{"88":2}}],["rope提供良好的相对位置感知",{"5":{"88":2}}],["rope提出了关键性的设计",{"5":{"88":2}}],["rope建议使用",{"5":{"88":2}}],["rope是一种针对长上下文外推的优化技术",{"5":{"88":2}}],["rope与自注意力的集成有两种常见策略",{"5":{"88":2}}],["rope与正弦位置编码的对比分析",{"2":{"88":1},"5":{"88":1}}],["rope将位置信息编码为相位",{"5":{"88":2}}],["rope平移等变性",{"5":{"88":2}}],["rope能够同时捕捉这两种尺度的位置依赖",{"5":{"88":2}}],["rope公式中频率参数",{"5":{"88":2}}],["rope操作由",{"5":{"88":2}}],["rope最神奇的性质是",{"5":{"88":2}}],["rope的这一性质使得transformer能够自然地处理超出训练序列长度的输入",{"5":{"92":2}}],["rope的相对位置内积结构",{"5":{"88":1}}],["rope的相对位置内积结构只依赖于相对位置",{"5":{"88":1}}],["rope的旋转编码是一种",{"5":{"88":1}}],["rope的旋转编码是一种相位调制",{"5":{"88":1}}],["rope的实现简洁高效",{"5":{"88":2}}],["rope的精妙之处在于它将位置编码问题转化为旋转群上的问题",{"5":{"88":2}}],["rope的线性旋转结构可能无法最优地捕捉这类复杂的依赖关系",{"5":{"88":2}}],["rope的外推能力使其成为处理长上下文的有力工具",{"5":{"88":2}}],["rope的外推能力使得这种需求更容易满足",{"5":{"88":2}}],["rope的外推能力源于其旋转结构",{"5":{"88":2}}],["rope的平移等变性是严格可证的",{"5":{"88":2}}],["rope的核心性质是相对位置的内积不变性",{"5":{"92":2}}],["rope的核心公式是",{"5":{"88":2}}],["rope的核心思想可以概括为",{"5":{"88":2}}],["rope的核心思想",{"2":{"88":1},"5":{"88":1}}],["rope的第一步是构建位置索引",{"5":{"88":2}}],["rope的数学定义为",{"5":{"92":2}}],["rope的数学结构可以用群论的语言来描述",{"5":{"88":2}}],["rope的数学形式化",{"2":{"88":1},"5":{"88":1}}],["rope的位置编码是确定性的",{"5":{"88":2}}],["rope的洞察始于一个看似简单却极其深刻的问题",{"5":{"88":2}}],["rope的变体与扩展",{"2":{"88":1},"5":{"88":1}}],["rope的工程实现",{"2":{"88":1},"5":{"88":1}}],["rope的深层结构",{"2":{"88":1},"5":{"88":1}}],["rope",{"0":{"88":1},"4":{"88":1},"5":{"70":4,"85":5,"88":7,"89":2,"90":4,"91":2,"92":2,"97":2,"101":2}}],["rope等位置编码的相对位置敏感性",{"5":{"99":2,"101":2}}],["rmsprop",{"5":{"97":4}}],["rmsprop是adagrad的改进版本",{"5":{"48":2}}],["rms范数是更简单的统计量",{"5":{"41":2}}],["rmsnorm在保持性能的同时减少了计算量",{"5":{"41":2}}],["rmsnorm不需要计算均值",{"5":{"41":2}}],["rmsnorm具有以下优势",{"5":{"41":2}}],["rmsnorm的数学性质",{"5":{"41":2}}],["rmsnorm的数学动机是",{"5":{"41":2}}],["rmsnorm定义为",{"5":{"41":2}}],["rmsnorm",{"5":{"41":6}}],["rmsnorm与计算优化",{"2":{"41":1},"5":{"41":1}}],["rnn实际上需要进行",{"5":{"92":1}}],["rnn实际上需要进行步的顺序计算",{"5":{"92":1}}],["rnn建模长程依赖的",{"5":{"92":2}}],["rnn为",{"5":{"92":2}}],["rnn的",{"5":{"92":1}}],["rnn的函数类是",{"5":{"92":2}}],["rnn的复杂度只能建模",{"5":{"92":1}}],["rnn的梯度范数增长",{"5":{"41":2}}],["rnn的隐藏状态更新为",{"5":{"41":2}}],["rnn",{"5":{"41":2,"50":2,"91":2,"92":2,"95":2}}],["reparameterization",{"5":{"93":2}}],["representation",{"5":{"69":2}}],["reduction=",{"5":{"61":2}}],["reshape",{"5":{"50":2,"93":2}}],["residual",{"5":{"41":2,"92":2}}],["regression",{"5":{"47":2,"51":2}}],["receptive",{"5":{"92":2}}],["recomputation",{"5":{"86":2}}],["recursive",{"5":{"86":2}}],["recurrent",{"5":{"46":2,"92":2,"95":2}}],["rectified",{"5":{"42":2,"48":2}}],["reverse",{"5":{"44":2,"45":2}}],["relative",{"5":{"88":2,"91":2,"92":2}}],["relation",{"5":{"96":8}}],["relu特征映射",{"5":{"86":2}}],["relu对每个坐标独立操作",{"5":{"45":2}}],["relu网络表现为一个线性函数",{"5":{"45":2}}],["relu激活函数将",{"5":{"45":1}}],["relu激活函数将空间按个坐标轴分割成个象限",{"5":{"45":1}}],["relu激活的几何效果",{"5":{"45":2}}],["relu激活",{"5":{"44":1}}],["relu可以被解释为一种",{"5":{"71":2}}],["relu虽然数学形式简单",{"5":{"71":2}}],["relu及其变体",{"5":{"42":2}}],["relu函数定义为",{"5":{"42":2}}],["relu函数",{"5":{"42":2,"71":2}}],["relu函数的定义与导数",{"2":{"42":1},"5":{"42":1}}],["relu函数族的导数与性质",{"2":{"42":1},"5":{"42":1}}],["relu的稀疏激活特性",{"5":{"71":2}}],["relu的稀疏激活特性可能更有价值",{"5":{"41":2}}],["relu的信息选择性",{"5":{"71":2}}],["relu的数学形式极其简单",{"5":{"42":2}}],["relu的计算仅涉及比较和乘法",{"5":{"41":2}}],["relu的负区域完全抑制了信号",{"5":{"41":2}}],["relu",{"5":{"41":3,"42":6,"47":2,"71":2}}],["relu通过分段线性的设计实现了这一点",{"5":{"41":2}}],["rel=",{"5":{"21":10,"85":26}}],["reinforcement",{"5":{"99":2,"101":2}}],["rlhf使用人类偏好的正样本和负样本",{"5":{"101":2}}],["rlhf",{"2":{"99":1,"101":1},"5":{"97":3,"99":3,"101":3}}],["rlhf的数学框架",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["rlhf的核心思想是",{"5":{"99":2,"101":2}}],["rlhf的三阶段流程",{"5":{"99":2,"101":2}}],["rlhf的目标可以写为",{"5":{"99":2,"101":2}}],["rlhf的理论挑战",{"5":{"99":2,"101":2}}],["rlhf中的策略优化问题可以通过重新参数化直接求解",{"5":{"99":2,"101":2}}],["rlhf依赖于人类偏好数据的质量和一致性",{"5":{"99":2,"101":2}}],["rlhf损失函数中的kl散度项是一个关键的数学组件",{"5":{"99":1,"101":1}}],["rlhf损失函数中的kl散度项",{"5":{"99":1,"101":1}}],["right|",{"5":{"42":4}}],["right",{"5":{"42":22,"46":8,"61":14,"69":6,"86":2,"97":2}}],["riemannian",{"5":{"70":4}}],["joint",{"5":{"96":2}}],["ji",{"5":{"46":8}}],["jacobian",{"5":{"45":2,"48":2}}],["j=1",{"5":{"42":6,"46":2}}],["j",{"5":{"42":39,"46":9,"61":8,"97":10}}],["epsilon",{"5":{"97":6}}],["eta",{"5":{"97":2}}],["extrapolation",{"5":{"91":2}}],["extension",{"5":{"88":2}}],["expansion",{"5":{"93":2}}],["experts",{"5":{"93":2}}],["expectation",{"5":{"96":4}}],["exponential",{"5":{"48":2,"86":6,"96":2}}],["exp",{"5":{"42":56,"61":10,"97":6}}],["equivariant",{"5":{"88":2}}],["embedding",{"0":{"88":1},"4":{"88":1},"5":{"85":5,"88":5,"92":2,"94":2}}],["efficient",{"5":{"50":2}}],["effective",{"5":{"87":2,"92":2}}],["effect",{"5":{"92":2,"96":2}}],["element",{"5":{"91":2}}],["ell",{"5":{"46":26}}],["ell=1",{"5":{"46":10}}],["elu特征映射",{"5":{"86":2}}],["elu",{"5":{"41":2,"42":6,"86":2}}],["e^",{"5":{"46":4}}],["e^x",{"5":{"42":4}}],["edge",{"5":{"45":2}}],["error",{"5":{"44":2,"51":8}}],["erf",{"5":{"42":2}}],["e",{"5":{"42":8}}],["estimation",{"5":{"48":2,"61":4,"69":4,"96":2}}],["encoding",{"5":{"88":10,"89":2,"91":12,"92":8,"94":2}}],["end",{"5":{"42":20,"61":8,"86":4,"97":2}}],["entropy",{"5":{"47":2,"61":4,"70":2,"92":2,"96":8}}],["engineering",{"5":{"70":2}}],["=",{"5":{"42":98,"46":38,"61":48,"69":10,"86":18,"97":20,"99":2,"101":2}}],["zj",{"5":{"46":2}}],["z^",{"5":{"46":4}}],["zag",{"5":{"42":2}}],["zig",{"5":{"42":2}}],["zk",{"5":{"42":1,"61":2}}],["z",{"5":{"42":81,"46":11,"61":28,"97":12}}],["bf16",{"5":{"93":2}}],["block",{"5":{"86":4,"93":2}}],["bug",{"5":{"50":2}}],["broadcasting",{"5":{"50":2}}],["b测试可以用于评估模型对用户交互的实际影响",{"5":{"96":2}}],["b测试的核心是随机分组和控制变量",{"5":{"96":2}}],["b测试是互联网公司和研究机构常用的实验方法",{"5":{"96":2}}],["bootstrap",{"5":{"96":2}}],["boundary",{"5":{"47":2,"92":2}}],["bottleneck",{"5":{"71":2,"87":2}}],["bigbird",{"5":{"87":2}}],["bigbird等模型中被广泛使用",{"5":{"86":2}}],["bidirectional",{"5":{"86":2}}],["binary",{"5":{"61":2}}],["binomial",{"5":{"96":2}}],["bits",{"5":{"61":2}}],["bit",{"5":{"96":2}}],["biases",{"5":{"88":4}}],["bias",{"5":{"47":2,"50":2,"51":8,"92":2}}],["biological",{"5":{"47":2}}],["b",{"5":{"46":10}}],["basis",{"5":{"93":2}}],["band",{"5":{"86":2,"87":2}}],["backprop",{"5":{"48":4}}],["backpropagation",{"5":{"44":2}}],["backward",{"5":{"44":4}}],["batch内的样本",{"5":{"45":1}}],["batch依赖",{"5":{"41":1}}],["batch的统计量而非全局统计量",{"5":{"41":2}}],["batch中的激活值为",{"5":{"41":2}}],["batch",{"5":{"41":5,"44":2,"47":2,"50":6,"94":2,"96":4}}],["beta1",{"5":{"97":1}}],["beta",{"5":{"97":11}}],["beta先验和dirichlet先验可用于建模词汇概率的不确定性",{"5":{"96":2}}],["bernoulli",{"5":{"47":2,"96":2}}],["bert等",{"5":{"47":2}}],["bert",{"5":{"71":2}}],["bert的15",{"5":{"99":2,"101":2}}],["begin",{"5":{"42":20,"61":8,"86":4,"97":2}}],["bn",{"5":{"41":2}}],["bptt",{"5":{"41":2}}],["函数在注意力机制中扮演着核心角色",{"5":{"61":2}}],["函数定义为",{"5":{"61":2}}],["函数与交叉熵损失的数学联系",{"5":{"61":2}}],["函数的几个关键性质值得强调",{"5":{"61":2}}],["函数的定义为",{"5":{"61":2}}],["函数的二阶泰勒展开为",{"5":{"48":1}}],["函数的雅可比矩阵具有特殊的结构",{"5":{"61":2}}],["函数",{"5":{"48":2,"61":4,"70":1,"90":2,"92":2,"97":1}}],["函数值随某一特定变量变化的速率",{"5":{"48":2}}],["函数逼近基石",{"5":{"71":2}}],["函数是凸函数的充要条件是对于任意和任意",{"5":{"48":1}}],["函数是凸的",{"5":{"70":1,"97":1}}],["由卷积核大小决定",{"5":{"93":2}}],["由​参数化",{"5":{"92":1}}],["由相同的词元组成",{"5":{"92":2}}],["由公式",{"5":{"91":2}}],["由公式完全指定的",{"5":{"88":2}}],["由正弦等式可得",{"5":{"91":2}}],["由正弦函数给出",{"5":{"91":2}}],["由后续的线性变换自动学习",{"5":{"91":2}}],["由余弦函数给出",{"5":{"91":2}}],["由点积给出",{"5":{"90":2}}],["由特定频率的正弦余弦函数张成",{"5":{"89":2}}],["由不同频率的正弦余弦函数张成",{"5":{"89":2}}],["由softmax函数给出",{"5":{"61":1}}],["由softmax给出时",{"5":{"71":2}}],["由预测概率",{"5":{"61":1}}],["由预测概率决定",{"5":{"61":1}}],["由真实标签",{"5":{"61":1}}],["由kl散度度量",{"5":{"61":2}}],["由训练数据决定",{"5":{"61":2}}],["由训练数据近似",{"5":{"61":2}}],["由此可以得到",{"5":{"51":2}}],["由数据固有噪声决定的不可减少误差",{"5":{"51":2}}],["由jensen不等式保证",{"5":{"96":2}}],["由四阶中心矩归一化得到",{"5":{"96":2}}],["由三阶中心矩归一化得到",{"5":{"96":2}}],["由神经元的数量决定",{"5":{"47":1}}],["由超平面",{"5":{"47":1}}],["由超平面所界定",{"5":{"47":1}}],["由细胞体",{"5":{"47":2}}],["由多个全连接层堆叠而成",{"5":{"46":2}}],["由",{"5":{"45":2,"92":1}}],["由激活模式定义",{"5":{"45":2}}],["由描述",{"5":{"45":2}}],["由导数的定义",{"5":{"45":2}}],["由向量",{"5":{"71":1}}],["由向量描述",{"5":{"71":1}}],["由矩阵",{"5":{"71":1}}],["由矩阵描述",{"5":{"71":1}}],["由链式法则和雅可比矩阵的性质",{"5":{"44":2}}],["由链式法则",{"5":{"44":2}}],["由于hessian依赖于预测概率",{"5":{"97":2}}],["由于hessian是常数",{"5":{"97":2}}],["由于对于任意向量",{"5":{"97":2}}],["由于和独立",{"5":{"95":1}}],["由于有",{"5":{"94":1}}],["由于有个头",{"5":{"94":1}}],["由于​是随机矩阵",{"5":{"93":1}}],["由于​是独立学习的",{"5":{"93":1}}],["由于​通常不会精确为零",{"5":{"92":1}}],["由于​且",{"5":{"87":1}}],["由于除以了很大的数",{"5":{"91":2}}],["由于可以取任意值",{"5":{"91":1}}],["由于可以近似表示为低秩分解",{"5":{"87":1}}],["由于各频率",{"5":{"90":1}}],["由于各频率互不相同",{"5":{"90":1}}],["由于频率",{"5":{"90":1}}],["由于频率​是互不相同的无理数倍数",{"5":{"90":1}}],["由于余弦函数的周期性",{"5":{"90":2}}],["由于编码包含多个维度",{"5":{"91":2}}],["由于编码包含多个频率成分",{"5":{"90":2}}],["由于编码矩阵只学习了前",{"5":{"89":1}}],["由于编码矩阵只学习了前​行的参数",{"5":{"89":1}}],["由于这是一个单位复指数",{"5":{"90":2}}],["由于这些区别",{"5":{"87":2}}],["由于正弦编码矩阵",{"5":{"89":1}}],["由于正弦编码矩阵​的秩不超过",{"5":{"89":1}}],["由于正弦编码的秩为",{"5":{"89":2}}],["由于通常",{"5":{"89":2}}],["由于通常不是对称矩阵",{"5":{"87":1}}],["由于旋转操作是周期性的",{"5":{"88":2}}],["由于核函数的限制",{"5":{"87":2}}],["由于的每一行是一个概率分布",{"5":{"87":1}}],["由于在大多数学习任务中",{"5":{"61":2}}],["由于连乘在数值计算上容易产生下溢",{"5":{"61":2}}],["由于交叉项的期望为零",{"5":{"51":2}}],["由于训练数据通常被组织为序列",{"5":{"96":2}}],["由于参数数量巨大",{"5":{"48":2}}],["由于任意布尔函数都可以由and",{"5":{"47":2}}],["由于本身是线性的",{"5":{"45":1}}],["由于是对角非奇异矩阵",{"5":{"87":1}}],["由于是行随机矩阵",{"5":{"87":4}}],["由于是逐元素函数",{"5":{"44":1}}],["由于是非线性函数",{"5":{"71":1}}],["由于神经网络是一个深度复合函数",{"5":{"44":2}}],["由于",{"5":{"44":1,"45":1,"61":6,"70":4,"71":1,"87":9,"88":2,"91":1,"92":1,"93":6,"95":1,"97":2}}],["由于激活值的分布更加稳定",{"5":{"41":2}}],["由于归一化使用mini",{"5":{"41":2}}],["由于约一半的神经元输出为零",{"5":{"41":2}}],["由自注意力的对称性",{"5":{"99":2,"101":2}}],["向量的模长",{"5":{"88":1}}],["向量的模长对所有位置都保持不变",{"5":{"88":1}}],["向量的乘积需要",{"5":{"50":1}}],["向量空间中的mse",{"2":{"51":1},"5":{"51":1}}],["向量加法满足交换律和结合律",{"5":{"50":1}}],["向量加法是逐分量",{"5":{"50":2}}],["向量乘积",{"5":{"50":2}}],["向量化操作与kronecker积结合可以简化矩阵方程的表示",{"5":{"50":2}}],["向量化",{"5":{"50":2}}],["向量距离和相似度是度量特征空间中样本关系的基本工具",{"5":{"50":2}}],["向量是线性空间中最基本的元素",{"5":{"50":2}}],["向量链式法则",{"5":{"44":2}}],["向量",{"5":{"50":2,"88":1}}],["时减少",{"5":{"97":1}}],["时增加",{"5":{"97":1}}],["时的2倍",{"5":{"95":1}}],["时首先考虑的特征",{"5":{"94":2}}],["时域和频域之间的对偶关系",{"5":{"90":2}}],["时达到这一下界",{"5":{"61":2}}],["时仍能保持较大的梯度值",{"5":{"61":2}}],["时取等号",{"5":{"61":4}}],["时取严格小于号",{"5":{"48":1}}],["时kl散度为0",{"5":{"96":1}}],["时可归入任一模式",{"5":{"45":1}}],["时间复杂度为",{"5":{"92":2,"93":2}}],["时间复杂度",{"5":{"44":1,"45":1,"93":2}}],["时变化最慢",{"5":{"42":2}}],["时",{"5":{"41":13,"42":14,"45":1,"46":4,"47":8,"50":1,"61":11,"69":3,"70":13,"71":2,"86":2,"87":7,"88":2,"89":5,"90":5,"92":10,"94":1,"95":5,"96":9}}],["05",{"5":{"96":2}}],["044715x^3",{"5":{"42":2}}],["0",{"5":{"42":42,"47":6,"61":8,"69":4,"70":18}}],["07",{"5":{"69":2}}],["01",{"5":{"70":4}}],["梯度能够稳定地回传",{"5":{"95":2}}],["梯度无法有效地反向传播回前层网络",{"5":{"95":2}}],["梯度都能稳定地流动",{"5":{"94":2}}],["梯度从输出端通过value矩阵反向传播回query投影",{"5":{"94":2}}],["梯度从输出层向输入层传播时",{"5":{"42":2}}],["梯度从输出层向输入层传播",{"5":{"41":2}}],["梯度信号会高度相似",{"5":{"93":2}}],["梯度信号会指数级衰减",{"5":{"92":2}}],["梯度会变得非常小",{"5":{"95":1}}],["梯度会指数级增长",{"5":{"92":2}}],["梯度会指数级衰减",{"5":{"92":2}}],["梯度会均匀分布到所有位置",{"5":{"41":2}}],["梯度特性",{"5":{"61":1}}],["梯度较大",{"5":{"51":2}}],["梯度较小",{"5":{"51":2}}],["梯度累积在数学上等价于使用更大的批次进行训练",{"5":{"50":2}}],["梯度累积是另一种与批处理相关的技术",{"5":{"50":2}}],["梯度累积效应",{"5":{"41":2}}],["梯度估计的方差较大",{"5":{"96":2}}],["梯度估计的方差减小",{"5":{"96":2}}],["梯度下降的收敛速度为",{"5":{"97":2}}],["梯度下降算法可能沿着这个方向逃离鞍点",{"5":{"97":2}}],["梯度下降动力学",{"5":{"97":2}}],["梯度下降动力学与收敛分析",{"2":{"97":1},"5":{"97":1}}],["梯度下降可能收敛到较差的解",{"5":{"51":2}}],["梯度下降以",{"5":{"51":1}}],["梯度下降以的速度收敛到最优解附近",{"5":{"51":1}}],["梯度下降以线性速度收敛",{"5":{"48":2}}],["梯度下降等一阶优化算法在凸函数上具有良好的收敛保证",{"5":{"51":2}}],["梯度下降在凸优化中的收敛性有完善的理论保证",{"5":{"48":2}}],["梯度下降是最基本也最广泛使用的优化算法",{"5":{"48":2}}],["梯度下降与凸优化基础",{"2":{"48":1},"5":{"48":1}}],["梯度与等值面",{"5":{"48":2}}],["梯度与雅可比矩阵",{"5":{"44":2}}],["梯度向量与等值面垂直",{"5":{"48":2}}],["梯度向量指导着参数的更新方向",{"5":{"48":2}}],["梯度向量指向函数增长最快的方向",{"5":{"48":2}}],["梯度向量就是误差信号本身",{"5":{"44":2}}],["梯度是多元函数最陡上升方向的向量",{"5":{"48":2}}],["梯度和hessian等概念",{"5":{"48":2}}],["梯度值在fp16下为0",{"5":{"44":1}}],["梯度值溢出",{"5":{"44":1}}],["梯度值接近0",{"5":{"44":1}}],["梯度计算的优化策略",{"5":{"61":2}}],["梯度计算",{"5":{"44":1}}],["梯度缩放等技术都是保证大规模神经网络训练稳定性的重要手段",{"5":{"44":2}}],["梯度缩放",{"5":{"44":3}}],["梯度可能变得很大或很小",{"5":{"87":2}}],["梯度可能因精度限制而下溢",{"5":{"44":2}}],["梯度可以直接计算为",{"5":{"61":2}}],["梯度可以正可以负",{"5":{"42":2}}],["梯度可以稳定传播",{"5":{"41":2}}],["梯度可以简洁地写为",{"5":{"69":2}}],["梯度检查点",{"5":{"50":2}}],["梯度检查点技术通过在前向传播时只保存部分激活值",{"5":{"48":2}}],["梯度检查",{"5":{"44":4}}],["梯度消失或梯度爆炸等问题",{"5":{"44":2}}],["梯度消失",{"5":{"42":2,"44":1,"94":2,"95":2}}],["梯度几乎为零",{"5":{"42":2}}],["梯度关于",{"5":{"41":1}}],["梯度关于的雅可比矩阵的范数满足",{"5":{"41":1}}],["梯度稳定性分析",{"5":{"41":2}}],["梯度也被限制在这个流形的切空间中",{"5":{"41":2}}],["梯度也不会因为激活值的尺度变化而放大或缩小",{"5":{"41":2}}],["梯度尺度归一化",{"5":{"41":2}}],["梯度尺度的归一化",{"5":{"41":2}}],["梯度主要集中在少数方向",{"5":{"41":2}}],["梯度的范数缩放约为",{"5":{"92":2}}],["梯度的范数满足",{"5":{"41":2}}],["梯度的方向更加稳定",{"5":{"96":2}}],["梯度的性质对于理解优化过程至关重要",{"5":{"48":2}}],["梯度的大小也更加稳定",{"5":{"41":2}}],["梯度的累积效应可能导致不稳定",{"5":{"41":2}}],["梯度的显式计算",{"2":{"69":1},"5":{"69":1}}],["梯度的雅可比矩阵",{"5":{"70":2,"97":2}}],["梯度范数的可能变化范围越大",{"5":{"41":2}}],["梯度范数边界",{"5":{"41":2}}],["梯度范数可能衰减",{"5":{"41":2}}],["梯度范数可能放大",{"5":{"41":2}}],["梯度范数与奇异值",{"5":{"41":2}}],["梯度范数约为",{"5":{"41":2}}],["梯度流方程为",{"5":{"97":2}}],["梯度流是梯度下降的连续极限",{"5":{"97":2}}],["梯度流与离散动力系统",{"2":{"97":1},"5":{"97":1}}],["梯度流与信息保持",{"2":{"41":1},"5":{"41":1}}],["梯度流动",{"5":{"89":2}}],["梯度流通过注意力权重矩阵",{"5":{"41":1}}],["梯度流通过注意力权重矩阵进行",{"5":{"41":1}}],["梯度流更加稳定",{"5":{"41":2}}],["梯度流稳定",{"5":{"41":2}}],["梯度流的信息瓶颈",{"5":{"41":2}}],["梯度流",{"5":{"41":2,"97":2}}],["梯度裁剪需要考虑这些特殊结构",{"5":{"97":2}}],["梯度裁剪本质上限制了有效学习率的上界",{"5":{"97":2}}],["梯度裁剪是一种有效的抑制梯度爆炸的技术",{"5":{"97":2}}],["梯度裁剪是最常用的策略",{"5":{"41":2}}],["梯度裁剪与爆炸抑制",{"2":{"97":1},"5":{"97":1}}],["梯度裁剪就是将梯度向量的模长限制在某个阈值之内",{"5":{"50":2}}],["梯度裁剪将梯度范数限制在",{"5":{"44":1}}],["梯度裁剪将梯度范数限制在范围内",{"5":{"44":1}}],["梯度裁剪的边界效应",{"5":{"44":2}}],["梯度裁剪",{"5":{"44":7,"48":2,"97":2}}],["梯度裁剪有两种主要形式",{"5":{"41":2,"97":2}}],["梯度高速公路",{"5":{"41":2}}],["梯度至少可以通过恒等路径",{"5":{"41":2}}],["梯度需要沿时间步反向传播",{"5":{"41":2}}],["梯度爆炸是深度学习训练中的常见问题",{"5":{"97":2}}],["梯度爆炸可能通过多个渠道发生",{"5":{"41":2}}],["梯度爆炸",{"5":{"41":2,"44":1}}],["梯度爆炸在循环神经网络和transformer中尤为突出",{"5":{"41":2}}],["梯度爆炸指的是在反向传播过程中梯度值变得非常大",{"5":{"41":2}}],["梯度爆炸的表现包括",{"5":{"41":2}}],["梯度爆炸的阈值与裁剪策略",{"2":{"41":1},"5":{"41":1}}],["梯度爆炸的现象描述",{"2":{"41":1},"5":{"41":1}}],["梯度爆炸的数学机制",{"2":{"41":1},"5":{"41":1}}],["梯度衰减为",{"5":{"41":2,"42":2}}],["梯度衰减为原来的",{"5":{"41":2,"42":2}}],["梯度饱和意味着信息在反向传播过程中丢失",{"5":{"41":2}}],["梯度饱和可以在输出尚未饱和时发生",{"5":{"41":2}}],["梯度饱和",{"5":{"41":2,"42":2,"97":1}}],["梯度饱和是深度学习训练中的核心挑战之一",{"5":{"41":2,"70":2}}],["梯度饱和与梯度爆炸是深度学习训练中的两大核心挑战",{"5":{"41":2}}],["梯度饱和与梯度爆炸的数学根源<",{"5":{"21":1,"85":1}}],["梯度饱和与梯度爆炸的数学根源",{"0":{"41":1},"4":{"41":1},"5":{"21":4,"41":1,"85":4}}],["梯度饱和的数学机制",{"2":{"41":1},"5":{"41":1}}],["梯度饱和特性",{"2":{"70":1},"5":{"70":1}}],["梯度",{"5":{"44":2,"95":1}}],["梯度归一化",{"5":{"70":2,"99":2,"101":2}}],["梯度归一化与帕累托优化",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["梯度归一化方法通过调整每个任务的梯度范数来实现平衡",{"5":{"70":2}}],["梯度归一化方法通过调整梯度范数来实现任务间的平衡",{"5":{"99":2,"101":2}}],["梯度归一化方法的一个理论优势是",{"5":{"99":2,"101":2}}],["梯度归一化的更新方向为",{"5":{"99":2,"101":2}}],["梯度归一化的推广",{"5":{"99":2,"101":2}}],["梯度结构的对比分析",{"2":{"70":1},"5":{"70":1}}],["梯度还受到",{"5":{"70":2}}],["梯度在概率饱和区域被过度压缩",{"5":{"70":2}}],["梯度仍然需要经过",{"5":{"92":1}}],["梯度仍然需要经过次矩阵乘法的累积",{"5":{"92":1}}],["梯度仍然可以通过",{"5":{"41":1}}],["梯度仍然可以通过路径传递到前一层",{"5":{"41":1}}],["梯度仍为",{"5":{"70":2}}],["梯度仍保持与成正比",{"5":{"70":1}}],["梯度仍保持与",{"5":{"70":1}}],["梯度为零的点",{"5":{"97":2}}],["梯度为",{"5":{"70":4}}],["梯度被压缩",{"5":{"70":2}}],["梯度被放大",{"5":{"70":2}}],["梯度解释",{"5":{"99":2,"101":2}}],["距离近的位置倾向于有更高的注意力权重",{"5":{"91":2}}],["距离衰减",{"5":{"91":2}}],["距离越远的token之间的依赖越弱",{"5":{"88":2}}],["距离最小的点",{"5":{"51":1}}],["距离",{"5":{"51":2,"61":4,"70":2,"71":2,"90":2,"96":2}}],["因式分解",{"5":{"93":2}}],["因果掩码的引入使得注意力计算仍然保持矩阵形式",{"5":{"95":2}}],["因果掩码的数学形式",{"2":{"95":1},"5":{"95":1}}],["因果掩码",{"5":{"95":2}}],["因果位置编码通过某种方式打破这种对称性",{"5":{"91":2}}],["因果位置编码",{"5":{"91":2}}],["因果性",{"5":{"91":2}}],["因果注意力掩码确保位置只能关注位置的token",{"5":{"99":1,"101":1}}],["因果注意力掩码确保位置",{"5":{"99":1,"101":1}}],["因其在微分运算中的便利性而被广泛采用",{"5":{"61":2}}],["因此均方误差的",{"5":{"97":2}}],["因此增加头数意味着减小每个头的维度",{"5":{"93":2}}],["因此最终输出中各头的信息是相互交织的",{"5":{"93":2}}],["因此最小化交叉熵等价于最小化kl散度",{"5":{"96":2}}],["因此输出维度为",{"5":{"93":2}}],["因此输出饱和往往伴随梯度饱和",{"5":{"41":2}}],["因此从",{"5":{"92":1}}],["因此从到的路径长度恒为1",{"5":{"92":1}}],["因此是理论上的最优复杂度",{"5":{"92":1}}],["因此点积较大",{"5":{"91":2}}],["因此没有被广泛采用",{"5":{"88":2}}],["因此不如策略一常用",{"5":{"88":2}}],["因此秩最多为",{"5":{"87":1}}],["因此秩最多为​",{"5":{"87":1}}],["因此特征值可以是复数",{"5":{"87":2}}],["因此可以保留更多的原始信息",{"5":{"94":2}}],["因此可以直接适应特定任务的需求",{"5":{"93":2}}],["因此可以学习到不同的特征表示和关联模式",{"5":{"93":2}}],["因此可以视为一种",{"5":{"87":1}}],["因此可以使用更大的学习率进行训练",{"5":{"41":2}}],["因此我们可以将数据生成过程建模为",{"5":{"61":2}}],["因此它不是真正意义上的",{"5":{"61":2}}],["因此敏感性较低",{"5":{"51":2}}],["因此单个层的计算量是相当可观的",{"5":{"50":2}}],["因此现代gpu都针对矩阵运算进行了专门的硬件优化",{"5":{"50":2}}],["因此深入掌握线性代数的基本概念和运算规则是理解模型工作原理的必要前提",{"5":{"50":2}}],["因此标准化操作是合理的",{"5":{"96":2}}],["因此共轭梯度法和拟牛顿法等方法在处理大规模优化问题时具有优势",{"5":{"48":2}}],["因此得名",{"5":{"46":2}}],["因此在实践中表现出更强的表示能力和泛化性能",{"5":{"71":2}}],["因此需要使用更大的权重方差",{"5":{"41":1}}],["因此需要使用更大的权重方差来保持激活的稳定性",{"5":{"41":1}}],["因此",{"5":{"42":6,"44":10,"45":8,"46":2,"47":6,"51":2,"61":4,"69":2,"70":4,"71":8,"86":2,"87":9,"88":8,"89":2,"90":12,"91":8,"92":9,"93":2,"95":4,"99":3,"101":3}}],["因此受到更强的",{"5":{"69":2}}],["因此​",{"5":{"99":1,"101":1}}],["因为负曲率方向上的梯度通常很小",{"5":{"97":2}}],["因为假设均值为0",{"5":{"95":2}}],["因为key和value的缓存可以大幅减少",{"5":{"93":2}}],["因为一边没有足够的",{"5":{"92":2}}],["因为是固定的",{"5":{"91":1}}],["因为编码不使用极高频率",{"5":{"90":2}}],["因为维度限制",{"5":{"89":2}}],["因为这些领域的",{"5":{"89":2}}],["因为公式定义在所有整数上",{"5":{"89":2,"91":2}}],["因为不依赖于",{"5":{"89":1}}],["因为非零奇异值较少",{"5":{"89":2}}],["因为所有位置编码向量都位于由前",{"5":{"89":1}}],["因为所有位置编码向量都位于由前​个基向量张成的子空间中",{"5":{"89":1}}],["因为所有其他位置都参与比较",{"5":{"69":2}}],["因为模型从未学习过如何处理那种位置与内容的新组合",{"5":{"88":2}}],["因为每层可以学习不同的低秩变换",{"5":{"87":2}}],["因为每个头的投影矩阵维度为",{"5":{"86":2}}],["因为指数函数是单调变换",{"5":{"87":2}}],["因为1是特征值",{"5":{"87":2}}],["因为行和为1且元素非负",{"5":{"87":2}}],["因为对于任意向量",{"5":{"61":2}}],["因为两个点",{"5":{"51":1}}],["因为两个点和通过线性关系连接",{"5":{"51":1}}],["因为其使用绝对值而非平方",{"5":{"51":2}}],["因为前向和后向替换的复杂度是",{"5":{"50":1}}],["因为前向和后向替换的复杂度是而不是高斯消元的",{"5":{"50":1}}],["因为梯度是线性的",{"5":{"50":2}}],["因为长序列可能消耗大量内存",{"5":{"50":2}}],["因为正弦函数对所有实数都有定义",{"5":{"88":2}}],["因为正规方程",{"5":{"50":1}}],["因为正规方程的解等价于求解",{"5":{"50":1}}],["因为正交矩阵的奇异值全部为1",{"5":{"50":2}}],["因为在数值上计算高阶多项式的根非常困难",{"5":{"50":2}}],["因为数值稳定性较差且计算成本高昂",{"5":{"50":2}}],["因为异或问题的正例点位于对角位置",{"5":{"47":2}}],["因为它能够适应训练过程中梯度分布的变化",{"5":{"97":2}}],["因为它能够捕捉语义方向的相似性而不受词频等因素的影响",{"5":{"50":2}}],["因为它对频繁出现的特征使用较小的学习率",{"5":{"97":2}}],["因为它为每个绝对位置",{"5":{"91":1}}],["因为它为每个绝对位置生成一个独立的编码向量",{"5":{"91":1}}],["因为它们的位置差都是1",{"5":{"91":2}}],["因为它们在推理过程中不变",{"5":{"89":2}}],["因为它已经与内容信息纠缠在一起",{"5":{"88":2}}],["因为它考虑到了概率分布的归一化约束和概率质量在各个类别间的分配方式",{"5":{"61":2}}],["因为它可以将不同样本的计算分配到不同的处理单元上同时执行",{"5":{"50":2}}],["因为它将复杂的矩阵运算转化为标准的矩阵",{"5":{"50":2}}],["因为它决定了信息放大或衰减的上界",{"5":{"50":2}}],["因为它不满足对称性和三角不等式",{"5":{"96":2}}],["因为它的数学形式最为清晰",{"5":{"46":2}}],["因为它的稀疏激活特性和线性区域的恒定导数有利于深层网络的训练",{"5":{"71":2}}],["因为它保留了梯度的主要方向信息",{"5":{"41":2,"97":2}}],["因为语言中的许多规律具有位置平移不变性",{"5":{"88":2}}],["因为语言",{"5":{"71":2}}],["因为层归一化位于关键路径上",{"5":{"41":2}}],["因为或",{"5":{"41":1}}],["因为",{"5":{"41":3,"44":2,"61":4,"70":1,"87":2,"89":1,"90":2,"91":1,"95":2}}],["因为我们可以将期望操作与梯度操作交换次序",{"5":{"96":2}}],["因为我们在推导中直接对",{"5":{"70":2}}],["因为被",{"5":{"70":1}}],["因为真实分布固定",{"5":{"96":2}}],["因为真实类别是1",{"5":{"70":2}}],["k采样和top",{"5":{"96":2}}],["kaiming初始化",{"5":{"89":2}}],["knowledge",{"5":{"87":2}}],["kernel",{"5":{"86":2,"87":6,"88":2,"93":2}}],["key空间有不同的维度",{"5":{"94":2}}],["key提供了信息片段的标识符",{"5":{"94":2}}],["key投影和value投影的参数数量也各为",{"5":{"93":2}}],["key向量进行逐元素乘法",{"5":{"88":1}}],["key向量进行逐元素乘法具体而言",{"5":{"88":1}}],["key内积就自然地只依赖于相对位置",{"5":{"88":2}}],["key内积的差异",{"5":{"88":2}}],["key内积运算",{"5":{"88":2}}],["key内积自动地只与相对位置有关",{"5":{"88":2}}],["key内积",{"5":{"88":6}}],["key维度限制",{"5":{"87":2}}],["key矩阵和value矩阵",{"5":{"94":2}}],["key矩阵",{"5":{"87":2}}],["key和value共享一个",{"5":{"93":1}}],["key和value共享一个​维投影",{"5":{"93":1}}],["key和value投影各为",{"5":{"86":2}}],["key和value的缓存也可以采用稀疏格式",{"5":{"86":2}}],["key",{"0":{"94":1},"4":{"94":1},"5":{"61":2,"69":2,"70":4,"71":2,"85":5,"86":6,"88":2,"91":4,"93":18,"94":21,"95":6,"97":4}}],["key相似度分数转换为概率分布",{"5":{"71":2}}],["key相似度分数转换为注意力权重",{"5":{"69":2}}],["kl",{"5":{"61":4,"70":14,"97":2}}],["kl散度诱导的几何结构是",{"5":{"61":1}}],["kl散度诱导的几何结构是黎曼几何结构",{"5":{"61":1}}],["kl散度定义了所谓的",{"5":{"61":2}}],["kl散度定义为",{"5":{"96":2}}],["kl散度度量了两个概率分布之间的",{"5":{"61":2}}],["kl散度越小",{"5":{"61":2}}],["kl散度的物理意义可以从",{"5":{"61":2}}],["kl散度的定义为",{"5":{"96":2}}],["kl散度的几个关键性质对其应用至关重要",{"5":{"61":2}}],["kl散度的几何意义",{"5":{"70":2}}],["kl散度的类比",{"5":{"70":2}}],["kl散度不满足对称性和三角不等式",{"5":{"61":2}}],["kl散度不对称",{"5":{"96":2}}],["kl散度可以理解为使用分布",{"5":{"96":1}}],["kl散度可以理解为使用分布来编码来自分布的样本时所需要的额外信息量",{"5":{"96":1}}],["kl散度在不同场景下的不同形式反映了其灵活性",{"5":{"96":2}}],["kl散度用于衡量学生模型和教师模型输出分布之间的差异",{"5":{"96":2}}],["kl散度用于限制策略更新的幅度",{"5":{"96":2}}],["kl散度有多种重要应用",{"5":{"96":2}}],["kl散度是非负的",{"5":{"96":2}}],["kl散度衡量两个概率分布之间的",{"5":{"96":2}}],["kl散度与交叉熵",{"2":{"96":1},"5":{"96":1}}],["kl散度",{"5":{"61":4,"69":2,"70":3,"71":2,"96":2}}],["kl散度约束的引入增加了优化目标复杂性",{"5":{"97":2}}],["kl散度约束的数学分析",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["kl散度项的作用",{"5":{"99":2,"101":2}}],["kl约束项使得优化后的策略不是简单地选择奖励最高的动作",{"5":{"99":2,"101":2}}],["kurtosis",{"5":{"96":2}}],["kullback",{"2":{"61":1},"5":{"61":3,"96":2}}],["kronecker积可以帮助我们将大矩阵运算分解为多个小矩阵运算的组合",{"5":{"50":2}}],["kronecker积的主要应用包括权重共享",{"5":{"50":2}}],["kronecker积具有许多有用的代数性质",{"5":{"50":2}}],["kronecker积是两个矩阵之间的特殊二元运算",{"5":{"50":2}}],["kronecker积与向量化",{"2":{"50":1},"5":{"50":1}}],["kronecker",{"5":{"48":2,"92":2}}],["k",{"5":{"42":78,"46":6,"48":3,"61":76,"86":17,"94":2}}],["k个位置",{"5":{"86":2}}],["k个非零权重",{"5":{"69":2}}],["k个最大的分数",{"5":{"99":2,"101":2}}],["k^",{"5":{"69":6}}],["k相似度",{"5":{"69":1}}],["与transformer架构的协同",{"5":{"101":2}}],["与transformer架构的联系",{"5":{"71":2}}],["与利普希茨常数",{"5":{"97":1}}],["与非凸损失函数的对比",{"5":{"97":2}}],["与query和key不同",{"5":{"94":2}}],["与query更相似的key获得更高的权重",{"5":{"69":2}}],["与单头配置的参数数量相同",{"5":{"94":2}}],["与单独使用可学习编码相同",{"5":{"89":2}}],["与堆叠后的投影矩阵相乘",{"5":{"93":1}}],["与头数无关",{"5":{"93":2}}],["与固定的基函数",{"5":{"93":2}}],["与固定形式的正弦余弦位置编码不同",{"5":{"89":2}}],["与输入序列长度和模型维度成正比",{"5":{"93":2}}],["与输入向量的外积",{"5":{"44":2}}],["与相乘",{"5":{"93":1}}],["与混合专家模型的类比",{"2":{"93":1},"5":{"93":1}}],["与卷积网络相比",{"5":{"92":2}}],["与卷积矩阵的比较",{"2":{"87":1},"5":{"87":1}}],["与内容编码兼容",{"5":{"92":2}}],["与内容向量相加或拼接",{"5":{"88":2}}],["与之对应的是不同",{"5":{"92":2}}],["与硬选择",{"5":{"92":2}}],["与rnn相比",{"5":{"92":2}}],["与rnn的迭代依赖结构不同",{"5":{"92":2}}],["与rope结合的可能性",{"5":{"88":2}}],["与它们在序列中的距离无关",{"5":{"92":2}}],["与后面的",{"5":{"92":2}}],["与标准注意力框架兼容",{"5":{"91":2}}],["与标准softmax注意力不同",{"5":{"87":2}}],["与假设矛盾",{"5":{"91":2}}],["与注意力分数的交互分析",{"2":{"91":1},"5":{"91":1}}],["与注意力机制的联系",{"5":{"61":4}}],["与注意力机制的设计协同",{"2":{"41":1},"5":{"41":1}}],["与注意力机制的梯度流",{"2":{"41":1},"5":{"41":1}}],["与注意力机制的数学联系",{"2":{"70":1},"5":{"70":1}}],["与注意力机制中缩放因子的比较",{"5":{"69":2}}],["与频率域中频率分辨率",{"5":{"90":2}}],["与频率域之间的深刻对偶关系",{"5":{"90":2}}],["与正弦余弦编码各有优劣",{"5":{"89":2}}],["与正弦余弦编码的平滑性质一致",{"5":{"89":2}}],["与正弦余弦编码的多尺度频率结构有某种对应关系",{"5":{"89":2}}],["与正弦余弦编码相比",{"5":{"89":2}}],["与正弦余弦编码不同",{"5":{"89":2}}],["与正弦编码的比较分析",{"2":{"89":1},"5":{"89":1}}],["与绝对位置无关",{"5":{"90":2}}],["与绝对位置和无关",{"5":{"90":1,"91":1}}],["与绝对位置",{"5":{"88":2,"90":1,"91":1}}],["与序列长度",{"5":{"87":1}}],["与序列长度无关",{"5":{"86":2,"87":1}}],["与起始位置",{"5":{"87":1}}],["与起始位置无关",{"5":{"87":1}}],["与初始位置无关",{"5":{"87":2}}],["与核矩阵的比较",{"2":{"87":1},"5":{"87":1}}],["与gram矩阵的比较",{"2":{"87":1},"5":{"87":1}}],["与欧几里得距离不同",{"5":{"61":2}}],["与模型分布",{"5":{"61":2}}],["与模型无关",{"5":{"61":2}}],["与",{"5":{"61":7,"69":1,"88":4,"92":5,"93":2,"94":2,"95":2,"99":1,"101":1}}],["与传统距离度量不同",{"5":{"61":2}}],["与预测空间",{"5":{"51":1}}],["与全局最小值相近",{"5":{"51":2}}],["与mse相比",{"5":{"51":2}}],["与随机高斯初始化或xavier初始化不同",{"5":{"50":2}}],["与向量的大小无关",{"5":{"50":2}}],["与pmf不同",{"5":{"96":2}}],["与目标",{"5":{"48":1}}],["与凸优化问题不同",{"5":{"48":2}}],["与完整的hessian相比",{"5":{"48":2}}],["与麦肯罗皮层神经元使用阶跃函数不同",{"5":{"47":2}}],["与权重矩阵区别对待",{"5":{"46":2}}],["与权重初始化的关系",{"5":{"41":2}}],["与其手工设计位置编码的形式",{"5":{"89":2}}],["与其通过标准正态累积分布函数",{"5":{"71":1}}],["与其他行无关",{"5":{"45":2}}],["与其他选项无关",{"5":{"69":2}}],["与其他位置之间的互信息",{"5":{"69":1}}],["与其上下文之间的相对关系",{"5":{"99":1,"101":1}}],["与数值微分",{"5":{"45":2,"48":2}}],["与前向传播相反",{"5":{"44":1,"45":1}}],["与前向传播相同",{"5":{"44":1,"45":1}}],["与损失函数的联系",{"5":{"71":2}}],["与第二项转置相关",{"5":{"91":2}}],["与第四章第一节讨论的均方误差不同",{"5":{"61":2}}],["与第四章损失函数的联系",{"5":{"71":2}}],["与第五章注意力的联系",{"5":{"71":2}}],["与梯度消失问题的联系",{"5":{"42":2}}],["与梯度饱和相反",{"5":{"41":2}}],["与sigmoid的梯度比较",{"5":{"42":2}}],["与sigmoid的数学关系",{"2":{"42":1},"5":{"42":1}}],["与sigmoid导数",{"5":{"42":1}}],["与sigmoid导数在形式上非常相似",{"5":{"42":1}}],["与sigmoid导数的比较",{"5":{"42":2}}],["与sigmoid相比",{"5":{"41":2,"42":4}}],["与层归一化相比",{"5":{"41":2}}],["与批归一化的比较",{"5":{"41":2}}],["与批归一化不同",{"5":{"41":2}}],["与下游任务的契合",{"2":{"41":1},"5":{"41":1}}],["与交叉熵组合的梯度计算",{"5":{"61":2}}],["与交叉熵的等价性",{"5":{"69":2}}],["与交叉熵的联系",{"5":{"99":2,"101":2}}],["与查询更相似的键获得更大的权重",{"5":{"69":2}}],["与均方误差的比较",{"5":{"61":2}}],["与均方误差和交叉熵的比较",{"5":{"69":2}}],["与最相似的键获得接近1的概率",{"5":{"69":1}}],["与位置距离无关",{"5":{"92":2}}],["与位置距离的直觉一致",{"5":{"91":2}}],["与位置无关",{"5":{"91":2}}],["与位置之间的距离存在特定的数学关系",{"5":{"91":2}}],["与位置编码的协同",{"5":{"101":2}}],["与位置编码的信息论联系",{"5":{"71":2}}],["与位置编码的联合设计",{"5":{"41":2}}],["与位置语义相关",{"5":{"69":1}}],["与位置",{"5":{"69":1}}],["与所有其他样本",{"5":{"69":1}}],["与真实分布",{"5":{"61":1}}],["与真实互信息之间的关系",{"5":{"69":1}}],["与每个",{"5":{"69":1}}],["与某种",{"5":{"69":2}}],["与某个负样本",{"5":{"69":1}}],["与键",{"5":{"69":1}}],["与任务类型匹配",{"5":{"70":2}}],["与参数无关",{"5":{"70":1,"97":1}}],["与参数",{"5":{"70":1,"97":1}}],["与具有相同的函数形式",{"5":{"99":1,"101":1}}],["与自然语言中位置依赖的统计特性相符",{"5":{"90":2}}],["与自回归语言建模不同",{"5":{"99":2,"101":2}}],["与infonce的联系",{"5":{"99":2,"101":2}}],["信息标识",{"5":{"94":2}}],["信息需求",{"5":{"94":2}}],["信息需要沿着序列依次传递",{"5":{"95":2}}],["信息需要沿着序列逐步传递",{"5":{"92":2}}],["信息需要经过路径长度与依赖跨度成正比的逐步传递",{"5":{"92":2}}],["信息中继",{"5":{"92":2}}],["信息只能从相邻位置逐步传递",{"5":{"92":2}}],["信息传递路径必须沿着序列顺序",{"5":{"92":2}}],["信息传递路径的数学分析",{"2":{"92":1},"5":{"92":1}}],["信息必须逐步传递",{"5":{"92":2}}],["信息必须经过所有中间位置的隐藏状态才能传递",{"5":{"92":2}}],["信息必须沿着序列逐步传递",{"5":{"91":2}}],["信息量",{"5":{"89":2}}],["信息被编码在载波信号的相位变化中",{"5":{"88":2}}],["信息到不同维度",{"5":{"87":2}}],["信息瓶颈的目标函数为",{"5":{"87":2}}],["信息瓶颈理论认为",{"5":{"71":2,"87":2}}],["信息压缩",{"5":{"87":2}}],["信息流动矩阵为",{"5":{"87":2}}],["信息流动的几何描述",{"2":{"45":1},"5":{"45":1}}],["信息会收敛到平稳分布",{"5":{"87":2}}],["信息几何",{"5":{"61":2,"70":4}}],["信息几何是研究概率分布空间几何性质的理论框架",{"5":{"71":2}}],["信息几何视角",{"2":{"70":1},"5":{"70":1}}],["信息编码",{"5":{"61":2}}],["信息丰富程度",{"5":{"61":2}}],["信息经过变换后丢失了一些细节",{"5":{"92":2}}],["信息经过",{"5":{"45":1}}],["信息经过次非线性变换后到达输出层",{"5":{"45":1}}],["信息论的核心概念之一是",{"5":{"61":1}}],["信息论的核心概念之一是自信息",{"5":{"61":1}}],["信息论的基本概念始于熵",{"5":{"96":2}}],["信息论的概念",{"5":{"70":2}}],["信息论",{"5":{"61":2}}],["信息论基础",{"2":{"61":1},"5":{"61":1,"69":2}}],["信息论提供了量化信息",{"5":{"71":2}}],["信息论视角与激活函数",{"2":{"71":1},"5":{"71":1}}],["信息贡献",{"5":{"69":2}}],["信息矩阵的形式",{"5":{"70":2}}],["信息矩阵的结构",{"5":{"70":2}}],["信息矩阵",{"5":{"70":2,"97":2}}],["信息矩阵定义为",{"5":{"70":2}}],["信息矩阵定义了概率流形上的黎曼度量",{"5":{"70":1}}],["信息矩阵定义了概率流形上的",{"5":{"70":1}}],["信息矩阵给出",{"5":{"70":2}}],["信息",{"5":{"70":2,"90":2}}],["信息加权的距离",{"5":{"70":2}}],["信息较大的方向上",{"5":{"70":2}}],["信息较小的方向上",{"5":{"70":2}}],["有着有趣的类比",{"5":{"94":2}}],["有​",{"5":{"91":1}}],["有深刻的联系",{"5":{"90":2}}],["有时略优",{"5":{"89":2}}],["有时我们需要的是校准的预测概率",{"5":{"70":1}}],["有时我们需要的是区分性的预测",{"5":{"70":1}}],["有时我们需要的是",{"5":{"70":2}}],["有可训练参数的",{"5":{"89":2}}],["有副作用的",{"5":{"88":2}}],["有些样本更关键",{"5":{"51":2}}],["有多种理论解释",{"5":{"48":2}}],["有限样本量带来的统计波动自然地起到了正则化的作用",{"5":{"61":2}}],["有限",{"5":{"44":1,"45":1,"96":3}}],["有损编码",{"5":{"71":1}}],["有利于梯度下降的收敛",{"5":{"71":2}}],["有界",{"5":{"71":2}}],["有界振荡性",{"5":{"71":2}}],["有更强的梯度信号",{"5":{"42":2}}],["有效复杂度",{"5":{"92":4}}],["有效批大小",{"5":{"50":2}}],["有效自由度",{"5":{"50":2}}],["有效秩度量了需要多少个奇异值才能",{"5":{"87":2}}],["有效秩的定义基于奇异值的分布",{"5":{"87":2}}],["有效秩与衰减奇异值",{"2":{"87":1},"5":{"87":1}}],["有效秩与梯度衰减",{"5":{"41":2}}],["有效秩与信息传递",{"2":{"41":1},"5":{"41":1}}],["有效秩衡量了矩阵中",{"5":{"41":2}}],["有效秩定义为",{"5":{"41":2}}],["有效秩",{"5":{"41":2,"87":2}}],["有效秩决定了网络能够捕获的信息维度",{"5":{"41":2}}],["有效地",{"5":{"69":1}}],["有效避免了梯度消失问题",{"5":{"70":2}}],["有",{"5":{"44":2,"45":4,"47":2,"48":2,"51":4,"61":8,"70":8,"87":2,"88":4,"90":2,"91":1,"96":5,"97":4,"99":2,"101":2}}],["有以下效果",{"5":{"69":2}}],["有助于数值稳定性",{"5":{"86":2}}],["有助于捕获全局依赖",{"5":{"86":2}}],["有助于逃离平坦区域",{"5":{"48":2}}],["有助于学习清晰的表示空间",{"5":{"69":2}}],["有助于模型学习边界情况",{"5":{"70":2}}],["有其独立的query",{"5":{"69":1}}],["有各自的特点",{"5":{"99":2,"101":2}}],["则softmax操作定义为",{"5":{"95":2}}],["则标准差为1",{"5":{"95":2}}],["则标准化和",{"5":{"96":1}}],["则标准化和的分布趋向于标准正态分布",{"5":{"96":1}}],["则矩阵乘法的结果为",{"5":{"95":2}}],["则矩阵的frobenius范数为",{"5":{"89":2}}],["则代表了源语言各个词语所携带的实际信息内容",{"5":{"95":2}}],["则每个元素的方差初始化为",{"5":{"94":2}}],["则每个头的投影矩阵维度为",{"5":{"94":2}}],["则每层的梯度传递因子约为",{"5":{"41":2}}],["则输入嵌入矩阵",{"5":{"94":1}}],["则输入嵌入矩阵可以表示为",{"5":{"94":1}}],["则输出为",{"5":{"86":2}}],["则输出向量为",{"5":{"71":2}}],["则它们的联合信息量是各自信息量之和",{"5":{"93":2}}],["则它们的线性组合可以逼近任意连续函数",{"5":{"71":2}}],["则query张量的计算为",{"5":{"93":2}}],["则多头注意力的计算过程为",{"5":{"93":2}}],["则归一化后的注意力权重为",{"5":{"92":2}}],["则不显式编码绝对位置",{"5":{"91":2}}],["则各部分的参数数量分析如下",{"5":{"93":2}}],["则各维度对应的频率参数",{"5":{"91":1}}],["则各维度对应的频率参数为",{"5":{"91":1}}],["则各层的计算定义为",{"5":{"46":2}}],["则带有位置信息的输入表示为",{"5":{"91":2}}],["则平移序列",{"5":{"90":1}}],["则平移序列的dft为",{"5":{"90":1}}],["则傅里叶变换变为",{"5":{"90":2}}],["则实部和虚部正好对应编码的奇数维度和偶数维度",{"5":{"90":2}}],["则编码向量的期望范数为",{"5":{"89":2}}],["则编码矩阵可以表示为",{"5":{"89":2}}],["则新位置的编码可以通过已学习位置的线性组合来构造",{"5":{"89":2}}],["则存在信息冗余",{"5":{"93":2}}],["则存在",{"5":{"89":1}}],["则存在个正交基向量",{"5":{"89":1}}],["则存在某个隐空间使得注意力计算等价于该隐空间中的线性运算",{"5":{"86":2}}],["则协方差矩阵为",{"5":{"89":2}}],["则质心位于原点",{"5":{"89":2}}],["则可学习位置编码矩阵定义为",{"5":{"89":2}}],["则可以理解为在​空间中的表示",{"5":{"45":1}}],["则可以认为反向传播实现正确",{"5":{"44":2}}],["则卷积矩阵",{"5":{"87":1}}],["则卷积矩阵是一个带状矩阵",{"5":{"87":1}}],["则注意力分数可以重新写为",{"5":{"91":2}}],["则注意力输出为",{"5":{"92":2}}],["则注意力输出",{"5":{"87":1}}],["则注意力输出满足",{"5":{"87":1}}],["则注意力矩阵是核矩阵经过行归一化得到的",{"5":{"87":2}}],["则是低秩矩阵",{"5":{"87":1}}],["则是一个的列向量",{"5":{"50":1}}],["则原始注意力分数矩阵为",{"5":{"87":2}}],["则稀疏注意力矩阵为",{"5":{"87":2}}],["则稀疏注意力矩阵",{"5":{"86":1}}],["则稀疏注意力矩阵定义为",{"5":{"86":1}}],["则总编码容量约为",{"5":{"89":1}}],["则总编码容量约为比特",{"5":{"89":1}}],["则总复杂度为",{"5":{"86":2}}],["则总损失",{"5":{"41":2}}],["则邻域定义为",{"5":{"86":2}}],["则整个数据集的负对数似然",{"5":{"61":2}}],["则整个网络等价于一个单一的仿射变换",{"5":{"47":2}}],["则属于负类",{"5":{"61":2}}],["则最终的输入表示为",{"5":{"94":2}}],["则最小化交叉熵",{"5":{"61":2}}],["则最大似然估计等价于最小化mae",{"5":{"51":2}}],["则雅可比矩阵",{"5":{"61":2}}],["则这种编码浪费的信息越少",{"5":{"61":2}}],["则kl散度定义为",{"5":{"61":2}}],["则从整体上刻画了一个概率分布的信息特征",{"5":{"61":2}}],["则预测向量为",{"5":{"51":2}}],["则加权mse可以写为",{"5":{"51":2}}],["则加权均方误差定义为",{"5":{"51":2}}],["则需要",{"5":{"92":2}}],["则需要额外的编码开销",{"5":{"61":2}}],["则需要除以权重总和",{"5":{"51":2}}],["则需要进一步分析",{"5":{"48":2}}],["则上式直接给出了加权平均的平方误差",{"5":{"51":2}}],["则两者的mse可以表示为",{"5":{"51":2}}],["则参数量从",{"5":{"50":1}}],["则参数量从减少到",{"5":{"50":1}}],["则复制整个模型在多个数据批次上并行训练",{"5":{"50":2}}],["则一个批次的表示就是形状为",{"5":{"50":2}}],["则plu分解总是存在",{"5":{"50":2}}],["则lu分解存在",{"5":{"50":2}}],["则模型利用了更多的参数自由度",{"5":{"50":2}}],["则系统是不稳定的",{"5":{"50":2}}],["则系统是稳定的",{"5":{"50":2}}],["则称是等变的",{"5":{"88":1}}],["则称是的特征向量",{"5":{"50":1}}],["则称",{"5":{"50":1,"88":1}}],["则称为标准正交基",{"5":{"50":2}}],["则称它们是正交的",{"5":{"50":2}}],["则称该网络存在梯度爆炸问题",{"5":{"41":2}}],["则会在较小张量的左侧自动补",{"5":{"50":2}}],["则样本均值",{"5":{"96":1}}],["则样本均值依概率收敛于对任意成立",{"5":{"96":1}}],["则鼓励",{"5":{"96":1}}],["则给定",{"5":{"96":1}}],["则给定时的条件分布仍然是高斯分布",{"5":{"96":1}}],["则任意分量",{"5":{"96":1}}],["则任意分量服从一维高斯分布",{"5":{"96":1}}],["则拒绝原假设",{"5":{"96":2}}],["则记为",{"5":{"96":2}}],["则其香农熵定义为",{"5":{"61":2}}],["则其元素可以表示为",{"5":{"50":1}}],["则其元素可以表示为​",{"5":{"50":1}}],["则其和为",{"5":{"50":1}}],["则其和为向量加法满足交换律和结合律",{"5":{"50":1}}],["则其pdf为",{"5":{"96":2}}],["则其pdf为当",{"5":{"96":2}}],["则其hessian也与fisher信息相关",{"5":{"70":2}}],["则该头的注意力权重矩阵有",{"5":{"93":1}}],["则该头的注意力权重矩阵有个自由度",{"5":{"93":1}}],["则该维度可以广播",{"5":{"50":2}}],["则该点是鞍点",{"5":{"48":2}}],["则该点是局部极大值",{"5":{"48":2}}],["则该点是局部极小值",{"5":{"48":2}}],["则该层的输出为",{"5":{"47":2}}],["则该词的所有子词都被掩码",{"5":{"99":2,"101":2}}],["则hessian矩阵是对称的",{"5":{"48":2}}],["则沿的方向导数为",{"5":{"48":1}}],["则沿时间步",{"5":{"41":1}}],["则沿时间步反向传播后",{"5":{"41":1}}],["则在这个映射下计算输入的某种",{"5":{"87":2}}],["则在点处的梯度定义为",{"5":{"48":1}}],["则在该区域输入的微小变化不会引起输出的显著变化",{"5":{"41":2,"42":2}}],["则在该区域输入的饱和称为输出饱和",{"5":{"41":2}}],["则梯度主要流向被",{"5":{"89":2}}],["则梯度会均匀地流向所有位置",{"5":{"89":2}}],["则梯度会按指数级衰减",{"5":{"42":2}}],["则梯度的分布趋向于高斯分布",{"5":{"96":2}}],["则梯度",{"5":{"48":1}}],["则梯度是一个与形状相同的三阶张量",{"5":{"48":1}}],["则梯度很小",{"5":{"69":2}}],["则关于的偏导数为",{"5":{"48":1}}],["则关于变量在点处的偏导数定义为",{"5":{"48":1}}],["则第",{"5":{"47":1}}],["则第层的输出为",{"5":{"47":1}}],["则提供了灵活的阈值调节能力",{"5":{"47":1}}],["则将整个超平面平移",{"5":{"47":1}}],["则仿射变换可以统一表示为",{"5":{"47":2}}],["则当且仅当两个输入均为1时",{"5":{"47":2}}],["则全连接层可以表示为",{"5":{"46":2}}],["则按比例缩放",{"5":{"44":2}}],["则对任意整数平移量",{"5":{"88":2}}],["则对任意矩阵",{"5":{"96":2}}],["则对于任意的",{"5":{"95":2}}],["则对于任意",{"5":{"94":2}}],["则对于任意有限的点集",{"5":{"86":2}}],["则对于任意位置置换",{"5":{"99":2,"101":2}}],["则对的雅可比矩阵为",{"5":{"45":1}}],["则对的导数为",{"5":{"45":1}}],["则对的梯度为",{"5":{"44":1}}],["则对应的概率为",{"5":{"71":2}}],["则层次化特征学习可以表示为",{"5":{"71":2}}],["则深度神经网络在表示效率上可能显著优于浅层网络",{"5":{"71":2}}],["则网络的前向传播可以表示为两个连续的线性变换",{"5":{"71":2}}],["则只能为正或零",{"5":{"42":1}}],["则反向传播的梯度变换为",{"5":{"41":1}}],["则反向传播的梯度变换为​",{"5":{"41":1}}],["则和",{"5":{"41":1}}],["则的秩与相同",{"5":{"87":1}}],["则的梯度为​",{"5":{"45":1}}],["则的各元素方差为",{"5":{"41":1}}],["则的特征值接近1",{"5":{"41":1}}],["则随指数衰减",{"5":{"41":1}}],["则随指数增长",{"5":{"41":1}}],["则​​可以表示为前个基向量的线性组合",{"5":{"89":1}}],["则​",{"5":{"41":1,"69":1,"87":1,"93":1}}],["则",{"5":{"41":12,"42":3,"44":9,"45":8,"47":8,"48":6,"50":1,"51":2,"61":6,"69":1,"70":2,"87":13,"89":3,"90":6,"92":4,"93":5,"94":4,"96":10,"99":5,"101":5}}],["则位置编码可以表示为",{"5":{"91":2}}],["则位置感知的嵌入表示为",{"5":{"71":2}}],["则位置的输出为",{"5":{"69":1}}],["则位置",{"5":{"69":1}}],["则均方误差定义为",{"5":{"70":2}}],["则交叉熵损失",{"5":{"96":1}}],["则交叉熵损失关于的偏导数为",{"5":{"96":1}}],["则交叉熵损失相对于",{"5":{"96":1}}],["则交叉熵损失相对于的梯度为​",{"5":{"96":1}}],["则交叉熵定义为",{"5":{"70":2}}],["则误差仅为0",{"5":{"70":2}}],["则条件众数预测也是最优的",{"5":{"70":2}}],["则仅是位置的token集合的函数",{"5":{"99":1,"101":1}}],["则mse为",{"5":{"51":2}}],["则mlm的目标是预测被掩码的token",{"5":{"99":2,"101":2}}],["则mlm损失变为",{"5":{"99":2,"101":2}}],["则期望的掩码比例为",{"5":{"99":2,"101":2}}],["则wwm的掩码策略为",{"5":{"99":2,"101":2}}],["且局部极小值的数量随参数数量指数级增长",{"5":{"97":2}}],["且它们之间存在依赖",{"5":{"92":1}}],["且的每一行只依赖于对应的查询行和整个键矩阵",{"5":{"92":1}}],["且不依赖于词序",{"5":{"92":2}}],["且不易被内容信息所干扰",{"5":{"88":2}}],["且由于需要顺序计算",{"5":{"92":2}}],["且计算难以并行化",{"5":{"91":2}}],["且便于进行复变函数分析",{"5":{"90":2}}],["且在长上下文外推上表现出色",{"5":{"88":2}}],["且所有特征值的模不超过1",{"5":{"87":2}}],["且对应的特征向量为",{"5":{"87":2}}],["且加速效果随序列长度增加而增大",{"5":{"86":2}}],["且通常归一化使得",{"5":{"51":1}}],["且通常归一化使得或",{"5":{"51":1}}],["且这些局部最小值之间由平缓的区域连接",{"5":{"51":2}}],["且特征向量矩阵是正交的",{"5":{"50":2}}],["且当且仅当",{"5":{"96":1}}],["且当且仅当时kl散度为0",{"5":{"96":1}}],["且同样唯一确定概率分布",{"5":{"96":2}}],["且最优解集是凸集",{"5":{"48":2}}],["且满足分解式",{"5":{"45":2}}],["且",{"5":{"42":1,"45":2,"61":3,"70":2,"71":2,"87":1,"92":7,"96":1}}],["且具有唯一的全局最小值",{"5":{"70":2}}],["且优化的收敛行为可以精确预测",{"5":{"70":2}}],["且任何局部最小值都是全局最小值",{"5":{"70":2}}],["且可以任意大",{"5":{"70":2}}],["且损失值被有效位置的数量归一化",{"5":{"99":2,"101":2}}],["或收敛过慢",{"5":{"97":2}}],["或学习率衰减",{"5":{"97":2}}],["或称he初始化",{"5":{"94":2}}],["或称单位高斯分布",{"5":{"96":2}}],["或glorot初始化",{"5":{"94":2}}],["或对称位置编码",{"5":{"92":2}}],["或对于分类任务",{"5":{"44":1}}],["或接近序列结尾",{"5":{"92":2}}],["或距离呈某种函数关系",{"5":{"92":2}}],["或4096",{"5":{"91":1}}],["或使用可学习的因果位置编码来实现",{"5":{"91":2}}],["或​维的空间",{"5":{"91":1}}],["或狄拉克",{"5":{"90":1}}],["或狄拉克函数",{"5":{"90":1}}],["或在最后一维单独处理",{"5":{"88":2}}],["或其他能够生成凸预测空间的模型",{"5":{"51":2}}],["或其中一个维度为",{"5":{"50":2}}],["或其变体",{"5":{"69":2,"70":2}}],["或超曲面",{"5":{"51":2}}],["或更低精度",{"5":{"50":2}}],["或更宽",{"5":{"41":1}}],["或和",{"5":{"96":2}}],["或平均值",{"5":{"96":2}}],["或证据",{"5":{"96":2}}],["或奈特",{"5":{"96":2}}],["或均匀分布",{"5":{"96":2}}],["或均方根",{"5":{"41":2}}],["或均方根为1",{"5":{"41":2}}],["或经过dropout操作后某个神经元是否被激活等",{"5":{"96":2}}],["或等值线",{"5":{"48":2}}],["或等价地表示为",{"5":{"61":2}}],["或等价地",{"5":{"45":2}}],["或回归",{"5":{"47":2}}],["或行向量",{"5":{"50":2}}],["或行",{"5":{"45":2}}],["或层次化结构",{"5":{"71":2}}],["或过小",{"5":{"41":2}}],["或之后",{"5":{"41":2}}],["或",{"5":{"41":3,"44":3,"45":2,"50":2,"51":1,"61":4,"69":1,"70":8,"71":4,"86":1,"87":2,"88":2,"89":2,"91":1,"92":4,"93":4,"95":2,"96":3,"97":2}}],["或同一图像的不同增强视图",{"5":{"69":2}}],["或者端到端联合训练两者",{"5":{"89":2}}],["或者更能够代表目标任务的需求",{"5":{"51":2}}],["或者等效地",{"5":{"69":2}}],["或者修改为",{"5":{"69":2}}],["或者在批量形式下",{"5":{"70":2}}],["或完全",{"5":{"70":2}}],["或排名损失",{"5":{"70":2}}],["或直接偏好优化损失",{"5":{"99":2,"101":2}}],["熵最小",{"5":{"92":2}}],["熵最大",{"5":{"92":2}}],["熵度量了注意力分布的",{"5":{"92":2}}],["熵和kl散度的定义都依赖于对数运算",{"5":{"61":2}}],["熵和互信息的数学框架",{"5":{"71":2}}],["熵在",{"5":{"61":1}}],["熵在均匀分布时取得最大值",{"5":{"61":1}}],["熵具有",{"5":{"61":1}}],["熵具有非负性",{"5":{"61":1}}],["熵",{"5":{"61":2,"70":2}}],["熵函数在这个流形上的等高线描述了不同概率分布的",{"5":{"61":2}}],["熵可以被视为概率单纯形",{"5":{"61":2}}],["熵的若干重要性质值得深入探讨",{"5":{"61":2}}],["熵实际上是自信息的数学期望",{"5":{"61":2}}],["熵值越小",{"5":{"61":2}}],["熵值越大",{"5":{"61":2}}],["熵代表了分布的",{"5":{"61":2}}],["熵与散度的数学定义",{"2":{"61":1},"5":{"61":1}}],["熵为0",{"5":{"96":2}}],["熵越小",{"5":{"96":2}}],["熵越大",{"5":{"96":2}}],["熵衡量了随机变量不确定性的大小",{"5":{"96":2}}],["熵定义为",{"5":{"96":2}}],["而动态损失缩放根据梯度范数自适应调整",{"5":{"97":2}}],["而后期的大学习率",{"5":{"97":2}}],["而键向量",{"5":{"95":1}}],["而键向量在同一维度上的分量也较大",{"5":{"95":1}}],["而缩放操作可以保持点积的方差稳定",{"5":{"95":2}}],["而缩放确保了这种对齐程度的度量不受维度数量的影响",{"5":{"95":2}}],["而矩阵乘法的梯度为",{"5":{"94":2}}],["而value则承载了实际的信息内容",{"5":{"94":2}}],["而注意力机制的投影是有监督的",{"5":{"94":2}}],["而注意力矩阵是数据依赖的动态矩阵",{"5":{"93":2}}],["而注意力的感受野是全局的",{"5":{"93":2}}],["而注意力的",{"5":{"92":2}}],["而注意力的路径长度恒为1",{"5":{"92":2}}],["而gpt",{"5":{"93":2}}],["而标准的多头注意力在所有头上都进行完整的计算",{"5":{"93":2}}],["而在注意力机制中",{"5":{"94":2}}],["而在多查询注意力中",{"5":{"93":2}}],["而在标准的多头注意力中",{"5":{"93":2}}],["而在极端值",{"5":{"42":2}}],["而这取决于训练数据",{"5":{"92":2}}],["而这个旋转完全由相对位置",{"5":{"88":2}}],["而这个最小化过程正是通过梯度下降等优化算法实现的",{"5":{"51":2}}],["而这个映射的几何性质决定了神经网络的学习动态和表达能力",{"5":{"71":2}}],["而rnn的建造成本与依赖跨度成正比",{"5":{"92":2}}],["而rope正是这一探索的集大成之作",{"5":{"88":2}}],["而可学习编码可能在外推时表现不佳",{"5":{"89":2}}],["而可学习编码的函数空间由训练数据决定",{"5":{"89":2}}],["而中间相隔数十个词",{"5":{"88":2}}],["而ntk",{"5":{"88":2}}],["而绝对位置编码提供额外的绝对位置信息",{"5":{"88":2}}],["而相位是循环的",{"5":{"88":2}}],["而相对位置的范围在推理时与训练时是相同的",{"5":{"88":2}}],["而最高频的维度",{"5":{"88":2}}],["而内积运算本身就会",{"5":{"88":2}}],["而复数的模长",{"5":{"88":2}}],["而与它们在序列中的绝对位置无关",{"5":{"88":2}}],["而稀疏矩阵运算可能无法充分利用硬件并行能力",{"5":{"86":2}}],["而分类任务使用softmax将logits转换为类别概率分布",{"5":{"61":2}}],["而香农熵",{"5":{"61":1}}],["而对于概率极小的事件",{"5":{"61":2}}],["而其他样本的总误差约为",{"5":{"51":2}}],["而且在实际计算中也更加高效",{"5":{"51":2}}],["而且并非所有方阵都有逆矩阵",{"5":{"50":2}}],["而数据并行",{"5":{"50":2}}],["而奇异值分解适用于任意矩阵",{"5":{"50":2}}],["而零空间则包含了可能被",{"5":{"50":2}}],["而正规方程的求解等价于计算矩阵的伪逆",{"5":{"50":2}}],["而反向",{"5":{"96":1}}],["而反向则鼓励找到一个主要的模式并紧密地拟合它",{"5":{"96":1}}],["而强大数定律保证样本均值几乎必然收敛于期望",{"5":{"96":2}}],["而真实分布是",{"5":{"96":2}}],["而连续空间中的嵌入表示和隐藏状态则是连续的",{"5":{"96":2}}],["而​",{"5":{"48":1}}],["而是首先构建一个未归一化的实数向量",{"5":{"96":2}}],["而是数据驱动的",{"5":{"94":2}}],["而是数学必然",{"5":{"88":2}}],["而是可以通过投影变换到隐空间中形成有意义的匹配",{"5":{"94":2}}],["而是通过可学习的线性变换从输入数据中投射到不同的",{"5":{"94":2}}],["而是通过架构设计",{"5":{"88":2}}],["而是遍历整个球面",{"5":{"91":2}}],["而是固定的常数",{"5":{"91":1}}],["而是使用不同频率的正弦波叠加来表示位置",{"5":{"90":2}}],["而是直接修改注意力计算",{"5":{"91":2}}],["而是直接指定了频率分布",{"5":{"90":2}}],["而是直接计算低秩近似下的输出",{"5":{"87":2}}],["而是不同频率正弦波的叠加",{"5":{"90":2}}],["而是经过精心设计的频率成分",{"5":{"90":2}}],["而是经过深思熟虑的设计",{"5":{"88":2}}],["而是经过深思熟虑的数学设计",{"5":{"71":2}}],["而是具有某种对数或平方根性质",{"5":{"88":2}}],["而是具有深刻的物理直觉",{"5":{"88":2}}],["而是让每个位置对应一个",{"5":{"88":1}}],["而是让每个位置对应一个旋转操作",{"5":{"88":1}}],["而是让复杂性自然涌现",{"5":{"88":2}}],["而是缩放因子",{"5":{"51":1}}],["而是取连续的实数值",{"5":{"50":2}}],["而是一种非线性变换",{"5":{"69":2}}],["而是将所有位置作为候选",{"5":{"69":2}}],["而是由fisher信息加权的距离",{"5":{"71":2}}],["而是由",{"5":{"70":2}}],["而是从机器学习的基本原则",{"5":{"70":2}}],["而是在奖励和策略多样性之间进行权衡",{"5":{"99":2,"101":2}}],["而是作用于整个生成策略",{"5":{"99":2,"101":2}}],["而各种信息量",{"5":{"70":2,"71":2}}],["而深度网络仅需多项式规模",{"5":{"71":2}}],["而非总是选择最确定的答案",{"5":{"96":2}}],["而非显式地依赖于输入",{"5":{"93":2}}],["而非它们的绝对位置",{"5":{"92":2}}],["而非它们在序列中的具体位置",{"5":{"91":2}}],["而非绝对位置",{"5":{"92":2}}],["而非绝对位置的函数",{"5":{"90":2}}],["而非卷积那种",{"5":{"92":2}}],["而非可学习的参数",{"5":{"91":2}}],["而非训练位置的",{"5":{"90":2}}],["而非完全自由的",{"5":{"90":2}}],["而非完整",{"5":{"89":1}}],["而非过拟合训练位置的特殊模式",{"5":{"89":2}}],["而非仅由它们的差",{"5":{"88":1}}],["而非仅由它们的差决定",{"5":{"88":1}}],["而非手工设计",{"5":{"88":2}}],["而非经验性的",{"5":{"88":2}}],["而非经验性的观察",{"5":{"88":2}}],["而非这种混合方案",{"5":{"88":2}}],["而非幅度变化中",{"5":{"88":2}}],["而非所有位置",{"5":{"86":2}}],["而非",{"5":{"51":2,"99":1,"101":1}}],["而非预先指定的",{"5":{"45":2}}],["而非批维度",{"5":{"41":2}}],["而非单一数值",{"5":{"70":2}}],["而非对角线元素",{"5":{"95":1}}],["而非对角线元素也接近0",{"5":{"95":1}}],["而非对角元素",{"5":{"41":1}}],["而非对角元素也非常小",{"5":{"41":1}}],["而非对特定位置的精确记忆",{"5":{"90":2}}],["而非对",{"5":{"70":2}}],["而非具体序列顺序的函数",{"5":{"99":2,"101":2}}],["而当注意力分布趋于one",{"5":{"41":2}}],["而梯度饱和是激活函数导数的行为",{"5":{"41":2}}],["而梯度饱和则关注导数趋近于零导致的梯度消失问题",{"5":{"41":2}}],["而",{"5":{"48":1,"50":2,"51":1,"61":1,"70":4,"91":1,"97":2}}],["而不重要的特征或噪声会对应于较小的奇异值",{"5":{"94":2}}],["而不增加计算成本",{"5":{"93":2}}],["而不需要重新训练或特殊的插值策略",{"5":{"88":2}}],["而不需要重新训练或复杂的插值策略",{"5":{"88":2}}],["而不需要存储",{"5":{"61":2}}],["而不需要显式计算",{"5":{"61":2}}],["而不需要显式的奖励模型",{"5":{"99":2,"101":2}}],["而不显式地计算和存储完整的hessian矩阵",{"5":{"48":2}}],["而不是每层独立学习",{"5":{"89":2}}],["而不是",{"5":{"61":2}}],["而不是高斯消元的",{"5":{"50":1}}],["而不是最小化全局kl散度",{"5":{"96":2}}],["而不是点估计",{"5":{"96":2}}],["而不是独立地掩码单个token",{"5":{"99":2,"101":2}}],["而不是依赖词内的子词相关性",{"5":{"99":2,"101":2}}],["而不仅仅是点估计",{"5":{"70":2}}],["而多个负样本散布在周围",{"5":{"69":1}}],["而多个负样本",{"5":{"69":1}}],["而预测",{"5":{"70":2}}],["而交叉熵认为比严重得多",{"5":{"70":1}}],["而交叉熵认为",{"5":{"70":1}}],["而目标部分使用因果注意力",{"5":{"99":2,"101":2}}],["而均方误差则无法享受这一特性",{"5":{"70":2}}],["而均方误差的裁剪是基于梯度的范数约束",{"5":{"99":2,"101":2}}],["单头注意力可以被视为一个特定的核函数",{"5":{"93":2}}],["单一的注意力头难以同时捕获所有这些不同层面的关联信息",{"5":{"93":2}}],["单一的注意力头在表达能力上存在固有的局限性",{"5":{"93":2}}],["单纯增加上下文长度会导致性能下降",{"5":{"88":2}}],["单位矩阵",{"5":{"88":2,"89":2}}],["单位元",{"5":{"88":2}}],["单位为哈特利",{"5":{"61":2}}],["单位为纳特",{"5":{"61":2}}],["单位为比特",{"5":{"61":2,"96":2}}],["单调性表明如果",{"5":{"61":1}}],["单调性",{"5":{"47":2,"61":1,"69":2}}],["单层的限制",{"5":{"87":2}}],["单层注意力能够表示这个函数吗",{"5":{"92":2}}],["单层注意力能够表示的函数类受到低秩结构的限制",{"5":{"87":2}}],["单层注意力能够建模任意复杂的长程依赖关系吗",{"5":{"92":2}}],["单层注意力即可完成",{"5":{"92":2}}],["单层注意力就能提供全局的感受野",{"5":{"92":2}}],["单层注意力可能无法精确表示",{"5":{"87":2}}],["单层网络的矩阵表示",{"2":{"46":1},"5":{"46":1}}],["单层网络的计算图",{"5":{"45":2}}],["单层transformer的flops约为",{"5":{"45":2}}],["单层前向传播的flops",{"5":{"45":2}}],["单层flops",{"5":{"44":1}}],["单层神经网络可以逼近任意连续函数到任意精度",{"5":{"71":2}}],["单次前向传播的flops约为",{"5":{"45":1}}],["单次前向传播的flops约为量级",{"5":{"45":1}}],["单次前向传播的flops为",{"5":{"45":2}}],["单次反向传播的浮点运算数为",{"5":{"44":2}}],["单次参数更新就可能将权重推向无穷大或负无穷",{"5":{"41":2}}],["单隐藏层神经网络就可以以任意精度逼近任意连续函数",{"5":{"71":2}}],["单个事件的自信息仅描述了单一结果的信息量",{"5":{"61":2}}],["单个麦肯罗皮层神经元可以实现所有的基本逻辑运算",{"5":{"47":2}}],["单个神经元实现了一个超平面分类器",{"5":{"47":2}}],["单个神经元无法解决异或问题",{"5":{"47":2}}],["单个神经元无法找到正确的分类边界",{"5":{"47":2}}],["单个神经元",{"5":{"47":2}}],["单个神经元执行的是一个超平面分类",{"5":{"47":2}}],["单个神经元可以模拟任意单变量布尔函数",{"5":{"47":2}}],["单个神经元只形成了一个超平面决策边界",{"5":{"46":2}}],["单个神经元的能力是有限的",{"5":{"46":2}}],["单个感知机只能解决线性可分的问题",{"5":{"46":2}}],["单个噪声样本不太可能同时激活多个稀疏神经元",{"5":{"71":2}}],["单个样本的注意力计算涉及形状为",{"5":{"50":1}}],["单个样本的注意力计算涉及形状为的查询",{"5":{"50":1}}],["单个样本",{"5":{"41":1}}],["多核学习",{"5":{"93":2}}],["多头机制在保持参数总量不变的情况下",{"5":{"93":2}}],["多头机制与子空间学习",{"2":{"93":1},"5":{"93":1}}],["多头注意力输出中的信息流动可以通过信息论的工具来分析",{"5":{"93":2}}],["多头注意力通过学习不同的投影矩阵",{"5":{"93":2}}],["多头注意力通过设置多个独立的注意力头来解决这个问题",{"5":{"93":2}}],["多头注意力与卷积运算之间存在着深刻的数学联系",{"5":{"93":2}}],["多头注意力与卷积运算的关系",{"2":{"93":1},"5":{"93":1}}],["多头注意力与moe也有所不同",{"5":{"93":2}}],["多头注意力与mixture",{"5":{"93":2}}],["多头注意力可以被视为对动态注意力矩阵的一种",{"5":{"93":2}}],["多头注意力可以被视为一种",{"5":{"93":2}}],["多头注意力可以被解释为多组并行的",{"5":{"69":1}}],["多头注意力可以被解释为",{"5":{"69":1}}],["多头注意力可以自然地被分解为块级别的计算",{"5":{"93":2}}],["多头注意力可以优雅地表示为一个紧凑的运算序列",{"5":{"93":2}}],["多头注意力引入了额外的参数",{"5":{"93":2}}],["多头注意力则可以被视为多个核函数的组合",{"5":{"93":2}}],["多头注意力中的",{"5":{"93":2}}],["多头注意力中的拼接和输出投影操作",{"5":{"93":2}}],["多头注意力中的多个头可以学习互补的低秩结构",{"5":{"87":2}}],["多头注意力中的不同头可以专门化于处理不同类型的任务",{"5":{"99":2,"101":2}}],["多头注意力的",{"5":{"93":2}}],["多头注意力的基函数是从数据中学习的",{"5":{"93":2}}],["多头注意力的设计天然适合并行计算",{"5":{"93":2}}],["多头注意力的设计是transformer架构的核心创新之一",{"5":{"93":2}}],["多头注意力的空间复杂度为",{"5":{"93":2}}],["多头注意力的空间复杂度分析与时间复杂度类似",{"5":{"93":2}}],["多头注意力的总时间复杂度为",{"5":{"93":2}}],["多头注意力的时间复杂度分析需要分别考虑各个计算阶段",{"5":{"93":2}}],["多头注意力的输出可以被视为在多个子空间中进行注意力计算的融合结果",{"5":{"93":2}}],["多头注意力的投影计算可以通过矩阵运算高效地并行完成",{"5":{"93":2}}],["多头注意力的函数逼近视角",{"2":{"93":1},"5":{"93":1}}],["多头注意力的复杂度",{"2":{"93":1},"5":{"93":1}}],["多头注意力的张量计算形式",{"2":{"93":1},"5":{"93":1}}],["多头注意力的数学定义如公式",{"5":{"93":2}}],["多头注意力的数学定义",{"2":{"93":1},"5":{"93":1}}],["多头注意力的数学结构",{"2":{"69":1},"5":{"69":1}}],["多头注意力的表达能力提升可以从子空间学习的角度来理解",{"5":{"93":2}}],["多头注意力的表达能力",{"5":{"86":2}}],["多头注意力的矩阵推导与表达能力分析<",{"5":{"85":1}}],["多头注意力的矩阵推导与表达能力分析",{"0":{"93":1},"4":{"93":1},"5":{"85":4,"93":1}}],["多头注意力的梯度复合",{"5":{"41":2}}],["多头注意力",{"5":{"69":2,"93":2}}],["多尺度频率结构可以看作是一种高效的编码方案",{"5":{"90":2}}],["多次加载到sram",{"5":{"86":2}}],["多查询注意力等",{"5":{"93":2}}],["多查询注意力与分组查询注意力",{"2":{"93":1},"5":{"93":1}}],["多查询注意力可以将参数量减少约",{"5":{"86":2}}],["多查询注意力的参数数量从",{"5":{"93":1}}],["多查询注意力的参数数量从减少到​",{"5":{"93":1}}],["多查询注意力的query投影参数量为",{"5":{"86":2}}],["多查询注意力的数学定义",{"2":{"86":1},"5":{"86":1}}],["多查询注意力",{"5":{"86":4,"93":2}}],["多少",{"5":{"61":2}}],["多重比较谬误",{"5":{"96":2}}],["多重比较问题是在进行多个假设检验时需要考虑的重要问题",{"5":{"96":2}}],["多项式核可以捕获特征之间的高阶交互",{"5":{"86":2}}],["多项式特征映射",{"5":{"86":2}}],["多项式分布的似然函数为",{"5":{"71":2}}],["多项分布的共轭先验是dirichlet分布",{"5":{"96":2}}],["多项logit模型",{"5":{"69":2}}],["多元高斯分布具有许多重要的性质",{"5":{"96":2}}],["多元高斯分布的pdf为",{"5":{"96":2}}],["多元高斯分布是一维高斯分布向多维空间的推广",{"5":{"96":2}}],["多元函数梯度与hessian",{"2":{"48":1},"5":{"48":1}}],["多数特征值很小",{"5":{"48":2}}],["多节点",{"5":{"45":1}}],["多gpu",{"5":{"45":1}}],["多层叠加和梯度存储会使内存压力进一步增大",{"5":{"95":2}}],["多层堆叠中的表示变换",{"2":{"94":1},"5":{"94":1}}],["多层堆叠使得网络能够表示极其复杂的函数",{"5":{"71":2}}],["多层感知机",{"5":{"47":2}}],["多层线性网络的恒等性",{"5":{"47":2}}],["多层神经网络叠加",{"5":{"97":2}}],["多层神经网络可以学习复杂的非线性决策边界",{"5":{"47":2}}],["多层神经网络",{"5":{"46":2}}],["多层网络的复合函数表示",{"5":{"46":2}}],["多层网络的表达能力超越了单层网络",{"5":{"46":2}}],["多层网络的矩阵形式",{"2":{"46":1},"5":{"46":1}}],["多层网络的flops",{"5":{"45":2}}],["多个头可以通过各自的非线性变换组合出更丰富的交互模式",{"5":{"93":2}}],["多个头的梯度会复合",{"5":{"41":2}}],["多个层面理解信息",{"5":{"93":2}}],["多个层的组合则构成了这些映射的复合",{"5":{"45":2}}],["多个注意力层被堆叠在一起",{"5":{"92":2}}],["多个样本的信息被平均",{"5":{"47":2}}],["多个神经元的组合形成了一个丰富的特征基",{"5":{"47":2}}],["多个这样的超平面组合",{"5":{"71":2}}],["多模态学习",{"5":{"41":2}}],["多分类交叉熵损失为",{"5":{"61":2}}],["多分类交叉熵损失的完整推导",{"2":{"61":1},"5":{"61":1}}],["多分类情况的证明类似",{"5":{"44":2}}],["多分类任务要求模型为每个样本预测其在所有",{"5":{"61":1}}],["多分类任务要求模型为每个样本预测其在所有个类别上的概率分布",{"5":{"61":1}}],["多分类任务与softmax函数",{"2":{"61":1},"5":{"61":1}}],["多分类任务",{"5":{"44":2}}],["多组并行的",{"5":{"69":1}}],["多任务学习中的损失函数组合",{"2":{"70":1,"99":1,"101":1},"5":{"70":1,"99":1,"101":1}}],["多任务学习",{"5":{"99":2,"101":2}}],["多任务学习的数学框架",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["多任务学习的总体损失为",{"5":{"99":2,"101":2}}],["多任务损失",{"5":{"99":2,"101":2}}],["表面上看",{"5":{"88":2}}],["表明注意力机制的优化与分类任务的优化具有相似的数学性质",{"5":{"97":2}}],["表明它们在",{"5":{"92":4}}],["表明它是输入",{"5":{"71":1}}],["表明高层表示更加紧凑",{"5":{"87":2}}],["表明高层表示更加",{"5":{"87":2}}],["表明如果",{"5":{"61":1}}],["表明",{"5":{"61":3,"87":2}}],["表明其发生带来了显著的信息增益",{"5":{"61":2}}],["表达能力来源和计算特性",{"5":{"93":2}}],["表达能力与计算能力界限",{"2":{"92":1},"5":{"92":1}}],["表达能力越弱",{"5":{"89":2}}],["表达能力越强",{"5":{"89":2}}],["表达能力受限",{"5":{"89":2}}],["表达能力最强",{"5":{"89":2}}],["表达能力分析",{"2":{"89":1},"5":{"89":1}}],["表达能力",{"5":{"87":2}}],["表达能力的理论界限",{"2":{"87":1},"5":{"87":1}}],["表达能力更强的激活函数可能提供更紧的下界",{"5":{"71":2}}],["表达能力更强",{"5":{"71":2}}],["表达式",{"5":{"47":1}}],["表达式是线性变换与平移的组合",{"5":{"47":1}}],["表2",{"5":{"45":2}}],["表4",{"5":{"44":6,"45":2}}],["表示查询位置",{"5":{"95":1}}],["表示查询位置对键位置的注意力权重",{"5":{"95":1}}],["表示值矩阵",{"5":{"95":2}}],["表示键矩阵",{"5":{"95":2}}],["表示各片段携带的实际信息内容",{"5":{"94":2}}],["表示各个信息片段的标识符",{"5":{"94":2}}],["表示信息需求的抽象表示",{"5":{"94":2}}],["表示熵",{"5":{"93":1}}],["表示分块对角矩阵",{"5":{"93":1}}],["表示互信息",{"5":{"93":2}}],["表示输入在特定子空间中的关联模式",{"5":{"93":2}}],["表示每个头被选中的概率或权重",{"5":{"93":2}}],["表示序列中各位置之间的信息流动关系",{"5":{"92":2}}],["表示序列中的位置索引",{"5":{"91":2}}],["表示维度索引",{"5":{"91":2}}],["表示从",{"5":{"87":1}}],["表示从到经过步间接传播的比例",{"5":{"87":1}}],["表示模型对第",{"5":{"61":1}}],["表示使用基于分布",{"5":{"61":2}}],["表示在第",{"5":{"97":1}}],["表示在第步使用的学习率",{"5":{"97":1}}],["表示在计算第",{"5":{"61":2}}],["表示在sigmoid曲线上各点的斜率",{"5":{"42":2}}],["表示该事件的发生几乎不提供新的信息",{"5":{"61":2}}],["表示第个键向量",{"5":{"95":1}}],["表示第个查询向量",{"5":{"95":1}}],["表示第",{"5":{"51":1,"90":1,"92":1,"95":2}}],["表示对给定",{"5":{"51":1}}],["表示对给定的的期望",{"5":{"51":1}}],["表示对训练数据集的期望",{"5":{"51":2}}],["表示期望运算",{"5":{"51":2}}],["表示矩阵转置",{"5":{"51":2}}],["表示完全正相关",{"5":{"96":1}}],["表示完全负相关",{"5":{"96":2}}],["表示线性无关",{"5":{"96":2}}],["表示生成该词的可能性",{"5":{"96":2}}],["表示取特定值的概率",{"5":{"96":1}}],["表示",{"5":{"45":1,"50":2,"51":2,"96":1}}],["表示​范数",{"5":{"51":1}}],["表示​",{"5":{"45":1}}],["表示数据",{"5":{"45":2}}],["表示数学运算",{"5":{"45":2}}],["表示空间的层次结构",{"5":{"45":2}}],["表示逐元素乘法",{"5":{"44":1,"48":2,"51":2,"97":1}}],["表示样本属于第",{"5":{"71":1}}],["表示的形式",{"5":{"42":1}}],["表示sigmoid曲线在各点的斜率",{"5":{"42":2}}],["表示位置",{"5":{"92":2,"99":2,"101":2}}],["表示位置是填充token",{"5":{"99":1,"101":1}}],["表示两个在语义上相关的样本",{"5":{"69":2}}],["表示除位置",{"5":{"99":1,"101":1}}],["从预训练阶段的语言建模损失和掩码语言建模损失",{"5":{"101":2}}],["从连续时间动力学的角度分析梯度下降有助于理解其收敛行为",{"5":{"97":2}}],["从优化的视角审视",{"5":{"96":2}}],["从优化的角度来看",{"5":{"94":2}}],["从优化的角度",{"5":{"91":2,"93":2}}],["从优化的角度看",{"5":{"96":2}}],["从logits到概率分布的转换通过",{"5":{"96":1}}],["从logits到概率分布的转换通过softmax函数实现",{"5":{"96":1}}],["从logits到概率的完整流程",{"2":{"61":1},"5":{"61":1}}],["从雅可比矩阵的结构可以看出",{"5":{"95":2}}],["从原始的嵌入空间",{"5":{"94":1}}],["从原始输入空间",{"5":{"47":1}}],["从文章中提取相关信息",{"5":{"94":2}}],["从表示学习的角度来看",{"5":{"94":2}}],["从表达能力的角度分析",{"5":{"93":2}}],["从表达能力角度",{"5":{"91":2}}],["从理论分析的角度",{"5":{"93":2}}],["从理论上看",{"5":{"96":2}}],["从理论上分析",{"5":{"99":2,"101":2}}],["从输入空间到查询空间的映射可以写为",{"5":{"93":2}}],["从输入embedding通过线性变换得到",{"5":{"88":2}}],["从重参数化",{"5":{"93":2}}],["从单头到多头的动机",{"2":{"93":1},"5":{"93":1}}],["从整个序列聚合的信息",{"5":{"92":2}}],["从到",{"5":{"92":1}}],["从到的仿射变换定义为",{"5":{"47":1}}],["从到的",{"5":{"70":1}}],["从图论的角度来看",{"5":{"92":2}}],["从归纳偏置角度",{"5":{"91":2}}],["从泛化能力角度",{"5":{"91":2}}],["从泛化角度",{"5":{"89":2}}],["从位置",{"5":{"92":3}}],["从位置到位置的信息传递路径是中的一条有向路径",{"5":{"92":1}}],["从位置到位置",{"5":{"92":1}}],["从位置差0时的",{"5":{"91":2}}],["从位置域看",{"5":{"90":2}}],["从这个示例可以看出",{"5":{"91":2}}],["从这个点向预测直线作垂线",{"5":{"51":2}}],["从另一个角度",{"5":{"91":2}}],["从0到",{"5":{"91":2}}],["从0开始或从1开始",{"5":{"91":2}}],["从维度角度",{"5":{"91":2}}],["从覆盖范围的角度",{"5":{"90":2}}],["从群论的角度",{"5":{"90":2}}],["从相位角度理解位置编码",{"5":{"90":2}}],["从傅里叶分析的角度",{"5":{"90":2}}],["从傅里叶分析的角度来看",{"5":{"90":2}}],["从分辨率的角度",{"5":{"90":4}}],["从对数频率的角度来看",{"5":{"90":2}}],["从正态分布",{"5":{"89":1}}],["从正态分布中采样",{"5":{"89":1}}],["从实验结果来看",{"5":{"89":2}}],["从计算复杂度的角度看",{"5":{"93":2}}],["从计算复杂度的角度",{"5":{"92":2}}],["从计算角度",{"5":{"89":2}}],["从计算到函数逼近",{"2":{"45":1},"5":{"45":1}}],["从函数空间角度",{"5":{"89":2}}],["从函数逼近理论的角度分析",{"5":{"93":2}}],["从函数逼近的角度来看",{"5":{"92":2}}],["从函数逼近的角度",{"5":{"87":2,"89":2}}],["从函数逼近论的角度",{"5":{"71":2}}],["从参数效率角度",{"5":{"91":2}}],["从参数效率的角度",{"5":{"89":4,"90":2}}],["从参数化角度",{"5":{"89":2,"91":2}}],["从二阶条件",{"5":{"89":2}}],["从秩的角度",{"5":{"89":2}}],["从微分几何的角度",{"5":{"89":2}}],["从信号处理的角度重新思考位置编码",{"5":{"88":1}}],["从信息流的角度",{"5":{"92":2}}],["从信息聚合的角度来看",{"5":{"92":2}}],["从信息融合角度",{"5":{"91":2}}],["从信息瓶颈",{"5":{"87":2}}],["从信息传播的角度",{"5":{"87":2}}],["从信息编码的角度",{"5":{"61":2}}],["从信息论视角",{"5":{"61":2}}],["从信息论的角度来看",{"5":{"92":2,"95":2}}],["从信息论的角度看",{"5":{"88":6}}],["从信息论的角度",{"5":{"69":2,"71":4,"89":4,"90":2}}],["从信息论角度",{"5":{"41":2,"61":2,"71":8,"99":2,"101":2}}],["从信息几何的角度",{"5":{"70":2}}],["从4k到32k",{"5":{"88":2}}],["从历史角度看",{"5":{"88":2}}],["从工程角度看",{"5":{"88":2}}],["从更高的视角来看",{"5":{"88":2}}],["从频率域看",{"5":{"90":2}}],["从频率域的角度",{"5":{"90":6}}],["从频率分析的角度",{"5":{"90":2}}],["从频谱的角度",{"5":{"90":2}}],["从频谱分析的角度看",{"5":{"88":2}}],["从频域分析的角度看",{"5":{"88":2}}],["从抽象代数的角度看",{"5":{"88":2}}],["从2017年vaswani等人提出的原始正弦位置编码",{"5":{"88":2}}],["从幅度到相位的范式转换",{"2":{"88":1},"5":{"88":1}}],["从谱分析的角度",{"5":{"87":2}}],["从谱性质的角度",{"5":{"87":2}}],["从核方法",{"5":{"87":2}}],["从线性代数的视角来看",{"5":{"95":2}}],["从线性代数的角度",{"5":{"90":2,"91":4,"93":2}}],["从线性代数的角度来看",{"5":{"87":2,"94":2}}],["从线性映射的局限性出发",{"5":{"71":2}}],["从不同角度对标准注意力进行了改进或扩展",{"5":{"86":2}}],["从模型输出到最终损失值的完整数学流程可以表示为以下变换链",{"5":{"61":2}}],["从偏微分的角度分析",{"5":{"61":2}}],["从经验风险最小化的角度",{"5":{"61":2}}],["从融合计算的角度",{"5":{"61":2}}],["从根本上解决了数值溢出问题",{"5":{"61":2,"97":2}}],["从期望的形式来看",{"5":{"61":2}}],["从kl散度视角的再解释",{"2":{"61":1},"5":{"61":1}}],["从kl散度的角度看",{"5":{"96":2}}],["从它的数学定义",{"5":{"51":2}}],["从拉普拉斯分布假设出发",{"5":{"51":2}}],["从统计学的角度",{"5":{"92":2}}],["从统计学习理论的角度",{"5":{"87":2}}],["从统计学中的最小二乘法到深度学习中的回归任务",{"5":{"51":2}}],["从统计学角度来看",{"5":{"51":2}}],["从统计学角度",{"5":{"69":2}}],["从张量代数的角度来看",{"5":{"93":2}}],["从张量的角度来看",{"5":{"50":2}}],["从张量网络的角度来看",{"5":{"50":2}}],["从矩阵计算的角度来看",{"5":{"95":2}}],["从矩阵分析的角度",{"5":{"61":2}}],["从矩阵运算的角度来看",{"5":{"51":2}}],["从矩阵乘法的并行计算到张量运算的维度变换",{"5":{"50":2}}],["从transformer架构的注意力机制到词嵌入的向量表示",{"5":{"50":2}}],["从中心极限定理的角度看",{"5":{"96":2}}],["从公式可以看出",{"5":{"96":2}}],["从应用上看",{"5":{"96":2}}],["从梯度的角度",{"5":{"92":2}}],["从梯度分析的角度来看",{"5":{"92":2}}],["从梯度流动角度",{"5":{"91":2}}],["从梯度计算的期望到模型不确定性的估计",{"5":{"96":2}}],["从梯度角度分析",{"5":{"71":2}}],["从梯度公式可以看出",{"5":{"69":1}}],["从梯度公式",{"5":{"69":1}}],["从很小的值线性增长到目标学习率",{"5":{"48":2}}],["从",{"5":{"47":1,"70":3,"92":1}}],["从概率角度看",{"5":{"47":2}}],["从概率论的视角来看",{"5":{"95":2}}],["从概率论的视角",{"5":{"61":2}}],["从概率论的角度",{"5":{"71":4}}],["从概率论视角审视自信息的定义",{"5":{"61":2}}],["从概率论角度",{"5":{"71":2}}],["从概率的角度理解",{"5":{"69":2}}],["从麦肯罗皮层神经元的二值模型出发",{"5":{"47":2}}],["从特征学习的角度来看",{"5":{"47":2}}],["从生物神经元到数学抽象",{"2":{"47":1},"5":{"47":1}}],["从神经元到层",{"2":{"46":1},"5":{"46":1}}],["从前向传播的数学描述到实际代码实现",{"5":{"45":1}}],["从链式法则的矩阵形式出发",{"5":{"44":2}}],["从多项式分布的角度",{"5":{"71":2}}],["从贝叶斯推断的角度",{"5":{"71":2,"90":2}}],["从直观上看",{"5":{"71":2}}],["从而允许使用更大的学习率",{"5":{"97":2}}],["从而允许我们使用概率论的统一工具进行分析和优化",{"5":{"69":2}}],["从而确保位置",{"5":{"95":1}}],["从而确保位置只能关注位置到",{"5":{"95":1}}],["从而产生较高的注意力分数",{"5":{"95":2}}],["从而产生更大的梯度来",{"5":{"69":2}}],["从而调整",{"5":{"94":1}}],["从而调整的结构",{"5":{"94":1}}],["从而极大地增强了模型的表达能力",{"5":{"93":2}}],["从而实现灵活的注意力计算",{"5":{"94":2}}],["从而实现信息聚合",{"5":{"92":2}}],["从而实现高效的存储和计算",{"5":{"50":2}}],["从而正确处理词序承载的语法和语义信息",{"5":{"91":2}}],["从而改善外推性能",{"5":{"88":2}}],["从而避免条件数过大",{"5":{"87":4}}],["从而避免下溢",{"5":{"44":2}}],["从而加速注意力计算",{"5":{"86":2}}],["从而加速收敛并减少震荡",{"5":{"48":2}}],["从而对应不同的注意力行为",{"5":{"86":2}}],["从而在实际应用中做出更好的选择",{"5":{"89":2}}],["从而在多个计算设备上分布式执行",{"5":{"50":2}}],["从而在反向传播时为梯度提供了一条恒等映射路径",{"5":{"50":2}}],["从而约束了网络函数的lipschitz常数",{"5":{"50":2}}],["从而缓解了梯度消失问题",{"5":{"50":2}}],["从而使得机器学习算法能够在这些向量上进行运算和优化",{"5":{"50":2}}],["从而大幅降低计算和存储成本",{"5":{"48":2}}],["从而解决异或等线性不可分问题",{"5":{"47":2}}],["从而捕获更抽象的语义信息",{"5":{"45":2}}],["从而开启了深度学习的现代时代",{"5":{"44":2}}],["从而更准确地估计和最大化互信息",{"5":{"71":2}}],["从而逼近任意形状的函数曲面",{"5":{"71":2}}],["从而通过stone",{"5":{"71":2}}],["从而影响各种损失函数的收敛速度和最终性能",{"5":{"101":2}}],["从而影响模型对位置模式的捕获能力",{"5":{"71":2}}],["从而影响学习动态和最终表示的质量",{"5":{"69":2}}],["从而传递更多信息",{"5":{"69":2}}],["从而间接影响损失函数的收敛速度和最终性能",{"5":{"70":2}}],["从而打破了上述对称性",{"5":{"99":2,"101":2}}],["从而提高数值稳定性",{"5":{"61":2}}],["从而提高mlm的预测准确性",{"5":{"99":2,"101":2}}],["从而导致策略优化走向错误的方向",{"5":{"99":2,"101":2}}],["从数学上看",{"5":{"96":2}}],["从数学上分析",{"5":{"71":4}}],["从数学角度分析",{"5":{"93":2}}],["从数学角度分析这种分工现象",{"5":{"93":2}}],["从数学角度",{"5":{"86":2,"89":2}}],["从数学角度来看",{"5":{"44":2,"45":2,"89":2,"93":2}}],["从数学角度看",{"5":{"47":2,"71":2,"86":2,"88":2}}],["从简单的曲线到高维空间中的复杂流形",{"5":{"71":2}}],["从变换的角度",{"5":{"42":2}}],["从某种意义上",{"5":{"42":2}}],["从极限的角度分析sigmoid函数的边界行为",{"5":{"42":2}}],["从导数公式",{"5":{"41":1}}],["从导数公式可以看出",{"5":{"41":1}}],["从几何视角理解",{"5":{"61":2}}],["从几何视角来看",{"5":{"47":2}}],["从几何角度",{"5":{"90":2,"91":2}}],["从几何角度来看",{"5":{"48":2,"50":2,"91":2,"94":2}}],["从几何角度分析",{"5":{"42":2}}],["从几何角度看",{"5":{"42":2,"45":2,"46":2,"47":2,"69":2}}],["从几何角度理解",{"5":{"41":2,"71":4}}],["从几何上看",{"5":{"42":2}}],["从nce到infonce的理论演进",{"2":{"69":1},"5":{"69":1}}],["从softmax统一性看transformer的设计哲学",{"2":{"69":1},"5":{"69":1}}],["从infonce到注意力的数学桥梁",{"2":{"69":1},"5":{"69":1}}],["从infonce的统一视角",{"5":{"69":2}}],["从损失函数的定义到模型性能的评价",{"5":{"96":2}}],["从损失函数的角度",{"5":{"70":2}}],["从1到0",{"5":{"70":2}}],["从1减小到0时",{"5":{"70":1}}],["从均匀分布",{"5":{"89":1}}],["从均匀分布中采样每个编码元素",{"5":{"89":1}}],["从均方误差的信息论起源开始",{"5":{"99":2,"101":2}}],["从dpo的目标函数可以反推出一个隐式的奖励函数",{"5":{"99":2,"101":2}}],["自注意力",{"5":{"101":2}}],["自注意力中的softmax操作使得注意力权重",{"5":{"97":1}}],["自注意力中的softmax操作使得注意力权重对所有的和为1",{"5":{"97":1}}],["自注意力的输出为",{"5":{"92":2}}],["自注意力的核心运算是缩放后的点积注意力",{"5":{"88":2}}],["自注意力机制",{"5":{"88":2}}],["自注意力机制需要计算查询向量与键向量的相似度",{"5":{"50":2}}],["自注意力机制通过计算每个位置的输出",{"5":{"69":1}}],["自注意力机制通过softmax权重的",{"5":{"69":2}}],["自注意力机制通过",{"5":{"69":1}}],["自注意力机制可以被解释为一种互信息最大化的机制",{"5":{"69":1}}],["自注意力机制可以被解释为一种",{"5":{"69":1}}],["自由度从",{"5":{"87":1}}],["自由度从减少到",{"5":{"87":1}}],["自信息",{"5":{"61":3}}],["自信息的数学定义为",{"5":{"61":2}}],["自信息的概率论定义",{"2":{"61":1},"5":{"61":1}}],["自助法可用于估计困惑度",{"5":{"96":2}}],["自助法",{"5":{"96":2}}],["自适应优化算法根据历史梯度信息为每个参数调整学习率",{"5":{"97":2}}],["自适应优化算法",{"5":{"97":4}}],["自适应优化算法的收敛性质",{"2":{"97":1},"5":{"97":1}}],["自适应稀疏模式",{"5":{"86":2}}],["自适应学习率等技术有助于跳出较差的局部最小值",{"5":{"51":2}}],["自适应学习率方法为每个参数单独调整学习率",{"5":{"48":2}}],["自适应激活函数",{"5":{"41":2}}],["自适应损失函数",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["自适应损失函数可以写为",{"5":{"99":2,"101":2}}],["自适应多任务损失",{"5":{"99":2,"101":2}}],["自动微分结合了数值计算的效率和符号计算的精确性",{"5":{"48":2}}],["自动微分将函数分解为基本操作的序列",{"5":{"48":2}}],["自动微分分为前向模式",{"5":{"45":2}}],["自动微分利用链式法则和计算图的结构",{"5":{"45":2}}],["自动微分",{"5":{"45":4,"48":2}}],["自动微分主要分为两种模式",{"5":{"44":2}}],["自动微分的分类",{"5":{"44":2}}],["自身表示导数",{"5":{"42":1}}],["自化",{"5":{"42":4}}],["自然对数",{"5":{"61":2}}],["自然",{"5":{"41":1,"44":2}}],["自然涌现的结构",{"5":{"70":2}}],["自回归模型",{"5":{"41":2}}],["自回归或掩码方式",{"5":{"99":2,"101":2}}],["自回归语言建模是大语言模型预训练的核心任务",{"5":{"99":2,"101":2}}],["自回归语言建模的形式化定义",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["自回归语言建模的目标是建模序列的联合概率分布",{"5":{"99":2,"101":2}}],["自回归语言建模",{"5":{"99":2,"101":2}}],["自回归分解",{"5":{"99":2,"101":2}}],["反而可能是有益的正则化",{"5":{"94":2}}],["反之",{"5":{"61":2,"90":2}}],["反之亦然",{"5":{"41":2,"69":2,"71":2,"93":2}}],["反幂迭代",{"5":{"50":2}}],["反向模式的核心是链式法则",{"5":{"45":1}}],["反向模式的核心是链式法则​",{"5":{"45":1}}],["反向模式的微分计算",{"5":{"45":2}}],["反向模式自动微分是深度学习中反向传播算法的理论基础",{"5":{"45":2}}],["反向模式自动微分",{"5":{"44":1}}],["反向模式自动微分从输出节点开始",{"5":{"44":2}}],["反向模式",{"5":{"44":2,"45":1}}],["反向传播中的梯度计算",{"2":{"94":1},"5":{"94":1}}],["反向传播遵循链式法则",{"5":{"51":2}}],["反向传播从输出层开始",{"5":{"48":2}}],["反向传播时对权重和偏置的梯度更新可能使用不同的学习率或正则化系数",{"5":{"46":2}}],["反向传播正是沿此图的反向边计算各节点对损失的梯度",{"5":{"44":2}}],["反向传播是反向模式自动微分",{"5":{"44":2}}],["反向传播是自动微分",{"5":{"44":2}}],["反向传播和参数更新",{"5":{"44":2}}],["反向传播循环",{"5":{"44":2}}],["反向传播算法沿着计算图反向遍历",{"5":{"48":2}}],["反向传播算法对应于反向模式自动微分在神经网络中的应用",{"5":{"44":2}}],["反向传播算法可以描述为",{"5":{"44":2}}],["反向传播算法",{"5":{"44":2}}],["反向传播的计算复杂度约为前向传播的两倍",{"5":{"44":2}}],["反向传播的核心",{"5":{"44":1,"45":1}}],["反向传播的核心是链式法则的高效应用",{"5":{"44":2}}],["反向传播的核心数学工具是链式法则",{"5":{"44":2}}],["反向传播的总flops约为前向传播的两倍",{"5":{"44":2}}],["反向传播的主要计算包括两部分",{"5":{"44":2}}],["反向传播的flops",{"5":{"44":2}}],["反向传播的梯度为",{"5":{"41":1}}],["反向传播的梯度为​",{"5":{"41":1}}],["反向传播的梯度满足某种形式的尺度约束",{"5":{"41":2}}],["反向传播的梯度满足",{"5":{"41":2}}],["反向传播通过巧妙地利用链式法则和动态规划思想",{"5":{"44":2}}],["反向传播本质上是链式法则在复合函数梯度计算中的高效应用",{"5":{"44":2}}],["反向传播",{"5":{"44":3}}],["反向传播与自动微分的关系",{"2":{"44":1},"5":{"44":1}}],["反向传播后",{"5":{"41":1}}],["反向传播梯度推导<",{"5":{"21":1,"85":1}}],["反向传播梯度推导",{"0":{"44":1},"4":{"44":1},"5":{"21":4,"44":1,"85":4}}],["反映了行随机矩阵的基本性质",{"5":{"87":2}}],["反映了增长的速率",{"5":{"48":2}}],["反映了一种深刻的设计哲学",{"5":{"69":2}}],["系统推导scaled",{"5":{"95":2}}],["系统推导mse的统计学基础",{"5":{"51":2}}],["系统地分析正弦余弦位置编码的频谱结构",{"5":{"90":2}}],["系统探讨可学习位置编码的数学原理和性质",{"5":{"89":2}}],["系统分析注意力矩阵的谱性质",{"5":{"87":2}}],["系统性",{"5":{"51":2}}],["系统性地探讨损失函数的优化性质",{"5":{"97":2}}],["系统性地剖析rope的设计哲学",{"5":{"88":2}}],["系统性地推导了交叉熵损失函数的数学定义",{"5":{"61":2}}],["系统性地推导交叉熵的数学定义",{"5":{"61":2}}],["系统性地构建这一统一框架",{"5":{"69":2}}],["系统阐述神经元从生物原型到数学抽象的演化过程",{"5":{"47":2}}],["系统阐述反向传播算法的完整理论框架",{"5":{"44":2}}],["分组注意力",{"5":{"93":2}}],["分组注意力将头分为若干组",{"5":{"93":2}}],["分组多头注意力的计算可以表示为",{"5":{"93":2}}],["分组多头注意力",{"5":{"93":2}}],["分组查询注意力是多查询注意力的折中方案",{"5":{"93":2}}],["分组查询注意力",{"5":{"86":2}}],["分组查询注意力的平衡设计",{"2":{"86":1},"5":{"86":1}}],["分块对角结构意味着不同头的投影在输入空间中是不相交的",{"5":{"93":2}}],["分而治之",{"5":{"93":2}}],["分层位置编码",{"5":{"89":2}}],["分散",{"5":{"89":2}}],["分离",{"5":{"88":2}}],["分离度",{"5":{"70":2,"97":2}}],["分解为低秩矩阵的乘积",{"5":{"87":2}}],["分解则将张量表示为一个核心张量与多个因子矩阵在各模上的乘积",{"5":{"50":2}}],["分解将一个张量表示为若干个秩一张量的和",{"5":{"50":2}}],["分解",{"5":{"50":4}}],["分片",{"5":{"45":2}}],["分别表示键空间和值空间的维度",{"5":{"95":1}}],["分别定义了键空间和值空间的投影",{"5":{"94":1}}],["分别左乘这三个投影矩阵",{"5":{"94":1}}],["分别出现在不同的项中",{"5":{"88":1}}],["分别为第",{"5":{"46":1}}],["分别编码各层次的位置信息",{"5":{"88":2}}],["分别编码",{"5":{"71":1}}],["分别编码粗粒度和细粒度的位置关系",{"5":{"71":2}}],["分别是两个头的输出",{"5":{"93":1}}],["分别是输入和输出的维度",{"5":{"41":1}}],["分别是",{"5":{"41":1}}],["分段线性",{"5":{"71":3}}],["分析模型行为",{"5":{"95":2}}],["分析不同头之间的信息互补性",{"5":{"93":2}}],["分析不同层之间隐藏状态的相关性",{"5":{"96":2}}],["分析单个头的信息容量",{"5":{"93":2}}],["分析其表达能力",{"5":{"87":2}}],["分析其几何意义和梯度特性",{"5":{"42":2}}],["分析了隐式正则化效应",{"5":{"97":2}}],["分析了各自的优缺点和适用场景",{"5":{"91":2}}],["分析了各个参数",{"5":{"91":2}}],["分析了两种范式的数学差异和适用场景",{"5":{"89":2}}],["分析了它与正弦余弦编码的逼近关系",{"5":{"89":2}}],["分析了其参数空间和自由度",{"5":{"89":2}}],["分析了其基本性质",{"5":{"87":2}}],["分析了softmax变换对秩的影响",{"5":{"87":2}}],["分析了",{"5":{"61":2}}],["分析现代llm训练中使用的各种损失函数的数学本质及其设计原理",{"5":{"99":2,"101":2}}],["分布变得更加尖锐",{"5":{"96":2}}],["分布集中在",{"5":{"95":1}}],["分布集中在区间内",{"5":{"95":1}}],["分布趋于尖锐",{"5":{"95":2}}],["分布趋于平坦",{"5":{"95":2}}],["分布间差异的信息度量",{"2":{"61":1},"5":{"61":1}}],["分布用pdf表示",{"5":{"96":2}}],["分布式训练",{"5":{"41":2}}],["分布越集中",{"5":{"61":2}}],["分布越均匀",{"5":{"61":2}}],["分布越",{"5":{"96":4}}],["分布",{"5":{"69":2,"95":2}}],["分布锐度效应",{"5":{"69":2}}],["分布外",{"5":{"99":2,"101":2}}],["分类任务",{"5":{"69":2,"99":2,"101":2}}],["分类任务在分类任务中",{"5":{"69":1}}],["分类任务的最大似然推导",{"2":{"61":1},"5":{"61":1}}],["分类任务的数学结构",{"5":{"70":2}}],["分类任务的输出空间是概率单纯形",{"5":{"70":1}}],["分类任务的输出空间是",{"5":{"70":1}}],["分类任务选择交叉熵或其变体",{"5":{"70":2}}],["分类任务中的交叉熵损失",{"5":{"69":2}}],["分类任务中的温度参数",{"5":{"70":2}}],["分割到多个计算设备上",{"5":{"45":2}}],["分割函数的对偶",{"5":{"70":2}}],["分词器对损失函数的影响",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["分词器的选择直接影响损失函数的结构和模型的学习行为",{"5":{"99":2,"101":2}}],["分词后的mlm损失",{"5":{"99":2,"101":2}}],["数值示例与可视化",{"2":{"91":1},"5":{"91":1}}],["数值计算更稳定",{"5":{"89":2}}],["数值计算可能不稳定",{"5":{"89":2}}],["数值计算不稳定",{"5":{"87":2}}],["数值不稳定",{"5":{"87":2}}],["数值上更加稳定",{"5":{"61":2}}],["数值稳定",{"5":{"87":2}}],["数值稳定的softmax实现",{"5":{"44":1}}],["数值稳定性问题与logsumexp技巧",{"2":{"61":1},"5":{"61":1}}],["数值稳定性",{"5":{"47":2,"97":1}}],["数值稳定性是反向传播实现中的关键考虑",{"5":{"44":2}}],["数值稳定性与实现细节",{"2":{"44":1},"5":{"44":1}}],["数十亿甚至万亿级",{"5":{"48":2}}],["数据分布的复杂性和噪声水平",{"5":{"97":2}}],["数据分布为两个交织的螺旋形状",{"5":{"71":2}}],["数据特性",{"5":{"97":2}}],["数据被投影到方差最大的正交方向上",{"5":{"94":2}}],["数据增强和正则化技术被用来引导模型学习有意义的依赖关系",{"5":{"92":2}}],["数据层面",{"5":{"61":2}}],["数据的似然函数为",{"5":{"51":2}}],["数据上的平均损失",{"5":{"51":2}}],["数据变化敏感的方向",{"5":{"70":2}}],["数学表达式更加紧凑",{"5":{"90":2}}],["数学结构",{"5":{"90":2}}],["数学最优",{"5":{"88":2}}],["数学最优解",{"5":{"88":2}}],["数学的自然结果就是平移等变性",{"5":{"88":2}}],["数学定义如下",{"5":{"86":2}}],["数学上",{"5":{"47":1,"50":6,"86":2,"91":2,"92":4,"93":2,"95":2}}],["数学上的优雅性",{"5":{"88":1}}],["数学上的优雅性是首要因素",{"5":{"88":1}}],["数学上的这种统一性并非巧合",{"5":{"61":2}}],["数学上的统一性",{"5":{"70":2}}],["数学评估指标",{"5":{"41":2}}],["数学基础",{"4":{"48":1,"50":1,"96":1},"5":{"21":6,"85":6}}],["数学本质",{"5":{"70":2}}],["数学形式统一",{"5":{"99":2,"101":2}}],["c^",{"5":{"97":8}}],["cyclic",{"5":{"92":2}}],["circle",{"5":{"90":2}}],["curriculum",{"5":{"51":2}}],["curvature",{"5":{"48":2}}],["cp分解可以将一个",{"5":{"50":1}}],["cp分解可以将一个的三阶张量表示为个秩一张量的和",{"5":{"50":1}}],["cp",{"5":{"50":4}}],["central",{"5":{"96":2}}],["critical",{"5":{"48":2}}],["crossentropyloss",{"5":{"61":2}}],["crossentropy",{"5":{"61":12,"97":2}}],["crossentropy联合优化",{"2":{"61":1},"5":{"61":1}}],["cross",{"5":{"47":2,"61":2,"70":2,"86":2,"87":2,"93":2,"96":2}}],["checkpointing",{"5":{"50":2}}],["checking",{"5":{"44":2}}],["cholesky分解的数值稳定性好",{"5":{"50":2}}],["cholesky分解是lu分解的特例",{"5":{"50":2}}],["chain",{"5":{"48":2}}],["casual",{"5":{"91":2}}],["cases",{"5":{"42":24,"61":4}}],["causal",{"5":{"91":2,"95":2}}],["candecomp",{"5":{"50":2}}],["categorical",{"5":{"96":2}}],["calculus",{"5":{"47":2}}],["class",{"5":{"61":4}}],["class=",{"5":{"21":10,"85":26}}],["claude",{"5":{"61":2}}],["clt",{"5":{"96":2}}],["clipping",{"5":{"44":2,"48":2}}],["c",{"5":{"42":2,"97":2}}],["core对半精度矩阵乘法的加速使得多头注意力的计算效率大幅提升",{"5":{"93":2}}],["core进行矩阵乘法加速",{"5":{"93":2}}],["core的更充分利用",{"5":{"86":2}}],["core",{"5":{"86":2}}],["correlation",{"5":{"96":2}}],["colorcoded",{"5":{"61":4}}],["column",{"5":{"50":2}}],["coefficient",{"5":{"96":2}}],["covariance",{"5":{"96":2}}],["covariate",{"5":{"41":2}}],["cosine",{"5":{"48":2,"97":2}}],["combination",{"5":{"92":2}}],["combinedscene",{"5":{"48":4}}],["complete",{"5":{"92":2}}],["completion",{"5":{"50":2}}],["component",{"5":{"50":2}}],["composite",{"5":{"45":2}}],["compositional",{"5":{"71":2}}],["comp",{"5":{"45":4}}],["computational",{"5":{"45":2}}],["compact",{"5":{"70":2}}],["coordinates",{"5":{"46":2,"47":2}}],["convex",{"5":{"92":2}}],["convolution",{"5":{"87":2}}],["convolutional",{"5":{"46":2}}],["constant",{"5":{"89":2}}],["contrastive",{"5":{"61":2,"69":6}}],["contraction",{"5":{"50":2,"93":2}}],["condition",{"5":{"51":2,"87":2}}],["conditional",{"5":{"96":2}}],["connected",{"5":{"46":2,"92":2}}],["connection",{"5":{"41":2,"92":2}}],["concatenation",{"5":{"91":2}}],["concat",{"5":{"69":2,"86":2}}],["coding",{"5":{"69":2}}],["cdots",{"5":{"46":4}}],["cdot",{"5":{"42":14,"61":18}}],["l2正则化倾向于学习较小的编码值",{"5":{"89":2}}],["l2正则化在损失函数中添加编码范数的惩罚",{"5":{"89":2}}],["ls",{"5":{"97":2}}],["lse",{"5":{"61":2}}],["lstm和gru通过门控机制设计",{"5":{"50":2}}],["lu分解将矩阵分解为下三角矩阵",{"5":{"50":1}}],["lu分解将矩阵分解为下三角矩阵和上三角矩阵的乘积",{"5":{"50":1}}],["lu分解的主要用途是高效求解线性方程组",{"5":{"50":2}}],["llm",{"5":{"50":4}}],["llama系列",{"5":{"88":1}}],["llama系列是rope的典型使用者",{"5":{"88":1}}],["llama",{"5":{"71":2,"88":2,"92":2,"93":2}}],["large",{"5":{"96":2,"99":2,"101":2}}],["law",{"5":{"96":2}}],["label",{"5":{"96":2}}],["layer或fully",{"5":{"46":2}}],["layer",{"5":{"41":4,"46":12}}],["language",{"5":{"99":6,"101":6}}],["long",{"5":{"88":2,"92":2}}],["local",{"5":{"86":2}}],["low",{"5":{"50":2,"87":2,"91":2}}],["loss",{"5":{"47":2,"61":2,"70":8}}],["logsumexp",{"5":{"61":10}}],["logsumexp技巧的数学原理",{"2":{"61":1},"5":{"61":1}}],["logistic",{"5":{"47":2}}],["logical",{"5":{"47":2}}],["logit",{"5":{"61":2,"69":2}}],["logit函数",{"5":{"42":2}}],["logits的定义域是整个实数空间",{"5":{"96":2}}],["logits的生成过程如下",{"5":{"96":2}}],["logits的分量之间是相对的",{"5":{"96":2}}],["logits向量具有两个显著特点",{"5":{"96":2}}],["logits这一术语源自统计学中的logit函数",{"5":{"96":2}}],["logits与概率分布的生成",{"2":{"96":1},"5":{"96":1}}],["logits与概率的变换",{"5":{"71":2}}],["logits可以理解为",{"5":{"71":2}}],["logits空间",{"5":{"42":2}}],["logits",{"5":{"61":12,"69":5,"70":10,"71":6,"96":5}}],["log",{"2":{"61":1},"5":{"61":13,"71":2,"97":12}}],["l层前馈网络",{"5":{"46":2}}],["ldots",{"5":{"42":2,"69":2,"86":2}}],["lt",{"5":{"42":2}}],["ln的挑战",{"5":{"41":2}}],["ln的数学优势",{"5":{"41":2}}],["ln结构中",{"5":{"41":2}}],["ln结构更有利于训练非常深的网络",{"5":{"41":2}}],["ln",{"5":{"41":12}}],["limit",{"5":{"96":2}}],["linearinseparablescene",{"5":{"47":4}}],["linearly",{"5":{"47":2}}],["linear",{"5":{"42":2,"86":8,"87":2,"88":4}}],["link",{"5":{"21":10,"85":26}}],["likelihood",{"5":{"61":12,"96":2}}],["l",{"5":{"42":14,"46":30,"61":18,"97":4}}],["length",{"5":{"50":2,"91":2}}],["leibler散度",{"2":{"61":1},"5":{"61":3}}],["leibler",{"5":{"96":2}}],["learned",{"5":{"88":2,"89":2,"91":2}}],["learning",{"5":{"48":2,"51":2,"69":2,"87":2,"93":2,"99":4,"101":4}}],["learn",{"5":{"45":4}}],["leakyrelu",{"5":{"42":4}}],["leaky",{"5":{"41":2}}],["leq",{"5":{"42":8,"69":2}}],["left|",{"5":{"42":4}}],["left",{"5":{"42":22,"46":8,"61":14,"69":6,"86":2,"97":2}}],["mt",{"5":{"97":1}}],["mb",{"5":{"86":1}}],["mb的存储",{"5":{"86":1}}],["mqa",{"5":{"86":2}}],["mutual",{"5":{"69":2,"96":2}}],["multiple",{"5":{"93":2}}],["multiqueryattention",{"5":{"86":2}}],["multinomial",{"5":{"69":2,"96":2}}],["multi",{"5":{"46":2,"70":2,"86":2,"93":6,"99":2,"101":2}}],["mgf",{"5":{"96":2}}],["moa使用一个门控网络为每个输入动态地选择",{"5":{"93":2}}],["moa",{"5":{"93":2}}],["moe通常需要为每个输入样本选择活跃的专家子集",{"5":{"93":2}}],["moe",{"5":{"93":2}}],["modulation",{"5":{"88":2}}],["mode",{"5":{"44":4,"45":4}}],["model",{"5":{"69":2,"99":2,"101":2}}],["models",{"5":{"99":2,"101":2}}],["modeling",{"5":{"99":2,"101":2}}],["mobilenet中的深度可分离卷积可以用kronecker积来表示",{"5":{"50":2}}],["moment",{"5":{"48":2,"96":4}}],["momentum",{"5":{"48":2}}],["mcculloch",{"5":{"47":4}}],["mixed",{"5":{"97":2}}],["mixture",{"5":{"93":2}}],["micro",{"5":{"50":2}}],["mirsky定理",{"5":{"50":2}}],["mid",{"5":{"46":2,"69":4}}],["min",{"5":{"42":2}}],["mini",{"5":{"41":1,"44":2}}],["msg",{"5":{"45":4,"96":4}}],["mse不是从信息论原理导出的损失函数",{"5":{"61":2}}],["mse关于",{"5":{"51":2}}],["mse关于是凸函数吗",{"5":{"51":1}}],["mse关于是严格的凸函数",{"5":{"51":1}}],["mse作为",{"5":{"51":1}}],["mse作为的函数",{"5":{"51":1}}],["mse梯度的一个特点是梯度幅度与误差大小成正比",{"5":{"51":2}}],["mse定义为",{"5":{"51":2}}],["mse是合适的选择",{"5":{"51":2}}],["mse对异常值",{"5":{"51":2}}],["mse最优解的几何解释是其最深刻的结果之一",{"5":{"51":2}}],["mse度量的是预测向量与目标向量之间的",{"5":{"51":2}}],["mse正是这两点之间欧几里得距离的平方除以样本数",{"5":{"51":2}}],["mse虽然是回归任务的标准损失",{"5":{"51":2}}],["mse与高斯分布之间存在深刻的联系",{"5":{"51":2}}],["mse与其他损失函数的联系",{"2":{"51":1},"5":{"51":1}}],["mse可以分为总体mse和样本mse两种形式",{"5":{"51":2}}],["mse可以用向量形式更加简洁地表示",{"5":{"51":2}}],["mse的几何解释是本节的核心内容之一",{"5":{"51":2}}],["mse的几何解释是理解其数学本质的关键视角",{"5":{"51":2}}],["mse的梯度计算是深度学习反向传播的基础",{"5":{"51":2}}],["mse的梯度特性",{"2":{"51":1},"5":{"51":1}}],["mse的统计学意义可以从最优预测理论的角度来理解",{"5":{"51":2}}],["mse的核心思想非常直观",{"5":{"51":2}}],["mse以其优雅的数学形式和清晰的几何解释",{"5":{"51":2}}],["mse",{"0":{"51":1},"4":{"51":1},"5":{"51":11,"61":4,"70":12,"85":5}}],["mamba等架构采用这类方法",{"5":{"88":2}}],["magnitude",{"5":{"88":2}}],["map",{"5":{"86":2}}],["mae或huber损失可能是更好的选择",{"5":{"51":2}}],["mae为",{"5":{"51":2}}],["mae对异常值的敏感性较低",{"5":{"51":2}}],["mae对异常值的惩罚较轻",{"5":{"51":2}}],["mae",{"5":{"51":2,"70":4}}],["mahalanobis",{"5":{"96":2}}],["mathcal",{"5":{"97":2}}],["mathbf",{"5":{"46":44}}],["mathbb",{"5":{"42":4}}],["match",{"5":{"95":2}}],["matrix",{"5":{"45":2,"48":2,"50":2,"86":2,"87":14,"95":6}}],["max",{"5":{"42":6}}],["maximum",{"5":{"61":2,"96":2}}],["mass",{"5":{"96":2}}],["mask",{"5":{"91":2,"95":4}}],["masked",{"5":{"99":2,"101":2}}],["masking",{"5":{"99":6,"101":6}}],["masking后的mlm损失",{"5":{"99":2,"101":2}}],["manimce",{"5":{"47":8,"48":4}}],["manifold",{"5":{"70":2}}],["müssen",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["mle是相合的",{"5":{"96":2}}],["mle是训练的标准方法",{"5":{"96":2}}],["mle",{"5":{"51":2,"61":2,"96":2}}],["mlp输出等",{"5":{"69":2}}],["mlm同时利用左右上下文来预测被掩码的token",{"5":{"99":2,"101":2}}],["mlm损失函数",{"5":{"99":2,"101":2}}],["mlm损失为",{"5":{"99":2,"101":2}}],["m",{"5":{"69":5,"97":13}}],["mechanism",{"5":{"93":2,"95":2}}],["memory",{"5":{"89":2}}],["mercer定理",{"5":{"87":2}}],["method",{"5":{"87":2}}],["metric",{"5":{"70":2,"87":2}}],["mean",{"5":{"41":2,"51":4}}],["损失感知",{"5":{"101":2}}],["损失缩放通过在反向传播前将损失乘以一个缩放因子",{"5":{"97":1}}],["损失缩放通过在反向传播前将损失乘以一个缩放因子来解决这个问题",{"5":{"97":1}}],["损失通常是非凸的",{"5":{"97":2}}],["损失景观的全局结构",{"5":{"97":2}}],["损失景观的形状决定了局部极小值的质量",{"5":{"97":2}}],["损失景观的宽碗",{"5":{"97":2}}],["损失景观通常非常复杂",{"5":{"97":2}}],["损失景观中良好的区域",{"5":{"97":2}}],["损失景观中鞍点和局部极小值的分布直接影响优化的难度和最终解的质量",{"5":{"97":2}}],["损失景观中存在大量的",{"5":{"51":2}}],["损失为",{"5":{"51":2}}],["损失都是正的",{"5":{"51":2}}],["损失nan",{"5":{"44":1}}],["损失函数定义了优化的目标",{"5":{"101":2}}],["损失函数与模型架构的协同设计",{"2":{"101":1},"5":{"101":1}}],["损失函数优化性质的比较分析",{"2":{"97":1},"5":{"97":1}}],["损失函数将变为",{"5":{"61":2}}],["损失函数作为模型参数的标量函数",{"5":{"51":1}}],["损失函数扮演着衡量模型预测与真实目标之间差距的核心角色",{"5":{"51":2}}],["损失函数包含重构项和kl正则项",{"5":{"96":2}}],["损失函数的收敛速度和最终性能之间存在复杂的权衡关系",{"5":{"97":2}}],["损失函数的梯度可能非常小",{"5":{"97":2}}],["损失函数的凸性是优化理论中最重要的概念之一",{"5":{"97":2}}],["损失函数的优化性质不仅决定了训练过程的效率和稳定性",{"5":{"97":2}}],["损失函数的优化性质<",{"5":{"85":1}}],["损失函数的优化性质",{"0":{"97":1},"4":{"97":1},"5":{"85":4,"97":1}}],["损失函数的等值面在高维空间中可能比直觉上预期的更加连通",{"5":{"48":2}}],["损失函数的hessian通常具有极值的特征值分布",{"5":{"48":2}}],["损失函数的曲率特性直接影响优化的收敛速度和稳定性",{"5":{"70":2}}],["损失函数的边界行为决定了训练的后期动态",{"5":{"70":2}}],["损失函数的几何分析与位置编码的几何分析在方法论上是相通的",{"5":{"70":2}}],["损失函数的选择应遵循以下原则",{"5":{"70":2}}],["损失函数的组合方式直接影响注意力层的学习",{"5":{"70":2}}],["损失函数的数学结构与注意力机制",{"5":{"70":2}}],["损失函数的理论极限",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["损失函数衡量输出与目标之间的差异",{"5":{"48":1}}],["损失函数会以多快的速度变化",{"5":{"48":2}}],["损失函数数学",{"2":{"85":1},"4":{"51":1,"61":1,"69":1,"70":1,"97":1,"98":1,"99":1,"100":1,"101":1},"5":{"85":13}}],["损失函数数学结构对比",{"0":{"70":1},"4":{"70":1},"5":{"70":1,"85":4}}],["损失函数数学结构对比<",{"5":{"85":1}}],["损失函数数学性质的系统性对比",{"2":{"70":1},"5":{"70":1}}],["损失函数为",{"5":{"44":2,"89":2}}],["损失函数度量预测与真实标签之间的差异",{"5":{"44":1}}],["损失函数关于注意力参数",{"5":{"97":2}}],["损失函数关于logits的梯度具有极其简洁的形式",{"5":{"96":2}}],["损失函数关于输入的梯度涉及注意力矩阵的逆或其伪逆",{"5":{"87":2}}],["损失函数关于模型参数的梯度是反向传播算法的基础",{"5":{"61":2}}],["损失函数关于网络参数的梯度需要通过激活函数的导数逐层传递",{"5":{"42":2}}],["损失函数关于第一层权重的梯度为",{"5":{"41":1}}],["损失函数值出现nan或inf",{"5":{"41":2}}],["损失函数",{"5":{"41":1,"44":3,"48":1,"51":1,"70":1,"97":1}}],["损失函数梯度计算",{"5":{"45":1}}],["损失函数选择的原则",{"2":{"70":1},"5":{"70":1}}],["损失函数​",{"5":{"70":1}}],["损失函数对权重的梯度可以分解为损失函数对净输入的梯度",{"5":{"44":2}}],["损失函数对不同位置误差的惩罚会通过这些正交基函数传播",{"5":{"70":2}}],["损失函数前沿与未来方向",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["损失也不会趋于",{"5":{"70":2}}],["损失越大",{"5":{"70":2}}],["损失会非常大",{"5":{"70":2}}],["损失增长越来越快",{"5":{"70":2}}],["损失是不可微的",{"5":{"70":2}}],["损失的可微代理",{"5":{"70":2}}],["损失主导",{"5":{"70":2}}],["损失",{"5":{"70":2}}],["损失出发",{"5":{"70":2}}],["损失值或验证性能动态调整",{"5":{"99":2,"101":2}}],["症状",{"5":{"44":1}}],["误差很小",{"5":{"87":2}}],["误差向量",{"5":{"51":1}}],["误差向量与预测空间正交",{"5":{"51":1}}],["误差最小化",{"5":{"51":2}}],["误差传播",{"5":{"44":1}}],["误差信号逐层反向传播",{"5":{"51":2}}],["误差信号传播",{"5":{"44":2}}],["误差信号矩阵需要除以批量大小",{"5":{"44":2}}],["误差信号会被衰减",{"5":{"44":2}}],["误差信号从前向传播的最后一层",{"5":{"44":2}}],["误差信号从前一层传播到当前层",{"5":{"44":2}}],["误差信号从输出层向输入层逐层传播",{"5":{"44":2}}],["误差信号有不同的简化形式",{"5":{"44":2}}],["误差信号为",{"5":{"44":2}}],["误差信号包含了损失函数对该层激活值变化的敏感程度信息",{"5":{"44":2}}],["误差信号",{"5":{"44":4}}],["误差信号的传播",{"2":{"44":1},"5":{"44":1}}],["误差为",{"5":{"70":2}}],["充分利用硬件并行性",{"5":{"44":2}}],["约128mb",{"5":{"95":2}}],["约为2倍前向传播",{"5":{"44":2}}],["约有一半的神经元输出为零",{"5":{"71":2}}],["返回分数最高的",{"5":{"86":1}}],["返回分数最高的个索引",{"5":{"86":1}}],["返回",{"5":{"44":2}}],["路由",{"5":{"44":2}}],["路径为",{"5":{"92":2}}],["路径长度与序列长度成正比",{"5":{"95":2}}],["路径长度与依赖关系的跨度成正比",{"5":{"92":2}}],["路径长度与依赖建模",{"2":{"92":1},"5":{"92":1}}],["路径长度恒为1",{"5":{"92":2}}],["路径长度为",{"5":{"92":2}}],["路径长度是路径中边的数量",{"5":{"92":2}}],["路径长度指的是信息从一个位置传递到另一个位置所需经过的中间节点数",{"5":{"92":2}}],["路径传递到前一层",{"5":{"41":1}}],["路径深度",{"5":{"41":2}}],["路径归一化确保所有路径的深度相近",{"5":{"41":2}}],["路径归一化",{"5":{"41":2}}],["路径归一化与深度网络稳定性",{"5":{"41":2}}],["恒等激活",{"5":{"44":2}}],["推导和性质",{"5":{"91":2}}],["推导而来",{"5":{"61":2}}],["推断",{"5":{"88":2}}],["推理时需要处理的位置为",{"5":{"89":1}}],["推理时需要处理的位置为​",{"5":{"89":1}}],["推理",{"5":{"44":2}}],["推论2",{"5":{"44":5,"45":2}}],["推力",{"5":{"69":2}}],["推力的大小与对该负样本的关注程度成正比",{"5":{"69":1}}],["推力的大小与",{"5":{"69":1}}],["推拉",{"5":{"69":2}}],["推开",{"5":{"69":2}}],["杂志上发表了关于反向传播算法的开创性论文",{"5":{"44":2}}],["威廉姆斯",{"5":{"44":2}}],["鲁梅尔哈特",{"5":{"44":2}}],["辛顿",{"5":{"44":2}}],["杰弗里",{"5":{"44":2}}],["应当与内容编码保持适当的数学关系",{"5":{"91":2}}],["应用rope时",{"5":{"88":1}}],["应用softmax和value加权",{"5":{"88":1}}],["应用softmax和value加权策略二",{"5":{"88":1}}],["应用sigmoid",{"5":{"42":2}}],["应用于位置",{"5":{"88":2}}],["应用链式法则",{"5":{"61":2}}],["应用",{"5":{"61":2}}],["应该设置为略大于训练时的最大序列长度",{"5":{"89":2}}],["应该增加正则化或收集更多数据",{"5":{"51":2}}],["应该增加模型容量",{"5":{"51":2}}],["应该最大化",{"5":{"71":1}}],["控制着注意力分布的",{"5":{"95":1}}],["控制",{"5":{"71":1}}],["控制相似度分数的",{"5":{"69":2}}],["解释了为什么纯注意力机制需要位置编码来感知序列顺序",{"5":{"91":2}}],["解释了为什么低维度使用低频",{"5":{"90":2}}],["解释为伯努利分布的成功概率",{"5":{"71":1}}],["解决方案",{"5":{"44":1}}],["解这个不等式得到两个解",{"5":{"41":2}}],["解码器可以看到前缀的所有token",{"5":{"99":2,"101":2}}],["解析解",{"5":{"99":2,"101":2}}],["宽度为",{"5":{"71":1}}],["宽度为的全连接网络",{"5":{"71":1}}],["能力",{"5":{"91":2}}],["能量集中在有限的频率点",{"5":{"90":2}}],["能量集中",{"5":{"90":2}}],["能量",{"5":{"90":2}}],["能够处理各种类型的依赖关系",{"5":{"92":2}}],["能够处理训练时未见过的长度",{"5":{"92":2}}],["能够充分利用内存带宽",{"5":{"92":2}}],["能够建立跨整个序列的长程关联",{"5":{"92":2}}],["能够同时捕获全局和局部的位置特征",{"5":{"91":2}}],["能够精确区分相近的位置",{"5":{"91":2}}],["能够精细区分相近的位置",{"5":{"90":4}}],["能够编码位置的整体结构",{"5":{"91":2}}],["能够唯一表示",{"5":{"90":1}}],["能够唯一表示个位置的最小频率间隔",{"5":{"90":1}}],["能够覆盖整个长序列",{"5":{"90":2}}],["能够覆盖较长的位置范围",{"5":{"90":2}}],["能够区分非常接近的位置",{"5":{"90":2}}],["能够区分非常远的位置",{"5":{"88":2}}],["能够区分相邻位置",{"5":{"88":2}}],["能够捕捉长距离的位置依赖",{"5":{"88":2}}],["能够捕获输入之间的复杂依赖关系",{"5":{"71":1}}],["能够在输入空间中形成",{"5":{"71":1}}],["能够更好地捕获位置",{"5":{"99":1,"101":1}}],["能区分任意两点",{"5":{"71":2}}],["偶数维度",{"5":{"71":2}}],["双精度浮点数的最大值约为",{"5":{"61":2}}],["双重非线性",{"5":{"71":2}}],["双曲正切函数",{"5":{"42":4,"47":2}}],["双向线性注意力",{"5":{"86":2}}],["双向",{"5":{"99":2,"101":2}}],["普遍选择gelu作为",{"5":{"71":2}}],["估计和最大化互信息",{"5":{"71":2}}],["参考",{"5":{"61":2}}],["参见第六章",{"5":{"71":4}}],["参见第四章",{"5":{"71":2}}],["参数随时间演化的梯度流方程为",{"5":{"97":1}}],["参数通常表示序列长度",{"5":{"95":1}}],["参数通过梯度下降优化",{"5":{"86":1}}],["参数更新",{"5":{"97":2}}],["参数更新方向几乎相同",{"5":{"93":2}}],["参数更新幅度过大导致模型权重溢出",{"5":{"41":2}}],["参数数量为",{"5":{"93":2,"94":2}}],["参数数量与维度配置",{"2":{"93":1},"5":{"93":1}}],["参数效率为100",{"5":{"91":2}}],["参数效率与实践考量",{"2":{"89":1},"5":{"89":1}}],["参数由数学公式确定",{"5":{"91":2}}],["参数节省显著",{"5":{"89":2}}],["参数化为低秩分解",{"5":{"89":1}}],["参数化的灵活性是可学习编码的主要优势",{"5":{"89":2}}],["参数化投影",{"5":{"69":1}}],["参数量为",{"5":{"89":1}}],["参数量为​",{"5":{"89":1}}],["参数量从",{"5":{"89":2}}],["参数量从减少到",{"5":{"89":2}}],["参数量约为",{"5":{"89":4}}],["参数压缩",{"5":{"89":2}}],["参数变换",{"5":{"87":2}}],["参数复杂度从",{"5":{"50":1}}],["参数复杂度从降低到",{"5":{"50":1}}],["参数高效微调和特定网络结构的设计",{"5":{"50":2}}],["参数高效微调以及模型蒸馏等场景中具有重要应用",{"5":{"50":2}}],["参数优化和性能评估的全过程",{"5":{"96":2}}],["参数优化需要计算损失函数对各参数的梯度",{"5":{"44":2}}],["参数梯度可以通过误差信号与前向传播保存的激活值计算",{"5":{"44":2}}],["参数梯度计算",{"5":{"44":2}}],["参数梯度的完整推导",{"2":{"44":1},"5":{"44":1}}],["参数",{"5":{"41":1,"71":1,"86":1,"95":1,"97":1}}],["参数可以通过反向传播自动学习",{"5":{"41":1}}],["参数的后验分布为",{"5":{"96":2}}],["参数的后验分布与似然函数和先验的乘积成正比",{"5":{"71":1}}],["参数的更新也会变得非常缓慢",{"5":{"70":2}}],["参数依赖的",{"5":{"70":1}}],["参与前向传播计算",{"5":{"99":2,"101":2}}],["硬",{"5":{"69":4,"71":2,"87":2,"92":2,"95":2}}],["硬件实现",{"5":{"45":1}}],["软性",{"5":{"61":2}}],["软",{"5":{"70":2,"71":2,"87":2}}],["软选择保留了所有位置的信息",{"5":{"92":2}}],["软选择",{"5":{"71":2,"92":2}}],["软化",{"5":{"69":2,"87":2}}],["软infonce",{"5":{"69":2}}],["软正样本分布",{"5":{"69":2}}],["软对比",{"5":{"69":6}}],["软最大化",{"5":{"96":2}}],["软最大",{"5":{"69":4}}],["软最优策略",{"5":{"99":2,"101":2}}],["限制了它在分类任务中的应用",{"5":{"97":2}}],["限制了可分析的频率范围和频率分辨率",{"5":{"90":1}}],["限制了过拟合的风险",{"5":{"89":2}}],["限制了输出值的范围",{"5":{"71":2}}],["限制梯度的范数以防止梯度爆炸",{"5":{"48":2}}],["限制在",{"5":{"71":1}}],["限制在区间内",{"5":{"71":1}}],["限制策略更新的幅度",{"5":{"99":2,"101":2}}],["成为现代大语言模型的核心计算单元",{"5":{"95":2}}],["成为大语言模型中不可或缺的数学基础",{"5":{"95":2}}],["成为",{"5":{"92":1}}],["成为transformer架构中的经典设计",{"5":{"91":2}}],["成为理解损失函数理论的理想切入点",{"5":{"51":2}}],["成反比",{"5":{"90":1}}],["成分",{"5":{"87":2}}],["成正比",{"5":{"44":1,"69":2,"70":3}}],["成功概率",{"5":{"71":1}}],["成功概率的对数几率",{"5":{"71":1}}],["成立时",{"5":{"61":1}}],["成立",{"5":{"50":1,"61":4,"71":1,"87":2,"90":2,"96":2}}],["已被证明在许多任务上优于单一核函数的使用",{"5":{"93":2}}],["已有的编码从缓存读取",{"5":{"89":2}}],["已有理论结果表明",{"5":{"71":2}}],["已在2",{"5":{"44":2}}],["已成为llama等现代大语言模型的默认选择",{"5":{"41":2}}],["已经与",{"5":{"69":1}}],["优化难度",{"5":{"97":1}}],["优化算法可能收敛到不同的局部最优解",{"5":{"97":2}}],["优化收敛变慢",{"5":{"97":2}}],["优化景观的几何分析",{"2":{"97":1},"5":{"97":1}}],["优化过程更加自由和稳定",{"5":{"96":2}}],["优化过程可以调整所有位置的重要性",{"5":{"92":2}}],["优化过程变得更加稳定",{"5":{"96":2}}],["优化模型效率",{"5":{"87":2}}],["优化目标",{"5":{"61":2}}],["优化目标就是最小化",{"5":{"96":1}}],["优化目标就是最小化或",{"5":{"96":1}}],["优化的目标是最小化这个距离",{"5":{"51":2}}],["优化趋于收敛",{"5":{"69":2}}],["优化轨迹是线性的",{"5":{"70":2}}],["优化变慢",{"5":{"70":4,"97":2}}],["优化器的更新方向为",{"5":{"70":2}}],["优化相对简单",{"5":{"70":2}}],["优化更加灵活高效",{"5":{"42":2}}],["优化更复杂但梯度特性更好",{"5":{"70":2}}],["优雅的注意力计算范式",{"5":{"95":2}}],["优雅",{"5":{"71":2}}],["光滑的损失函数",{"5":{"51":2}}],["光滑",{"5":{"71":2}}],["历史上",{"5":{"71":2}}],["抑制区",{"5":{"71":2}}],["功能",{"5":{"71":2}}],["支持向量机",{"5":{"71":2}}],["折线或更复杂的形状",{"5":{"71":2}}],["折叠",{"5":{"45":2,"90":2,"91":2}}],["图像等数据中充满了非线性模式",{"5":{"71":2}}],["曲率和优化难度上的差异",{"5":{"97":2}}],["曲率条件数",{"5":{"97":2}}],["曲率与逃逸动力学",{"5":{"97":2}}],["曲率特性",{"5":{"70":2}}],["曲面",{"5":{"71":2}}],["曲面或更复杂的几何结构",{"5":{"71":2}}],["曲线几乎为常数",{"5":{"91":2}}],["曲线也趋于水平",{"5":{"42":2}}],["曲线趋于水平",{"5":{"42":2}}],["曲线趋近于渐近线",{"5":{"41":4}}],["曲线上凸",{"5":{"42":2}}],["曲线下凸",{"5":{"42":2}}],["曲线在该点由上凸转为下凸",{"5":{"42":2}}],["曲线两端的平坦性正是梯度饱和的几何表现",{"5":{"41":2}}],["调整",{"5":{"89":2}}],["调整多少超参数",{"5":{"71":2}}],["调整位置",{"5":{"71":2}}],["调整参数以使逼近目标函数",{"5":{"45":1}}],["调整参数",{"5":{"45":1}}],["万有逼近定理",{"5":{"71":2}}],["局部极小值的质量",{"5":{"97":2}}],["局部极小值的质量分析",{"5":{"97":2}}],["局部的",{"5":{"93":2}}],["局部性",{"5":{"92":2,"93":2}}],["局部函数",{"5":{"92":2}}],["局部注意力适合捕获短语结构",{"5":{"92":2}}],["局部注意力类似于卷积操作",{"5":{"92":2}}],["局部注意力满足",{"5":{"92":2}}],["局部注意力指的是注意力权重集中在相邻或接近的位置",{"5":{"92":2}}],["局部注意力",{"5":{"92":2}}],["局部注意力与全局注意力",{"2":{"92":1},"5":{"92":1}}],["局部",{"5":{"92":4}}],["局部稀疏",{"5":{"87":2}}],["局部位置只与邻近位置交互",{"5":{"86":2}}],["局部混合注意力",{"5":{"86":2}}],["局部最小值可能不是全局最小值",{"5":{"51":2}}],["局部最小值的等价性是大规模深度学习中的一个有趣现象",{"5":{"48":2}}],["局部曲面叠加",{"5":{"71":2}}],["局部探测器",{"5":{"71":2}}],["局部基函数",{"5":{"71":2}}],["局部基函数与函数重构",{"2":{"71":1},"5":{"71":1}}],["增大而减小",{"5":{"90":1}}],["增大时",{"5":{"42":1,"92":1}}],["增长率为频率",{"5":{"90":1}}],["增长率为频率​",{"5":{"90":1}}],["增长",{"5":{"86":1,"95":1}}],["增强了模型的表达能力",{"5":{"93":2}}],["增强表达能力",{"5":{"86":2}}],["增强对困难负样本的关注",{"5":{"69":2}}],["增加了生成的多样性",{"5":{"96":2}}],["增加了表示的丰富性",{"5":{"94":2}}],["增加了函数的",{"5":{"41":2}}],["增加",{"5":{"93":2}}],["增加头数不增加计算成本",{"5":{"93":2}}],["增加时",{"5":{"91":1}}],["增加1时",{"5":{"90":1}}],["增加模型复杂度可以显著降低mse",{"5":{"51":2}}],["增加到",{"5":{"70":2}}],["增加对",{"5":{"99":2,"101":2}}],["构成标准正交基",{"5":{"50":2}}],["构成的函数族在适当条件下可以满足这些要求",{"5":{"71":1}}],["构成了现代神经网络激活函数的主流选择",{"5":{"42":2}}],["补",{"5":{"42":6}}],["浪费计算资源",{"5":{"42":2}}],["钟形",{"5":{"42":2}}],["证毕",{"5":{"42":8}}],["证明注意力机制在理论上具有强大的长程依赖建模能力",{"5":{"92":2}}],["证明了点积的方差随维度",{"5":{"95":1}}],["证明了点积的方差随维度线性增长",{"5":{"95":1}}],["证明了不同位置具有不同的编码向量",{"5":{"91":2}}],["证明了1始终是特征值",{"5":{"87":2}}],["证明了它可以分解为偏差平方",{"5":{"51":2}}],["证明了在高斯噪声假设下",{"5":{"51":2}}],["证明了激活函数对于突破神经网络线性瓶颈的关键作用",{"5":{"47":2}}],["证明了通过错误信号的反向传播可以有效训练多层神经网络",{"5":{"44":2}}],["证明",{"5":{"42":8,"44":24,"45":26,"46":8,"47":14,"88":2,"99":2,"101":2}}],["证明思路",{"5":{"69":2}}],["天然具有概率解释",{"5":{"42":2}}],["稳定性和最终性能",{"5":{"42":2}}],["否则会引入虚假信息",{"5":{"95":2}}],["否则可能失败",{"5":{"89":2}}],["否则",{"5":{"89":2}}],["否则形状不兼容",{"5":{"50":2}}],["否则为0",{"5":{"96":2}}],["否",{"5":{"41":1}}],["很高",{"5":{"97":1}}],["很大时",{"5":{"51":1,"69":1}}],["很大而没有进行适当的缩放",{"5":{"41":1}}],["很大",{"5":{"70":1}}],["很小",{"5":{"41":1}}],["很相似",{"5":{"69":1}}],["针对特定的损失函数",{"5":{"101":2}}],["针对特定任务",{"5":{"41":2}}],["针对文档级别的长上下文设计",{"5":{"88":1}}],["针对relu激活函数进行了优化",{"5":{"41":2}}],["程度",{"5":{"41":2,"61":2,"94":2}}],["弯曲",{"5":{"41":2}}],["虽然标准的transformer架构没有显式地使用温度调度",{"5":{"95":2}}],["虽然transformer中没有使用relu",{"5":{"94":2}}],["虽然总参数数量不变",{"5":{"93":2}}],["虽然投影矩阵在结构上是分块对角的",{"5":{"93":2}}],["虽然注意力的渐进复杂度",{"5":{"92":1}}],["虽然注意力的渐进复杂度高于rnn的",{"5":{"92":1}}],["虽然通过堆叠多层卷积可以扩大感受野",{"5":{"92":2}}],["虽然隐含在编码中",{"5":{"91":2}}],["虽然位置编码不需要实际计算fft",{"5":{"90":2}}],["虽然rope理论上支持任意长度的外推",{"5":{"88":2}}],["虽然某些语言现象确实需要绝对位置",{"5":{"88":2}}],["虽然复数提供了优雅的数学语言",{"5":{"88":2}}],["虽然正弦函数的差可以表示为另一个正弦函数",{"5":{"88":2}}],["虽然相对位置是注意力计算的核心",{"5":{"88":2}}],["虽然词汇相同",{"5":{"88":2}}],["虽然",{"5":{"87":1}}],["虽然理论上注意力矩阵的秩不超过",{"5":{"87":2}}],["虽然的计算结果是一个的矩阵",{"5":{"87":1}}],["虽然计算高效",{"5":{"86":2}}],["虽然自然语言处理很少直接使用傅里叶变换",{"5":{"50":2}}],["虽然在大规模语言模型中sigmoid已被relu及其变体取代",{"5":{"42":2}}],["虽然训练高效",{"5":{"41":2}}],["虽然这看起来是显然的",{"5":{"92":2}}],["虽然这里的",{"5":{"61":2}}],["虽然这种传递不是直接的",{"5":{"92":2}}],["虽然这种改变不是直接作用于注意力权重",{"5":{"99":2,"101":2}}],["虽然均方误差和交叉熵源自完全不同的数学框架",{"5":{"70":2}}],["虽然两种损失函数源自不同的理论框架",{"5":{"70":2}}],["效果相当",{"5":{"41":2}}],["效应量",{"5":{"96":2}}],["效应",{"5":{"41":2}}],["效应使得非常深的网络成为可能",{"5":{"41":2}}],["效用",{"5":{"69":2}}],["节省计算",{"5":{"93":2}}],["节省了约30倍的内存带宽",{"5":{"86":2}}],["节省了加法和除法操作",{"5":{"41":2}}],["节点",{"5":{"45":2}}],["节点的值只依赖于其前驱节点",{"5":{"45":2}}],["节",{"5":{"70":2}}],["减小了协变量偏移",{"5":{"41":1}}],["减小了协变量偏移post",{"5":{"41":1}}],["减少到",{"5":{"50":1,"87":2,"89":3,"93":1}}],["减少了过拟合的风险",{"5":{"88":2}}],["减少了协变量偏移问题",{"5":{"42":2}}],["减少了梯度偏移问题",{"5":{"42":2}}],["减少了层与层之间的依赖关系",{"5":{"41":2}}],["减少内部协变量偏移",{"5":{"41":2}}],["减少对",{"5":{"99":2,"101":2}}],["除非",{"5":{"87":2}}],["除非有平移分量",{"5":{"71":2}}],["除了计算复杂度",{"5":{"92":2}}],["除了softmax",{"2":{"61":1},"5":{"61":1}}],["除了前面介绍的奇异值分解",{"5":{"50":2}}],["除了在极端负值区域",{"5":{"71":2}}],["除最大特征值外其余特征值趋近于零",{"5":{"41":2}}],["死神经元问题",{"5":{"41":1}}],["死神经元",{"5":{"41":4,"71":2}}],["瓶颈",{"5":{"41":2}}],["捷径",{"5":{"41":2,"92":2}}],["放大或保持稳定",{"5":{"41":2}}],["裁剪后的有效学习率为",{"5":{"97":2}}],["裁剪后的梯度范数始终不超过",{"5":{"44":2}}],["裁剪后的梯度方向与原梯度方向相同",{"5":{"41":2}}],["裁剪阈值",{"5":{"41":1}}],["裁剪阈值的选择是一个超参数调优问题",{"5":{"41":1}}],["裁剪阈值的选择",{"5":{"41":2}}],["裁剪",{"5":{"99":2,"101":2}}],["裁剪操作确保当偏离1太远时",{"5":{"99":1,"101":1}}],["裁剪操作确保当",{"5":{"99":1,"101":1}}],["全覆盖",{"5":{"88":2}}],["全连接假设",{"5":{"92":2}}],["全连接结构与信息直达",{"2":{"92":1},"5":{"92":1}}],["全连接注意力模式等",{"5":{"86":2}}],["全连接层就是典型的线性变换加上非线性激活函数的组合",{"5":{"50":2}}],["全连接层",{"5":{"46":4,"61":2}}],["全连接层是神经网络中最基本的层类型",{"5":{"46":2}}],["全连接层的数学定义为",{"5":{"46":2}}],["全连接层的计算展开",{"5":{"46":2}}],["全连接层的矩阵运算等价于对每个输出神经元进行独立的仿射变换计算",{"5":{"46":2}}],["全连接层的矩阵表示是多个神经元并行计算的紧凑写法",{"5":{"46":2}}],["全连接层可以统一表示为单一的矩阵乘法",{"5":{"46":2}}],["全连接",{"5":{"46":2}}],["全局函数",{"5":{"92":2}}],["全局注意力适合捕获跨越长距离的语义关联",{"5":{"92":2}}],["全局注意力满足",{"5":{"92":2}}],["全局注意力指的是注意力权重分布在整个序列上",{"5":{"92":2}}],["全局注意力",{"5":{"92":2}}],["全局稀疏",{"5":{"87":2}}],["全局位置",{"5":{"86":2}}],["全局",{"5":{"86":2,"92":2}}],["全局梯度裁剪将梯度的范数裁剪到一个固定的上界",{"5":{"41":2,"97":2}}],["全局梯度裁剪",{"5":{"41":2}}],["全局上下文",{"5":{"99":2,"101":2}}],["残差",{"5":{"89":2}}],["残差部分",{"5":{"51":2}}],["残差路径的梯度可以直接通过恒等连接传递",{"5":{"41":2}}],["残差路径的恒等保证",{"5":{"41":2}}],["残差的梯度下界",{"5":{"41":2}}],["残差的梯度流分析",{"5":{"41":2}}],["残差连接确保了单位矩阵",{"5":{"92":1}}],["残差连接确保了单位矩阵的存在",{"5":{"92":1}}],["残差连接提供了一条梯度传播的",{"5":{"92":2}}],["残差连接提供了一条",{"5":{"92":2}}],["残差连接保留的原始信息",{"5":{"92":2}}],["残差连接与梯度流动",{"2":{"92":1},"5":{"92":1}}],["残差连接是向量加法的典型应用",{"5":{"50":2}}],["残差连接的数学分析展示了它如何通过恒等路径保证梯度流的稳定",{"5":{"41":2}}],["残差连接的数学作用",{"2":{"41":1},"5":{"41":1}}],["残差连接的核心数学保证是",{"5":{"41":2}}],["残差连接定义为",{"5":{"41":2}}],["残差连接",{"5":{"41":6,"44":1,"92":2}}],["循环神经网络的梯度流问题",{"2":{"92":1},"5":{"92":1}}],["循环神经网络",{"5":{"41":2,"91":2}}],["循环层",{"5":{"46":2}}],["僵尸状态",{"5":{"41":2}}],["斜率的最大值出现在",{"5":{"42":1}}],["斜率的最大值出现在处",{"5":{"42":1}}],["斜率同样趋近于零",{"5":{"41":2}}],["斜率趋近于零",{"5":{"41":2}}],["揭示了不同损失函数在全局最优性",{"5":{"97":2}}],["揭示了两者在结构上的相似性和差异性",{"5":{"93":2}}],["揭示了一个重要的数学事实",{"5":{"93":2}}],["揭示了局部注意力与全局注意力的区别",{"5":{"92":2}}],["揭示了传统循环结构在处理长距离依赖时的根本性困难",{"5":{"92":2}}],["揭示了模型捕获的依赖类型",{"5":{"92":2}}],["揭示了点积",{"5":{"90":1}}],["揭示了点积​作为相对位置的函数",{"5":{"90":1}}],["揭示了编码向量可以视为不同频率正弦波的叠加",{"5":{"90":2}}],["揭示了编码矩阵的秩不超过嵌入维度",{"5":{"89":1}}],["揭示了编码矩阵的秩不超过嵌入维度​这一重要性质",{"5":{"89":1}}],["揭示了mse的最优解对应于目标向量在预测空间上的正交投影",{"5":{"51":2}}],["揭示了大语言模型中非线性变换的数学本质",{"5":{"71":2}}],["揭示了激活函数导数特性与饱和现象的内在联系",{"5":{"41":2}}],["揭示了",{"5":{"70":2}}],["揭示其低秩结构的成因和理论含义",{"5":{"87":2}}],["揭示其作为最小二乘估计量的数学本质",{"5":{"51":2}}],["揭示其作为",{"5":{"71":2}}],["揭示其与网络架构和初始化策略的关系",{"5":{"41":2}}],["揭示其与激活函数导数特性的内在联系",{"5":{"41":2}}],["揭示这些技术如何从根本上改善深层网络的训练动态",{"5":{"41":2}}],["揭示信息在网络中传递和保持的数学原理",{"5":{"41":2}}],["揭示它为何能够成为连接分类损失",{"5":{"69":2}}],["揭示它们与统计学和信息论之间的深层联系",{"5":{"71":2}}],["揭示它们与激活函数特性",{"5":{"41":2}}],["揭示它们的内在联系与本质区别",{"5":{"70":2}}],["导致下溢",{"5":{"97":2}}],["导致强凸性系数很小",{"5":{"97":2}}],["导致信息损失",{"5":{"94":2}}],["导致长程依赖难以建模",{"5":{"91":2}}],["导致不同位置的编码变得难以区分",{"5":{"88":2}}],["导致不同训练集产生截然不同的预测",{"5":{"51":2}}],["导致平移等变性不再严格成立",{"5":{"88":2}}],["导致内积同时受到两个绝对位置的影响",{"5":{"88":2}}],["导致数值计算不稳定",{"5":{"87":2}}],["导致溢出",{"5":{"86":1}}],["导致",{"5":{"61":4,"86":1}}],["导致系统性的预测偏差",{"5":{"51":2}}],["导致测试集上的实际mse可能与训练mse有所不同",{"5":{"51":2}}],["导致深层网络的训练困难",{"5":{"42":2}}],["导致表示过度平均",{"5":{"41":2}}],["导致softmax的输出接近one",{"5":{"41":2}}],["导致梯度消失问题",{"5":{"61":2,"92":2}}],["导致梯度消失",{"5":{"41":6,"44":2,"70":2}}],["导致梯度爆炸问题",{"5":{"92":2}}],["导致梯度爆炸",{"5":{"41":4}}],["导致梯度接近0",{"5":{"70":2}}],["导致参数更新幅度过大",{"5":{"41":2}}],["导致参数更新极其缓慢甚至停止",{"5":{"41":2}}],["导致这些层无法学习",{"5":{"41":2}}],["导致训练不稳定",{"5":{"87":2,"97":2}}],["导致训练开始时梯度就很小",{"5":{"41":2}}],["导致训练停滞",{"5":{"70":2}}],["导致有效梯度非常小",{"5":{"70":2}}],["导数表达式变得越来越复杂",{"5":{"42":2}}],["导数可以用函数自身来表示",{"5":{"42":2}}],["导数推导与性质分析",{"2":{"42":1},"5":{"42":1}}],["导数推导与自化性质",{"2":{"42":1},"5":{"42":1}}],["导数推导与梯度特性<",{"5":{"21":1,"85":1}}],["导数推导与梯度特性",{"0":{"42":1},"4":{"42":1},"5":{"21":4,"42":1,"85":4}}],["导数为常数1",{"5":{"41":2}}],["导数最小值接近0而非0",{"5":{"41":2}}],["导数达到最大值0",{"5":{"41":2}}],["导数达到最大值而非最小值",{"5":{"41":2}}],["导数趋近于零",{"5":{"41":4}}],["导数的值取决于",{"5":{"41":1}}],["导数的值取决于和的乘积",{"5":{"41":1}}],["导数的调制",{"5":{"70":6}}],["导数",{"5":{"70":2}}],["保留了更多的表达能力",{"5":{"93":2}}],["保留了内容编码和位置编码的相对重要性",{"5":{"91":2}}],["保留前64",{"5":{"89":2}}],["保留关于目标",{"5":{"71":1}}],["保留关于目标的信息",{"5":{"71":1}}],["保证质量",{"5":{"93":2}}],["保证其可以作为神经网络的输出层进行端到端优化",{"5":{"61":1}}],["保证这是一个有效的概率分布",{"5":{"61":2}}],["保证了求和结果至少为",{"5":{"61":2}}],["保证二次可导和光滑性",{"5":{"51":2}}],["保证梯度计算的可操作性",{"5":{"47":2}}],["保证网络的表达能力",{"5":{"47":2}}],["保存所有层的",{"5":{"44":1}}],["保存所有层的和",{"5":{"44":1}}],["保持在较大的值",{"5":{"95":1}}],["保持在合理的分布范围内",{"5":{"94":1}}],["保持核心计算不变",{"5":{"95":2}}],["保持了跨不同维度模型的可比性",{"5":{"95":2}}],["保持维度不变",{"5":{"94":2}}],["保持完整",{"5":{"88":1}}],["保持梯度流稳定性",{"2":{"41":1},"5":{"41":1}}],["保持模型生成的多样性",{"5":{"99":2,"101":2}}],["饱和状态",{"5":{"95":2}}],["饱和的网络层叠效应",{"5":{"41":2}}],["饱和效应与权重初始化密切相关",{"5":{"41":2}}],["饱和效应会逐层累积放大",{"5":{"41":2}}],["饱和效应的累积放大机制",{"2":{"41":1},"5":{"41":1}}],["饱和行为呈现对称分布",{"5":{"41":2}}],["饱和深度的量化分析",{"5":{"41":2}}],["饱和区域",{"5":{"44":2}}],["饱和区域为",{"5":{"41":6}}],["饱和区域可以精确计算",{"5":{"41":2}}],["饱和现象的数学定义",{"2":{"41":1},"5":{"41":1}}],["1^t",{"5":{"97":2}}],["1傅里叶分析基础",{"2":{"90":1},"5":{"90":1}}],["1矩阵的和来描述",{"5":{"89":2}}],["1始终是特征值",{"5":{"87":2}}],["17",{"2":{"92":1,"93":1},"5":{"46":2,"92":1,"93":1}}],["16",{"2":{"87":1,"92":1,"93":1},"5":{"46":2,"87":1,"92":1,"93":1}}],["18",{"2":{"87":1,"92":1,"93":1},"5":{"42":2,"46":2,"86":2,"87":1,"92":1,"93":1}}],["1948年",{"5":{"61":2}}],["1943年",{"5":{"47":2}}],["1958年",{"5":{"47":2}}],["19",{"2":{"87":1,"92":1,"93":1},"5":{"42":2,"47":8,"48":4,"87":1,"92":1,"93":1}}],["1986年",{"5":{"44":2}}],["14",{"2":{"51":1,"87":1,"92":1,"93":1,"95":1},"5":{"41":2,"51":1,"87":1,"92":1,"93":1,"95":1,"97":2,"99":2,"101":2}}],["1",{"0":{"47":1,"48":1,"50":2,"51":1,"71":1,"91":1,"95":1,"96":1},"2":{"41":1,"42":1,"44":1,"45":1,"46":1,"47":8,"48":4,"50":27,"51":16,"61":1,"69":1,"70":1,"71":7,"86":1,"87":1,"88":1,"89":1,"91":7,"92":1,"93":1,"94":1,"95":15,"96":9,"97":1,"99":1,"101":1},"4":{"47":1,"48":1,"50":2,"51":1,"71":1,"91":1,"95":1,"96":1},"5":{"21":27,"41":11,"42":45,"44":19,"45":9,"46":27,"47":67,"48":9,"50":35,"51":17,"61":35,"69":9,"70":15,"71":40,"85":42,"86":3,"87":1,"88":5,"89":1,"91":18,"92":1,"93":5,"94":1,"95":28,"96":10,"97":22,"99":7,"101":7}}],["10步前的位置",{"5":{"92":2}}],["10",{"2":{"50":1,"51":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1},"5":{"41":2,"44":2,"45":2,"47":2,"50":1,"51":1,"71":2,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"97":2,"99":2,"101":2}}],["10^",{"5":{"46":2}}],["100",{"5":{"70":2}}],["11",{"2":{"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1},"5":{"41":2,"44":2,"45":2,"46":4,"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"97":2,"99":2,"101":2}}],["128个奇异值通常能够达到接近原始编码的性能",{"5":{"89":2}}],["12",{"2":{"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1},"5":{"41":2,"44":2,"45":2,"46":2,"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"99":2,"101":2}}],["12288",{"5":{"46":4}}],["13",{"2":{"50":1,"51":1,"87":1,"92":1,"93":1,"95":1},"5":{"41":2,"45":2,"46":2,"50":1,"51":1,"86":2,"87":1,"92":1,"93":1,"95":1,"99":2,"101":2}}],["1节的分析",{"5":{"89":2}}],["1节的scaled",{"5":{"69":2}}],["1节中引入的缩放因子",{"5":{"94":1}}],["1节中引入的缩放因子​​",{"5":{"94":1}}],["1节中给出",{"5":{"94":2}}],["1节中看到的",{"5":{"46":2}}],["1节中定义的单个神经元的数学形式",{"5":{"46":2}}],["1节和2",{"5":{"45":2}}],["1节和4",{"5":{"69":2}}],["15",{"2":{"51":1,"86":1,"87":1,"92":1,"93":1},"5":{"46":2,"51":1,"86":1,"87":1,"92":1,"93":1,"97":2,"99":2,"101":2}}],["1到6",{"5":{"96":2}}],["1到0",{"5":{"95":2,"99":2,"101":2}}],["1个正样本",{"5":{"69":3}}],["1个正样本个负样本",{"5":{"69":1}}],["1的简化形式",{"5":{"44":2}}],["1的预测误差和0",{"5":{"70":2}}],["1比预测0",{"5":{"70":2}}],["1损失",{"5":{"70":2}}],["2^t",{"5":{"97":2}}],["2等现代大语言模型就采用了分组查询注意力配置",{"5":{"93":2}}],["2采用了rope的变体",{"5":{"88":1}}],["2和llama",{"5":{"88":2}}],["2的主要改进包括",{"5":{"86":2}}],["2与flash注意力",{"2":{"86":1},"5":{"86":1}}],["28",{"5":{"42":2,"61":2}}],["26",{"5":{"42":2,"61":2}}],["24",{"2":{"87":1,"92":1},"5":{"42":2,"46":2,"61":2,"87":1,"92":1}}],["23",{"2":{"87":1,"92":1},"5":{"42":2,"46":2,"61":2,"87":1,"92":1}}],["22",{"2":{"87":1,"92":1},"5":{"42":2,"46":2,"61":2,"87":1,"92":1,"97":2}}],["21",{"2":{"87":1,"92":1},"5":{"42":2,"46":2,"87":1,"92":1}}],["2021年su等人发表rope论文后短短几年内",{"5":{"88":2}}],["20",{"2":{"87":1,"92":1},"5":{"42":2,"46":2,"87":1,"92":1,"97":2}}],["29",{"5":{"42":2,"61":2,"69":2}}],["27",{"5":{"42":2}}],["25",{"2":{"92":1},"5":{"41":4,"42":6,"61":2,"92":1}}],["2",{"0":{"42":1,"44":1,"45":1,"46":2,"47":1,"61":1,"90":1,"94":1,"96":1},"2":{"41":1,"42":4,"44":10,"45":8,"46":7,"47":8,"48":1,"50":1,"51":1,"61":8,"69":1,"70":1,"71":1,"86":1,"87":1,"88":1,"89":1,"90":8,"91":1,"92":1,"93":1,"94":13,"95":1,"96":9,"97":1,"99":1,"101":1},"4":{"42":1,"44":1,"45":1,"46":2,"47":1,"61":1,"90":1,"94":1,"96":1},"5":{"21":31,"41":9,"42":97,"44":27,"45":17,"46":77,"47":19,"48":1,"50":1,"51":1,"61":39,"69":9,"70":5,"71":7,"85":46,"86":1,"87":3,"88":6,"89":1,"90":9,"91":1,"92":1,"93":3,"94":14,"95":1,"96":10,"97":7,"99":9,"101":9}}],["2n",{"5":{"46":4}}],["2节中",{"5":{"45":2,"69":6}}],["2节中二元交叉熵的形式完全一致",{"5":{"99":2,"101":2}}],["2节的推导",{"5":{"99":2,"101":2}}],["3的96个头",{"5":{"93":2}}],["3的直接应用",{"5":{"44":2}}],["3则增加了头数到96个",{"5":{"93":2}}],["3中",{"5":{"88":2}}],["3采用的标准rope实现",{"5":{"88":2}}],["3针对hopper架构gpu",{"5":{"86":2}}],["3进一步引入了异步执行和tensor",{"5":{"86":2}}],["39",{"5":{"42":2}}],["38",{"5":{"42":2}}],["37",{"5":{"42":2}}],["36",{"5":{"42":2}}],["35",{"5":{"42":2}}],["34",{"5":{"42":2}}],["33",{"5":{"42":2}}],["32",{"5":{"42":2,"69":2}}],["31",{"5":{"42":2,"61":2,"69":2}}],["30",{"5":{"42":2,"61":2,"69":2}}],["3节中证明",{"5":{"44":2}}],["3节中",{"5":{"44":2,"69":2}}],["3",{"0":{"41":2,"42":1,"45":1,"48":1,"70":1,"71":1,"89":1,"93":1},"2":{"41":13,"42":4,"44":1,"45":8,"46":1,"47":1,"48":4,"50":1,"51":1,"61":1,"69":1,"70":7,"71":7,"86":2,"87":1,"88":1,"89":8,"90":1,"91":1,"92":1,"93":20,"94":1,"95":1,"96":1,"99":1,"101":1},"4":{"41":2,"42":1,"45":1,"48":1,"70":1,"71":1,"89":1,"93":1},"5":{"21":27,"41":73,"42":59,"44":11,"45":65,"46":5,"47":11,"48":5,"50":1,"51":1,"61":3,"69":3,"70":16,"71":12,"85":42,"86":2,"87":1,"88":4,"89":9,"90":1,"91":1,"92":1,"93":27,"94":1,"95":1,"96":1,"97":2,"99":5,"101":5}}],["3规模的模型",{"5":{"45":2}}],["第次谐波的角频率为​",{"5":{"90":1}}],["第行第列元素为",{"5":{"89":1}}],["第六章",{"2":{"85":1},"5":{"85":1}}],["第六章位置编码",{"5":{"71":2}}],["第6章",{"4":{"88":1,"89":1,"90":1,"91":1},"5":{"85":8}}],["第5章",{"4":{"86":1,"87":1,"92":1,"93":1,"94":1,"95":1},"5":{"85":12}}],["第4章",{"4":{"51":1,"61":1,"69":1,"70":1,"97":1,"98":1,"99":1,"100":1,"101":1},"5":{"85":12}}],["第五章",{"2":{"85":1},"5":{"85":1}}],["第五",{"5":{"44":2,"91":2}}],["第层偏置的梯度为",{"5":{"44":2}}],["第层权重的梯度为",{"5":{"44":2}}],["第层误差信号",{"5":{"44":1}}],["第层的感受野大小约为",{"5":{"92":1}}],["第层的误差信号矩阵为",{"5":{"44":1}}],["第层的计算定义为",{"5":{"44":1}}],["第层的计算为",{"5":{"41":1}}],["第层的输入为",{"5":{"46":1}}],["第3章",{"4":{"41":1,"42":1,"71":1},"5":{"21":6,"85":6}}],["第1章",{"4":{"48":1,"50":1,"96":1},"5":{"21":6,"85":6}}],["第一阶段是查询",{"5":{"95":2}}],["第一步是矩阵乘法",{"5":{"95":2}}],["第一种是掩码作用于注意力分数",{"5":{"91":2}}],["第一种是均匀注意力",{"5":{"87":2}}],["第一种情况直接给出",{"5":{"91":2}}],["第一是稀疏性",{"5":{"87":2}}],["第一是非负性",{"5":{"87":2}}],["第一项",{"5":{"91":3}}],["第一项是原始内容之间的注意力分数",{"5":{"91":1}}],["第一项最大化输出与目标的相关性",{"5":{"87":2}}],["第一项为",{"5":{"44":2}}],["第一个奇异值远大于其他",{"5":{"87":2}}],["第一个特征向量",{"5":{"87":2}}],["第一个神经元学习",{"5":{"46":2}}],["第一部分是真实分布",{"5":{"61":2}}],["第一",{"5":{"44":2,"45":2,"46":2,"47":2,"51":2,"61":2,"69":2,"70":2,"87":4,"88":4,"89":2,"90":2,"91":4,"92":2,"96":2}}],["第一章",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["第一层的输出是第二层的输入",{"5":{"93":2}}],["第一层学习到某种",{"5":{"92":2}}],["第一层有两个神经元",{"5":{"46":2}}],["第一层神经元1",{"5":{"46":2}}],["第一层神经元2",{"5":{"46":2}}],["第一层输出为",{"5":{"46":8}}],["第二阶段是缩放与归一化",{"5":{"95":2}}],["第二步是缩放与softmax归一化",{"5":{"95":2}}],["第二种是掩码同时作用于位置编码",{"5":{"91":2}}],["第二种是one",{"5":{"87":2}}],["第二种情况给出",{"5":{"91":2}}],["第二是数据依赖性",{"5":{"87":2}}],["第二是行随机性",{"5":{"87":2}}],["第二个向量取共轭",{"5":{"90":2}}],["第二个特征向量",{"5":{"87":2}}],["第二个神经元学习",{"5":{"46":2}}],["第二部分是kl散度",{"5":{"61":2}}],["第二项最小化输出与输入的冗余",{"5":{"87":2}}],["第二项",{"5":{"91":1}}],["第二项中",{"5":{"44":2}}],["第二项是内容与位置之间的交叉项",{"5":{"91":1}}],["第二",{"5":{"44":2,"45":2,"46":2,"47":2,"51":2,"61":2,"69":2,"70":2,"87":4,"88":4,"89":2,"90":2,"91":6,"92":2,"96":2}}],["第二章",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["第二层的输入为",{"5":{"93":2}}],["第二层有一个神经元",{"5":{"46":2}}],["第二层",{"5":{"46":2}}],["第二层输出为0",{"5":{"46":4}}],["第二层输出为1",{"5":{"46":4}}],["第三阶段是值聚合",{"5":{"95":2}}],["第三步是矩阵乘法与值聚合",{"5":{"95":2}}],["第三项",{"5":{"91":1}}],["第三项是位置与内容之间的交叉项",{"5":{"91":1}}],["第三项为对角矩阵",{"5":{"44":2}}],["第三是分辨率对偶",{"5":{"90":2}}],["第三是平移不变性",{"5":{"87":2}}],["第三是对称性的缺失",{"5":{"87":2}}],["第三章",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["第三",{"5":{"41":4,"44":2,"45":2,"46":2,"47":2,"48":4,"50":8,"51":2,"61":4,"69":4,"70":4,"71":2,"87":4,"88":2,"89":4,"90":4,"91":6}}],["第四项",{"5":{"91":3}}],["第四项是纯粹的位置",{"5":{"91":1}}],["第四章详细讨论的交叉熵损失",{"5":{"71":1}}],["第四章详细讨论的交叉熵损失正是衡量两个概率分布",{"5":{"71":1}}],["第四章损失函数",{"5":{"71":2}}],["第四章",{"2":{"85":1},"5":{"85":1}}],["第四",{"5":{"44":2,"45":2,"47":2,"48":2,"50":2,"69":2,"70":2,"88":2,"91":2}}],["第2章",{"4":{"44":1,"45":1,"46":1,"47":1},"5":{"21":8,"85":8}}],["第2",{"5":{"45":2}}],["第",{"5":{"41":1,"44":7,"45":1,"46":1,"89":1,"90":1,"92":1,"99":1,"101":1}}],["第个样本的第个输出",{"5":{"45":1}}],["第个token的分布仅依赖于其前面的token",{"5":{"99":1,"101":1}}],["4倍的加速",{"5":{"86":2}}],["42",{"5":{"42":2}}],["41",{"5":{"42":2}}],["40",{"5":{"42":2}}],["4",{"0":{"44":1,"51":1,"61":1,"69":2,"70":1,"88":1,"92":1,"97":1,"99":1,"101":1},"2":{"41":1,"44":10,"45":1,"47":1,"50":1,"51":16,"61":8,"69":9,"70":7,"71":1,"86":1,"87":1,"88":11,"89":1,"90":1,"91":1,"92":26,"93":1,"94":1,"95":1,"96":1,"97":4,"99":7,"101":8},"4":{"44":1,"51":1,"61":1,"69":2,"70":1,"88":1,"92":1,"97":1,"98":1,"99":1,"100":1,"101":1},"5":{"21":5,"41":7,"42":4,"44":69,"45":5,"46":4,"47":7,"50":1,"51":17,"61":39,"69":53,"70":18,"71":3,"85":44,"86":1,"87":1,"88":15,"89":1,"90":1,"91":1,"92":27,"93":1,"94":1,"95":1,"96":1,"97":27,"99":46,"101":47}}],["4节中",{"5":{"45":2}}],["4节详细讨论",{"5":{"45":2}}],["4节",{"5":{"45":2,"99":4,"101":4}}],["4的递归形式可得",{"5":{"46":2}}],["4n",{"5":{"46":4}}],["49152",{"5":{"46":4}}],["58",{"5":{"91":2}}],["54",{"5":{"91":2}}],["5x",{"5":{"42":2}}],["5为中心",{"5":{"42":2}}],["5附近时变化最快",{"5":{"42":2}}],["5",{"0":{"86":1,"87":2,"92":1,"93":1,"94":1,"95":1,"99":1,"101":1},"2":{"41":1,"44":1,"45":1,"47":1,"50":1,"51":1,"61":1,"70":1,"71":1,"86":14,"87":49,"88":1,"89":1,"90":1,"91":1,"92":26,"93":20,"94":13,"95":15,"96":1,"97":1,"99":7,"101":8},"4":{"86":1,"87":2,"92":1,"93":1,"94":1,"95":1,"98":1,"99":1,"100":1,"101":1},"5":{"41":9,"44":7,"45":5,"47":5,"50":1,"51":1,"61":3,"69":2,"70":1,"71":3,"85":34,"86":19,"87":51,"88":1,"89":1,"90":1,"91":1,"92":27,"93":27,"94":14,"95":22,"96":1,"97":3,"99":44,"101":45}}],["5的输入点集",{"5":{"47":2}}],["5的变化量等于0",{"5":{"70":2}}],["5到0的变化量",{"5":{"70":2}}],["6本节小结",{"2":{"41":1},"5":{"41":1}}],["6",{"0":{"86":1,"88":1,"89":1,"90":1,"91":1,"97":1},"2":{"44":1,"45":1,"47":1,"50":1,"51":1,"61":1,"70":1,"71":1,"86":14,"87":1,"88":11,"89":8,"90":8,"91":7,"92":1,"93":1,"94":1,"95":1,"96":1,"97":5,"99":1,"101":1},"4":{"86":1,"88":1,"89":1,"90":1,"91":1,"97":1},"5":{"41":4,"44":7,"45":5,"47":5,"50":1,"51":1,"61":1,"70":1,"71":3,"85":26,"86":19,"87":1,"88":12,"89":9,"90":9,"91":16,"92":1,"93":1,"94":1,"95":1,"96":1,"97":30,"99":3,"101":3}}],["754",{"5":{"61":2}}],["7",{"2":{"44":1,"45":1,"47":1,"50":1,"51":1,"61":1,"86":1,"87":2,"88":1,"89":1,"90":1,"92":1,"93":1,"94":1,"95":1,"96":1,"101":1},"5":{"41":2,"44":5,"45":5,"47":5,"50":1,"51":1,"61":1,"71":2,"86":1,"87":2,"88":1,"89":1,"90":1,"92":1,"93":1,"94":1,"95":1,"96":1,"99":2,"101":3}}],["神经活动中内在思想的逻辑演算",{"5":{"47":2}}],["神经生理学家沃伦",{"5":{"47":2}}],["神经架构搜索和语言模型微调中有着应用",{"5":{"96":2}}],["神经架构搜索",{"5":{"41":2}}],["神经元",{"5":{"47":1}}],["神经元是神经网络的基本计算单元",{"5":{"47":2}}],["神经元模型不仅可以从几何角度理解",{"5":{"47":2}}],["神经元模型的矩阵表示",{"2":{"47":1},"5":{"47":1}}],["神经元可以理解为将输入向量从原始输入空间映射到新的特征空间的变换",{"5":{"47":1}}],["神经元被激活并通过轴突向其他神经元传递信号",{"5":{"47":2}}],["神经元的矩阵表示使得批量处理成为可能",{"5":{"47":2}}],["神经元的特征空间映射",{"5":{"47":2}}],["神经元的几何分离能力",{"5":{"47":2}}],["神经元的几何解释",{"2":{"47":1},"5":{"47":1}}],["神经元的数学模型",{"0":{"47":1},"4":{"47":1},"5":{"21":4,"47":1,"85":4}}],["神经元的数学模型<",{"5":{"21":1,"85":1}}],["神经元级",{"5":{"45":2}}],["神经元级并行",{"5":{"45":1}}],["神经网络定义了一个参数化的概率分布",{"5":{"61":2}}],["神经网络本质上是一个嵌套的复合函数",{"5":{"48":2}}],["神经网络",{"5":{"47":2}}],["神经网络通常具有非凸的损失景观",{"5":{"51":2}}],["神经网络通过编码器",{"5":{"71":1}}],["神经网络通过编码器和分别编码和",{"5":{"71":1}}],["神经网络通用逼近能力的核心定理表明",{"5":{"71":2}}],["神经网络之所以被称为",{"5":{"71":2}}],["神经网络之所以能够拟合任意复杂的函数模式",{"5":{"71":2}}],["神经网络的层数等来衡量",{"5":{"51":2}}],["神经网络的激活值等",{"5":{"96":2}}],["神经网络的训练目标是最小化某个损失函数",{"5":{"44":2}}],["神经网络的目标通常是最大化输入不同部分之间的互信息",{"5":{"71":2}}],["神经网络的表达能力与其参数矩阵的有效秩密切相关",{"5":{"41":2}}],["神经网络的矩阵形式",{"0":{"46":1},"4":{"46":1},"5":{"21":4,"46":1,"85":4}}],["神经网络的矩阵形式<",{"5":{"21":1,"85":1}}],["神经网络作为复合函数",{"5":{"45":2}}],["神经网络可以表示为各层映射的复合",{"5":{"45":2}}],["神经网络层",{"5":{"46":2}}],["的和为1",{"5":{"97":1}}],["的比值",{"5":{"97":1}}],["的鞍点",{"5":{"97":1}}],["的噪声可能减慢收敛但有助于逃离鞍点",{"5":{"97":2}}],["的优化涉及策略优化和奖励模型学习两个相互关联的过程",{"5":{"97":2}}],["的性能对温度参数和负样本选择非常敏感",{"5":{"97":2}}],["的自注意力机制中",{"5":{"97":2}}],["的自注意力层中",{"5":{"91":2}}],["的非负性保证了输出概率的非负性",{"5":{"96":1}}],["的非主特征值很小",{"5":{"41":1}}],["的状态空间大小",{"5":{"96":1}}],["的统计学原理",{"5":{"95":1}}],["的下三角矩阵",{"5":{"95":1}}],["的速度增长",{"5":{"95":1}}],["的速度线性增长",{"5":{"95":1}}],["的速度收敛到最优解附近",{"5":{"51":1}}],["的速度收敛",{"5":{"48":1}}],["的投影矩阵",{"5":{"95":1,"97":2}}],["的特性",{"5":{"96":2}}],["的特例",{"5":{"95":2}}],["的特征表示",{"5":{"94":2}}],["的特征",{"5":{"92":2}}],["的特征值的最大模",{"5":{"87":1}}],["的特征值的平方根",{"5":{"50":1}}],["的特征值分解",{"5":{"87":1}}],["的特征值决定",{"5":{"50":1,"97":1}}],["的特征值接近1",{"5":{"41":1}}],["的特征向量",{"5":{"45":1,"50":1}}],["的量级",{"5":{"94":1}}],["的softmax梯度矩阵",{"5":{"94":1}}],["的softmax变换",{"5":{"87":2}}],["的约数",{"5":{"93":1}}],["的场景",{"5":{"93":1}}],["的各维度之间存在相关性",{"5":{"93":1}}],["的各元素方差为",{"5":{"41":1}}],["的各元素独立同分布且方差为",{"5":{"41":1}}],["的query",{"5":{"93":1}}],["的query向量",{"5":{"87":1,"93":1}}],["的视角",{"5":{"93":2}}],["的视角来看",{"5":{"87":2}}],["的头子集",{"5":{"93":2}}],["的头往往具有较大的有效依赖跨度",{"5":{"92":2}}],["的头往往具有较小的有效依赖跨度",{"5":{"92":2}}],["的多头注意力变体通过只激活部分头来降低计算成本",{"5":{"93":2}}],["的多项式方程",{"5":{"50":1}}],["的能力增强是多头注意力设计的精妙之处",{"5":{"93":2}}],["的能力正是通用逼近能力的几何解释",{"5":{"71":2}}],["的策略使得模型能够从多个角度同时建模输入数据",{"5":{"93":2}}],["的策略与人类处理语言的认知过程有相似之处",{"5":{"92":2}}],["的处所",{"5":{"93":2}}],["的主题",{"5":{"93":2}}],["的重要性",{"5":{"92":1}}],["的每一行只依赖于对应的查询行和整个键矩阵",{"5":{"92":1}}],["的每一行是一个概率分布",{"5":{"87":1,"92":2}}],["的键",{"5":{"92":1}}],["的存储空间",{"5":{"95":1}}],["的存储和计算都不可行",{"5":{"92":2}}],["的存在",{"5":{"92":1}}],["的贡献也较小",{"5":{"92":1}}],["的卷积为例",{"5":{"92":1}}],["的距离有多远",{"5":{"92":1}}],["的路径长度恒为1",{"5":{"92":2}}],["的路径长度为",{"5":{"92":2}}],["的引入是必要的",{"5":{"92":2}}],["的引入正是为了解决这一根本性问题",{"5":{"91":2}}],["的依赖关系",{"5":{"92":2}}],["的融合",{"5":{"92":2}}],["的时间复杂度如下",{"5":{"92":2}}],["的发展进一步增强了注意力机制建模位置相关依赖的能力",{"5":{"92":2}}],["的大小可以衡量依赖关系的强度",{"5":{"92":2}}],["的大小受到卷积核尺寸的限制",{"5":{"92":2}}],["的大矩阵",{"5":{"50":1}}],["的参数量",{"5":{"91":1}}],["的增加",{"5":{"91":1}}],["的配对",{"5":{"91":1}}],["的配置",{"5":{"89":2}}],["的同一频率振动",{"5":{"91":1}}],["的内容key",{"5":{"91":1}}],["的内容query",{"5":{"91":1}}],["的内容嵌入向量",{"5":{"91":1}}],["的内积",{"5":{"70":2,"87":1}}],["的先验假设",{"5":{"92":4}}],["的先验",{"5":{"91":2}}],["的先验分布为",{"5":{"96":2}}],["的热力图",{"5":{"91":2}}],["的结构",{"5":{"94":1}}],["的结构取决于",{"5":{"70":2}}],["的结果",{"5":{"91":1}}],["的结论",{"5":{"91":2}}],["的趋势",{"5":{"91":2}}],["的思想",{"5":{"91":2}}],["的思想一致",{"5":{"90":2}}],["的几何直觉可以通过圆周运动来理解",{"5":{"90":1}}],["的dft为",{"5":{"90":2}}],["的周期函数",{"5":{"90":1}}],["的周期为10000",{"5":{"90":2}}],["的周期为1",{"5":{"90":2}}],["的相邻位置差较大",{"5":{"90":2}}],["的相邻位置差较小",{"5":{"90":2}}],["的相似度决定",{"5":{"61":2}}],["的相似度",{"5":{"69":2}}],["的相似度分数转换为归一化的注意力权重",{"5":{"70":2}}],["的奇异值分解中的对应项",{"5":{"89":1}}],["的奇异值分解为",{"5":{"41":1,"89":1}}],["的逼近",{"5":{"89":1}}],["的前",{"5":{"89":2}}],["的设计原理和数学直觉",{"5":{"91":2}}],["的设计哲学",{"5":{"88":2}}],["的设计选择背后有深刻的数学原因",{"5":{"71":2}}],["的旋转块独立运作",{"5":{"88":1}}],["的旋转矩阵",{"5":{"88":3}}],["的转变",{"5":{"88":2}}],["的四个元素",{"5":{"88":2}}],["的逆元是反向旋转",{"5":{"88":2}}],["的群论表述",{"5":{"88":1}}],["的群论基础",{"5":{"70":2}}],["的群结构由以下性质刻画",{"5":{"88":2}}],["的取值范围是整个实数轴",{"5":{"96":2}}],["的取值范围是",{"5":{"88":2}}],["的框架中",{"5":{"88":2}}],["的深刻转变",{"5":{"88":2}}],["的诞生",{"5":{"88":2}}],["的维度均为",{"5":{"93":2}}],["的维度较低",{"5":{"87":1}}],["的维度为",{"5":{"44":1}}],["的截断svd",{"5":{"87":1}}],["的低秩矩阵有",{"5":{"87":1}}],["的低秩结构意味着",{"5":{"87":1}}],["的秩最大为",{"5":{"94":1}}],["的秩最多为",{"5":{"87":2}}],["的秩是影响其性质的关键参数",{"5":{"89":1}}],["的秩",{"5":{"89":1}}],["的秩为",{"5":{"87":1,"89":1}}],["的秩与",{"5":{"87":1}}],["的子空间中",{"5":{"87":1}}],["的任意行的",{"5":{"87":1}}],["的任意特征值",{"5":{"87":1}}],["的key向量",{"5":{"87":1}}],["的注意力分数矩阵",{"5":{"95":1}}],["的注意力分数设为",{"5":{"91":1}}],["的注意力输出",{"5":{"93":1}}],["的注意力计算",{"5":{"93":1}}],["的注意力主要集中在哪些位置",{"5":{"92":1}}],["的注意力权重矩阵为",{"5":{"92":1}}],["的注意力权重",{"5":{"92":3,"95":1}}],["的注意力矩阵计算",{"5":{"87":1}}],["的注意力矩阵",{"5":{"87":1}}],["的注意力矩阵有更快的衰减",{"5":{"87":2}}],["的注意力矩阵通常比浅层",{"5":{"87":2}}],["的注意力模式时",{"5":{"87":2}}],["的注意力模式",{"5":{"86":2}}],["的角度审视多头注意力",{"5":{"93":2}}],["的角度",{"5":{"87":2}}],["的角度理解",{"5":{"61":2}}],["的模式来推断该头负责的依赖类型",{"5":{"92":1}}],["的模式与现代gpu的架构特性高度匹配",{"5":{"92":2}}],["的模式",{"5":{"87":2}}],["的基向量相关",{"5":{"87":2}}],["的循环依赖",{"5":{"86":1}}],["的原始嵌入",{"5":{"92":1}}],["的原始注意力分数",{"5":{"86":1}}],["的原因有以下几点",{"5":{"91":2}}],["的原理是选择使对数似然最大的参数值",{"5":{"61":1}}],["的邻域集合",{"5":{"86":1}}],["的输入序列",{"5":{"89":1}}],["的输入",{"5":{"86":1,"92":1}}],["的输出时只能依赖于位置",{"5":{"95":1}}],["的输出计算",{"5":{"92":1}}],["的输出",{"5":{"92":1}}],["的输出可以解释为给定输入",{"5":{"47":1}}],["的输出为",{"5":{"69":1}}],["的序列长度",{"5":{"95":1}}],["的序列上",{"5":{"90":1}}],["的序列",{"5":{"86":2,"92":2}}],["的稀疏注意力模式",{"5":{"86":2}}],["的复杂度成为性能瓶颈",{"5":{"95":2}}],["的复杂度成为效率和可扩展性的主要障碍",{"5":{"86":2}}],["的复数编码",{"5":{"90":1}}],["的复数编码向量",{"5":{"90":1}}],["的复数",{"0":{"88":1},"4":{"88":1},"5":{"85":5,"88":1}}],["的复合",{"5":{"45":2}}],["的解释",{"5":{"61":2}}],["的解等价于求解",{"5":{"50":1}}],["的位置编码是一个有前景的研究方向",{"5":{"101":2}}],["的位置编码向量",{"5":{"91":1}}],["的位置对",{"5":{"92":1}}],["的位置key",{"5":{"91":1}}],["的位置query",{"5":{"91":1}}],["的位置信息",{"5":{"91":2}}],["的位置",{"5":{"61":2,"69":2,"89":3,"92":2}}],["的熵",{"5":{"61":2}}],["的事件",{"5":{"61":1}}],["的工具",{"5":{"61":2}}],["的加权平均",{"5":{"61":2,"92":1}}],["的行方向归一化",{"5":{"61":2}}],["的行为类似于信息选择器",{"5":{"71":2}}],["的本质",{"5":{"61":2}}],["的值向量",{"5":{"92":3}}],["的值",{"5":{"92":1}}],["的值被限制在",{"5":{"61":2}}],["的值会是一个绝对值很大的负数",{"5":{"61":2}}],["的值会很大",{"5":{"61":2}}],["的值很大时",{"5":{"61":2}}],["的值很大",{"5":{"61":2}}],["的值趋近于0",{"5":{"61":2}}],["的期望",{"5":{"51":1}}],["的列空间",{"5":{"51":1}}],["的列向量决定了",{"5":{"94":2}}],["的列向量",{"5":{"50":1}}],["的列向量称为右奇异向量",{"5":{"50":2}}],["的列向量称为左奇异向量",{"5":{"50":2}}],["的列向量是",{"5":{"45":1}}],["的集合",{"5":{"51":1}}],["的点集",{"5":{"61":1}}],["的点",{"5":{"51":1}}],["的标量函数",{"5":{"51":1}}],["的标准自注意力配置",{"5":{"95":1}}],["的标准定义",{"5":{"61":2}}],["的标准",{"5":{"50":2}}],["的标准差为1时",{"5":{"41":1}}],["的联系",{"5":{"51":2,"69":2}}],["的敏感性是其最著名的性质之一",{"5":{"51":2}}],["的局部最小值",{"5":{"51":2}}],["的平稳分布",{"5":{"87":1}}],["的平方",{"5":{"51":2,"70":1}}],["的平方范数",{"5":{"51":2}}],["的平方范数加上不可解释部分",{"5":{"51":2}}],["的平面相交得到的一元函数曲线的切线斜率",{"5":{"48":1}}],["的平均",{"5":{"70":2}}],["的查询",{"5":{"50":1,"92":1}}],["的三阶段范式",{"5":{"95":2}}],["的三阶张量表示为",{"5":{"50":1}}],["的三维张量运算",{"5":{"50":1}}],["的三维张量",{"5":{"50":3,"93":1}}],["的稳定性由矩阵",{"5":{"50":1}}],["的稳定性由矩阵的特征值决定",{"5":{"50":1}}],["的二次复杂度",{"5":{"95":1}}],["的二次增长对应于范数的平方",{"5":{"70":1}}],["的二次增长对应于",{"5":{"70":1}}],["的二维张量",{"5":{"50":1}}],["的二维张量重塑为",{"5":{"50":1}}],["的二阶泰勒展开为",{"5":{"48":1}}],["的二阶偏导数连续",{"5":{"48":1}}],["的张量",{"5":{"50":2}}],["的乘法可以看作是收缩掉两个张量的一个公共维度",{"5":{"50":2}}],["的乘积",{"5":{"41":1,"42":2,"50":3}}],["的概率分布为",{"5":{"61":1}}],["的概率",{"5":{"96":3}}],["的概率为1",{"5":{"96":1}}],["的概率为",{"5":{"61":6,"69":2,"96":1}}],["的概念推广到任意维度",{"5":{"50":2}}],["的概念",{"5":{"69":2}}],["的方差为",{"5":{"95":1}}],["的方差较大时",{"5":{"94":1}}],["的方差很大",{"5":{"69":2}}],["的方式",{"5":{"92":2}}],["的方阵",{"5":{"50":1,"94":1}}],["的方阵才是可逆的",{"5":{"50":2}}],["的方向导数为",{"5":{"48":1}}],["的方向",{"5":{"71":4}}],["的运算",{"5":{"50":2,"88":2}}],["的支持集",{"5":{"96":1}}],["的支持集必须包含",{"5":{"96":1}}],["的样本时所需要的额外信息量",{"5":{"96":1}}],["的高斯噪声",{"5":{"96":1}}],["的高斯分布",{"5":{"96":1}}],["的条件期望",{"5":{"51":1}}],["的条件下",{"5":{"51":2}}],["的条件分布仍然是高斯分布",{"5":{"96":1}}],["的条件数定义为",{"5":{"41":1}}],["的pdf为",{"5":{"96":1}}],["的语言模型",{"5":{"96":1}}],["的效果",{"5":{"96":2}}],["的渐近行为",{"5":{"96":2}}],["的one",{"5":{"96":2}}],["的定义直接求导验证",{"5":{"61":2}}],["的定义",{"5":{"96":2}}],["的一次试验版本",{"5":{"96":2}}],["的一种特殊情况",{"5":{"44":2}}],["的一种形式",{"5":{"70":2}}],["的单调性来保证收敛性",{"5":{"48":1}}],["的等值面上",{"5":{"48":1}}],["的等式依次代入",{"5":{"46":1}}],["的矩阵乘法",{"5":{"95":2}}],["的矩阵组合成一个",{"5":{"50":1}}],["的矩阵与一个",{"5":{"50":1}}],["的矩阵",{"5":{"48":1,"50":3,"87":2,"93":3,"94":1,"95":1}}],["的偏导数的乘积之和",{"5":{"48":1}}],["的偏导数与它对",{"5":{"48":1}}],["的偏导数为",{"5":{"48":1,"96":1}}],["的总影响是通过所有中间变量",{"5":{"48":1}}],["的总flops约为前向传播的4倍",{"5":{"44":2}}],["的变换",{"5":{"47":1,"94":2}}],["的摩尔",{"5":{"47":1}}],["的仿射变换定义为",{"5":{"47":1}}],["的出现才得到解决",{"5":{"47":2}}],["的问题",{"5":{"47":2,"88":2}}],["的操作",{"5":{"47":2}}],["的抽象与简化之上",{"5":{"47":2}}],["的所有行和为",{"5":{"61":2}}],["的所有模式",{"5":{"96":1}}],["的所有列",{"5":{"45":1}}],["的数学原理",{"5":{"88":2,"89":2,"93":2}}],["的数学公式<",{"5":{"85":1}}],["的数学公式",{"0":{"95":1},"4":{"95":1},"5":{"85":4,"95":1}}],["的数学本质",{"5":{"61":2}}],["的数学基础建立在对生物神经元",{"5":{"47":2}}],["的数学基础与几何解释",{"0":{"51":1},"4":{"51":1},"5":{"51":3,"85":4}}],["的数学基础与几何解释<",{"5":{"85":1}}],["的数学协同关系",{"5":{"71":2}}],["的数学改进",{"5":{"99":2,"101":2}}],["的数学框架涉及如何组合不同任务的损失函数",{"5":{"99":2,"101":2}}],["的flops约为",{"5":{"44":2}}],["的fisher信息矩阵正好是softmax雅可比矩阵与自身转置的乘积",{"5":{"71":1}}],["的误差信号",{"5":{"44":1}}],["的过程",{"5":{"44":2}}],["的后验分布与似然函数和先验的乘积成正比",{"5":{"71":1}}],["的网络可以等效于宽度为指数级别的浅层网络",{"5":{"71":1}}],["的全连接网络",{"5":{"71":1}}],["的全连接层",{"5":{"45":1}}],["的形式",{"5":{"69":2,"71":1,"99":4,"101":4}}],["的形式包含类似的项",{"5":{"70":1}}],["的形式包含类似",{"5":{"70":1}}],["的hessian矩阵与fisher信息矩阵密切相关",{"5":{"71":2}}],["的显式注入",{"5":{"71":2}}],["的连续调节能力",{"5":{"71":2}}],["的激活函数可能需要更少的神经元",{"5":{"71":2}}],["的",{"5":{"51":1,"61":4,"69":3,"70":4,"71":3,"87":3,"88":3,"92":5,"97":8}}],["的理论基础",{"5":{"71":2}}],["的交替结构是深度学习成功的关键数学基础",{"5":{"71":2}}],["的绝对值很大时",{"5":{"42":1}}],["的均方根",{"5":{"41":1}}],["的谱性质决定了梯度如何被",{"5":{"41":1}}],["的谱半径",{"5":{"41":1}}],["的尺度与",{"5":{"41":1}}],["的尺度与相关",{"5":{"41":1}}],["的雅可比矩阵的范数满足",{"5":{"41":1}}],["的雅可比矩阵为",{"5":{"45":1}}],["的饱和特性与sigmoid类似",{"5":{"41":1}}],["的第一个维度",{"5":{"94":1}}],["的第行对应序列中第个位置的嵌入表示",{"5":{"94":1}}],["的第行为为行向量",{"5":{"47":1}}],["的第行仅依赖于的第行和",{"5":{"45":1}}],["的第",{"5":{"44":2,"45":4,"47":3,"87":2,"89":1,"92":1,"94":1,"95":4}}],["的第列仅依赖于的第列和的所有列",{"5":{"45":1}}],["的函数",{"5":{"51":1,"61":2,"71":1,"90":3,"96":1,"99":1,"101":1}}],["的梯度形式具有优美的残差结构",{"5":{"96":2}}],["的梯度可以写为",{"5":{"94":2}}],["的梯度可以直接传播",{"5":{"41":2}}],["的梯度在预测概率接近0或1时会变得非常小",{"5":{"61":2}}],["的梯度具有简洁的形式",{"5":{"70":2,"96":2}}],["的梯度特性使得gelu特别适合训练非常深的网络",{"5":{"71":2}}],["的梯度很小",{"5":{"41":1}}],["的梯度完全消失",{"5":{"41":1}}],["的梯度为",{"5":{"41":1,"44":1,"45":1,"51":1,"61":2,"96":1,"99":1,"101":1}}],["的梯度",{"5":{"69":7,"70":1,"89":1}}],["的梯度结构",{"5":{"70":1}}],["的梯度结构与上述交叉熵关于",{"5":{"70":1}}],["的梯度结构高度相似",{"5":{"70":1}}],["的梯度是",{"5":{"70":1}}],["的梯度高度相似",{"5":{"99":2,"101":2}}],["的梯度向量",{"5":{"99":1,"101":1}}],["的梯度与ppo目标的梯度在期望意义下是相近的",{"5":{"99":1,"101":1}}],["的信息通过位置",{"5":{"92":1}}],["的信息就可以",{"5":{"92":1}}],["的信息对位置",{"5":{"92":1}}],["的信息有多少能够传递到位置",{"5":{"92":1}}],["的信息传递路径是",{"5":{"92":1}}],["的信息传递",{"5":{"92":1}}],["的信息整合方式",{"5":{"92":2}}],["的信息聚合方式",{"5":{"92":2}}],["的信息流动比例",{"5":{"87":1}}],["的信息时",{"5":{"61":5}}],["的信息量受到其维度",{"5":{"93":1}}],["的信息量",{"5":{"69":1,"71":1}}],["的信息",{"5":{"50":2,"71":1,"91":4,"93":1,"95":1}}],["的对数",{"5":{"91":2}}],["的对数几率为",{"5":{"71":1}}],["的对数几率",{"5":{"71":1}}],["的对称半正定矩阵",{"5":{"96":1}}],["的对角线元素",{"5":{"50":2}}],["的对角线分类",{"5":{"46":4}}],["的对角元素",{"5":{"41":1}}],["的最终表示保留了原始嵌入的直接成分",{"5":{"92":1}}],["的最小非零特征值",{"5":{"97":1}}],["的最小路径长度为",{"5":{"92":1}}],["的最小值为",{"5":{"61":2}}],["的最小和最大奇异值",{"5":{"41":1}}],["的最小特征值",{"5":{"70":1}}],["的最优预测",{"5":{"51":1}}],["的最优预测是",{"5":{"51":1}}],["的最优问题",{"5":{"51":1}}],["的最佳近似",{"5":{"50":1,"89":1}}],["的最大特征值的平方根",{"5":{"50":1}}],["的最大奇异值",{"5":{"41":1,"87":1,"92":1}}],["的最大化互信息的目标",{"5":{"70":2}}],["的计算过程可以分解为三个连续的矩阵运算步骤",{"5":{"95":2}}],["的计算与单头注意力相同",{"5":{"93":1}}],["的计算为",{"5":{"88":2}}],["的计算结果",{"5":{"95":1}}],["的计算结果是一个",{"5":{"87":1}}],["的计算结果与",{"5":{"46":1}}],["的计算",{"5":{"61":2,"87":1}}],["的计算会遇到严重的数值问题",{"5":{"61":2}}],["的计算是数值稳定的",{"5":{"61":2}}],["的计算涉及",{"5":{"61":2}}],["的计算涉及softmax操作",{"5":{"70":1}}],["的计算量",{"5":{"50":1}}],["的计算就隐含了条件期望的概念",{"5":{"96":2}}],["的计算也是并行的",{"5":{"45":2}}],["的逐层求值",{"5":{"45":2}}],["的分量都是独立同分布的随机变量",{"5":{"95":1}}],["的分离",{"5":{"94":2}}],["的分层混合",{"5":{"89":2}}],["的分子分母分别计算时是稳定的",{"5":{"61":2}}],["的分布特性",{"5":{"95":1}}],["的分布趋向于标准正态分布",{"5":{"96":1}}],["的分布范围可能达到",{"5":{"41":1}}],["的分布范围可能达到或更宽",{"5":{"41":1}}],["的分析揭示了它们如何通过稳定激活分布来改善梯度传播",{"5":{"41":2}}],["的分段线性逼近有相似之处",{"5":{"45":2}}],["的表示与原始位置",{"5":{"92":1}}],["的表示可能影响训练动态和最终性能",{"5":{"89":2}}],["的表示捕获数据的局部",{"5":{"45":2}}],["的表示编码全局",{"5":{"45":2}}],["的流动",{"5":{"45":2}}],["的导数为",{"5":{"42":2,"45":1}}],["的伪逆",{"5":{"45":1}}],["的线性子空间上的所有函数",{"5":{"89":1}}],["的线性变换中最大的拉伸因子",{"5":{"50":1}}],["的线性函数",{"5":{"45":2}}],["的线性投影",{"5":{"45":1}}],["的伴随值为",{"5":{"45":2}}],["的元素为对角元素的对角矩阵",{"5":{"61":2}}],["的元素为",{"5":{"61":2}}],["的元素",{"5":{"46":1}}],["的隐藏状态",{"5":{"92":1,"99":1,"101":1}}],["的选择并非随意",{"5":{"88":2}}],["的选择是一个重要的权衡问题",{"5":{"93":2}}],["的选择是一个超参数调优问题",{"5":{"41":1}}],["的选择是一个关键的设计决策",{"5":{"91":1}}],["的选择是一个关键的超参数",{"5":{"99":1,"101":1}}],["的选择对性能有显著影响",{"5":{"69":1}}],["的选择直接影响优化轨迹和最终性能",{"5":{"99":1,"101":1}}],["的核心运算是query",{"5":{"88":2}}],["的核心思想是通过替换softmax函数",{"5":{"86":2}}],["的核心思想是通过限制每个位置只能关注少数其他位置",{"5":{"86":2}}],["的核心正是通过这种矩阵表示实现高效的神经网络计算",{"5":{"47":2}}],["的核心组件",{"5":{"69":2}}],["的infonce",{"5":{"69":2}}],["的编码是对称的",{"5":{"91":1}}],["的编码是初始编码经过各频率成分独立旋转",{"5":{"90":1}}],["的编码与位置",{"5":{"91":1}}],["的编码可以表示为初始编码",{"5":{"90":1}}],["的编码映射到位置",{"5":{"90":1}}],["的编码分辨率与频率",{"5":{"90":2}}],["的编码矩阵",{"5":{"89":1}}],["的编码向量视为一个离散序列",{"5":{"90":1}}],["的编码向量",{"5":{"89":1,"90":2,"91":3,"92":2}}],["的编码方案来编码来自分布",{"5":{"61":3}}],["的编码",{"5":{"69":1,"89":1,"90":3}}],["的锐度",{"5":{"69":1}}],["的正样本的软概率",{"5":{"69":1}}],["的机制",{"5":{"69":1}}],["的区域对应类别0",{"5":{"47":2}}],["的区域对应类别1",{"5":{"47":2}}],["的区域保持不变",{"5":{"45":2}}],["的区域被压缩到零点",{"5":{"45":2}}],["的区分",{"5":{"70":2}}],["的超平面与第一卦限相交形成的三角形区域",{"5":{"70":2}}],["的不同理解",{"5":{"70":2}}],["的负对数似然",{"5":{"70":2}}],["的目标是找到参数",{"5":{"61":2}}],["的目标是学习一个编码函数",{"5":{"70":2}}],["的目标是计算一个加权平均",{"5":{"70":2}}],["的组合",{"5":{"70":1}}],["的项",{"5":{"70":1,"97":1}}],["的映射函数",{"5":{"70":1}}],["的映射",{"5":{"70":1,"92":2}}],["的预测概率",{"5":{"70":1}}],["的预测",{"5":{"70":1}}],["的具体数值完全由训练数据决定",{"5":{"89":1}}],["的具体场景",{"5":{"99":2,"101":2}}],["的具体身份",{"5":{"99":1,"101":1}}],["的功能",{"5":{"99":2,"101":2}}],["的估计",{"5":{"99":2,"101":2}}],["的token",{"5":{"99":2,"101":2}}],["的token嵌入经过多层自注意力计算后的隐藏状态",{"5":{"99":1,"101":1}}],["的token集合的函数",{"5":{"99":1,"101":1}}],["的双向上下文表示",{"5":{"99":1,"101":1}}],["的损失函数为",{"5":{"99":1,"101":1}}],["的权重矩阵被近似为",{"5":{"50":1}}],["的权重",{"5":{"99":1,"101":1}}],["aggregate",{"5":{"95":2}}],["aware的思想来改善长上下文性能",{"5":{"88":2}}],["aware",{"5":{"88":6}}],["action",{"5":{"88":2}}],["activation",{"5":{"47":2,"93":2}}],["activity",{"5":{"47":2}}],["argument",{"5":{"88":2}}],["absolute",{"5":{"51":2,"88":4,"91":2}}],["amsgrad是adam的一个修改版本",{"5":{"48":2}}],["amp",{"5":{"42":30,"61":18,"86":5,"97":4}}],["anaphora",{"5":{"88":2}}],["another",{"5":{"88":2}}],["annealing",{"5":{"48":2,"97":2}}],["and",{"5":{"47":2}}],["affine",{"5":{"47":2}}],["axon",{"5":{"47":2}}],["a",{"5":{"47":2,"96":6}}],["all",{"5":{"91":2}}],["alibi也有明显的局限",{"5":{"88":2}}],["alibi的优势在于实现极其简单",{"5":{"88":2}}],["alibi",{"5":{"88":2}}],["alibi等都属于这一类别",{"5":{"88":2}}],["aligned",{"5":{"86":8}}],["align",{"5":{"42":16,"61":12,"97":4}}],["alpha",{"5":{"42":12}}],["alt=",{"5":{"44":1,"45":2,"47":2,"48":3,"50":2,"61":1,"96":3}}],["alternatives",{"5":{"69":2}}],["a>",{"5":{"21":10,"85":26}}],["addition",{"5":{"91":2}}],["additivity",{"5":{"71":2}}],["adaptation",{"5":{"50":2}}],["adaptive",{"5":{"48":2,"86":2}}],["adam的超参数",{"5":{"48":1}}],["adam的超参数和通常使用默认值",{"5":{"48":1}}],["adam维护两个指数移动平均",{"5":{"48":2}}],["adam",{"5":{"48":4,"70":4,"97":4}}],["adagrad",{"5":{"97":6}}],["adagrad特别适合处理稀疏特征和非均匀分布的梯度",{"5":{"48":2}}],["adagrad为每个参数维护一个累积梯度平方和",{"5":{"48":2}}],["adjoint",{"5":{"45":2}}],["advantage",{"5":{"99":2,"101":2}}],["attention以其简洁的数学形式",{"5":{"95":2}}],["attention算法通过分块计算和io感知优化",{"5":{"93":2}}],["attention计算",{"5":{"93":2}}],["attention计算过程中的核心中间结果",{"5":{"87":2}}],["attention等算法通过精心设计的数据分块策略",{"5":{"92":2}}],["attention的完整计算流程可以用三个矩阵运算阶段来描述",{"5":{"95":2}}],["attention的标准形式",{"5":{"95":2}}],["attention的关键创新之一",{"5":{"95":2}}],["attention的每一个组成部分",{"5":{"95":2}}],["attention的数学定义与推导",{"5":{"95":2}}],["attention的数学定义与query",{"5":{"93":2}}],["attention的数学定义如公式",{"5":{"95":2}}],["attention的数学公式",{"5":{"94":2}}],["attention的框架下",{"5":{"93":2}}],["attention的高效实现",{"5":{"88":2}}],["attention的兼容性",{"5":{"88":2}}],["attention的定义",{"5":{"86":2}}],["attention中",{"5":{"61":2,"69":4}}],["attention定义为",{"5":{"71":2}}],["attention",{"0":{"95":1},"4":{"95":1},"5":{"46":2,"61":10,"69":2,"70":2,"85":5,"86":34,"87":6,"88":8,"91":2,"93":10,"95":5}}],["approximate",{"5":{"48":2}}],["approx",{"5":{"42":2,"46":8}}],["automatic",{"5":{"44":2,"45":2,"48":2}}],["auc",{"5":{"70":2}}],["is",{"5":{"91":2}}],["implicit",{"5":{"88":2}}],["immanent",{"5":{"47":2}}],["ib",{"5":{"87":2}}],["ieee",{"5":{"61":2}}],["iteration",{"5":{"50":4}}],["ideas",{"5":{"47":2}}],["ik",{"5":{"42":2}}],["i^2",{"5":{"42":2}}],["inductive",{"5":{"92":2}}],["index",{"4":{"85":1}}],["independence",{"5":{"69":2}}],["invariant",{"5":{"91":2,"92":2}}],["inverse",{"5":{"50":2}}],["interaction",{"5":{"93":2}}],["interpolation",{"5":{"88":2}}],["internal",{"5":{"21":10,"41":2,"85":26}}],["int8",{"5":{"50":2}}],["input",{"5":{"47":2}}],["info",{"5":{"61":4}}],["infonce",{"5":{"61":4,"69":4,"70":8,"97":5}}],["infonce的数学定义与推导",{"2":{"69":1},"5":{"69":1}}],["infonce的互信息下界性质",{"2":{"69":1},"5":{"69":1}}],["infonce的梯度结构与优化动力学",{"2":{"69":1},"5":{"69":1}}],["infonce的梯度会",{"5":{"69":2}}],["infonce的梯度同样具有这种简洁的形式",{"5":{"69":2}}],["infonce的梯度为",{"5":{"99":2,"101":2}}],["infonce的一个重要理论性质是它与互信息",{"5":{"69":2}}],["infonce的目标是从所有其他位置中选择出与",{"5":{"69":2}}],["infonce的目标是最大化互信息的下界",{"5":{"99":2,"101":2}}],["infonce损失提供了互信息的下界估计",{"5":{"71":2}}],["infonce损失函数的数学基础",{"2":{"69":1},"5":{"69":1}}],["infonce损失",{"5":{"69":2}}],["infonce损失定义为",{"5":{"69":2}}],["infonce损失与互信息满足以下关系",{"5":{"69":2}}],["infonce损失为",{"5":{"69":4}}],["infonce将nce从二分类扩展到多分类",{"5":{"69":2}}],["infonce与注意力的softmax统一",{"0":{"69":1},"4":{"69":1},"5":{"61":4,"69":1,"70":2,"85":4}}],["infonce与注意力的softmax统一<",{"5":{"85":1}}],["infonce与交叉熵的等价性是本节核心论点的基础",{"5":{"69":2}}],["infonce作为交叉熵的一种特殊形式",{"5":{"69":2}}],["infonce是互信息的下界",{"5":{"69":2}}],["infonce最小化的正是这个预测分布与",{"5":{"69":2}}],["infonce最小化这个分布与",{"5":{"69":2}}],["infonce通过正负样本对比来最大化互信息的下界",{"5":{"69":2}}],["infonce可以被解释为一个多分类问题的交叉熵损失",{"5":{"69":2}}],["infonce可能无法有效区分真正相似的样本和表面相似的样本",{"5":{"99":2,"101":2}}],["information",{"5":{"61":2,"69":4,"70":2,"71":2,"87":2,"96":2}}],["ij",{"5":{"42":4,"46":2}}],["i=1",{"5":{"42":4,"46":8,"61":10}}],["irreducible",{"5":{"51":4}}],["irrelevant",{"5":{"69":2}}],["iia",{"5":{"69":2}}],["和第八章",{"5":{"101":2}}],["和注意力权重矩阵",{"5":{"95":1}}],["和注意力维度",{"5":{"50":1}}],["和键向量",{"5":{"95":1}}],["和的第个分量的乘积​的期望为",{"5":{"95":1}}],["和的偏导数等于偏导数的和",{"5":{"48":2}}],["和value",{"5":{"94":2}}],["和头维度",{"5":{"93":1}}],["和专门的推理引擎",{"5":{"93":2}}],["和转置操作",{"5":{"93":2}}],["和内容",{"5":{"92":1}}],["和位置",{"5":{"91":2,"92":4,"93":1}}],["和考虑位置的编码向量",{"5":{"91":1}}],["和余弦序列",{"5":{"90":1}}],["和虚部序列",{"5":{"90":1}}],["和嵌入维度",{"5":{"90":1}}],["和维度索引",{"5":{"88":1}}],["和模型维度",{"5":{"88":1}}],["和保持完整",{"5":{"88":1}}],["和正交性",{"5":{"88":2}}],["和核方法中的核心概念",{"5":{"87":2}}],["和0",{"5":{"87":2}}],["和低秩结构",{"5":{"87":2}}],["和低秩矩阵恢复问题",{"5":{"50":2}}],["和重计算",{"5":{"86":2}}],["和预测向量",{"5":{"51":1}}],["和预测分布",{"5":{"71":1}}],["和不可减少误差",{"5":{"51":2}}],["和是正交矩阵",{"5":{"94":1}}],["和是向量的第和个分量",{"5":{"92":1}}],["和是原始的query和key向量",{"5":{"88":1}}],["和是垂直的",{"5":{"51":1}}],["和是可学习的逐元素缩放和偏移参数",{"5":{"41":1}}],["和是可学习的缩放和偏移参数",{"5":{"41":1}}],["和对称性",{"5":{"51":2}}],["和对角矩阵",{"5":{"50":2}}],["和上三角矩阵",{"5":{"50":2}}],["和标量",{"5":{"50":1}}],["和连接节点的边",{"5":{"50":2}}],["和防止对抗攻击都很重要",{"5":{"50":2}}],["和transformer中",{"5":{"50":2}}],["和qr算法来计算特征值和特征向量",{"5":{"50":2}}],["和张量收缩",{"5":{"50":2}}],["和矩阵",{"5":{"50":2}}],["和分别表示键空间和值空间的维度",{"5":{"95":1}}],["和分别为第层的权重矩阵和偏置向量",{"5":{"46":1}}],["和分组查询注意力",{"5":{"93":2}}],["和分层张量分解",{"5":{"50":2}}],["和分配律",{"5":{"50":2}}],["和随机变量",{"5":{"96":1}}],["和归一性",{"5":{"96":1}}],["和归一化操作",{"5":{"48":2}}],["和相关系数",{"5":{"96":2}}],["和备择假设",{"5":{"96":2}}],["和渐近有效的",{"5":{"96":2}}],["和交叉熵在分类任务中的应用",{"5":{"61":2}}],["和交叉熵",{"5":{"70":2,"96":2}}],["和向量4",{"5":{"88":1}}],["和向量",{"5":{"96":2}}],["和中位数都等于均值",{"5":{"96":2}}],["和线性无关",{"5":{"96":1}}],["和倾向于反向变化",{"5":{"96":1}}],["和倾向于同向变化",{"5":{"96":1}}],["和逻辑非",{"5":{"47":2}}],["和突触",{"5":{"47":2}}],["和数学家沃尔特",{"5":{"47":2}}],["和罗纳德",{"5":{"44":2}}],["和其上下文",{"5":{"71":1}}],["和其他深度学习高级主题奠定了坚实的理论基础",{"5":{"45":2}}],["和激活",{"5":{"71":1}}],["和奇数维度",{"5":{"71":2}}],["和表示的形式",{"5":{"42":1}}],["和输出梯度",{"5":{"41":1}}],["和​分别是行归一化和列归一化操作",{"5":{"86":1}}],["和​",{"5":{"41":1,"69":1,"86":1}}],["和传递",{"5":{"41":2}}],["和",{"5":{"41":7,"42":1,"44":2,"45":4,"46":1,"47":2,"48":6,"50":12,"51":3,"61":8,"69":11,"70":6,"71":9,"86":3,"87":8,"88":12,"89":3,"90":5,"91":16,"92":14,"93":8,"94":2,"95":4,"96":10,"97":5,"99":2,"101":4}}],["和符号微分",{"5":{"45":2,"48":2}}],["和任意标量",{"5":{"71":1}}],["和任意",{"5":{"48":1,"70":1,"71":1,"97":1}}],["和平移变换",{"5":{"45":2}}],["和反向模式",{"5":{"45":2}}],["和个负样本键",{"5":{"69":1}}],["和key向量",{"5":{"88":2}}],["和key",{"5":{"69":1}}],["sm",{"5":{"93":2}}],["smoothing",{"5":{"96":2}}],["sram的容量",{"5":{"86":2}}],["src=",{"5":{"44":1,"45":2,"47":2,"48":3,"50":2,"61":1,"96":3}}],["sliding",{"5":{"86":2}}],["shannon",{"5":{"61":4}}],["shift",{"5":{"41":2}}],["subtraction问题",{"2":{"61":1},"5":{"61":1}}],["sumj",{"5":{"97":2}}],["sum",{"5":{"42":38,"46":22,"50":2,"61":22,"97":8}}],["surrogate",{"5":{"70":2}}],["sample",{"5":{"51":2}}],["svd",{"5":{"50":2,"94":2}}],["system",{"5":{"50":2}}],["synapse",{"5":{"47":2}}],["sne和umap",{"5":{"50":2}}],["spline",{"5":{"45":2}}],["sparse",{"5":{"86":6,"87":2,"93":8}}],["space",{"5":{"50":4,"90":2}}],["span",{"5":{"92":2,"99":2,"101":2}}],["span掩码",{"5":{"99":2,"101":2}}],["span掩码策略连续掩码一个token片段",{"5":{"99":2,"101":2}}],["spectral",{"5":{"50":8,"87":4}}],["specialization",{"5":{"70":2}}],["s",{"5":{"96":2}}],["skewness",{"5":{"96":2}}],["sgd",{"5":{"97":2}}],["sgd倾向于收敛到平坦的局部最小值",{"5":{"48":2}}],["sgd的随机性本身就提供了一种隐式的噪声源",{"5":{"48":2}}],["stationary",{"5":{"87":2}}],["standard",{"5":{"96":2}}],["structure",{"5":{"87":2}}],["step",{"5":{"48":2,"97":2}}],["stochastic",{"5":{"44":2,"87":2}}],["stone",{"5":{"71":2}}],["schwarz不等式",{"5":{"87":2}}],["schmidt正交化",{"5":{"50":2}}],["schmidt正交化可以移除表示中的冗余成分",{"5":{"50":2}}],["schmidt正交化过程是构建正交基的标准算法",{"5":{"50":2}}],["scheduling",{"5":{"48":2}}],["scaling",{"5":{"44":2}}],["scaled",{"0":{"95":1},"4":{"95":1},"5":{"61":4,"71":2,"85":5,"88":2,"95":9}}],["selection",{"5":{"92":2}}],["self",{"5":{"21":10,"42":2,"44":1,"45":2,"47":2,"48":3,"50":2,"61":3,"85":26,"88":2,"96":3}}],["sequence",{"5":{"50":2,"95":4}}],["separable",{"5":{"47":2}}],["setminus",{"5":{"69":2}}],["soft",{"5":{"92":2}}],["softmax归一化",{"5":{"95":2}}],["softmax可以写为",{"5":{"95":2}}],["softmax可能过于",{"5":{"69":2}}],["softmax和输出投影则通常使用全精度以保持数值稳定性",{"5":{"93":2}}],["softmax和交叉熵通常会被",{"5":{"61":1}}],["softmax和交叉熵通常会被融合计算",{"5":{"61":1}}],["softmax和交叉熵的组合在数学上等价于最大似然估计",{"5":{"47":2}}],["softmax变换可以视为核矩阵的",{"5":{"87":2}}],["softmax变换是逐行进行的",{"5":{"87":2}}],["softmax变换对秩的影响",{"2":{"87":1},"5":{"87":1}}],["softmax变换正是这种对数几率的指数化和归一化",{"5":{"71":2}}],["softmax注意力可以表示负定的交互模式",{"5":{"86":2}}],["softmax与离散分布采样",{"2":{"71":1},"5":{"71":1}}],["softmax与交叉熵的梯度计算",{"2":{"61":1},"5":{"61":1}}],["softmax与交叉熵的组合源于信息论的基本原理",{"5":{"69":2}}],["softmax数值不稳定的根源",{"2":{"61":1},"5":{"61":1}}],["softmax神经元与多项分布",{"5":{"47":2}}],["softmax激活",{"5":{"44":2}}],["softmax等激活函数与概率分布的内在联系",{"5":{"71":2}}],["softmax提供token间的交互非线性",{"5":{"71":2}}],["softmax提供了一种可微的采样近似",{"5":{"71":2}}],["softmax趋向于均匀分布",{"5":{"69":4,"71":2}}],["softmax趋向于one",{"5":{"71":2}}],["softmax趋向于",{"5":{"69":2}}],["softmax样本定义为",{"5":{"71":2}}],["softmax分布",{"5":{"71":2}}],["softmax及其梯度特性与交叉熵损失的配合至关重要",{"5":{"41":2}}],["softmax的雅可比矩阵在对角线上的元素接近1",{"5":{"95":2}}],["softmax的梯度流分析",{"2":{"95":1},"5":{"95":1}}],["softmax的梯度形式已在7",{"5":{"94":2}}],["softmax的梯度为",{"5":{"89":2}}],["softmax的非线性变换使得注意力权重具有一定的",{"5":{"92":2}}],["softmax的非线性变换可能",{"5":{"87":2}}],["softmax的非线性确保了注意力权重具有归一化的概率解释",{"5":{"71":2}}],["softmax的数学性质",{"5":{"47":2}}],["softmax的数学性质由温度参数",{"5":{"71":1}}],["softmax的数学性质由温度参数控制",{"5":{"71":1}}],["softmax的输出将满足",{"5":{"95":1}}],["softmax的输出将满足对于所有",{"5":{"95":1}}],["softmax的输出将趋向于one",{"5":{"95":2}}],["softmax的输出分布不会过于",{"5":{"95":2}}],["softmax的输出",{"5":{"71":1}}],["softmax的输出满足概率分布的所有公理",{"5":{"71":1}}],["softmax的极端梯度",{"5":{"41":2}}],["softmax的",{"5":{"69":2}}],["softmax函数在注意力计算中扮演着至关重要的角色",{"5":{"95":2}}],["softmax函数具有",{"5":{"96":2}}],["softmax函数具有一个重要特性",{"5":{"95":2}}],["softmax函数具有以下重要性质",{"5":{"47":2}}],["softmax函数的分母对所有分子的指数项进行归一化",{"5":{"96":2}}],["softmax函数的梯度可以表示为雅可比矩阵的形式",{"5":{"95":2}}],["softmax函数的输入都能保持在一个合理的范围内",{"5":{"95":2}}],["softmax函数的定义为",{"5":{"95":2}}],["softmax函数的饱和行为分析",{"2":{"95":1},"5":{"95":1}}],["softmax函数",{"5":{"61":2,"69":2,"96":1}}],["softmax函数将空间中的向量映射到概率单纯形上",{"5":{"96":1}}],["softmax函数将相似度转换为概率分布",{"5":{"95":2}}],["softmax函数将模型的原始输出",{"5":{"96":2}}],["softmax函数将",{"5":{"47":1,"96":1}}],["softmax函数将个神经元的输出归一化为概率分布",{"5":{"47":1}}],["softmax函数将任意实数向量映射到概率单纯形上",{"5":{"69":1}}],["softmax函数将任意实数向量",{"5":{"69":1}}],["softmax函数是大语言模型",{"5":{"47":2}}],["softmax函数是sigmoid在多类别情况下的推广",{"5":{"71":2}}],["softmax函数定义为",{"5":{"69":2,"71":2}}],["softmax函数与概率单纯形",{"5":{"71":2}}],["softmax函数与多项式分布",{"2":{"71":1},"5":{"71":1}}],["softmax",{"2":{"61":1,"69":1},"5":{"42":4,"48":2,"61":67,"69":9,"70":28,"96":2,"97":8}}],["softmax操作可以在行级别并行执行",{"5":{"95":2}}],["softmax操作可以在每个线程束内高效地归约求和",{"5":{"93":2}}],["softmax操作的几何与概率本质",{"2":{"69":1},"5":{"69":1}}],["softmax操作",{"5":{"69":2}}],["softmax不是简单的线性投影",{"5":{"69":2}}],["softmax是非线性变换",{"5":{"87":4}}],["softmax是gumbel分布和softmax分布的结合",{"5":{"71":2}}],["softmax是多项logit模型",{"5":{"69":2}}],["softmax将不可避免地进入饱和区域",{"5":{"95":2}}],["softmax将每一行归一化为概率分布",{"5":{"71":2}}],["softmax将网络的logits输出转换为类别概率分布",{"5":{"69":2}}],["softmax将query",{"5":{"69":2}}],["softmax将这些相似度转换为概率分布",{"5":{"69":2}}],["softmax同样用于将相似度分数转换为概率分布",{"5":{"69":2}}],["softmax在三大场景中的统一性",{"2":{"69":1},"5":{"69":1}}],["softmax在transformer中的广泛使用",{"5":{"69":2}}],["softmax输出保持在合理的分布范围内",{"5":{"94":1}}],["softmax输出的每个元素都是正的",{"5":{"92":2}}],["softmax输出的概率",{"5":{"71":1}}],["softmax输出的概率表示样本属于第类的概率估计",{"5":{"71":1}}],["softmax输出为",{"5":{"96":1}}],["softmax输出为​",{"5":{"96":1}}],["softmax输出总是有效的概率分布",{"5":{"69":2}}],["softmax输出",{"5":{"69":1,"94":1}}],["softmax输入",{"5":{"69":1}}],["softmax结构的统一性",{"2":{"70":1},"5":{"70":1}}],["softmax对所有位置加权求和",{"5":{"99":2,"101":2}}],["softmax策略",{"5":{"99":2,"101":2}}],["soma",{"5":{"47":2}}],["swish函数",{"5":{"41":1}}],["swish函数通过可学习的参数提供了自适应的激活特性",{"5":{"41":1}}],["squared",{"5":{"51":2}}],["square",{"5":{"41":2}}],["sqrt",{"5":{"42":4,"46":4,"69":2,"97":2}}],["s形",{"5":{"41":2,"71":2}}],["sinusoidal",{"5":{"88":2,"91":2,"92":2}}],["sinkhorn迭代定义为",{"5":{"86":2}}],["sinkhorn稀疏化是一种基于最优传输的稀疏化方法",{"5":{"86":2}}],["singular",{"5":{"50":2,"94":2}}],["single",{"5":{"46":2}}],["simplex",{"5":{"61":2,"70":2}}],["signal",{"5":{"44":2}}],["sigmoid和softmax激活函数的输出可以解释为分类概率",{"5":{"47":2}}],["sigmoid神经元",{"5":{"47":1}}],["sigmoid神经元的输出可以解释为给定输入时",{"5":{"47":1}}],["sigmoid神经元与伯努利分布的对应",{"5":{"47":2}}],["sigmoid激活",{"5":{"44":2}}],["sigmoid变换可以理解为将先验信息",{"5":{"71":2}}],["sigmoid输出常用于二分类任务的概率预测",{"5":{"71":2}}],["sigmoid输出",{"5":{"42":1}}],["sigmoid输出区间",{"5":{"42":1}}],["sigmoid导数",{"5":{"42":2}}],["sigmoid导数的这种",{"5":{"42":2}}],["sigmoid导数的自化形式",{"5":{"42":2}}],["sigmoid以0",{"5":{"42":2}}],["sigmoid的二阶导数",{"5":{"42":2}}],["sigmoid的饱和区域",{"5":{"41":2}}],["sigmoid的导数变得很小",{"5":{"70":1}}],["sigmoid的导数是",{"5":{"70":2}}],["sigmoid的导数",{"5":{"70":1}}],["sigmoid曲线在其两端进入平坦区域",{"5":{"41":2}}],["sigmoid函数可以拟合任意单调递增的概率函数",{"5":{"47":2}}],["sigmoid函数可以将任意实数",{"5":{"71":1}}],["sigmoid函数可以将任意实数解释为伯努利分布的成功概率",{"5":{"71":1}}],["sigmoid函数恰好可以将神经元的净输入映射为概率值",{"5":{"47":2}}],["sigmoid函数将线性组合",{"5":{"71":1}}],["sigmoid函数将线性组合转换为概率",{"5":{"71":1}}],["sigmoid函数将实数域映射到区间",{"5":{"71":1}}],["sigmoid函数将整个实数轴压缩到",{"5":{"42":1}}],["sigmoid函数将整个实数轴压缩到区间",{"5":{"42":1}}],["sigmoid函数与伯努利分布",{"2":{"71":1},"5":{"71":1}}],["sigmoid函数是单调递增函数",{"5":{"42":2}}],["sigmoid函数是神经网络中最经典的激活函数之一",{"5":{"42":2}}],["sigmoid函数是分析梯度饱和的经典案例",{"5":{"41":1}}],["sigmoid函数的输出范围为",{"5":{"47":2}}],["sigmoid函数的逆函数正是对数几率的变换",{"5":{"71":2}}],["sigmoid函数的二阶导数为",{"5":{"42":2}}],["sigmoid函数的高阶导数可以递归地通过低阶导数表示",{"5":{"42":2}}],["sigmoid函数的分母是分子的积分",{"5":{"42":2}}],["sigmoid函数的导数取值范围是",{"5":{"42":2}}],["sigmoid函数的导数为",{"5":{"42":1}}],["sigmoid函数的导数具有一个极为优美的性质",{"5":{"42":2}}],["sigmoid函数的导数与性质",{"2":{"42":1},"5":{"42":1}}],["sigmoid函数的反函数为对数几率函数",{"5":{"42":2}}],["sigmoid函数的数学定义",{"2":{"42":1},"5":{"42":1}}],["sigmoid函数的梯度饱和区域定义为",{"5":{"41":2}}],["sigmoid函数的饱和区域分析",{"2":{"41":1},"5":{"41":1}}],["sigmoid函数",{"5":{"41":1,"42":5,"47":2,"71":1}}],["sigmoid",{"5":{"41":1,"48":2,"61":2,"70":12}}],["sigma",{"5":{"42":14,"46":4,"86":6}}],["size可能为1",{"5":{"41":2}}],["size",{"5":{"41":2,"50":2,"96":2}}],["其收敛速度由",{"5":{"97":1}}],["其收敛速度由的特征值决定",{"5":{"97":1}}],["其优化景观包含多个局部极小值",{"5":{"97":2}}],["其绝对数值本身并不具有直接的概率意义",{"5":{"96":2}}],["其标准差会以",{"5":{"95":1}}],["其标准差会以的速度增长",{"5":{"95":1}}],["其每一行都是一个有效的概率分布",{"5":{"95":2}}],["其每一行是一个",{"5":{"95":3}}],["其每一行是一个维的值向量",{"5":{"95":1}}],["其每一行是一个维的键向量",{"5":{"95":1}}],["其每一行是一个​维的查询向量",{"5":{"95":1}}],["其输出接近确定性分布",{"5":{"95":2}}],["其输出可以写为",{"5":{"93":2}}],["其softmax梯度与缩放后的版本有所不同",{"5":{"94":2}}],["其方差为",{"5":{"94":1,"95":4}}],["其方差为​",{"5":{"94":1}}],["其方向决定了模型在评估",{"5":{"94":2}}],["其设计源于对点积运算统计学特性的深入分析",{"5":{"95":2}}],["其设计目标是保持每层激活值和梯度的方差在各层之间稳定",{"5":{"94":2}}],["其设计天然契合概率分布间的差异度量需求",{"5":{"61":2}}],["其对角线元素为奇异值",{"5":{"94":2}}],["其对角元素为",{"5":{"44":2}}],["其对角元素为非负实数",{"5":{"45":2}}],["其信息也可能被其他位置",{"5":{"92":2}}],["其信息量应当越大",{"5":{"61":2}}],["其隐藏状态的更新公式为",{"5":{"92":2}}],["其值保持不变",{"5":{"91":2}}],["其值与样本的",{"5":{"70":2,"97":2}}],["其角速度为",{"5":{"90":1}}],["其角速度为​",{"5":{"90":1}}],["其频率为",{"5":{"90":1}}],["其频率为​",{"5":{"90":1}}],["其傅里叶变换为",{"5":{"90":2}}],["其辐角为",{"5":{"90":2}}],["其效果是两个点的",{"5":{"90":2}}],["其模为1",{"5":{"90":2}}],["其dft定义为",{"5":{"90":2}}],["其秩为",{"5":{"89":1}}],["其秩为的最佳近似",{"5":{"89":1}}],["其性质取决于注意力机制的实现",{"5":{"89":2}}],["其性质可以被严格证明",{"5":{"88":2}}],["其理论依据是",{"5":{"88":2}}],["其实现与llama类似",{"5":{"88":2}}],["其实际性能显著优于标准实现",{"5":{"86":2}}],["其原因可能在于",{"5":{"88":2}}],["其原因在于",{"5":{"88":2}}],["其历史渊源可以追溯到正弦位置编码的原始论文",{"5":{"88":2}}],["其左特征向量和右特征向量不同",{"5":{"87":2}}],["其特征向量可能与离散余弦变换",{"5":{"87":2}}],["其特征值非负",{"5":{"87":2}}],["其特征值和特征向量可以通过傅里叶变换分析",{"5":{"87":2}}],["其特征值分布在单位圆附近",{"5":{"41":2}}],["其特征值为和​",{"5":{"70":1,"97":1}}],["其特征值为",{"5":{"70":1,"97":1}}],["其行列式可以表示为特征值的乘积",{"5":{"87":2}}],["其谱性质可以用toeplitz矩阵的理论分析",{"5":{"87":2}}],["其谱性质因输入而异",{"5":{"87":2}}],["其谱范数至少为1",{"5":{"87":2}}],["其谱半径为1",{"5":{"41":2}}],["其核心思想是让所有注意力头共享同一个key和value投影",{"5":{"86":2}}],["其核心思想是在计算当前块的同时预加载下一块的数据",{"5":{"86":2}}],["其核心思想是通过分块计算",{"5":{"86":2}}],["其核心创新在于引入了参数学习机制",{"5":{"47":2}}],["其非零元素集中在主对角线附近的",{"5":{"86":1}}],["其非零元素集中在主对角线附近的条对角线上",{"5":{"86":1}}],["其非线性程度比二次函数更高",{"5":{"70":2}}],["其余为",{"5":{"61":2}}],["其余为0",{"5":{"61":2}}],["其定义为",{"5":{"61":2}}],["其定义域为",{"5":{"42":4}}],["其定义可以延伸到概率空间",{"5":{"70":2}}],["其度量张量由fisher信息矩阵给出",{"5":{"61":2}}],["其真实类别是唯一确定的",{"5":{"61":2}}],["其真值表为",{"5":{"47":2}}],["其二是后续的",{"5":{"61":2}}],["其一是计算出的",{"5":{"61":2}}],["其自信息",{"5":{"61":2}}],["其mse为",{"5":{"51":2}}],["其列向量",{"5":{"50":2}}],["其维度分别代表批量大小",{"5":{"50":2}}],["其维度为",{"5":{"61":2,"71":2}}],["其协方差矩阵",{"5":{"96":1}}],["其协方差矩阵是一个的对称半正定矩阵",{"5":{"96":1}}],["其他所有token作为隐式负样本",{"5":{"101":2}}],["其他高频成分仍然能够区分不同位置",{"5":{"90":2}}],["其他形式的损失函数可以转化为mse或与之等价",{"5":{"51":2}}],["其他为0",{"5":{"96":2}}],["其他情况输出为0",{"5":{"47":2}}],["其他键的概率接近0",{"5":{"69":2}}],["其pdf为",{"5":{"96":2}}],["其pmf记为",{"5":{"96":2}}],["其期望定义为",{"5":{"96":2}}],["其元素表示生成词汇表中每个词的概率",{"5":{"96":2}}],["其上方的图构成一个凸集",{"5":{"48":2}}],["其长度",{"5":{"48":2}}],["其权重矩阵为",{"5":{"47":2}}],["其损失函数为交叉熵损失",{"5":{"47":2}}],["其连接强度决定了信号传递的效率",{"5":{"47":2}}],["其表达能力与参数量的关系如何",{"5":{"89":2}}],["其表达能力等价于单层线性变换",{"5":{"47":2}}],["其表达能力远超单个超平面",{"5":{"46":2}}],["其表达能力不再受限于线性变换的复合",{"5":{"71":2}}],["其所有依赖值已经就绪",{"5":{"45":2}}],["其后继节点之前被处理",{"5":{"45":2}}],["其计算图包含以下节点",{"5":{"45":2}}],["其雅可比矩阵",{"5":{"45":2}}],["其雅可比矩阵为",{"5":{"44":2}}],["其数学形式是在注意力分数矩阵上应用一个掩码矩阵",{"5":{"95":2}}],["其数学形式为",{"5":{"47":2,"88":2}}],["其数学性质直接决定了注意力计算的行为和效率",{"5":{"87":2}}],["其数学性质更容易分析",{"5":{"41":2}}],["其数学定义看似简单",{"5":{"51":2}}],["其数学定义为",{"5":{"47":2}}],["其数学表示可以清晰地揭示线性变换的本质局限性",{"5":{"71":2}}],["其第个元素表示第个查询向量与第个键向量的点积相似度",{"5":{"95":1}}],["其第行第列元素为",{"5":{"89":1}}],["其第行为样本的误差信号​",{"5":{"44":1}}],["其第",{"5":{"44":1,"89":1,"95":1}}],["其基本思想是用有限差分近似计算梯度",{"5":{"44":2}}],["其梯度为行向量",{"5":{"44":2}}],["其注意力层和前馈层堆叠形成深层结构",{"5":{"71":2}}],["其参数数量为",{"5":{"71":2}}],["其参数集合为",{"5":{"45":2}}],["其能表示的函数就被限制在这个有限维的函数空间中",{"5":{"71":2}}],["其根本原因在于激活函数引入的非线性变换",{"5":{"71":2}}],["其激活值对输入的微小变化不敏感",{"5":{"42":2}}],["其解为",{"5":{"41":1}}],["其解为​",{"5":{"41":1}}],["其导数的",{"5":{"42":2}}],["其导数在大多数区域都接近1",{"5":{"41":2}}],["其导数通常也接近零",{"5":{"41":2}}],["其复杂性源于归一化操作的非线性",{"5":{"41":2}}],["其",{"5":{"41":2}}],["其中在填充位置为",{"5":{"95":1}}],["其中query",{"5":{"94":2}}],["其中query与所有key的相似度被用来计算加权平均",{"5":{"69":2}}],["其中通常远小于",{"5":{"93":1}}],["其中旋转角度随位置非线性变化",{"5":{"88":2}}],["其中旋转角度",{"5":{"88":2}}],["其中核函数为",{"5":{"87":2}}],["其中由真实标签决定",{"5":{"61":1}}],["其中对数底数",{"5":{"61":1}}],["其中对数底数决定了信息的计量单位",{"5":{"61":1}}],["其中查询向量与键向量在特征维度上进行点积运算",{"5":{"50":2}}],["其中条件均值为",{"5":{"96":2}}],["其中表示分块对角矩阵",{"5":{"93":1}}],["其中表示模型对第",{"5":{"61":1}}],["其中表示第个样本的权重",{"5":{"51":1}}],["其中表示完全正相关",{"5":{"96":1}}],["其中表示逐元素乘法",{"5":{"44":1,"97":1}}],["其中表示位置是有效token",{"5":{"99":1,"101":1}}],["其中表示除位置外所有token的序列",{"5":{"99":1,"101":1}}],["其中第",{"5":{"96":1}}],["其中第个元素为",{"5":{"96":1}}],["其中且",{"5":{"96":1}}],["其中节点表示变量或操作",{"5":{"48":2}}],["其中行索引",{"5":{"46":1}}],["其中行索引对应输出神经元的索引",{"5":{"46":1}}],["其中的计算结果与完全相同",{"5":{"46":1}}],["其中的列向量是的特征向量",{"5":{"45":1}}],["其中来自矩阵乘法",{"5":{"45":1}}],["其中和的维度较低",{"5":{"87":1}}],["其中和是未编码位置的原始向量",{"5":{"88":1}}],["其中和是激活函数",{"5":{"48":1}}],["其中和是正交矩阵",{"5":{"45":1}}],["其中和均为可微函数",{"5":{"45":1}}],["其中和分别是输入和输出的维度",{"5":{"41":1}}],["其中和分别是的最小和最大奇异值",{"5":{"41":1}}],["其中矩阵乘法的顺序不可交换",{"5":{"44":2,"45":2}}],["其中​表示位置的位置编码向量",{"5":{"91":1}}],["其中​表示位置的内容嵌入向量",{"5":{"91":1}}],["其中​是第个频率成分在位置处的相位",{"5":{"90":1}}],["其中​是概率分布",{"5":{"44":1}}],["其中​",{"5":{"44":1,"48":1,"96":1}}],["其中假设",{"5":{"44":2}}],["其中每个分量",{"5":{"96":1}}],["其中每个分量代表模型对第个词元的原始置信度评分",{"5":{"96":1}}],["其中每个头",{"5":{"93":1}}],["其中每个头独立地进行注意力计算",{"5":{"93":1}}],["其中每个节点",{"5":{"92":2}}],["其中每个词被映射为空间中的一个点",{"5":{"50":2}}],["其中每个元素",{"5":{"50":2}}],["其中每个",{"5":{"48":2,"99":2,"101":2}}],["其中每个层映射​定义为",{"5":{"46":1}}],["其中每个层映射",{"5":{"46":1}}],["其中每个是来自词汇表的token",{"5":{"99":1,"101":1}}],["其中每个是子词单元",{"5":{"99":1,"101":1}}],["其中每一层的变换",{"5":{"71":1}}],["其中每一层的变换和激活将前一层的表示转换为新的表示",{"5":{"71":1}}],["其中包含所有奇异值",{"5":{"41":1}}],["其中为样本总数",{"5":{"61":1}}],["其中为真实分布",{"5":{"61":1}}],["其中为标准正态分布的累积分布函数",{"5":{"47":1}}],["其中为的摩尔",{"5":{"47":1}}],["其中为变换矩阵",{"5":{"47":1}}],["其中为激活函数",{"5":{"47":1}}],["其中为矩阵变量",{"5":{"44":1}}],["其中为输入信号",{"5":{"47":1}}],["其中为输入",{"5":{"44":1}}],["其中为权重矩阵",{"5":{"45":1,"46":2}}],["其中为网络输入",{"5":{"46":1}}],["其中",{"5":{"41":11,"42":1,"44":12,"45":8,"46":6,"47":13,"48":9,"50":15,"51":16,"61":23,"69":11,"70":11,"71":10,"86":17,"87":28,"88":8,"89":25,"90":17,"91":13,"92":21,"93":16,"94":10,"95":5,"96":15,"97":7,"99":16,"101":18}}],["其中是总训练步数",{"5":{"97":1}}],["其中是初始学习率",{"5":{"97":1}}],["其中是损失函数的利普希茨常数",{"5":{"97":1}}],["其中是注意力权重矩阵",{"5":{"97":1}}],["其中是注意力头的数量",{"5":{"50":1}}],["其中是缩放后注意力分数矩阵的softmax梯度矩阵",{"5":{"94":1}}],["其中是缩放因子",{"5":{"44":1}}],["其中是多头注意力中的头数",{"5":{"94":1}}],["其中是卷积矩阵",{"5":{"93":1}}],["其中是卷积核",{"5":{"93":1}}],["其中是某个可学习的距离函数",{"5":{"92":1}}],["其中是一个的下三角矩阵",{"5":{"95":1}}],["其中是一个可学习的参数",{"5":{"88":1}}],["其中是一个缩放因子",{"5":{"88":1}}],["其中是一个非线性函数",{"5":{"88":1}}],["其中是角度为",{"5":{"88":1}}],["其中是核函数",{"5":{"87":1}}],["其中是有效秩",{"5":{"87":1}}],["其中是数据矩阵",{"5":{"87":1}}],["其中是频率参数",{"5":{"86":1}}],["其中是逐元素乘方",{"5":{"86":1}}],["其中是逐元素累积的梯度平方",{"5":{"48":1}}],["其中是特征维度",{"5":{"86":1}}],["其中是one",{"5":{"61":1}}],["其中是投影矩阵",{"5":{"51":1}}],["其中是设计矩阵的最小非零特征值",{"5":{"97":1}}],["其中是设计矩阵",{"5":{"51":1,"70":1}}],["其中是序列长度",{"5":{"50":1}}],["其中是单位矩阵",{"5":{"50":1}}],["其中是标准正态分布的cdf",{"5":{"96":1}}],["其中是似然函数",{"5":{"96":1}}],["其中是未知参数",{"5":{"96":1}}],["其中是率参数",{"5":{"96":1}}],["其中是成功概率",{"5":{"96":1}}],["其中是衰减系数",{"5":{"48":1}}],["其中是速度向量",{"5":{"48":1}}],["其中是残差函数的雅可比矩阵",{"5":{"48":1}}],["其中是梯度与方向之间的夹角",{"5":{"48":1}}],["其中是外层函数关于中间变量的导数",{"5":{"48":1}}],["其中是全1向量",{"5":{"47":2,"89":1}}],["其中是仿射变换算子",{"5":{"47":1}}],["其中是批大小",{"5":{"50":1}}],["其中是批量大小",{"5":{"50":1}}],["其中是批量隐藏激活矩阵",{"5":{"44":1}}],["其中是批量预测矩阵",{"5":{"44":1}}],["其中是批均值",{"5":{"41":1}}],["其中是行向量还是列向量需要注意维度匹配",{"5":{"44":1}}],["其中是网络预测",{"5":{"44":1}}],["其中是网络的原始输出",{"5":{"71":1}}],["其中是网络对第类的",{"5":{"69":1}}],["其中是激活函数",{"5":{"71":1}}],["其中是负样本的数量",{"5":{"69":1,"71":1}}],["其中是随机噪声源",{"5":{"71":1}}],["其中是gumbel噪声",{"5":{"71":1}}],["其中是非线性激活函数",{"5":{"71":1}}],["其中是变换矩阵",{"5":{"71":1}}],["其中是两个权重矩阵的乘积",{"5":{"71":1}}],["其中是双曲正割函数",{"5":{"42":1}}],["其中是输入特征",{"5":{"51":1}}],["其中是输入神经元的数量",{"5":{"96":1}}],["其中是输入权重向量",{"5":{"71":1}}],["其中是输入层到隐藏层的权重矩阵",{"5":{"71":1}}],["其中是输入分布的支持域",{"5":{"41":1}}],["其中是的第列",{"5":{"47":1}}],["其中是的均方根",{"5":{"41":1}}],["其中是的最大奇异值",{"5":{"41":1}}],["其中是的最小特征值",{"5":{"70":1}}],["其中是裁剪阈值",{"5":{"41":1}}],["其中是均值向量",{"5":{"96":1}}],["其中是均值参数",{"5":{"96":1}}],["其中是均值",{"5":{"41":1}}],["其中是噪声样本与正样本的比例",{"5":{"69":1}}],["其中是由网络参数定义的函数映射",{"5":{"45":1}}],["其中是由softmax定义的概率分布",{"5":{"69":1}}],["其中是查询向量",{"5":{"69":1}}],["其中是查询与键的相似度",{"5":{"69":1}}],["其中是位置到向量的映射",{"5":{"91":1}}],["其中是位置与其他位置之间的互信息",{"5":{"69":1}}],["其中是位置的隐藏状态",{"5":{"99":1,"101":1}}],["其中是预测误差",{"5":{"70":1}}],["其中是常数",{"5":{"70":1,"97":1}}],["其中是黎曼度量张量",{"5":{"70":1}}],["其中是真实的目标函数",{"5":{"70":1}}],["其中是第个位置的输入embedding",{"5":{"88":1}}],["其中是第个任务的权重",{"5":{"70":1}}],["其中是第步的参数",{"5":{"48":1}}],["其中是模型的隐藏维度",{"5":{"95":1}}],["其中是模型参数",{"5":{"99":1,"101":1}}],["其中是被掩码的span集合",{"5":{"99":1,"101":1}}],["其中是共享参数",{"5":{"99":1,"101":1}}],["其中是任务的权重",{"5":{"99":1,"101":1}}],["其中是sigmoid函数",{"5":{"99":2,"101":2}}],["其中是归一化常数",{"5":{"99":1,"101":1}}],["其中注意力权重度量位置对位置的",{"5":{"69":1}}],["其中注意力权重",{"5":{"69":1}}],["其中也涉及",{"5":{"70":2}}],["其中到是前缀",{"5":{"99":1,"101":1}}],["其中被人类偏好于",{"5":{"99":1,"101":1}}],["其中权重由",{"5":{"61":2}}],["其中权重由相似度决定",{"5":{"70":2}}],["其中权重是时间的函数",{"5":{"99":1,"101":1}}],["其中权重",{"5":{"99":1,"101":1}}],["其次是卷积对偶",{"5":{"90":2}}],["其次分析谱范数",{"5":{"87":2}}],["其次",{"5":{"41":6,"42":2,"48":4,"50":10,"61":4,"69":2,"70":2,"71":2,"86":2,"87":2,"89":2,"90":2,"91":2,"93":2,"96":4}}],["其联合分布为",{"5":{"69":2}}],["其图像呈标准的s形曲线",{"5":{"42":2}}],["其图像是一个抛物面",{"5":{"70":2}}],["其任意两点间的弦位于函数图像上方",{"5":{"70":2,"97":2}}],["其hessian矩阵为",{"5":{"70":2,"97":2}}],["其hessian恰好是fisher信息矩阵",{"5":{"70":2}}],["其关于和的梯度结构与上述交叉熵关于的梯度结构高度相似",{"5":{"70":1}}],["其关于",{"5":{"70":1}}],["为预测概率",{"5":{"96":1}}],["为离散随机变量",{"5":{"96":1}}],["为真实标签",{"5":{"96":2}}],["为真实分布",{"5":{"61":1}}],["为注意力输出",{"5":{"94":1}}],["为注意力权重矩阵",{"5":{"92":2}}],["为某个阈值",{"5":{"92":2}}],["为值矩阵",{"5":{"92":2}}],["为简化展示",{"5":{"91":2}}],["为整数",{"5":{"91":2}}],["为深入理解位置编码的数学本质奠定坚实基础",{"5":{"91":2}}],["为深层网络的稳定训练提供了数学保障",{"5":{"41":2}}],["为经过rope编码的query和key向量",{"5":{"88":2}}],["为在实际应用中选择和设计合适的注意力机制提供理论指导",{"5":{"86":2}}],["为在某种语义上有意义的位置",{"5":{"69":2}}],["为样本总数",{"5":{"61":1}}],["为绝对值很大的负数时",{"5":{"61":2}}],["为模型分布",{"5":{"61":2}}],["为后续学习多头注意力",{"5":{"95":2}}],["为后续学习更复杂的位置编码方案",{"5":{"90":2}}],["为后续学习更复杂的损失函数奠定坚实的数学基础",{"5":{"51":2}}],["为后续讨论kl散度提供了几何背景",{"5":{"61":2}}],["为后续机器学习中的损失函数设计提供了深刻的理论启示",{"5":{"61":2}}],["为大语言模型提供了处理不确定性的理论基础",{"5":{"96":2}}],["为行向量",{"5":{"47":1}}],["为标准正态分布的累积分布函数",{"5":{"47":1}}],["为标量损失函数",{"5":{"44":1}}],["为变换矩阵",{"5":{"47":1}}],["为误差函数",{"5":{"47":2}}],["为平移向量",{"5":{"47":2}}],["为偏置项",{"5":{"47":2}}],["为偏置向量",{"5":{"46":2}}],["为权重向量",{"5":{"47":2}}],["为权重矩阵",{"5":{"45":1,"46":2}}],["为神经网络的现代发展奠定了基础",{"5":{"47":2}}],["为神经元输出",{"5":{"47":2}}],["为阶跃激活函数",{"5":{"47":2}}],["为激活阈值",{"5":{"47":2}}],["为激活函数",{"5":{"45":2,"46":2,"47":1}}],["为连接权重",{"5":{"47":2}}],["为网络输入",{"5":{"46":1}}],["为该层的输出向量",{"5":{"46":2}}],["为逐元素应用的激活函数",{"5":{"46":2}}],["为两个映射",{"5":{"45":1}}],["为理解和改进大语言模型的训练策略提供了理论基础",{"5":{"97":2}}],["为理解和优化大语言模型提供了核心的数学工具",{"5":{"48":2}}],["为理解投影矩阵的几何性质提供了强大的工具",{"5":{"94":2}}],["为理解反向传播提供必要的背景知识",{"5":{"45":2}}],["为理解大语言模型中非线性变换的数学本质奠定坚实基础",{"5":{"71":2}}],["为我们进一步学习反向传播算法",{"5":{"45":2}}],["为序列长度",{"5":{"45":2}}],["为序列中的每个位置分配数学表示",{"5":{"71":2}}],["为序列中的每个位置添加位置信息",{"5":{"71":2}}],["为输出向量",{"5":{"47":2}}],["为输出数",{"5":{"44":1,"45":1}}],["为输入信号",{"5":{"47":1}}],["为输入数",{"5":{"44":1,"45":1}}],["为输入",{"5":{"44":1}}],["为输入向量",{"5":{"44":2,"47":1}}],["为矩阵变量",{"5":{"44":1}}],["为克罗内克函数",{"5":{"44":2}}],["为什么需要反向传播",{"2":{"44":1},"5":{"44":1}}],["为基准",{"5":{"71":1}}],["为了便于分析和控制",{"5":{"95":2}}],["为了分析多层注意力如何建模长程依赖",{"5":{"92":2}}],["为了更精确地分析注意力机制在长程依赖建模方面的优势",{"5":{"92":2}}],["为了更直观地理解正弦余弦位置编码",{"5":{"91":2}}],["为了更直观地理解rope的运作机制",{"5":{"88":2}}],["为了更直观地理解mse的几何解释",{"5":{"51":2}}],["为了量化注意力机制建模长程依赖的能力",{"5":{"92":2}}],["为了理解缩放的必要性",{"5":{"95":2}}],["为了理解注意力机制的突破",{"5":{"92":2}}],["为了理解这一点",{"5":{"88":2}}],["为了增强线性注意力的表达能力",{"5":{"86":2}}],["为了在现代硬件上高效实现稀疏注意力",{"5":{"86":2}}],["为了解决这些问题并适应不同的应用需求",{"5":{"86":2}}],["为了校正偏差",{"5":{"48":2}}],["为了高效地实现这些计算",{"5":{"48":2}}],["为了将仿射变换统一表示为线性变换的形式",{"5":{"47":2}}],["为了突破这一限制",{"5":{"46":2}}],["为了保持方差稳定",{"5":{"41":2}}],["为了克服nce的局限性",{"5":{"69":2}}],["为了深刻理解这一数学本质",{"5":{"71":2}}],["为了深入理解infonce的优化特性",{"5":{"69":2}}],["为了处理变长序列",{"5":{"99":2,"101":2}}],["为了提高数值稳定性",{"5":{"96":2}}],["为了提高计算效率",{"5":{"99":2,"101":2}}],["为",{"5":{"47":2,"61":4,"69":2,"89":4,"90":5,"91":2,"92":2,"96":2}}],["为正",{"5":{"42":1}}],["为损失函数",{"5":{"41":1}}],["为饱和阈值",{"5":{"41":1}}],["为未来研究和实践提供了理论指导",{"5":{"41":2}}],["为负区域提供有限的梯度传递",{"5":{"41":2}}],["为设计和改进大语言模型的训练策略提供理论指导",{"5":{"41":2}}],["为第层的激活函数",{"5":{"46":1}}],["为第层的输入",{"5":{"41":1}}],["为第层激活函数的导数",{"5":{"44":1}}],["为第",{"5":{"41":1,"44":1,"45":1,"46":1}}],["为其他位置",{"5":{"69":2}}],["为所有样本",{"5":{"70":2}}],["为分析两种应用提供了共同的语言",{"5":{"70":2}}],["为类别标签",{"5":{"47":1}}],["为每个位置分配唯一的",{"5":{"99":2,"101":2}}],["定理2",{"5":{"44":24,"45":26,"46":8,"47":14}}],["定理的证明思路基于stone",{"5":{"71":2}}],["定理3",{"5":{"41":12,"42":8,"71":4}}],["定理",{"5":{"69":2,"88":2,"99":4,"101":4}}],["定义logits向量",{"5":{"96":2}}],["定义的二次型下",{"5":{"94":1}}],["定义从位置",{"5":{"92":1}}],["定义从位置到位置的最小路径长度为",{"5":{"92":1}}],["定义有效依赖跨度",{"5":{"92":2}}],["定义角频率",{"5":{"91":1}}],["定义角频率为",{"5":{"91":1}}],["定义位置",{"5":{"90":1}}],["定义位置的复数编码向量为",{"5":{"90":1}}],["定义复数形式的编码分量",{"5":{"90":2}}],["定义注意力输出为",{"5":{"86":2}}],["定义注意力权重的熵",{"5":{"92":2}}],["定义注意力权重",{"5":{"69":2}}],["定义",{"5":{"61":10,"69":12,"70":6,"97":14,"99":30,"101":30}}],["定义了一种依赖关系图",{"5":{"92":1}}],["定义了一个有向加权图",{"5":{"87":2,"92":1}}],["定义了一个预测函数",{"5":{"51":1}}],["定义了一个参数化的分布",{"5":{"69":2}}],["定义了query空间和key空间之间的映射",{"5":{"87":2}}],["定义了所有可能的预测向量集合",{"5":{"51":1}}],["定义了流形上的几何结构",{"5":{"70":2,"71":2}}],["定义了相似度函数",{"5":{"69":1}}],["定义2",{"5":{"44":13,"45":18,"46":8,"47":20}}],["定义激活函数的梯度稳定性指标",{"5":{"41":2}}],["定义激活函数的饱和度量来量化其饱和程度",{"5":{"41":2}}],["定义饱和深度函数来量化输入值距离饱和区域的远近",{"5":{"41":2}}],["定义3",{"5":{"41":28,"42":8,"71":20}}],["定义为矩阵",{"5":{"87":1}}],["定义为矩阵的特征值的最大模",{"5":{"87":1}}],["定义为奇异值的和",{"5":{"87":2}}],["定义为在给定参数",{"5":{"61":2}}],["定义为所有元素平方和的平方根",{"5":{"50":2}}],["定义为使后验概率等于0",{"5":{"47":2}}],["定义为​",{"5":{"71":1,"96":1}}],["定义为",{"5":{"42":6,"45":3,"51":4,"61":2,"70":2,"71":5,"86":1,"87":5,"92":2,"95":2,"96":5}}],["定义正样本的归一化分数为",{"5":{"69":2}}],["定义一个概率分布使得",{"5":{"69":1}}],["定义一个概率分布",{"5":{"69":1}}],["定义一个有效位置掩码​",{"5":{"99":1,"101":1}}],["定义一个有效位置掩码",{"5":{"99":1,"101":1}}],["ntk",{"5":{"88":4}}],["nuclear",{"5":{"50":2,"87":2}}],["null",{"5":{"50":2}}],["number",{"5":{"87":2}}],["numbers",{"5":{"96":2}}],["numpy",{"5":{"50":2}}],["n",{"5":{"46":36}}],["need",{"5":{"91":2}}],["neox",{"5":{"88":1}}],["neox的原始论文讨论了rope与flash",{"5":{"88":2}}],["neox同样使用rope",{"5":{"88":1}}],["negative",{"5":{"61":8}}],["nesterov动量是动量方法的一个变体",{"5":{"48":2}}],["net",{"5":{"47":2}}],["network",{"5":{"46":2,"47":2,"50":2,"71":2,"92":2,"95":2}}],["nervous",{"5":{"47":2}}],["neuron",{"5":{"47":4}}],["neural",{"5":{"46":2,"47":2,"88":2,"92":2,"95":2}}],["neq",{"5":{"42":8,"61":10}}],["next",{"5":{"99":2,"101":2}}],["nats",{"5":{"61":2}}],["nat",{"5":{"96":2}}],["nas被用于自动发现新的激活函数",{"5":{"41":2}}],["nas",{"5":{"41":2}}],["noise",{"5":{"61":2,"69":4}}],["none",{"5":{"61":2}}],["norm",{"5":{"50":6,"87":2}}],["normal",{"5":{"96":2}}],["normalization",{"5":{"41":8,"50":4,"94":2,"96":2}}],["not组合表示",{"5":{"47":2}}],["not",{"5":{"47":2}}],["node",{"5":{"45":2}}],["noopener",{"5":{"21":10,"85":26}}],["nofollow",{"5":{"21":10,"85":26}}],["nce损失",{"5":{"69":2}}],["nce将其标记为正样本",{"5":{"69":2}}],["nce的核心技巧是将密度估计问题转化为一个二分类问题",{"5":{"69":2}}],["nce的损失函数为这个二分类问题的二元交叉熵",{"5":{"69":2}}],["nce的理论保证需要",{"5":{"69":2}}],["​作为相关信息来源的概率",{"5":{"95":1}}],["​作为相对位置",{"5":{"90":1}}],["​都加上一个较大的常数",{"5":{"95":1}}],["​增大时",{"5":{"95":1}}],["​变换后得到query向量",{"5":{"94":1}}],["​位",{"5":{"93":1}}],["​将输入矩阵",{"5":{"93":1}}],["​与堆叠投影矩阵",{"5":{"93":1}}],["​与正弦序列",{"5":{"90":1}}],["​隐含地学习了一种加权方案",{"5":{"93":1}}],["​沿头维度拼接",{"5":{"93":1}}],["​就位于张量的第",{"5":{"93":1}}],["​接近0",{"5":{"93":2}}],["​接近1",{"5":{"93":2,"95":1}}],["​正确地反映了位置",{"5":{"92":1}}],["​参数化",{"5":{"92":1}}],["​只依赖于查询和键的相对位置",{"5":{"92":1}}],["​可比",{"5":{"93":1}}],["​可能很小",{"5":{"92":1}}],["​可以预先计算",{"5":{"91":1}}],["​可以视为频率域中的",{"5":{"90":1}}],["​可以通过将",{"5":{"90":1}}],["​可以分配给不同的设备",{"5":{"45":1}}],["​同时依赖于位置",{"5":{"92":1}}],["​既编码了位置信息",{"5":{"92":1}}],["​直接依赖于所有",{"5":{"92":1}}],["​直接依赖于所有​",{"5":{"92":1}}],["​能够感知到",{"5":{"92":1}}],["​很大时这是一笔不小的开销",{"5":{"91":1}}],["​进入注意力计算",{"5":{"91":1}}],["​维的查询向量",{"5":{"95":1}}],["​维的查询空间",{"5":{"94":1}}],["​维的子空间中",{"5":{"94":1}}],["​维的空间",{"5":{"91":1}}],["​维投影",{"5":{"93":1}}],["​维子空间的映射",{"5":{"93":1}}],["​维空间",{"5":{"89":1}}],["​不等于",{"5":{"91":1}}],["​对应的周期仍然大于序列长度",{"5":{"91":1}}],["​对应的频率索引",{"5":{"90":1}}],["​随维度",{"5":{"91":1}}],["​随位置",{"5":{"90":1}}],["​成反比",{"5":{"90":1}}],["​比特的信息",{"5":{"90":1}}],["​经过",{"5":{"90":1}}],["​在复平面上旋转角度",{"5":{"90":1}}],["​按频率成分展开",{"5":{"90":1}}],["​使得",{"5":{"90":1}}],["​共同决定了可用的频率范围",{"5":{"90":1}}],["​通过残差连接直接参与到最终层的表示",{"5":{"92":1}}],["​通过以下积分计算",{"5":{"90":1}}],["​通常不会精确为零",{"5":{"92":1}}],["​通常与模型的隐藏维度相当",{"5":{"87":2}}],["​较大时",{"5":{"95":1}}],["​较大",{"5":{"90":2,"91":2}}],["​较小",{"5":{"90":2,"92":3}}],["​这一重要性质",{"5":{"89":1}}],["​这种结构将层归一化置于残差分支内部",{"5":{"41":1}}],["​允许处理更长的序列",{"5":{"89":1}}],["​行的参数",{"5":{"89":1}}],["​替代",{"5":{"89":1}}],["​的期望为",{"5":{"95":1}}],["​的具体值",{"5":{"94":1}}],["​的第一列",{"5":{"94":1}}],["​的集合",{"5":{"93":1}}],["​的qkv投影",{"5":{"93":1}}],["​的维度为",{"5":{"93":2}}],["​的线性变换",{"5":{"92":1}}],["​的梯度为例",{"5":{"94":1}}],["​的梯度",{"5":{"92":2}}],["​的影响需要经过",{"5":{"92":1}}],["​的信息量最多为",{"5":{"93":1}}],["​的信息量最多为​位",{"5":{"93":1}}],["​的信息",{"5":{"92":1}}],["​的序列",{"5":{"91":1}}],["​的几何直觉是",{"5":{"91":1}}],["​的每个维度配对",{"5":{"90":1}}],["​的某些维度",{"5":{"89":1}}],["​的行",{"5":{"89":1}}],["​的奇异值分解为",{"5":{"89":1}}],["​的秩不超过",{"5":{"89":1}}],["​的选择需要权衡内存开销和泛化能力",{"5":{"89":2}}],["​时",{"5":{"89":4,"94":1}}],["​空间",{"5":{"89":1}}],["​空间中的表示",{"5":{"45":1}}],["​上的坐标",{"5":{"89":1}}],["​个基向量张成的子空间中",{"5":{"89":1}}],["​个输出元素的总flops为",{"5":{"45":1}}],["​个输出元素的总flops为​",{"5":{"45":1}}],["​且",{"5":{"87":1}}],["​满足",{"5":{"87":1,"92":1}}],["​表示查询矩阵",{"5":{"95":2}}],["​表示位置对位置的注意力权重",{"5":{"92":2}}],["​表示位置",{"5":{"91":2,"92":2}}],["​表示从位置",{"5":{"87":1}}],["​表示从位置到位置的信息流动比例",{"5":{"87":1}}],["​表示第",{"5":{"46":1}}],["​到来时",{"5":{"86":1}}],["​分别是行归一化和列归一化操作",{"5":{"86":1}}],["​视为空间中的两个点",{"5":{"51":1}}],["​来最小化期望损失",{"5":{"51":1}}],["​来自偏置加法",{"5":{"45":2}}],["​范数",{"5":{"51":1}}],["​附近",{"5":{"48":1}}],["​定义为",{"5":{"46":1,"91":1}}],["​var",{"5":{"46":1}}],["​zj",{"5":{"46":1}}],["​后合并结果",{"5":{"45":1}}],["​次flops",{"5":{"45":1}}],["​次加法",{"5":{"45":1}}],["​是随机矩阵",{"5":{"93":1}}],["​是单头注意力能够表示的函数类",{"5":{"93":1}}],["​是独立学习的",{"5":{"93":1}}],["​是多头注意力能够表示的函数类",{"5":{"93":2}}],["​是所有位置",{"5":{"92":1}}],["​是权重矩阵",{"5":{"92":1}}],["​是时刻",{"5":{"92":2}}],["​是时刻的输入",{"5":{"92":1}}],["​是时刻的隐藏状态",{"5":{"92":1}}],["​是旋转角度",{"5":{"92":2}}],["​是不同的",{"5":{"91":1}}],["​是可学习的投影矩阵",{"5":{"91":2}}],["​是可学习编码残差",{"5":{"89":2}}],["​是互不相同的无理数倍数",{"5":{"90":1}}],["​是与频率",{"5":{"90":1}}],["​是与频率​对应的频率索引",{"5":{"90":1}}],["​是第层的残差贡献",{"5":{"92":1}}],["​是第",{"5":{"90":3,"92":1}}],["​是第个频率成分的频率",{"5":{"90":1}}],["​是第个频率的周期参数",{"5":{"90":1}}],["​是表达能力的硬上限",{"5":{"89":1}}],["​是的奇异值分解中的对应项",{"5":{"89":1}}],["​是的前列",{"5":{"89":1}}],["​是的第行",{"5":{"87":2}}],["​是预设的最大序列长度",{"5":{"89":2}}],["​是左右奇异向量",{"5":{"87":1}}],["​是",{"5":{"87":2,"89":2,"92":6}}],["​是上三角矩阵",{"5":{"87":2}}],["​是位置处词元的嵌入向量",{"5":{"94":1}}],["​是位置的编码向量",{"5":{"92":1}}],["​是位置的位置key",{"5":{"91":1}}],["​是位置在基​上的坐标",{"5":{"89":1}}],["​是位置",{"5":{"86":1,"89":1,"90":1,"91":1,"92":1,"94":1}}],["​是位置对位置的原始注意力分数",{"5":{"86":1}}],["​是最大值",{"5":{"86":2}}],["​是各层映射的复合",{"5":{"45":1}}],["​是概率分布",{"5":{"44":1}}],["​是对角矩阵",{"5":{"44":1}}],["​为的第行",{"5":{"95":1}}],["​为零",{"5":{"86":2}}],["​为网络输出",{"5":{"46":2}}],["​为后向传播的中间值",{"5":{"45":1}}],["​为",{"5":{"45":1,"86":1,"95":2}}],["​为偏置向量",{"5":{"45":2,"46":2}}],["​为第",{"5":{"44":1}}],["​为输出",{"5":{"44":2}}],["​和分别定义了键空间和值空间的投影",{"5":{"94":1}}],["​和注意力权重的约束",{"5":{"93":1}}],["​和键向量",{"5":{"92":1}}],["​和​是权重矩阵",{"5":{"92":1}}],["​和​是左右奇异向量",{"5":{"87":1}}],["​和value块",{"5":{"86":1}}],["​和任意",{"5":{"51":1}}],["​和",{"5":{"44":1,"87":1,"90":4,"91":3,"92":2,"93":2,"94":2}}],["​j",{"5":{"42":1}}],["​f",{"5":{"42":1}}],["​",{"5":{"41":20,"42":13,"44":15,"45":11,"46":6,"47":6,"48":8,"50":3,"51":3,"61":2,"69":9,"70":3,"71":3,"86":10,"87":20,"88":8,"89":18,"90":21,"91":6,"92":7,"93":13,"94":8,"95":1,"96":12,"97":1,"99":2,"101":2}}],["​​时",{"5":{"95":1}}],["​​为缩放后的注意力分数矩阵",{"5":{"95":1}}],["​​成正比",{"5":{"95":1}}],["​​这一缩放因子是scaled",{"5":{"95":1}}],["​​可以被理解为一个温度参数",{"5":{"95":1}}],["​​可以表示为前",{"5":{"89":1}}],["​​可以视为一种核矩阵",{"5":{"87":1}}],["​​的设计正是为了抵消维度增长对点积量级的影响",{"5":{"95":1}}],["​​的方差被控制在一个稳定的范围内",{"5":{"94":2}}],["​​的范数决定了梯度在传递过程中的缩放程度",{"5":{"92":1}}],["​​的秩较低",{"5":{"89":1}}],["​​的引入部分解决了这个问题",{"5":{"87":1}}],["​​的引入部分原因就是为了控制注意力矩阵的条件数",{"5":{"87":1}}],["​​在训练时从未见过",{"5":{"89":1}}],["​​",{"5":{"41":1,"48":1,"50":2,"61":4,"69":2,"87":1,"89":1,"94":1}}],["​p",{"5":{"69":1}}],["只在输入和输出端使用",{"5":{"97":2}}],["只在关键步骤使用全精度",{"5":{"50":2}}],["只依赖于",{"5":{"94":2}}],["只依赖于查询和键向量的内容",{"5":{"92":1}}],["只依赖于相对位置",{"5":{"88":1,"90":3}}],["只关注位置",{"5":{"92":2}}],["只与位置本身有关",{"5":{"91":2}}],["只计算新增位置的编码",{"5":{"89":2}}],["只留下相对位置信息",{"5":{"88":2}}],["只重新定向它在高维空间中的指向",{"5":{"88":2}}],["只改变它的方向",{"5":{"88":2}}],["只改变方向",{"5":{"50":2}}],["只使用了",{"5":{"87":1}}],["只保留多个独立的query投影",{"5":{"86":2}}],["只保留rms归一化",{"5":{"41":2}}],["只适用于对称正定矩阵",{"5":{"50":2}}],["只需进行转置操作",{"5":{"50":2}}],["只需要简单的三角函数计算和逐元素乘法",{"5":{"88":2}}],["只需要计算三角函数并进行逐元素乘法",{"5":{"88":2}}],["只需要计算相对位置",{"5":{"88":2}}],["只需要计算预测误差",{"5":{"51":2}}],["只需要将更长的索引映射到更大的角度",{"5":{"88":2}}],["只需要存储稀疏的注意力权重和值向量",{"5":{"86":2}}],["只需要",{"5":{"61":2}}],["只需要训练",{"5":{"50":1}}],["只需要训练个参数而不是个参数",{"5":{"50":1}}],["只需要按顺序执行各层的矩阵运算即可",{"5":{"45":2}}],["只需要知道当前的",{"5":{"42":1}}],["只需要知道当前的值即可",{"5":{"42":1}}],["只需要正确排名即可",{"5":{"70":2}}],["只是概率质量的分布会根据logits的相对大小进行调整",{"5":{"96":2}}],["只是简单地修改输入矩阵",{"5":{"95":2}}],["只是根据重要性进行加权",{"5":{"92":2}}],["只是平移",{"5":{"87":2}}],["只是在不同的应用场景下使用",{"5":{"61":2}}],["只是需要明确哪个变量被当作中间变量",{"5":{"48":2}}],["只是relu网络的",{"5":{"45":2}}],["只是重排",{"5":{"99":2,"101":2}}],["只是输入和输出的形式有所不同",{"5":{"99":2,"101":2}}],["只是权重因子的计算方式不同",{"5":{"99":2,"101":2}}],["只有经过适当的变换后才能转化为有效的概率分布",{"5":{"96":2}}],["只有被选中的头参与输出计算",{"5":{"93":2}}],["只有位置相关的旋转介入内积计算",{"5":{"88":2}}],["只有可对角化矩阵",{"5":{"50":2}}],["只有满秩",{"5":{"50":2}}],["只有方阵才可能存在逆矩阵",{"5":{"50":2}}],["只有当",{"5":{"96":1}}],["只有当和不相关时等号才成立",{"5":{"96":1}}],["只有两个输入均为0时输出为0",{"5":{"47":2}}],["只有超过阈值的信号才能通过",{"5":{"71":2}}],["只有后层能够学习",{"5":{"41":2}}],["只要存在一种注意力权重配置",{"5":{"92":2}}],["只要序列长度",{"5":{"90":1}}],["只要序列长度不超过最低频率成分的周期",{"5":{"90":1}}],["只要样本量足够大",{"5":{"96":2}}],["只要至少有一个输入为1",{"5":{"47":2}}],["只要激活函数是",{"5":{"71":2}}],["只要激活函数满足一定的数学条件",{"5":{"71":2}}],["只要模型结构固定",{"5":{"71":2}}],["只要不使用激活函数",{"5":{"71":2}}],["只能",{"5":{"91":1}}],["只能处理线性可分问题",{"5":{"47":2}}],["只能解决线性可分",{"5":{"47":2}}],["只能捕获输入的线性可分特征",{"5":{"45":2}}],["只能学习直线或超平面作为决策边界",{"5":{"71":2}}],["只能为正或零",{"5":{"42":1}}],["只能关注位置到",{"5":{"92":2}}],["只能关注位置",{"5":{"92":2,"95":1,"99":1,"101":1}}],["只缩放其大小",{"5":{"41":2,"97":2}}],["当梯度较小时",{"5":{"97":2}}],["当梯度信号被压缩到接近零时",{"5":{"41":2}}],["当logits与交叉熵损失函数结合使用时",{"5":{"96":2}}],["当点积被适当地缩放后",{"5":{"95":2}}],["当softmax的输入量级较大时",{"5":{"95":2}}],["当softmax输出接近one",{"5":{"41":2}}],["当维度",{"5":{"95":2}}],["当维度较大时",{"5":{"95":1}}],["当维度​增大时",{"5":{"95":1}}],["当维度匹配时",{"5":{"50":2}}],["当的方差较大时",{"5":{"94":1}}],["当的标准差为1时",{"5":{"41":1}}],["当头数增加时",{"5":{"93":2}}],["当然",{"5":{"93":2}}],["当达到百万甚至十亿级别时",{"5":{"92":1}}],["当前的transformer架构",{"5":{"101":2}}],["当前位置",{"5":{"92":2}}],["当前一步的计算未完成时",{"5":{"92":2}}],["当某些位置的重要性显著高于其他位置时",{"5":{"92":2}}],["当某个​接近1",{"5":{"95":1}}],["当某个",{"5":{"61":2,"95":1}}],["当某个类别的预测概率接近",{"5":{"70":2}}],["当信息需要经过很多步传递时",{"5":{"92":2}}],["当​​时",{"5":{"95":1}}],["当​很大时这是一笔不小的开销",{"5":{"91":1}}],["当​时",{"5":{"89":4,"94":1}}],["当位置差",{"5":{"91":3}}],["当位置差增加时",{"5":{"91":1}}],["当位置差较大时",{"5":{"91":1}}],["当位置差较小时",{"5":{"91":1}}],["当位置从",{"5":{"90":1}}],["当位置从变为时",{"5":{"90":1}}],["当大时",{"5":{"91":1}}],["当足够大时",{"5":{"90":1}}],["当增加1时",{"5":{"90":1}}],["当增大时",{"5":{"42":1,"92":1}}],["当处理离散序列",{"5":{"90":2}}],["当处理长序列",{"5":{"86":2}}],["当编码向量线性无关时",{"5":{"89":2}}],["当上下文长度从8k扩展到32k时",{"5":{"88":2}}],["当上一层输出的均值不为零时",{"5":{"42":2}}],["当序列长度超出训练范围时",{"5":{"88":2}}],["当是自注意力且时",{"5":{"87":1}}],["当是真实分布而是模型预测分布时",{"5":{"96":1}}],["当条件数很大时",{"5":{"87":4,"89":2}}],["当注意力完全集中于一个位置时",{"5":{"92":2}}],["当注意力均匀分布在所有位置时",{"5":{"92":2}}],["当注意力模式过于",{"5":{"87":2}}],["当注意力权重分布相对",{"5":{"95":2}}],["当注意力权重接近one",{"5":{"87":2}}],["当注意力权重接近均匀分布时",{"5":{"87":2}}],["当注意力权重趋于极端分布时",{"5":{"87":2}}],["当注意力权重趋于均匀分布时",{"5":{"87":2}}],["当注意力权重越集中时",{"5":{"87":2}}],["当注意力权重越均匀分布时",{"5":{"87":2}}],["当注意力分布趋于均匀时",{"5":{"41":2}}],["当所有行都是one",{"5":{"87":2}}],["当新输入",{"5":{"86":1}}],["当新输入​到来时",{"5":{"86":1}}],["当真实类别的预测概率",{"5":{"61":2}}],["当真实的数据生成过程超出这个空间时",{"5":{"71":2}}],["当对所有成立时",{"5":{"61":1}}],["当预测偏离真实值时",{"5":{"51":2}}],["当预测接近真实值时",{"5":{"51":2}}],["当预测分布",{"5":{"71":2}}],["当数据矩阵的列高度相关时",{"5":{"97":2}}],["当数据质量较高且需要更精确的拟合时",{"5":{"51":2}}],["当数据中可能存在异常值或噪声较大时",{"5":{"51":2}}],["当很大时",{"5":{"51":1,"69":1}}],["当测试mse较高时",{"5":{"51":2}}],["当模型生成目标语言的某个词时",{"5":{"95":2}}],["当模型学习到过于",{"5":{"87":2}}],["当模型复杂度很高时",{"5":{"51":2}}],["当模型复杂度很低时",{"5":{"51":2}}],["当模型复杂度适中时",{"5":{"51":2}}],["当模型对某个预测非常有信心",{"5":{"70":2}}],["当模型对一个样本完全",{"5":{"70":2}}],["当模型容量足够大且训练数据足够多时",{"5":{"51":2}}],["当模型容量有限时",{"5":{"99":2,"101":2}}],["当批大小受限于gpu内存时",{"5":{"50":2}}],["当批量太小时",{"5":{"96":2}}],["当批量大小足够大时",{"5":{"96":4}}],["当单个样本太大以至于无法在gpu内存中容纳时",{"5":{"50":2}}],["当两个形状不同的张量进行逐元素运算时",{"5":{"50":2}}],["当样本量趋向无穷时",{"5":{"51":1}}],["当样本量",{"5":{"51":1,"96":1}}],["当样本量时",{"5":{"96":1}}],["当独立同分布的随机变量样本量趋向无穷时",{"5":{"96":2}}],["当使用自然对数时",{"5":{"96":2}}],["当使用以2为底的对数时",{"5":{"96":2}}],["当dropout率为",{"5":{"96":2}}],["当我们想要从该信息源中提取与某个特定查询相关的信息时",{"5":{"95":2}}],["当我们在位置",{"5":{"88":1}}],["当我们在位置应用rope时",{"5":{"88":1}}],["当我们同时平移两个位置",{"5":{"88":2}}],["当我们使用交叉熵作为损失函数时",{"5":{"61":2}}],["当我们用kl散度作为优化目标时",{"5":{"61":2}}],["当我们训练一个神经网络时",{"5":{"51":2}}],["当我们比较多个模型或在多个测试集上评估时",{"5":{"96":2}}],["当我们计算两个加了位置编码的向量的内积时",{"5":{"88":2}}],["当我们计算批次中所有样本的前向传播时",{"5":{"50":2}}],["当我们计算验证集上的性能指标时",{"5":{"96":2}}],["当我们计算损失函数关于某个特定参数的偏导数时",{"5":{"48":2}}],["当我们设计一个神经网络架构并训练其参数时",{"5":{"45":2}}],["当多个这样的曲面进行线性组合时",{"5":{"71":2}}],["当且仅当分布为退化分布",{"5":{"61":2}}],["当且仅当和独立时互信息为0",{"5":{"96":1}}],["当且仅当和独立时互信息为零",{"5":{"71":1}}],["当且仅当",{"5":{"61":6,"71":1,"96":1}}],["当且仅当对于任意向量",{"5":{"71":1}}],["当且仅当对于任意向量和任意标量",{"5":{"71":1}}],["当且仅当对于任意和任意",{"5":{"70":1,"97":1}}],["当且仅当对于任意",{"5":{"70":1,"97":1}}],["当且时",{"5":{"42":1}}],["当网络较深时",{"5":{"42":2}}],["当输入值的量级较大时",{"5":{"95":2}}],["当输入信号的累积效应超过某个阈值时",{"5":{"47":2}}],["当输入",{"5":{"42":1}}],["当输入的绝对值很大时",{"5":{"42":1}}],["当这些梯度在拼接处合并时",{"5":{"41":2}}],["当接近1时",{"5":{"41":1}}],["当接近0或1时",{"5":{"41":1,"70":1}}],["当较大时",{"5":{"41":1,"86":1,"90":1,"95":1}}],["当较小时",{"5":{"41":1,"95":1}}],["当激活函数输出接近其渐近边界时",{"5":{"41":2}}],["当时可归入任一模式",{"5":{"45":1}}],["当时",{"5":{"41":8,"42":5,"45":1,"46":4,"47":6,"50":1,"61":3,"69":3,"70":2,"71":2,"87":4,"89":1,"90":2,"92":3,"94":1,"95":1,"96":5}}],["当",{"5":{"41":15,"42":11,"45":2,"46":4,"47":6,"50":1,"51":1,"61":18,"69":5,"70":6,"71":2,"86":1,"87":7,"88":2,"89":9,"90":5,"91":2,"92":5,"94":3,"95":4,"96":8}}],["当负样本数量足够大时",{"5":{"69":1}}],["当负样本数量",{"5":{"69":1}}],["当负样本数量有限时",{"5":{"99":2,"101":2}}],["当与某个负样本相似度很高时",{"5":{"69":1}}],["当远离",{"5":{"70":1}}],["当不同任务的损失函数具有不同的尺度时",{"5":{"70":2,"99":2,"101":2}}],["当不同类别的样本高度重叠时",{"5":{"70":2,"97":2}}],["当损失函数的梯度趋近于零时",{"5":{"70":2}}],["当或时",{"5":{"70":1}}],["当从1减小到0时",{"5":{"70":1}}],["当偏好差异较大时",{"5":{"99":1,"101":1}}],["当偏好差异较小时",{"5":{"99":2,"101":2}}],["当偏好差异",{"5":{"99":1,"101":1}}],["这影响了",{"5":{"97":2}}],["这影响了最优学习率的选择和超参数调优的难度",{"5":{"97":2}}],["这增加了优化的复杂性",{"5":{"97":2}}],["这被称为",{"5":{"97":2}}],["这被认为有助于模型在早期阶段找到一个更好的参数区域",{"5":{"48":2}}],["这使其不受概率公理的限制",{"5":{"96":2}}],["这使得优化过程可预测且稳定",{"5":{"97":2}}],["这使得优化过程具有可预测的动态特性",{"5":{"70":2}}],["这使得梯度更新不受概率边界约束的限制",{"5":{"96":2}}],["这使得梯度可以",{"5":{"92":2}}],["这使得学习到的特征直接与下游任务相关",{"5":{"94":2}}],["这使得线性注意力特别适合在线推理和流式处理场景",{"5":{"86":2}}],["这使得它能够自然地处理变长序列和流式输入",{"5":{"86":2}}],["这使得它在以下场景中特别有用",{"5":{"41":2}}],["这使得求解正交矩阵的逆矩阵变得极其简单",{"5":{"50":2}}],["这使得贝叶斯推断在计算上更加方便",{"5":{"96":2}}],["这使得高斯分布成为最易于处理的分布之一",{"5":{"96":2}}],["这使得在保持二阶信息的同时实现了可扩展性",{"5":{"48":2}}],["这使得tanh在训练初期通常比sigmoid收敛更快",{"5":{"42":2}}],["这使得",{"5":{"41":2,"61":2}}],["这使得负输入也会进入饱和区域",{"5":{"41":2}}],["这使得模型可以根据输入动态地调整计算分配",{"5":{"93":2}}],["这使得模型能够学习到与相对位置相关的注意力模式",{"5":{"92":2}}],["这使得模型能够学习到与距离相关的注意力模式",{"5":{"92":2}}],["这使得模型能够学习与位置无关的相对依赖模式",{"5":{"90":2}}],["这使得模型能够更有效地学习位置相关的模式",{"5":{"70":2}}],["这使得模型的输出具有清晰的概率意义",{"5":{"69":2}}],["这体现了transformer架构的优雅性",{"5":{"95":2}}],["这体现了sigmoid函数的代数闭合性",{"5":{"42":2}}],["这也是后续出现各种稀疏注意力",{"5":{"95":2}}],["这也是大语言模型能够实现高效训练和推理的关键技术基础",{"5":{"50":2}}],["这三个阶段的计算可以自然地并行化",{"5":{"95":2}}],["这三个步骤的数学本质和物理意义将在后续小节中逐一详细阐述",{"5":{"95":2}}],["这三个线性变换可以统一表示为矩阵乘法的形式",{"5":{"94":2}}],["这三个向量并非凭空产生",{"5":{"94":2}}],["这三个向量构成了注意力机制的",{"5":{"94":2}}],["这三个项之间存在内在的权衡关系",{"5":{"51":2}}],["这实际上是一种参数冗余",{"5":{"93":2}}],["这等于",{"5":{"93":2}}],["这同样与单头注意力相同",{"5":{"93":2}}],["这句话中存在着多种语义关系",{"5":{"93":2}}],["这句话的序列",{"5":{"93":2}}],["这催生了各种稀疏注意力和线性注意力变体的研究",{"5":{"92":2}}],["这解决了深层网络的梯度消失问题",{"5":{"92":2}}],["这解决了adagrad学习率单调递减的问题",{"5":{"48":2}}],["这确保了无论模型的隐藏维度如何变化",{"5":{"95":2}}],["这确保了底层的信息不会被高层完全覆盖",{"5":{"92":2}}],["这确保了各任务对参数更新的贡献在尺度上是一致的",{"5":{"99":2,"101":2}}],["这不仅导致计算效率低下",{"5":{"92":2,"95":2}}],["这不仅提供了更深刻的理解",{"5":{"88":2}}],["这类头对位置编码敏感",{"5":{"92":2}}],["这类头往往表现出较强的局部注意力模式和明确的方向性",{"5":{"92":2}}],["这类依赖关系的跨度可能跨越数十个词元",{"5":{"92":2}}],["这类似于dropout的正则化效果",{"5":{"41":2}}],["这约能编码3400比特的信息",{"5":{"90":2}}],["这降低了工程实现的复杂度",{"5":{"88":2}}],["这带来了两个重要优势",{"5":{"88":2}}],["这带来了两个重要的数学优势",{"5":{"42":2}}],["这并非偶然",{"5":{"88":2}}],["这相当于一种自动的维度压缩",{"5":{"87":2}}],["这极大地简化了梯度表达式",{"5":{"61":2}}],["这反映了相对位置的对称性",{"5":{"90":2}}],["这反映了交叉熵对真实分布和模型分布的区别对待",{"5":{"61":2}}],["这反映了概率分布的约束条件",{"5":{"61":2}}],["这反映了分类任务中",{"5":{"70":2}}],["这有助于理解其参数效率",{"5":{"50":2}}],["这有助于提高模型的泛化能力",{"5":{"47":2}}],["这有效缓解了深层网络中的梯度消失问题",{"5":{"61":2}}],["这有效地限制了策略更新的步长",{"5":{"99":2,"101":2}}],["这本质上是一个概率分布",{"5":{"61":2}}],["这本质上就是计算查询矩阵与转置后的键矩阵的乘积",{"5":{"50":2}}],["这对训练生成对抗网络",{"5":{"50":2}}],["这对于主动学习和探索",{"5":{"96":2}}],["这对于训练深层网络和循环神经网络尤为重要",{"5":{"48":2}}],["这对于学习位置敏感的token交互模式至关重要",{"5":{"71":4}}],["这在不同参数方向上可能不是最优的",{"5":{"97":2}}],["这在某些初始化方案中是常见的",{"5":{"95":2}}],["这在某些数值精度下可能导致问题",{"5":{"61":2}}],["这在理论上消除了边界",{"5":{"92":2}}],["这在理论上可以防止初始化时的梯度消失或爆炸问题",{"5":{"50":2}}],["这在加性注入的框架下是困难的",{"5":{"88":2}}],["这在实践中被证明是困难的",{"5":{"88":2}}],["这在实际中是达不到的",{"5":{"69":2}}],["这在后续与最大似然估计的联系中将发挥关键作用",{"5":{"61":2}}],["这在将展平后的向量恢复为嵌入序列时非常有用",{"5":{"50":2}}],["这在近端策略优化",{"5":{"96":2}}],["这在许多情况下是计算上不可行的",{"5":{"69":2}}],["这从参数命名就可以看出",{"5":{"96":2}}],["这可能成为计算瓶颈",{"5":{"93":2}}],["这可能是因为神经网络具有大量的对称性",{"5":{"48":2}}],["这可以通过修改正弦编码的定义",{"5":{"91":2}}],["这可以通过矩阵运算高效地实现",{"5":{"47":2}}],["这可以看作是对编码幅度的约束",{"5":{"89":2}}],["这可以被视为一种",{"5":{"69":2}}],["这可以被理解为一种参数化投影",{"5":{"69":1}}],["这可以被理解为一种",{"5":{"69":1}}],["这可以从",{"5":{"70":2}}],["这可以加速优化过程",{"5":{"70":2}}],["这为架构设计提供了额外的灵活性",{"5":{"94":2}}],["这为损失函数的设计提供了理论基础",{"5":{"47":2}}],["这为使用梯度下降等优化算法提供了数学基础",{"5":{"47":2}}],["这为理解attention的信息整合能力提供了理论基础",{"5":{"69":2}}],["这为模型提供了坚实的理论基础",{"5":{"69":2}}],["这组神经元构成神经网络的一个层",{"5":{"46":2}}],["这需要对上述单样本梯度计算进行扩展",{"5":{"44":2}}],["这需要深入理解任务的数学特性和激活函数的数学性质",{"5":{"41":2}}],["这里对分数矩阵的每一行独立进行缩放和softmax归一化",{"5":{"95":2}}],["这里对应位置域",{"5":{"90":2}}],["这里是虚数单位",{"5":{"88":1}}],["这里为避免混淆使用不同字母",{"5":{"87":2}}],["这里我们关注激活函数在神经元模型中的整体角色",{"5":{"47":2}}],["这里",{"5":{"51":2,"61":2,"70":4,"71":2,"88":1,"93":2}}],["这里softmax的作用是",{"5":{"69":4}}],["这里softmax的作用同样是",{"5":{"69":2}}],["这里的",{"5":{"69":2}}],["这里没有",{"5":{"70":2}}],["这避免了sigmoid和tanh的梯度饱和问题",{"5":{"71":2}}],["这与深度学习中常见的",{"5":{"93":2}}],["这与rnn的",{"5":{"92":2}}],["这与编码向量",{"5":{"90":2}}],["这与正弦编码的多尺度频率结构有某种对应关系",{"5":{"89":2}}],["这与正弦余弦编码的极限相同",{"5":{"89":2}}],["这与位置依赖的平滑性假设一致",{"5":{"89":2}}],["这与通信系统中广泛使用的调制技术具有相同的数学本质",{"5":{"88":2}}],["这与人类记忆的遗忘特性有某种相似性",{"5":{"86":2}}],["这与最大化对数似然是等价的",{"5":{"61":2}}],["这与第六章讨论的交叉熵损失有着深刻的联系",{"5":{"47":2}}],["这与样条函数",{"5":{"45":2}}],["这与反向模式自动微分的定义完全一致",{"5":{"44":2}}],["这与gelu输入的范围需求相匹配",{"5":{"71":2}}],["这与多项式分布的参数空间完全一致",{"5":{"71":2}}],["这与infonce中温度参数的作用在数学上是等价的",{"5":{"69":2}}],["这与均方误差中使用的平方运算形成了鲜明对比",{"5":{"61":2}}],["这与均方误差关于预测值的梯度形式",{"5":{"70":2}}],["这与鲁棒统计中的思想形成对比",{"5":{"70":2}}],["这与概率的边界行为一致",{"5":{"47":2}}],["这与概率分布的归一化约束完全一致",{"5":{"70":1}}],["这与概率分布的归一化约束",{"5":{"70":1}}],["这与随机变量在条件分布下的期望具有相同的形式",{"5":{"70":1}}],["这与随机变量",{"5":{"70":1}}],["这与标准的分类任务中logit",{"5":{"99":2,"101":2}}],["这与4",{"5":{"99":2,"101":2}}],["这正好是概率值的取值范围",{"5":{"71":2}}],["这正是建模长程依赖所需要的",{"5":{"92":2}}],["这正是平移等变性的群论表述",{"5":{"88":1}}],["这正是旋转矩阵的可加性",{"5":{"88":2}}],["这正是最小化交叉熵与最大化似然估计等价的直接体现",{"5":{"61":2}}],["这正是二元交叉熵损失",{"5":{"61":1}}],["这正是多分类交叉熵损失函数",{"5":{"61":2}}],["这正是单样本仿射变换的定义",{"5":{"47":2}}],["这正是层网络的等效仿射变换形式",{"5":{"47":1}}],["这正是4",{"5":{"46":2}}],["这正是深度学习",{"5":{"46":2}}],["这正是异或函数的输出",{"5":{"46":2}}],["这正是复合函数的定义",{"5":{"45":2}}],["这正是矩阵乘法的定义",{"5":{"44":2,"45":2}}],["这正是",{"5":{"47":1,"61":1,"70":2,"71":2,"88":1}}],["这正是交叉熵损失的标准形式",{"5":{"69":2}}],["这正是对比学习的核心目标",{"5":{"69":2}}],["这正是回归任务的目标",{"5":{"70":2}}],["这正是著名的",{"5":{"99":2,"101":2}}],["这样可以避免softmax计算中指数运算可能导致的数值溢出问题",{"5":{"96":2}}],["这样语义完全不同的句子",{"5":{"91":2}}],["这样",{"5":{"50":4,"61":2,"93":2}}],["这一设计确保了梯度能够有效地反向传播",{"5":{"95":2}}],["这一设计确实能够在一定程度上覆盖不同尺度的位置信息",{"5":{"88":2}}],["这一步根据注意力权重对值矩阵进行加权求和",{"5":{"95":2}}],["这一步计算了所有查询",{"5":{"95":2}}],["这一范式具有清晰的语义解释",{"5":{"95":2}}],["这一趋势的背后有多重原因",{"5":{"88":2}}],["这一公式在形式上具有完美的对称性",{"5":{"88":2}}],["这一领域经历了从",{"5":{"88":2}}],["这一梯度表达式将在后续的梯度计算部分得到详细展开",{"5":{"61":2}}],["这一等价变换揭示了一个重要事实",{"5":{"61":2}}],["这一等价性建立了机器学习损失函数与经典统计推断之间的桥梁",{"5":{"61":2}}],["这一分解具有深刻的理论意义",{"5":{"61":2}}],["这一重要性质可以通过jensen不等式严格证明",{"5":{"61":2}}],["这一极值性质可以通过拉格朗日乘数法或jensen不等式严格证明",{"5":{"61":2}}],["这一几何性质不仅解释了mse的最优性",{"5":{"51":2}}],["这一小批样本的梯度是整体数据梯度的一个良好近似",{"5":{"96":2}}],["这一局限性直到多层神经网络",{"5":{"47":2}}],["这一局限性的根本原因在于",{"5":{"46":2}}],["这一过程称为反向传播",{"5":{"48":2}}],["这一过程可以与前向传播并行进行",{"5":{"45":2}}],["这一过程实现了损失信息从输出到输入的逆向流动",{"5":{"44":2}}],["这一观点与深度学习中的",{"5":{"45":2}}],["这一结论虽然难以给出严格的数学证明",{"5":{"45":2}}],["这一性质可以用数学语言精确描述",{"5":{"92":2}}],["这一性质对于语言模型捕捉局部模式",{"5":{"88":2}}],["这一性质的证明直接来源于对数函数的性质和概率的非负性",{"5":{"61":2}}],["这一性质被用于分析梯度分布",{"5":{"96":2}}],["这一性质被称为",{"5":{"42":2}}],["这一性质在变分推断和高斯过程回归中非常重要",{"5":{"96":2}}],["这一性质在反向传播算法的推导中至关重要",{"5":{"96":2}}],["这一性质大大简化了反向传播的实现",{"5":{"44":2}}],["这一原则直接来自于任务数学结构与损失函数的一致性",{"5":{"70":2}}],["这就是所谓的",{"5":{"95":2}}],["这就是为什么transformer在处理长序列时面临计算挑战",{"5":{"50":2}}],["这就是梯度下降算法的理论基础",{"5":{"48":2}}],["这就是著名的",{"5":{"69":1,"71":2}}],["这就是著名的独立于无关选择的特性",{"5":{"69":1}}],["这就是nce的理论基础",{"5":{"69":2}}],["这就是均方误差在分类任务中表现不佳的根本原因",{"5":{"70":2}}],["这就",{"5":{"42":2}}],["这具有某种",{"5":{"41":2}}],["这看似不会导致梯度爆炸",{"5":{"41":2}}],["这表明位置编码的点积只依赖于相对位置",{"5":{"92":2}}],["这表明每个频率成分具有相同的能量",{"5":{"90":2}}],["这表明优化过程倾向于学习平滑的位置表示",{"5":{"89":2}}],["这表明在给定方差的所有连续分布中",{"5":{"96":2}}],["这表明拐点处的曲率为零",{"5":{"42":2}}],["这表明tanh比sigmoid更早进入饱和区域",{"5":{"41":2}}],["这表明交叉熵的梯度与预测概率与真实标签的差值成正比",{"5":{"70":2}}],["这表明",{"5":{"70":2}}],["这是多头注意力设计的另一个精妙之处",{"5":{"93":2}}],["这是多数实际应用中的常见假设",{"5":{"48":2}}],["这是正弦位置编码难以做到的",{"5":{"92":2}}],["这是正交投影矩阵的两个关键性质",{"5":{"51":2}}],["这是建模长程依赖的重要能力",{"5":{"92":2}}],["这是一种",{"5":{"92":2}}],["这是一个常数矩阵",{"5":{"97":2}}],["这是一个关键发现",{"5":{"95":2}}],["这是一个关于",{"5":{"50":1}}],["这是一个关于的多项式方程",{"5":{"50":1}}],["这是一个简单的矩阵乘法梯度",{"5":{"94":2}}],["这是一个简单的乘法交互",{"5":{"71":2}}],["这是一个非常重要的特性",{"5":{"93":2}}],["这是一个非常重要的结论",{"5":{"87":2}}],["这是一个值得深入分析的问题",{"5":{"90":2}}],["这是一个值得反复品味的结论",{"5":{"88":2}}],["这是一个无约束优化问题",{"5":{"89":2}}],["这是一个相当可观的参数量",{"5":{"89":2}}],["这是一个极强的数学保证",{"5":{"88":2}}],["这是一个极其重要的性质",{"5":{"70":2}}],["这是一个秩1矩阵",{"5":{"87":4}}],["这是一个平凡特征向量",{"5":{"87":2}}],["这是一个重要的性质",{"5":{"96":2}}],["这是一个超平面",{"5":{"47":2}}],["这是一个",{"5":{"69":2,"70":4}}],["这是一个线性的梯度结构",{"5":{"70":1}}],["这是一个半负定矩阵",{"5":{"70":1}}],["这是一个半正定矩阵",{"5":{"70":1}}],["这是一个有限的常数值",{"5":{"70":2}}],["这是一个维的紧致流形",{"5":{"70":1}}],["这是常见配置",{"5":{"89":2}}],["这是其他位置编码方法难以企及的优势",{"5":{"88":2}}],["这是最直观的实现方式",{"5":{"88":2}}],["这是位置编码实用性的重要指标",{"5":{"88":2}}],["这是自注意力机制的优势",{"5":{"88":2}}],["这是transformer原始论文提出的设计",{"5":{"86":2}}],["这是实现高效训练的关键数学基础",{"5":{"61":2}}],["这是在用模型分布",{"5":{"61":2}}],["这是因为注意力分数",{"5":{"92":1}}],["这是因为注意力分数只依赖于查询和键向量的内容",{"5":{"92":1}}],["这是因为正弦和余弦函数的周期性导致编码向量不会总是聚集在球面的某个区域",{"5":{"91":2}}],["这是因为正弦编码天然支持外推",{"5":{"89":2}}],["这是因为可学习编码可以适应训练数据的特定分布",{"5":{"89":2}}],["这是因为当位置索引很大时",{"5":{"88":2}}],["这是因为softmax函数的分子是指数函数",{"5":{"87":2}}],["这是因为内存带宽瓶颈在长序列下更加严重",{"5":{"86":2}}],["这是因为",{"5":{"61":2,"93":2}}],["这是回归分析中",{"5":{"51":1}}],["这是回归分析中统计量的几何基础",{"5":{"51":1}}],["这是mse作为回归任务标准损失的统计学理论基础",{"5":{"51":2}}],["这是理解机器学习模型泛化误差的核心理论工具",{"5":{"51":2}}],["这是所谓的",{"5":{"96":2}}],["这是深度学习高效训练的关键技术基础",{"5":{"47":2}}],["这是不可能拟合复杂非线性函数的原因",{"5":{"47":2}}],["这是神经网络强大表达能力的关键来源",{"5":{"47":2}}],["这是神经网络理论中最深刻的结果之一",{"5":{"71":2}}],["这是线性代数中的基本定理",{"5":{"45":2}}],["这是微积分中的基本定理",{"5":{"45":2}}],["这是微积分基本定理的直接推论",{"5":{"44":2}}],["这是反向传播的最后一步",{"5":{"44":2}}],["这是曲线的",{"5":{"42":2}}],["这是导致深层神经网络训练困难的根本原因之一",{"5":{"41":2}}],["这个方程的解可以通过线性常微分方程的理论分析",{"5":{"97":2}}],["这个常微分方程描述了参数在损失函数梯度的负方向上的连续运动",{"5":{"97":2}}],["这个",{"5":{"96":2}}],["这个权重满足非负性和归一性",{"5":{"95":2}}],["这个区间对于softmax函数来说是",{"5":{"95":2}}],["这个求和运算遍历了查询向量和键向量的所有维度",{"5":{"95":2}}],["这个求和需要",{"5":{"45":1}}],["这个求和需要次乘法和次加法",{"5":{"45":1}}],["这个看似简洁的公式蕴含了注意力计算的完整逻辑",{"5":{"95":2}}],["这个视角将注意力的计算置于更一般的二次型框架中",{"5":{"94":2}}],["这个嵌入矩阵是整个注意力计算的起点",{"5":{"94":2}}],["这个复杂度分析与头数",{"5":{"93":1}}],["这个复杂度分析与头数无关",{"5":{"93":1}}],["这个复杂度与单头注意力相同",{"5":{"93":2}}],["这个复数编码与实数编码的关系为",{"5":{"90":2}}],["这个运算可以视为张量收缩",{"5":{"93":2}}],["这个运算被高度优化",{"5":{"50":2}}],["这个例子表明",{"5":{"92":2}}],["这个例子直观地说明了线性模型的表达能力边界",{"5":{"71":2}}],["这个函数可以表示为",{"5":{"92":2}}],["这个函数应当满足以下核心要求",{"5":{"88":2}}],["这个递推关系展示了信息如何在层间流动",{"5":{"92":2}}],["这个递推关系的解为",{"5":{"41":2}}],["这个归一化过程有两个重要作用",{"5":{"92":2}}],["这个度量表示它实际关注的",{"5":{"92":2}}],["这个梯度可以展开为",{"5":{"92":2}}],["这个梯度具有清晰的几何意义",{"5":{"69":2}}],["这个过程可以理解为位置信息的",{"5":{"91":2}}],["这个过程与sigmoid将logits转换为概率的过程在数学上是相似的",{"5":{"71":2}}],["这个值是通过实验调优确定的",{"5":{"91":2}}],["这个值的选择基于以下考量",{"5":{"91":2}}],["这个相位差正好对应点积公式中的",{"5":{"90":1}}],["这个相位差正好对应点积公式中的项",{"5":{"90":1}}],["这个间隔随",{"5":{"90":1}}],["这个间隔随增大而减小",{"5":{"90":1}}],["这个算子可以表示为",{"5":{"90":2}}],["这个算法清晰地展示了反向传播的执行流程",{"5":{"44":2}}],["这个结论的严格证明需要用到多重频率系统的唯一性定理",{"5":{"90":2}}],["这个结果具有深刻的数学意义",{"5":{"91":2}}],["这个结果具有深刻的统计学意义",{"5":{"51":2}}],["这个结果清楚地表明",{"5":{"90":2}}],["这个结果干净得多",{"5":{"88":2}}],["这个结果不依赖于任何可学习的参数",{"5":{"88":2}}],["这个结果可以通过",{"5":{"61":2}}],["这个结果与之前的推导一致",{"5":{"61":2}}],["这个结果与二分类情况完全一致",{"5":{"61":2}}],["这个结果表明",{"5":{"71":2,"97":2}}],["这个频谱表示表明",{"5":{"90":2}}],["这个雅可比矩阵的结构影响梯度流动",{"5":{"89":2}}],["这个位置应该有什么样的编码",{"5":{"89":2}}],["这个映射的复杂度取决于编码矩阵的秩",{"5":{"89":2}}],["这个映射满足群同态性质",{"5":{"88":2}}],["这个展开式揭示了问题的复杂性",{"5":{"88":2}}],["这个证明的关键步骤是利用了旋转矩阵的可交换性",{"5":{"88":2}}],["这个证明揭示了sigmoid导数自化性质的数学根源",{"5":{"42":2}}],["这个数值验证了公式",{"5":{"91":2}}],["这个数值足够大",{"5":{"88":2}}],["这个数学事实揭示了一个深刻的原理",{"5":{"71":2}}],["这个等式是rope理论的核心成果",{"5":{"88":2}}],["这个等式的右边只依赖于",{"5":{"88":2}}],["这个性质可以从kl散度的凸性直接推导",{"5":{"97":2}}],["这个性质在分析注意力机制的频率特性时很有用",{"5":{"90":2}}],["这个性质与位置编码中的相对位置表示密切相关",{"5":{"90":2}}],["这个性质具有深刻的含义",{"5":{"87":2}}],["这个性质称为正交性条件",{"5":{"51":2}}],["这个极限表明",{"5":{"87":2}}],["这个表达式表明",{"5":{"94":2}}],["这个表达式揭示了注意力分数的矩阵分解结构",{"5":{"94":2}}],["这个表达式清楚地揭示了位置编码的生成机制",{"5":{"90":2}}],["这个表达式清楚地表明",{"5":{"90":2}}],["这个表达式是深度学习分类任务中最常用的损失函数形式",{"5":{"61":2}}],["这个表达式在分类任务的训练中至关重要",{"5":{"71":2}}],["这个表达式可以解释为",{"5":{"99":2,"101":2}}],["这个损失函数具有清晰的似然解释",{"5":{"61":2}}],["这个平均值就越接近真实分布的熵",{"5":{"61":2}}],["这个矩阵的每一行对应一个位置",{"5":{"88":2}}],["这个矩阵的谱结构决定了梯度下降在概率空间中的几何行为",{"5":{"71":2}}],["这个矩阵类似于一个距离矩阵",{"5":{"87":2}}],["这个矩阵具有一些特殊的性质",{"5":{"87":2}}],["这个矩阵可以分解为对角矩阵与外积矩阵的差",{"5":{"61":2}}],["这个异常样本可能主导mse",{"5":{"51":2}}],["这个误差来源于数据的随机性",{"5":{"51":2}}],["这个分解揭示了注意力矩阵与输入数据内在结构的关系",{"5":{"87":2}}],["这个分解的推导过程如下",{"5":{"51":2}}],["这个分解将模型在未知数据上的期望mse分解为三个部分",{"5":{"51":2}}],["这个分解表明",{"5":{"51":2,"89":2}}],["这个分解利用了序列数据的有序性质",{"5":{"99":2,"101":2}}],["这个分布在重参数化梯度计算中扮演关键角色",{"5":{"71":2}}],["这个优化问题可以形式化为",{"5":{"51":2}}],["这个优化过程涉及对损失函数求导",{"5":{"48":2}}],["这个子流形就是",{"5":{"51":2}}],["这个推导表明",{"5":{"51":2,"87":2}}],["这个推论揭示了一个重要的简化",{"5":{"44":2}}],["这个向量随后经过softmax变换",{"5":{"96":2}}],["这个向量被旋转了一个角度",{"5":{"88":2}}],["这个向量形式的表示不仅在数学上更加简洁",{"5":{"51":2}}],["这个向量的每一个维度都编码了某种语义或语法特征",{"5":{"50":2}}],["这个近似将原始矩阵分解为k个秩一矩阵的和",{"5":{"50":2}}],["这个近似在局部区域非常准确",{"5":{"48":2}}],["这个估计越可靠",{"5":{"96":2}}],["这个简洁的梯度形式是交叉熵损失被广泛采用的重要原因之一",{"5":{"96":2}}],["这个公式中的底数10000和指数",{"5":{"88":2}}],["这个公式在计算方差时更加实用",{"5":{"96":2}}],["这个公式清楚地表明",{"5":{"48":2}}],["这个公式表明",{"5":{"48":2,"71":2}}],["这个信息对于确定参数更新的方向和幅度至关重要",{"5":{"48":2}}],["这个特征映射可以表示为复合函数",{"5":{"47":2}}],["这个决策边界始终是线性的",{"5":{"46":2}}],["这个变换将",{"5":{"94":1}}],["这个变换将从原始的嵌入空间",{"5":{"94":1}}],["这个变换可以分解为三个步骤",{"5":{"87":2}}],["这个变换包含两个步骤",{"5":{"47":2}}],["这个变换的逆变换为",{"5":{"71":1}}],["这个变换的逆变换为​",{"5":{"71":1}}],["这个变量在反向传播过程中起着桥梁作用",{"5":{"44":2}}],["这个选择背后有深刻的数学考量",{"5":{"71":2}}],["这个不等式表明",{"5":{"71":2}}],["这个不等式给出了梯度范数的精确边界",{"5":{"41":2}}],["这个不等式的含义是",{"5":{"69":2}}],["这个恒等式可以从代数上验证",{"5":{"71":2}}],["这个超平面将输入空间分为两个半空间",{"5":{"71":2}}],["这个定义的数学意义非常明确",{"5":{"51":2}}],["这个定义包含了线性映射的两个核心性质",{"5":{"71":2}}],["这个定义与标准交叉熵的区别在于",{"5":{"99":2,"101":2}}],["这个定理表明",{"5":{"47":2}}],["这个定理虽然使用了简化的阈值激活函数",{"5":{"46":2}}],["这个定理揭示了单层神经网络的根本局限性",{"5":{"47":2}}],["这个定理揭示了一个深刻的事实",{"5":{"47":2}}],["这个定理揭示了relu网络的一个重要性质",{"5":{"45":2}}],["这个定理揭示了前向传播的数学本质",{"5":{"45":2}}],["这个定理揭示了反向传播的本质机制",{"5":{"44":2}}],["这个定理是反向传播中权重梯度计算的核心公式",{"5":{"44":2}}],["这个定理的数学意义极其深远",{"5":{"71":2}}],["这个定理的关键洞察是",{"5":{"69":2}}],["这个关系表明",{"5":{"42":2}}],["这个反函数在逻辑回归中具有重要意义",{"5":{"42":2}}],["这个初始化确保了激活值的方差在前向传播中保持稳定",{"5":{"41":2}}],["这个下界确保了即使",{"5":{"41":1}}],["这个下界确保了即使的梯度很小",{"5":{"41":1}}],["这个下界会越来越紧",{"5":{"69":2}}],["这个理论结果解释了为什么对比学习能够学习到有用的表示",{"5":{"69":2}}],["这个缩放因子的作用与温度参数类似",{"5":{"69":2}}],["这个期望可以解释为",{"5":{"69":2}}],["这个流程在神经网络中的实现非常直接",{"5":{"61":2}}],["这个流形的几何性质",{"5":{"70":2}}],["这个更新可以理解为在欧几里得度量下进行的",{"5":{"70":2}}],["这个比例趋于平稳值",{"5":{"87":2}}],["这个比例保证了足够的上下文信息同时提供了足够的预测挑战",{"5":{"99":2,"101":2}}],["这个问题看似简单",{"5":{"45":2}}],["这个问题有一个解析解",{"5":{"99":2,"101":2}}],["这个奖励函数直接由策略和参考策略的比率定义",{"5":{"99":2,"101":2}}],["这种约束在反向传播中会产生特定的梯度模式",{"5":{"97":2}}],["这种约束有助于提高模型的稳定性和泛化能力",{"5":{"50":2}}],["这种自适应的学习率调整使优化过程更加稳定",{"5":{"97":2}}],["这种性质部分源于过参数化带来的隐式正则化效应",{"5":{"97":2}}],["这种参数依赖的曲率使得交叉熵的优化动态比均方误差更为复杂",{"5":{"97":2}}],["这种参数简化提高了样本效率",{"5":{"87":2}}],["这种恒定曲率的性质使得均方误差的优化具有可预测的动态特性",{"5":{"97":2}}],["这种掩码与因果掩码可以叠加使用",{"5":{"95":2}}],["这种温度控制的视角揭示了一个重要的训练技巧",{"5":{"95":2}}],["这种期望表达式的形式在理论上具有重要的价值",{"5":{"95":2}}],["这种学习到的相似度度量不同于简单的余弦相似度或欧氏距离",{"5":{"94":2}}],["这种初始化确保了信号和梯度在正向传播和反向传播过程中都不会过度放大或衰减",{"5":{"94":2}}],["这种初始化意味着初始位置编码不提供任何位置信息",{"5":{"89":2}}],["这种通过学习得到的特征选择机制是注意力机制强大表达能力的重要来源",{"5":{"94":2}}],["这种维度压缩并非缺点",{"5":{"94":2}}],["这种配置下的投影矩阵都是",{"5":{"94":1}}],["这种配置下的投影矩阵都是的方阵",{"5":{"94":1}}],["这种配置在多头注意力的语境下使用",{"5":{"94":2}}],["这种在多个子空间中的并行计算赋予了多头注意力更强的特征提取能力",{"5":{"93":2}}],["这种组合策略在核方法文献中被称为",{"5":{"93":2}}],["这种组合损失结合了两种损失函数的优点",{"5":{"51":2}}],["这种双重依赖使得注意力能够建模多种类型的长程依赖",{"5":{"92":2}}],["这种双重非线性的组合使得transformer能够学习极其复杂的函数映射",{"5":{"71":2}}],["这种全局性使得注意力机制在理论上能够建模任意复杂的长程依赖模式",{"5":{"92":2}}],["这种全连接结构与卷积结构形成鲜明对比",{"5":{"92":2}}],["这种全面的理解为学习后续章节",{"5":{"51":2}}],["这种直接依赖关系是注意力机制建模长程依赖的数学基础",{"5":{"92":2}}],["这种下降在低维度",{"5":{"91":2}}],["这种数值分布反映了多尺度编码的特性",{"5":{"91":2}}],["这种数学上的同构性并非偶然",{"5":{"61":2}}],["这种数学上的简洁性不仅在理论上令人赞叹",{"5":{"42":2}}],["这种数学上的相似性意味着",{"5":{"70":2}}],["这种数学上的",{"5":{"70":2}}],["这种数学结构的一致性意味着",{"5":{"61":2}}],["这种数学统一性意味着",{"5":{"70":2}}],["这种方式进一步确保了被掩码位置的信息完全不会泄露",{"5":{"91":2}}],["这种方式下",{"5":{"91":2}}],["这种方法在推理时能够处理更长的序列",{"5":{"92":2}}],["这种方法在模型压缩和少样本学习中有一定的应用价值",{"5":{"48":2}}],["这种方法需要对attention计算流程进行较大改动",{"5":{"88":2}}],["这种方法的核心思想是",{"5":{"89":2}}],["这种方法的时间复杂度为",{"5":{"88":2}}],["这种方法的数学基础正是低秩近似理论",{"5":{"50":2}}],["这种均匀分布确保了不同位置之间的区分性",{"5":{"91":2}}],["这种关系是位置编码能够表达位置信息的核心",{"5":{"91":2}}],["这种结合方式的数学意义在于",{"5":{"91":2}}],["这种结构化参数具有更好的泛化能力",{"5":{"90":2}}],["这种结构恰好对应于rope的块对角矩阵形式",{"5":{"88":2}}],["这种结构影响着优化算法的行为",{"5":{"48":2}}],["这种结构使得梯度计算可以高效地逐元素进行",{"5":{"71":2}}],["这种结构将层归一化置于残差分支内部",{"5":{"41":1}}],["这种结构在训练初期可能不稳定",{"5":{"41":2}}],["这种结构与交叉熵的",{"5":{"97":2}}],["这种结构与许多自然数据",{"5":{"71":2}}],["这种结构与infonce中计算正样本相似度的结构高度相似",{"5":{"69":2}}],["这种结构上的一致性暗示了一个更深层次的统一性",{"5":{"69":2}}],["这种隐式编码方式存在明显的局限性",{"5":{"91":2}}],["这种平滑性是泛化的重要保障",{"5":{"90":2}}],["这种先验防止模型学习过于高频的位置变化模式",{"5":{"90":2}}],["这种先验信息与损失函数的交互决定了模型的学习动态",{"5":{"70":2}}],["这种覆盖范围对于常见的序列长度",{"5":{"90":2}}],["这种编码利用不同频率的正弦和余弦函数来唯一标识每个位置",{"5":{"92":2}}],["这种编码利用不同频率的正弦和余弦函数来唯一标识序列中的每个位置",{"5":{"91":2}}],["这种编码方式在数学上具有完备性",{"5":{"90":2}}],["这种编码将位置信息嵌入到固定范围的正弦和余弦函数中",{"5":{"71":2}}],["这种构造方式具有深刻的数学理论基础和优雅的性质",{"5":{"90":2}}],["这种基于秩的外推假设了位置编码的平滑性",{"5":{"89":2}}],["这种采样偏差可能导致模型在某些位置上的表现不佳",{"5":{"89":2}}],["这种灵活性是可学习编码的主要优势",{"5":{"89":2}}],["这种灵活性使得softmax成为处理各种复杂模式的强大工具",{"5":{"69":2}}],["这种从经验到理论的发展",{"5":{"88":2}}],["这种调整的效果是",{"5":{"88":2}}],["这种调制方式具有抗噪声能力强",{"5":{"88":2}}],["这种实现方式充分利用了现代gpu的向量化计算能力",{"5":{"88":2}}],["这种变换可以分解为旋转",{"5":{"94":2}}],["这种变换是可逆的",{"5":{"88":2}}],["这种变换保持了几何结构的许多性质",{"5":{"71":2}}],["这种简单性掩盖了深层次的问题",{"5":{"88":2}}],["这种多阶段训练策略反映了损失函数优化性质的复杂性和实际应用的需求",{"5":{"97":2}}],["这种多样化的分布模式使得模型能够同时捕获全局信息和局部细节",{"5":{"92":2}}],["这种多尺度的波动模式是位置编码的核心特征",{"5":{"91":2}}],["这种多尺度的编码类似于小波变换",{"5":{"91":2}}],["这种多尺度的频率结构具有深刻的数学意义",{"5":{"90":2}}],["这种多尺度的频率表示与人类感知和处理信息的方式有着有趣的相似性",{"5":{"90":2}}],["这种多尺度的频率设计",{"5":{"88":2}}],["这种多尺度设计使得位置编码能够适应不同粒度的位置相关建模需求",{"5":{"90":2}}],["这种多分辨率分析与小波变换",{"5":{"90":2}}],["这种多频率的设计实现了频谱的",{"5":{"88":2}}],["这种表示具有平移不变性和对称性",{"5":{"90":2}}],["这种表示并非数学游戏",{"5":{"88":2}}],["这种表示方法不仅是深度学习框架实现高效计算的理论基础",{"5":{"46":2}}],["这种加法操作虽然简单直接",{"5":{"88":2}}],["这种并行性是transformer相比rnn在计算效率上具有巨大优势的根本原因",{"5":{"95":2}}],["这种并行性是深度学习在大规模数据和模型上高效训练的基础",{"5":{"45":2}}],["这种并行性恰恰是其致命缺陷的根源",{"5":{"88":2}}],["这种快速衰减的奇异值谱是低秩结构的直接证据",{"5":{"87":2}}],["这种快速饱和特性仍然是tanh的主要局限性",{"5":{"42":2}}],["这种形式的门控允许对不同位置",{"5":{"86":2}}],["这种形式的计算复杂度为",{"5":{"86":2}}],["这种递归形式的复杂度为",{"5":{"86":1}}],["这种递归形式的复杂度为每步",{"5":{"86":1}}],["这种递推关系揭示了sigmoid非线性变换的深层数学结构",{"5":{"42":2}}],["这种映射对应于高斯核或rbf核",{"5":{"86":2}}],["这种映射对应于多项式核函数",{"5":{"86":2}}],["这种映射的优点是数值稳定且计算简单",{"5":{"86":2}}],["这种映射关系使得sigmoid函数在概率建模中具有直接的应用价值",{"5":{"71":2}}],["这种格式可以利用密集矩阵乘法的高效实现",{"5":{"86":2}}],["这种策略的优点是保留了最重要的连接",{"5":{"86":2}}],["这种模式揭示了transformer处理长程依赖的策略",{"5":{"92":2}}],["这种模式不利于gpu的并行执行",{"5":{"92":2}}],["这种模式对应于非正定的gram矩阵",{"5":{"86":2}}],["这种模式在longformer",{"5":{"86":2}}],["这种模式可以在保持计算量不变的情况下扩大感受野",{"5":{"86":2}}],["这种模式引入了局部性的归纳偏置",{"5":{"86":2}}],["这种做法存在几个根本性的问题",{"5":{"61":2}}],["这种不对称性在机器学习中有其实际意义",{"5":{"61":2}}],["这种联系表明",{"5":{"87":2}}],["这种联系不仅在理论上具有重要意义",{"5":{"61":2}}],["这种联系揭示了为什么softmax与交叉熵的组合在优化上具有特殊的性质",{"5":{"71":2}}],["这种计算方式避免了显式计算任何",{"5":{"61":2}}],["这种计算方式在数学上与原始定义完全等价",{"5":{"61":2}}],["这种融合不仅减少了冗余计算",{"5":{"61":2}}],["这种饱和行为会导致严重的梯度问题",{"5":{"95":2}}],["这种饱和行为会导致两个问题",{"5":{"61":2}}],["这种饱和效应会逐层累积放大",{"5":{"42":2}}],["这种特性使得模型能够在生成过程中保持一定的随机性和多样性",{"5":{"96":2}}],["这种特性使得模型在训练过程中会特别关注那些预测偏差较大的样本",{"5":{"51":2}}],["这种特性使得训练过程自动适应样本的",{"5":{"51":2}}],["这种特性使得训练过程对困难样本更加关注",{"5":{"70":2}}],["这种敏感性差异在实际应用中有重要影响",{"5":{"51":2}}],["这种敏感性可以从数学上量化",{"5":{"51":2}}],["这种几何解释与主成分分析",{"5":{"94":2}}],["这种几何解释与旋转位置编码",{"5":{"90":2}}],["这种几何结构使得注意力机制能够自然地学习到与位置距离相关的注意力模式",{"5":{"91":2}}],["这种几何结构在统计学习理论中具有基础性的地位",{"5":{"61":2}}],["这种几何关系在任何维度上都成立",{"5":{"51":2}}],["这种几何视角揭示了mse的一个核心性质",{"5":{"51":2}}],["这种几何表示使得我们可以利用空间中的几何关系来理解和操作语义信息",{"5":{"50":2}}],["这种几何差异反映了两种损失函数对",{"5":{"70":2}}],["这种区分对于理解训练集损失与测试集性能的差异至关重要",{"5":{"51":2}}],["这种技术在训练大语言模型时尤为重要",{"5":{"50":2}}],["这种技术使得离散随机变量的重参数化成为可能",{"5":{"71":2}}],["这种转化在推导反向传播公式和优化算法时非常有用",{"5":{"50":2}}],["这种压缩不一定是坏事",{"5":{"87":2}}],["这种压缩是显著的",{"5":{"50":2}}],["这种压缩本质上是信息的",{"5":{"71":1}}],["这种压缩本质上是信息的有损编码",{"5":{"71":1}}],["这种密集表示使得模型能够在连续空间中学习平滑的语义关系",{"5":{"50":2}}],["这种运算在模型的权重初始化",{"5":{"50":2}}],["这种近似在分析神经网络的学习动态",{"5":{"96":2}}],["这种选择影响着生成样本的多样性和质量",{"5":{"96":2}}],["这种选择性与生物神经元的行为类似",{"5":{"71":2}}],["这种矩阵表示的优势在于它可以利用现代硬件",{"5":{"47":2}}],["这种视角对于理解神经网络的输出解释和损失函数设计具有重要意义",{"5":{"47":2}}],["这种视角揭示了激活函数在信息处理中的深层作用",{"5":{"71":2}}],["这种抽象使得我们可以利用线性代数和逻辑代数的工具来分析神经网络的行为",{"5":{"47":2}}],["这种抽象化过程正是深度学习成功的关键数学机制",{"5":{"71":2}}],["这种层级化可以通过信息论的工具进行量化",{"5":{"92":2}}],["这种层级化结构使得模型能够建模更加复杂的长程依赖",{"5":{"92":2}}],["这种层级化的模式与人类语言的层级结构",{"5":{"92":2}}],["这种层级化的依赖建模是transformer成功捕获复杂语言结构的关键因素",{"5":{"92":2}}],["这种层叠结构通过矩阵运算的复合来表示",{"5":{"46":2}}],["这种层次化的注意力学习是大语言模型能力的重要来源",{"5":{"71":2}}],["这种连接模式可以用矩阵乘法简洁地表示",{"5":{"46":2}}],["这种分离允许模型学习到高度非线性的匹配函数",{"5":{"94":2}}],["这种分布反映了编码向量在高维球面上的均匀性",{"5":{"91":2}}],["这种分布反映了位置编码的内在结构",{"5":{"89":2}}],["这种分布在多个数量级上提供了一致的分辨率",{"5":{"90":2}}],["这种分布表明编码存在明显的维度分层",{"5":{"89":2}}],["这种分布表明编码在各方向上均匀分散",{"5":{"89":2}}],["这种分布表明编码主要沿少数几个方向分散",{"5":{"89":2}}],["这种分解将参数数量从",{"5":{"87":1}}],["这种分解将参数数量从减少到",{"5":{"87":1}}],["这种分解将原始的",{"5":{"87":1}}],["这种分解将原始的计算转换为计算",{"5":{"87":1}}],["这种分解对于分析注意力机制的信息流动方向至关重要",{"5":{"87":2}}],["这种分析对于诊断模型过拟合和理解模型容量都很有价值",{"5":{"50":2}}],["这种分析不仅具有理论意义",{"5":{"71":2}}],["这种分割不会影响计算的数学正确性",{"5":{"45":2}}],["这种概率解释具有优雅的理论性质",{"5":{"95":2}}],["这种概率解释在逻辑回归",{"5":{"47":2}}],["这种概率解释与注意力机制中的softmax概率分布形成了数学上的呼应",{"5":{"71":2}}],["这种概率解释不仅加深了我们对激活函数数学本质的理解",{"5":{"71":2}}],["这种定义使得gelu具有概率解释",{"5":{"71":2}}],["这种有损压缩在机器学习中是有益的",{"5":{"71":2}}],["这种逐元素应用的方式保持了向量结构",{"5":{"71":2}}],["这种非线性的引入使得神经网络能够学习任意复杂的输入",{"5":{"71":2}}],["这种局限性在处理现实世界的复杂数据时尤为突出",{"5":{"71":2}}],["这种单向梯度可能导致优化过程的",{"5":{"42":2}}],["这种积分",{"5":{"42":2}}],["这种渐近行为表明sigmoid函数在正无穷处趋向于饱和值1",{"5":{"42":2}}],["这种规范化意味着激活值的尺度不会随网络深度增加而累积变化",{"5":{"41":2}}],["这种稳定性对于深层网络的成功训练至关重要",{"5":{"41":2}}],["这种极端分布使得反向传播时产生大的梯度",{"5":{"41":2}}],["这种",{"5":{"41":4,"69":4,"70":2,"71":8,"89":2,"91":4,"92":12,"93":4,"99":2,"101":2}}],["这种现象称为梯度饱和",{"5":{"41":2,"42":2}}],["这种设计哲学",{"5":{"95":2}}],["这种设计蕴含着深刻的语义洞见",{"5":{"94":2}}],["这种设计允许模型在不同的语义空间中表示同一个输入",{"5":{"94":2}}],["这种设计允许模型学习到复杂的头间交互模式",{"5":{"93":2}}],["这种设计将计算量从",{"5":{"93":1}}],["这种设计将计算量从降低到",{"5":{"93":1}}],["这种设计在保持大部分推理效率提升的同时",{"5":{"93":2}}],["这种设计在保持参数效率的同时",{"5":{"93":2}}],["这种设计在推理时特别有利",{"5":{"93":2}}],["这种设计引入了一种归纳偏置",{"5":{"93":2}}],["这种设计有两个好处",{"5":{"92":2}}],["这种设计使得模型能够学习到什么样的信息应该被",{"5":{"94":2}}],["这种设计使得位置编码成为模型的",{"5":{"91":2}}],["这种设计使得低维度能够编码位置的粗粒度信息",{"5":{"91":2}}],["这种设计使得编码能够在多个数量级上同时捕获位置信息",{"5":{"90":2}}],["这种设计确保了频率分辨率与位置分辨率的匹配",{"5":{"90":2}}],["这种设计与频率域和位置域的对偶性完全一致",{"5":{"90":2}}],["这种设计可以看作是",{"5":{"89":2}}],["这种设计没有引入额外的可学习参数",{"5":{"88":2}}],["这种设计的数学形式为",{"5":{"93":2}}],["这种设计的直觉是",{"5":{"89":2}}],["这种设计的动机是",{"5":{"88":2}}],["这种设计的优势在于简单直观",{"5":{"88":2}}],["这种设计的思路是",{"5":{"88":2}}],["这种设计体现了数学的优雅",{"5":{"88":2}}],["这种设计保持了卷积的空间结构",{"5":{"41":2}}],["这种设计选择带来了几个关键优势",{"5":{"69":2}}],["这种设计反映了实际应用场景",{"5":{"99":2,"101":2}}],["这种统一的数学结构将在本章第四节",{"5":{"61":2}}],["这种统一性不仅具有理论美感",{"5":{"69":2}}],["这种一致性有助于理解模型的内部机制",{"5":{"69":2}}],["这种对偶性使得位置编码能够同时在两个域中表示信息",{"5":{"90":2}}],["这种对偶性在位置编码中有深刻的体现",{"5":{"90":2}}],["这种对偶性体现在多个方面",{"5":{"90":2}}],["这种对数变换将概率空间的非线性关系转化为线性尺度上的度量",{"5":{"61":2}}],["这种对比不仅有助于深入理解损失函数的设计原理",{"5":{"70":2}}],["这种差异源于损失函数的数学结构和对数据分布的敏感性",{"5":{"97":2}}],["这种差异在数学上体现为信息传递的路径长度",{"5":{"92":2}}],["这种差异可以从链式法则的角度理解",{"5":{"70":2}}],["这种差异反映了强化学习与监督学习在优化目标上的根本区别",{"5":{"99":2,"101":2}}],["这种深层的数学联系揭示了",{"5":{"70":2}}],["这种跨章节的联系体现了",{"5":{"70":2}}],["这种条件依赖假设是自回归语言建模的理论基础",{"5":{"99":2,"101":2}}],["这种混合方法在某些任务上可能有所提升",{"5":{"88":2}}],["这种混合注意力模式与前缀语言建模损失的设计是一致的",{"5":{"99":2,"101":2}}],["这种归一化具有以下几个重要的数学效应",{"5":{"41":2}}],["这种归一化确保每个任务对更新方向的贡献在范数上是一致的",{"5":{"99":2,"101":2}}],["这种专业化现象部分源于随机初始化和训练过程中的随机梯度下降",{"5":{"93":2}}],["这种专业化是transformer强大表达能力的重要来源",{"5":{"92":2}}],["这种专门化是多任务学习成功的关键因素之一",{"5":{"99":2,"101":2}}],["这种相似性反映了两个函数在数学结构上的内在联系",{"5":{"42":2}}],["这种相似性表明",{"5":{"99":2,"101":2}}],["这种偏差",{"5":{"51":2}}],["这种偏好调整正是对齐的目标",{"5":{"99":2,"101":2}}],["这意味着当梯度较大时",{"5":{"97":2}}],["这意味着单层注意力需要存储约",{"5":{"95":1}}],["这意味着单层注意力需要存储约个浮点数",{"5":{"95":1}}],["这意味着梯度几乎为零",{"5":{"95":2}}],["这意味着梯度总是非负的",{"5":{"42":2}}],["这意味着微小的输入变化几乎不会引起输出的变化",{"5":{"95":2}}],["这意味着变换后的表示必然位于一个",{"5":{"94":1}}],["这意味着变换后的表示必然位于一个​维的子空间中",{"5":{"94":1}}],["这意味着无论模型深度如何或序列长度如何",{"5":{"94":2}}],["这意味着多头注意力在保持参数总量不变的情况下",{"5":{"94":2}}],["这意味着多头机制在",{"5":{"93":2}}],["这意味着多分类交叉熵损失简化为负对数似然",{"5":{"61":2}}],["这意味着各个头确实学习到了互补的信息",{"5":{"93":2}}],["这意味着从位置",{"5":{"92":1}}],["这意味着从位置到位置的路径长度恒为1",{"5":{"92":1}}],["这意味着任意两个位置之间都存在直接的信息传递路径",{"5":{"92":2}}],["这意味着每个位置的输出是值向量的凸组合",{"5":{"92":2}}],["这意味着每个维度都不是二元的或稀疏的",{"5":{"50":2}}],["这意味着对于相同的输入序列",{"5":{"91":2}}],["这意味着编码向量都位于高维球面上",{"5":{"91":2}}],["这意味着编码可以区分相邻位置",{"5":{"90":2}}],["这意味着位置编码隐式地编码了相对位置信息",{"5":{"91":2}}],["这意味着模型可以通过学习",{"5":{"95":1}}],["这意味着模型可以通过学习和的投影矩阵",{"5":{"95":1}}],["这意味着模型可以自然地处理任意长度的序列",{"5":{"88":2}}],["这意味着模型在高层通过注意力机制",{"5":{"92":2}}],["这意味着模型无法区分",{"5":{"91":2}}],["这意味着​可以通过将​在复平面上旋转角度得到",{"5":{"90":1}}],["这意味着相邻位置在编码中差异较小",{"5":{"90":2}}],["这意味着相邻位置在编码中会有显著差异",{"5":{"90":2}}],["这意味着频率在对数域上是等间隔分布的",{"5":{"90":2}}],["这意味着满秩的可学习编码可以精确表示正弦余弦编码",{"5":{"89":2}}],["这意味着向量",{"5":{"88":1}}],["这意味着向量绕原点逆时针旋转了",{"5":{"88":1}}],["这意味着采用rope的模型在理论上保证了相对位置感知的正确性",{"5":{"88":2}}],["这意味着我们不需要显式地计算旋转后的向量",{"5":{"88":2}}],["这意味着最低频的维度每个位置只旋转极小的角度",{"5":{"88":2}}],["这意味着rope注入位置信息的方式是",{"5":{"88":2}}],["这意味着两个token之间的注意力分数只取决于它们的相对位置",{"5":{"88":2}}],["这意味着输出表示位于最多",{"5":{"87":1}}],["这意味着输出表示位于最多维的子空间中",{"5":{"87":1}}],["这意味着注意力分数",{"5":{"92":1}}],["这意味着注意力分数​只依赖于查询和键的相对位置",{"5":{"92":1}}],["这意味着注意力权重必须是确定性的",{"5":{"92":2}}],["这意味着注意力矩阵可以用少数几个主成分近似表示",{"5":{"87":2}}],["这意味着注意力机制的信息压缩能力是常数级的",{"5":{"87":2}}],["这意味着至少有一个特征值为1",{"5":{"87":2}}],["这意味着是一个随机矩阵",{"5":{"87":1}}],["这意味着计算一个",{"5":{"50":1}}],["这意味着计算一个权重矩阵与向量的乘积需要次标量乘法和加法",{"5":{"50":1}}],["这意味着正交变换是一种保距变换",{"5":{"50":2}}],["这意味着如果我们沿着梯度方向移动",{"5":{"48":2}}],["这意味着堆叠多层网络不会带来任何额外的表达能力",{"5":{"47":2}}],["这意味着gelu不会像relu那样产生",{"5":{"71":2}}],["这意味着sigmoid",{"5":{"71":2}}],["这意味着在",{"5":{"42":1}}],["这意味着在附近",{"5":{"42":1}}],["这意味着在未饱和区域",{"5":{"41":2}}],["这意味着",{"5":{"42":2,"87":3,"88":2,"89":2,"90":1,"93":2,"95":2}}],["这意味着自注意力机制通过计算token之间的相似度",{"5":{"69":2}}],["这意味着反向传播的实现可以高度复用",{"5":{"69":2}}],["这意味着均方误差的曲率在整个参数空间中是不变的",{"5":{"70":2}}],["这意味着曲率在参数空间中不是恒定的",{"5":{"70":2,"97":2}}],["这意味着函数具有唯一的全局最小值",{"5":{"97":2}}],["这意味着函数具有全局最小值",{"5":{"70":2}}],["这意味着函数图上任意两点的连线位于函数图的上方",{"5":{"48":2}}],["这意味着即使模型完全正确分类",{"5":{"70":2}}],["这意味着交叉熵在饱和区域的梯度比均方误差大约",{"5":{"70":2}}],["这意味着交叉熵损失没有上界",{"5":{"70":2}}],["这意味着交叉熵优化的二阶信息直接对应于",{"5":{"70":2}}],["这些损失函数通过注意力机制实现信息聚合",{"5":{"101":2}}],["这些解在训练损失上可能相近",{"5":{"97":2}}],["这些配置的选择通常基于经验性的消融实验和计算资源约束",{"5":{"93":2}}],["这些头输出按维度拼接起来",{"5":{"93":2}}],["这些偏置决定了模型擅长学习哪类函数",{"5":{"92":2}}],["这些编码方式将位置映射到一个环面上",{"5":{"92":2}}],["这些编码向量的几何分布取决于训练过程中学习到的模式",{"5":{"89":2}}],["这些依赖可以是长距离的",{"5":{"92":2}}],["这些依赖通常是局部的",{"5":{"92":2}}],["这些数据可以一次性加载到高速缓存中",{"5":{"92":2}}],["这些数学工具为深入理解位置编码奠定了基础",{"5":{"90":2}}],["这些数学基础为理解后续章节中",{"5":{"61":2}}],["这些数学分析为理解大语言模型的训练过程提供了必要的理论基础",{"5":{"42":2}}],["这些可视化有助于建立对位置编码几何结构的直观理解",{"5":{"91":2}}],["这些值随位置变化显著",{"5":{"91":2}}],["这些向量之间存在特定的角度关系",{"5":{"91":2}}],["这些方程表明",{"5":{"90":2}}],["这些方向称为主成分",{"5":{"50":2}}],["这些序列的平均功率为",{"5":{"90":2}}],["这些差异可以从多个角度分析",{"5":{"89":2}}],["这些基向量是从训练数据中学习的",{"5":{"89":2}}],["这些矩阵是",{"5":{"94":2}}],["这些矩阵性质如何影响训练动态和泛化性能",{"5":{"89":2}}],["这些矩阵乘法的计算效率直接影响模型的整体性能",{"5":{"50":2}}],["这些旋转组合成一个块对角矩阵",{"5":{"88":2}}],["这些优势在rope的语境中表现为",{"5":{"88":2}}],["这些算法将计算复杂度从",{"5":{"87":1}}],["这些算法将计算复杂度从降低到或",{"5":{"87":1}}],["这些理论知识为进一步研究注意力机制的理论性质",{"5":{"87":2}}],["这些内容为深入理解注意力机制奠定了坚实的理论基础",{"5":{"86":2}}],["这些样本服从某个参数化的概率分布",{"5":{"61":2}}],["这些性质对于理解其数学行为至关重要",{"5":{"87":2}}],["这些性质对于在实际应用中选择和使用mse作为损失函数提供了理论指导",{"5":{"51":2}}],["这些性质为mle在大样本场景下的应用提供了理论保障",{"5":{"96":2}}],["这些梯度可以通过注意力输出对各矩阵的雅可比矩阵来计算",{"5":{"94":2}}],["这些梯度可以通过链式法则推导",{"5":{"41":2}}],["这些梯度的计算非常直接",{"5":{"51":2}}],["这些运算的吞吐量通常是全精度运算的数倍",{"5":{"50":2}}],["这些运算在大语言模型中有着直接的应用场景",{"5":{"50":2}}],["这些范数在稀疏优化和线性规划中有重要应用",{"5":{"50":2}}],["这些定理为机器学习中许多方法的合理性提供了理论依据",{"5":{"96":2}}],["这些高阶矩在统计分析中有一定应用",{"5":{"96":2}}],["这些统计量在机器学习的理论和实践中都有广泛应用",{"5":{"96":2}}],["这些最小值通常具有更好的泛化能力",{"5":{"48":2}}],["这些变体在保持注意力机制核心思想的同时",{"5":{"86":2}}],["这些变体反映了深度学习优化研究的持续进展",{"5":{"48":2}}],["这些变换的复合最终将输入数据映射到所需的输出空间",{"5":{"45":2}}],["这些调度策略的目标是在训练初期快速收敛",{"5":{"48":2}}],["这些点可能是极小值",{"5":{"48":2}}],["这些激活函数各有特点",{"5":{"47":2}}],["这些超平面的组合形成了一个复杂的决策边界",{"5":{"46":2}}],["这些神经元按照层次结构组织",{"5":{"46":2}}],["这些原则不仅解释了现有激活函数的设计逻辑",{"5":{"41":2}}],["这些约束意味着归一化后的表示位于一个特定的流形上",{"5":{"41":2}}],["这些机制可以从数学角度进行详细分析",{"5":{"41":2}}],["这些嵌入经过层归一化处理后",{"5":{"41":2}}],["这些分析有助于理解可学习编码如何从数据中学习最优的位置表示",{"5":{"89":2}}],["这些分析为后续章节",{"5":{"70":2,"71":2}}],["这些分析为理解大语言模型训练的动力学过程提供了坚实的数学基础",{"5":{"41":2}}],["这些分析建立了一个坚实的理论基础",{"5":{"69":2}}],["这些任务在数学上都可以统一为某种形式的预测任务",{"5":{"99":2,"101":2}}],["这将在下一节详细讨论",{"5":{"61":2}}],["这将在第四章详细讨论",{"5":{"47":2}}],["这将为理解infonce的改进提供必要的背景",{"5":{"69":2}}],["这限制了它在大规模数据集上的效率",{"5":{"69":2}}],["这两个定义密切相关但并不等价",{"5":{"41":2}}],["这两种损失函数分别适用于回归任务和分类任务",{"5":{"70":2}}],["这只是表面上的相似",{"5":{"70":2}}],["这涉及到部分和",{"5":{"50":2}}],["这涉及到如何组合不同的损失函数",{"5":{"70":2}}],["这通常超过了gpu",{"5":{"86":2}}],["这通常能够提高模型对词级语义的理解能力",{"5":{"99":2,"101":2}}],["它决定了优化的收敛速度",{"5":{"97":2}}],["它决定了每次更新的幅度",{"5":{"48":2}}],["它直接影响收敛速度和最终解的质量",{"5":{"97":2}}],["它直接关系到优化算法能否找到全局最优解",{"5":{"97":2}}],["它直接作用于概率分布",{"5":{"70":2}}],["它会趋向于",{"5":{"95":2}}],["它需要",{"5":{"95":2}}],["它需要保留足够的细节以供后续层使用",{"5":{"94":2}}],["它涉及到softmax和矩阵乘法的复合梯度",{"5":{"94":2}}],["它捕获了query空间和key空间之间的关系",{"5":{"94":2}}],["它假设依赖关系可以跨越任意距离",{"5":{"92":2}}],["它假设所有位置的旋转角度是均匀的",{"5":{"88":2}}],["它事先就知道序列中任意两个位置之间是否",{"5":{"92":1}}],["它确保注意力权重构成有效的概率分布",{"5":{"92":2}}],["它统一处理正弦和余弦分量",{"5":{"90":2}}],["它证明了任意周期函数都可以分解为不同频率正弦和余弦函数的线性组合",{"5":{"90":2}}],["它与现有的深度学习框架兼容良好",{"5":{"88":2}}],["它与随机变量",{"5":{"96":1}}],["它与随机变量具有相同的量纲",{"5":{"96":1}}],["它采用了一种极其简单的位置编码方式",{"5":{"88":2}}],["它采用的是加性注入方式",{"5":{"88":2}}],["它注意到rope的旋转性质可以在attention计算中直接利用",{"5":{"88":2}}],["它反映了概率分布空间的内在曲率",{"5":{"61":2}}],["它比简单的欧氏距离更适合描述概率分布间的差异",{"5":{"61":2}}],["它仅取决于真实分布本身",{"5":{"61":2}}],["它同样使用了",{"5":{"61":2}}],["它深刻揭示了熵作为",{"5":{"61":2}}],["它衡量的是按照该概率分布采样时",{"5":{"61":2}}],["它衡量的是一个随机事件发生所带来的",{"5":{"61":2}}],["它代表了给定输入下目标变量的",{"5":{"51":2}}],["它使用半精度浮点数",{"5":{"50":2}}],["它使得我们能够高效地计算复合函数对底层变量的梯度",{"5":{"45":2}}],["它使得大规模神经网络的端到端训练成为可能",{"5":{"44":2}}],["它允许我们用期望的形式表达聚合操作",{"5":{"95":2}}],["它允许序列中任意两个位置之间直接建立联系",{"5":{"92":2}}],["它允许同时处理多个样本以提高计算效率",{"5":{"50":2}}],["它允许使用更大的学习率",{"5":{"41":2}}],["它只能在同一组语义空间中计算输入序列各位置之间的关联强度",{"5":{"93":2}}],["它只能建模线性的距离衰减",{"5":{"88":2}}],["它只能处理线性可分的问题",{"5":{"47":2}}],["它只关注真实类别对应的预测概率的对数值",{"5":{"61":2}}],["它只旋转或反射空间而不改变向量的长度",{"5":{"50":2}}],["它用于权重归一化以稳定训练过程",{"5":{"50":2}}],["它可以视为复平面上单位圆上的一个点",{"5":{"90":2}}],["它可以直接解释为对数似然比的期望",{"5":{"61":2}}],["它可以看作是矩阵展开为向量后的l2范数",{"5":{"50":2}}],["它可以显著简化代码",{"5":{"50":2}}],["它可以被理解为",{"5":{"71":2}}],["它可以被解释为一种",{"5":{"70":2}}],["它考虑了各变量之间的相关性",{"5":{"96":2}}],["它总是存在的",{"5":{"96":2}}],["它唯一确定了随机变量的概率分布",{"5":{"96":2}}],["它结合了动量方法和rmsprop的优点",{"5":{"48":2}}],["它在梯度计算中同样发挥着重要作用",{"5":{"94":2}}],["它在更新之前先对梯度进行校正",{"5":{"48":2}}],["它在输入空间的某个区域激活",{"5":{"71":2}}],["它在统计估计中对应于最小二乘估计的优良性质",{"5":{"70":2}}],["它累积历史梯度来平滑参数更新",{"5":{"48":2}}],["它利用中心极限定理的原理来稳定训练过程",{"5":{"96":2}}],["它利用负梯度方向作为搜索方向来迭代地最小化目标函数",{"5":{"48":2}}],["它利用链式法则自动计算复杂函数的导数",{"5":{"48":2}}],["它描述了输入中的冗余信息",{"5":{"50":2}}],["它描述了输入数据如何逐层变换",{"5":{"45":2}}],["它描述了矩阵所能表示的所有输出",{"5":{"50":2}}],["它描述了当其他变量保持不变时",{"5":{"48":2}}],["它保持了向量空间的代数结构",{"5":{"88":2}}],["它保持了空间的平行性和比例关系",{"5":{"47":2}}],["它保证了优化问题的",{"5":{"51":2}}],["它保留正信息",{"5":{"71":2}}],["它保留输入的相对顺序",{"5":{"69":2}}],["它揭示了神经网络如何通过层次化的非线性变换逼近复杂的函数关系",{"5":{"45":2}}],["它本质上实现了从输入空间到输出空间的函数映射",{"5":{"45":2}}],["它告诉我们如何计算复合函数的导数",{"5":{"48":2}}],["它告诉我们",{"5":{"44":2}}],["它告诉我们给定当前参数下网络的输出是什么",{"5":{"44":2}}],["它度量了模型分布",{"5":{"61":1}}],["它度量了模型分布与真实分布之间的差异",{"5":{"61":1}}],["它度量了",{"5":{"71":1}}],["它度量了和之间的依赖程度",{"5":{"71":1}}],["它提供了数值稳定性",{"5":{"71":2}}],["它提供了一定的正则化效果",{"5":{"41":2}}],["它是连接神经网络的线性输出与概率分布的关键桥梁",{"5":{"96":2}}],["它是离散的二值模型",{"5":{"47":2}}],["它是一个非负矩阵",{"5":{"87":2}}],["它是一个复合函数的逐层求值过程",{"5":{"45":2}}],["它是一个无界的实数",{"5":{"71":2}}],["它是一个二分类方法",{"5":{"69":2}}],["它是一个二次函数",{"5":{"70":1}}],["它是一个对数函数的组合",{"5":{"70":1}}],["它是一个",{"5":{"70":2}}],["它是负对数似然的期望",{"5":{"71":2}}],["它正是交叉熵损失的核心组成部分",{"5":{"71":2}}],["它指出大量独立同分布随机变量之和",{"5":{"96":2}}],["它指出",{"5":{"71":2}}],["它指的是在训练过程中梯度值变得非常小",{"5":{"41":2}}],["它从数学上证明了深度学习的表达能力基础",{"5":{"71":2}}],["它从根本上改变了梯度传播的路径",{"5":{"41":2}}],["它无法表示复杂的曲线",{"5":{"71":2}}],["它移除了均值归一化",{"5":{"41":2}}],["它减少了内部协变量偏移",{"5":{"41":2}}],["它将参数更新视为在损失函数的负梯度方向上连续流动",{"5":{"97":2}}],["它将参数视为随机变量并使用贝叶斯公式进行推断",{"5":{"96":2}}],["它将注意力机制纳入到概率图模型的框架中",{"5":{"95":2}}],["它将原始的注意力分数转换为概率分布",{"5":{"95":2}}],["它将位置",{"5":{"90":1}}],["它将位置的编码映射到位置的编码",{"5":{"90":1}}],["它将位置信息编码为一种",{"5":{"88":2}}],["它将位置信息视为一种",{"5":{"88":2}}],["它将位置信息从隐式",{"5":{"71":2}}],["它将正弦和余弦统一为一个复指数运算",{"5":{"90":2}}],["它将正样本键拉向查询向量",{"5":{"69":1}}],["它将正样本键",{"5":{"69":1}}],["它将任意周期函数表示为不同频率正弦和余弦函数的叠加",{"5":{"90":2}}],["它将query头分为",{"5":{"86":1}}],["它将query头分为组",{"5":{"86":1}}],["它将一个",{"5":{"50":1}}],["它将一个的矩阵与一个的矩阵组合成一个的大矩阵",{"5":{"50":1}}],["它将矩阵分解为下三角矩阵与其转置的乘积",{"5":{"50":2}}],["它将权重矩阵的谱范数归一化为1",{"5":{"50":2}}],["它将标量",{"5":{"50":2}}],["它将某一子层的输入与该子层的输出直接相加",{"5":{"50":2}}],["它将后验分布的推断转化为优化问题",{"5":{"96":2}}],["它将硬标签",{"5":{"96":2}}],["它将随机试验的结果数值化",{"5":{"96":2}}],["它将hessian矩阵近似为两个小矩阵的kronecker积",{"5":{"48":2}}],["它将输入映射到一个特征空间后计算相似度",{"5":{"93":2}}],["它将输入空间划分为两个半空间",{"5":{"47":2}}],["它将输入数据依次通过",{"5":{"45":1}}],["它将输入数据依次通过层非线性变换",{"5":{"45":1}}],["它将仿射变换统一为线性变换的形式",{"5":{"46":2}}],["它将数学表达式显式地表示为节点和边的有向无环图",{"5":{"45":2}}],["它将",{"5":{"71":1}}],["它将维向量映射到概率单纯形",{"5":{"71":1}}],["它将概率值转换回实数域",{"5":{"42":2}}],["它将实数映射到区间",{"5":{"42":1}}],["它将实数映射到",{"5":{"42":3}}],["它将难以计算的kl散度最小化问题转化为一个可计算的二分类问题",{"5":{"69":2}}],["它们在测试集上的性能相近",{"5":{"97":2}}],["它们在数学性质和实际表现上有显著差异",{"5":{"91":2}}],["它们在数学结构上存在本质的差异",{"5":{"70":2}}],["它们是方阵变换",{"5":{"94":2}}],["它们包含了注意力机制需要学习的全部参数",{"5":{"94":2}}],["它们各自定义的子空间",{"5":{"93":1}}],["它们各自定义的子空间和可能相交",{"5":{"93":1}}],["它们能够建立的远距离依赖关系较少",{"5":{"92":2}}],["它们打破了注意力机制本应具有的平移等变性",{"5":{"88":2}}],["它们都建立在softmax",{"5":{"101":2}}],["它们都通过控制激活值的分布来稳定训练动态",{"5":{"94":2}}],["它们都是将",{"5":{"61":2}}],["它们都可以作为通用逼近器的核心组件",{"5":{"71":2}}],["它们都控制着softmax分布的锐度",{"5":{"69":2}}],["它们揭示了线性变换的本质特征",{"5":{"50":2}}],["它们描述了大量随机变量之和",{"5":{"96":2}}],["它们用于衡量两个概率分布之间的差异",{"5":{"96":2}}],["它们刻画了随机变量的集中趋势",{"5":{"96":2}}],["它们共享相同的输入",{"5":{"46":2}}],["它们的效果最终都可以追溯到logits向量的相对结构和softmax变换的特性",{"5":{"96":2}}],["它们的学习目标是最小化预测损失",{"5":{"94":2}}],["它们的输出表示会高度相关",{"5":{"93":2}}],["它们的点积为",{"5":{"91":2,"92":2}}],["它们的组合可以表示任意相位的同频率振动",{"5":{"91":2}}],["它们的组合形成了一个能够唯一表示序列位置的编码系统",{"5":{"90":2}}],["它们的编码向量分别为",{"5":{"90":1}}],["它们的编码向量分别为​和​",{"5":{"90":1}}],["它们的内积为",{"5":{"92":2}}],["它们的内积为零",{"5":{"51":2}}],["它们的内积",{"5":{"90":2}}],["它们的差向量为",{"5":{"90":2}}],["它们的rope编码向量分别为",{"5":{"88":2}}],["它们的比值也可能是不稳定的",{"5":{"61":2}}],["它们的kronecker积定义为",{"5":{"50":2}}],["它们的均值",{"5":{"96":2}}],["它们的复合",{"5":{"45":1}}],["它们的复合定义为",{"5":{"45":1}}],["它们的联合结构使得损失景观具有特定的几何形状",{"5":{"71":2}}],["它们定义了从输入空间到表示空间的映射",{"5":{"71":2}}],["它们决定了信息如何被编码和传递",{"5":{"71":2}}],["它们通过规范化激活值或梯度的分布来稳定训练过程",{"5":{"41":2}}],["它们直接决定了神经网络能否有效学习",{"5":{"41":2}}],["它通过在查询和键向量上应用旋转操作来编码位置信息",{"5":{"92":2}}],["它通过在前向传播时只保存部分层的激活值",{"5":{"50":2}}],["它通过绝对位置编码的形式",{"5":{"91":2}}],["它通过调整频率参数来改善模型在超出训练长度时的表现",{"5":{"88":2}}],["它通过将权重矩阵除以其谱范数来约束网络函数的lipschitz常数",{"5":{"50":2}}],["它通过匹配矩",{"5":{"96":2}}],["它通过对激活值进行归一化来稳定训练过程",{"5":{"41":2}}],["它通过限制梯度的范数来防止过度更新",{"5":{"41":2}}],["它表达的是",{"5":{"94":4}}],["它表明",{"5":{"88":2}}],["它表明infonce本质上是交叉熵在对比学习场景下的应用",{"5":{"69":2}}],["它表示使用基于分布",{"5":{"61":1}}],["它表示使用基于分布的编码方案来编码来自分布的信息时",{"5":{"61":1}}],["它表示损失函数对各层净输入的梯度",{"5":{"44":2}}],["它控制着相似度分数的",{"5":{"69":2}}],["它",{"5":{"69":2,"92":1}}],["它的梯度关于误差",{"5":{"70":2}}],["它包含一个归一化操作",{"5":{"70":2}}],["它关于",{"5":{"70":2}}],["它关于预测值是凸函数",{"5":{"70":1}}],["它关于预测值",{"5":{"70":1}}],["它有一个下界",{"5":{"70":2}}],["它对于理解优化的收敛行为和设计自适应优化算法至关重要",{"5":{"97":2}}],["它对应于",{"5":{"94":1}}],["它对应于的第一个维度",{"5":{"94":1}}],["它对应于从",{"5":{"50":1}}],["它对应于从到的线性变换中最大的拉伸因子",{"5":{"50":1}}],["它对应复数",{"5":{"88":2}}],["它对所有输入token一视同仁",{"5":{"88":2}}],["它对大误差的敏感度更高",{"5":{"70":2}}],["它测量的是两个概率向量之间的直线距离",{"5":{"70":2}}],["它不假设依赖关系只存在于邻近位置",{"5":{"92":2}}],["它不随输入序列的内容变化而变化",{"5":{"91":2}}],["它不改变向量的总体强度",{"5":{"88":2}}],["它不仅选择概率最高的类别",{"5":{"96":2}}],["它不仅在实践中取得了显著的性能提升",{"5":{"93":2}}],["它不仅告诉我们差异是否显著",{"5":{"96":2}}],["它不仅仅是在区分正负样本",{"5":{"69":2}}],["它不是为每个位置独立编码",{"5":{"90":2}}],["它不是对称的",{"5":{"70":2}}],["它不是凭空设计的",{"5":{"70":2}}],["它为注意力机制注入了位置信息",{"5":{"92":2}}],["它为模型提供了关于序列结构的先验信息",{"5":{"70":2}}],["它为每个位置提供唯一的身份标识",{"5":{"99":2,"101":2}}],["它意味着均方误差的优化问题是一个线性最小二乘问题",{"5":{"70":1}}],["它意味着均方误差的优化问题是一个",{"5":{"70":1}}],["它迫使模型基于完整的词级上下文进行预测",{"5":{"99":2,"101":2}}],["它能够并行处理序列中的所有位置",{"5":{"88":2}}],["它能够确保优化过程收敛到帕累托前沿上的某个点",{"5":{"99":2,"101":2}}],["它实现了",{"5":{"99":2,"101":2}}],["它绕过了显式的奖励模型",{"5":{"99":2,"101":2}}],["p采样等解码策略也都是在概率分布层面进行操作的各种启发式方法",{"5":{"96":2}}],["phase",{"5":{"88":2}}],["phi",{"5":{"42":6,"46":2}}],["pca是一个无监督的降维方法",{"5":{"94":2}}],["pca",{"5":{"50":6,"91":2,"94":2}}],["p值受样本量影响很大",{"5":{"96":2}}],["p值",{"5":{"96":2}}],["ppo",{"5":{"96":2}}],["ppo在rlhf中的应用",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["ppo损失",{"5":{"99":2,"101":2}}],["ppo的核心思想是",{"5":{"99":2,"101":2}}],["ppo的目标函数为",{"5":{"99":2,"101":2}}],["ppo的裁剪是基于策略比率的几何约束",{"5":{"99":2,"101":2}}],["ppo中的",{"5":{"99":2,"101":2}}],["pdf同样满足非负性",{"5":{"96":1}}],["pdf同样满足非负性和归一性",{"5":{"96":1}}],["pdf在单点的取值不代表概率",{"5":{"96":2}}],["pdf",{"5":{"96":2}}],["pmf必须满足两个基本性质",{"5":{"96":2}}],["pmf",{"5":{"96":2}}],["positivity",{"5":{"86":2}}],["positive",{"5":{"61":2,"86":2}}],["positional",{"5":{"88":12,"89":2,"91":12,"92":8,"94":2}}],["position",{"0":{"88":1},"4":{"88":1},"5":{"85":5,"88":5,"92":2}}],["post",{"5":{"41":3}}],["population",{"5":{"51":2}}],["power",{"5":{"50":2}}],["point",{"5":{"45":2,"48":2}}],["policy",{"5":{"99":2,"101":2}}],["png",{"5":{"44":4,"45":8,"47":8,"48":12,"50":8,"61":4,"96":12}}],["pytorch",{"5":{"50":2,"61":2}}],["processing",{"5":{"47":2,"50":2}}],["properties",{"5":{"87":2}}],["property",{"5":{"42":2}}],["propagation",{"5":{"45":2,"96":2}}],["product",{"0":{"95":1},"4":{"95":1},"5":{"61":8,"69":4,"71":2,"85":5,"86":2,"87":2,"88":2,"93":4,"94":2,"95":17}}],["probability",{"5":{"61":2,"70":2,"96":4}}],["proximal",{"5":{"99":2,"101":2}}],["precision",{"5":{"97":2}}],["prelu",{"5":{"42":2}}],["pre",{"5":{"41":8}}],["predictive",{"5":{"69":2}}],["prediction",{"5":{"99":2,"101":2}}],["prefixlm",{"5":{"99":2,"101":2}}],["preference",{"5":{"99":2,"101":2}}],["palm",{"5":{"88":2}}],["palm等现代大语言模型都采用了rope或其变体",{"5":{"92":2}}],["palm等",{"5":{"88":2}}],["pattern",{"5":{"86":2}}],["path",{"5":{"41":2}}],["parallelism",{"5":{"50":6}}],["parameter",{"5":{"50":2}}],["params",{"5":{"46":4}}],["parafac",{"5":{"50":2}}],["partial",{"5":{"42":68,"50":2,"61":52}}],["padding",{"5":{"95":2,"99":2,"101":2}}],["peft",{"5":{"50":2}}],["permutation",{"5":{"91":2,"92":2,"96":2}}],["perplexity",{"5":{"96":2}}],["perceptron",{"5":{"47":2}}],["pe",{"5":{"42":4}}],["pipeline",{"5":{"50":2}}],["pitts",{"5":{"47":4}}],["pi",{"5":{"42":2,"61":6}}],["p",{"5":{"42":98,"46":2,"61":126,"69":3,"97":8}}],["设位置编码矩阵为",{"5":{"94":2}}],["设位置投影为",{"5":{"91":2}}],["设表示熵",{"5":{"93":1}}],["设表示第层位置的表示与原始位置嵌入之间的互信息",{"5":{"92":1}}],["设门控函数为",{"5":{"93":2}}],["设每个头的query",{"5":{"93":2}}],["设当前矩阵形状为",{"5":{"93":2}}],["设多头注意力的头数为",{"5":{"93":2,"94":2}}],["设头数",{"5":{"93":1}}],["设头数被分为组",{"5":{"93":1}}],["设头",{"5":{"92":1}}],["设头的注意力权重矩阵为",{"5":{"92":1}}],["设原始注意力分数矩阵为",{"5":{"92":2}}],["设原始注意力矩阵为",{"5":{"86":2}}],["设嵌入维度",{"5":{"91":2}}],["设离散序列",{"5":{"90":2}}],["设离散随机变量",{"5":{"61":1}}],["设离散随机变量的概率分布为",{"5":{"61":1}}],["设transformer有",{"5":{"89":1}}],["设transformer有层",{"5":{"89":1}}],["设训练时使用的最大位置为",{"5":{"89":1}}],["设训练时使用的最大位置为​",{"5":{"89":1}}],["设任务为序列分类或语言建模",{"5":{"89":2}}],["设注意力权重矩阵",{"5":{"93":2}}],["设注意力输出为",{"5":{"89":2}}],["设注意力分数矩阵为",{"5":{"86":2}}],["设编码向量为",{"5":{"90":2}}],["设编码向量的维度为",{"5":{"89":1}}],["设编码向量的维度为​",{"5":{"89":1}}],["设编码矩阵的秩为",{"5":{"89":2}}],["设去均值化的编码为",{"5":{"89":2}}],["设非零奇异值为",{"5":{"89":2}}],["设序列",{"5":{"90":1}}],["设序列的dft为",{"5":{"90":1}}],["设序列的最大长度为",{"5":{"89":2}}],["设序列长度为",{"5":{"86":2,"91":2,"92":4,"93":2,"94":2}}],["设保留前",{"5":{"87":1}}],["设保留前个奇异值",{"5":{"87":1}}],["设稀疏模式由掩码矩阵",{"5":{"87":1}}],["设稀疏模式由掩码矩阵给出",{"5":{"87":1}}],["设稀疏模式的参数为",{"5":{"86":2}}],["设一维卷积核为",{"5":{"87":2}}],["设一个深度为",{"5":{"71":2}}],["设核函数为",{"5":{"87":2}}],["设query矩阵",{"5":{"87":2}}],["设块大小为",{"5":{"86":2}}],["设模型输出为",{"5":{"61":2}}],["设模型族",{"5":{"51":1}}],["设模型族定义了所有可能的预测向量集合",{"5":{"51":1}}],["设样本",{"5":{"61":1}}],["设样本的真实类别为",{"5":{"61":2}}],["设样本属于正类",{"5":{"61":1}}],["设样本量",{"5":{"51":2}}],["设我们有一个独立同分布的样本集",{"5":{"61":2}}],["设我们有观测数据",{"5":{"96":2}}],["设和是定义在同一概率空间上的两个离散概率分布",{"5":{"61":1}}],["设和为两个映射",{"5":{"45":1}}],["设最后一层的输出为",{"5":{"51":2}}],["设相同的数据",{"5":{"51":2}}],["设正常样本的误差为",{"5":{"51":2}}],["设对角权重矩阵",{"5":{"51":2}}],["设目标向量",{"5":{"51":2}}],["设单个样本的特征表示是形状为",{"5":{"50":1}}],["设单个样本的特征表示是形状为的二维张量",{"5":{"50":1}}],["设连续随机变量",{"5":{"96":1}}],["设连续随机变量的pdf为",{"5":{"96":1}}],["设参数",{"5":{"96":2}}],["设参数矩阵",{"5":{"41":1}}],["设参数矩阵的奇异值分解为",{"5":{"41":1}}],["设随机变量在个类别上服从类别分布",{"5":{"96":1}}],["设随机变量",{"5":{"96":3}}],["设的奇异值分解为",{"5":{"89":1}}],["设的秩为",{"5":{"87":1}}],["设的第行为",{"5":{"47":1}}],["设的伴随值为",{"5":{"45":1}}],["设有两个",{"5":{"50":1}}],["设有两个维向量",{"5":{"50":1}}],["设有",{"5":{"47":1,"99":1,"101":1}}],["设有一组神经元",{"5":{"46":2}}],["设有一个三阶张量",{"5":{"50":2}}],["设有一个层的前馈网络",{"5":{"47":1}}],["设有一个层的前馈神经网络",{"5":{"45":1,"46":1}}],["设有一个层前馈网络",{"5":{"44":1}}],["设有一个",{"5":{"44":1,"45":1,"46":1,"47":1}}],["设有个神经元的层",{"5":{"47":1}}],["设有个任务",{"5":{"99":1,"101":1}}],["设权重向量",{"5":{"51":2}}],["设权重",{"5":{"47":6}}],["设隐藏维度",{"5":{"45":2}}],["设隐藏层的输出为",{"5":{"71":2}}],["设计",{"5":{"101":2}}],["设计专门的模型架构",{"5":{"101":2}}],["设计专门的激活函数",{"5":{"41":2}}],["设计改进方案都具有重要意义",{"5":{"95":2}}],["设计新的编码方案",{"5":{"91":2}}],["设计哲学和工程实践",{"5":{"88":2}}],["设计优化算法和理解泛化性质时非常有用",{"5":{"96":2}}],["设计权重和偏置如下",{"5":{"46":2}}],["设计模型架构和优化计算资源至关重要",{"5":{"45":2}}],["设批量输入",{"5":{"44":2}}],["设标量函数",{"5":{"44":2}}],["设激活函数",{"5":{"71":1}}],["设激活函数逐元素应用于向量",{"5":{"71":1}}],["设网络的原始输出为",{"5":{"71":2}}],["设网络的一层为",{"5":{"41":2}}],["设是周期为的周期函数",{"5":{"90":1}}],["设是学习到的位置编码矩阵",{"5":{"89":1}}],["设是某层某头的注意力矩阵",{"5":{"87":1}}],["设是的任意特征值",{"5":{"87":1}}],["设是特征映射对应的核函数",{"5":{"86":1}}],["设是模型预测的期望",{"5":{"51":1}}],["设是预测空间",{"5":{"51":1}}],["设是独立同分布的随机变量",{"5":{"96":2}}],["设是第个类别的logit",{"5":{"96":1}}],["设是第层的表示向量",{"5":{"71":1}}],["设是一个有向图",{"5":{"92":1}}],["设是一个离散序列",{"5":{"90":1}}],["设是一个维随机向量",{"5":{"96":1}}],["设是一个单位向量",{"5":{"48":1}}],["设是一个可微函数",{"5":{"48":1}}],["设是一个多元函数",{"5":{"48":1}}],["设是",{"5":{"71":1}}],["设是隐藏状态向量",{"5":{"41":1}}],["设是隐藏状态",{"5":{"41":1}}],["设是位置编码矩阵",{"5":{"71":1}}],["设是位置的token嵌入经过多层自注意力计算后的隐藏状态",{"5":{"99":1,"101":1}}],["设是位置被掩码的概率分布",{"5":{"99":1,"101":1}}],["设是任意非常数的有界连续函数",{"5":{"71":1}}],["设是任务的梯度向量",{"5":{"99":1,"101":1}}],["设输入序列的长度为",{"5":{"93":2}}],["设输入序列为",{"5":{"92":2,"99":4,"101":4}}],["设输入是一个二值序列",{"5":{"92":2}}],["设输入嵌入张量为",{"5":{"93":2}}],["设输入嵌入为",{"5":{"87":2,"91":2}}],["设输入嵌入矩阵为",{"5":{"87":2,"91":2}}],["设输入矩阵",{"5":{"47":1}}],["设输入矩阵包含个样本",{"5":{"47":1}}],["设输入为",{"5":{"46":2,"71":2}}],["设输入空间为",{"5":{"71":2}}],["设输入向量为",{"5":{"46":2,"71":2,"94":2}}],["设输入的绝对值较大时",{"5":{"42":2}}],["设第一层的输入为",{"5":{"94":2}}],["设第一层的输出为",{"5":{"93":2}}],["设第一层的注意力权重为",{"5":{"92":2}}],["设第层的输入为",{"5":{"45":1,"92":1}}],["设第",{"5":{"45":1,"61":2,"92":1}}],["设平均每层的梯度衰减因子为",{"5":{"42":2}}],["设平均每层的梯度传递因子为",{"5":{"41":2}}],["设卷积输出的形状为",{"5":{"41":2}}],["设mini",{"5":{"41":2}}],["设​​为缩放后的注意力分数矩阵",{"5":{"95":1}}],["设​和分别是两个头的输出",{"5":{"93":1}}],["设​和​是位置和的编码",{"5":{"90":1}}],["设​是单头注意力能够表示的函数类",{"5":{"93":1}}],["设​是的最大奇异值",{"5":{"92":1}}],["设​的奇异值分解为",{"5":{"89":1}}],["设​为的第行",{"5":{"95":1}}],["设​为后向传播的中间值",{"5":{"45":1}}],["设​为第层的误差信号",{"5":{"44":1}}],["设​",{"5":{"41":1,"87":1}}],["设个头的输出拼接为",{"5":{"41":1}}],["设",{"5":{"41":10,"42":2,"44":6,"45":10,"47":8,"48":7,"51":2,"61":7,"71":4,"86":1,"87":10,"88":4,"89":5,"90":5,"91":2,"92":5,"93":3,"94":3,"95":4,"96":12,"99":6,"101":6}}],["设为预测概率",{"5":{"96":1}}],["设为离散随机变量的状态空间大小",{"5":{"96":1}}],["设为注意力输出",{"5":{"94":1}}],["设为注意力权重矩阵",{"5":{"92":2}}],["设为个类别之一",{"5":{"47":1}}],["设为输入向量",{"5":{"47":1}}],["设为第层的输出",{"5":{"45":1}}],["设为标量损失函数",{"5":{"44":1}}],["设为损失函数",{"5":{"41":1}}],["设为饱和阈值",{"5":{"41":1}}],["设为类别标签",{"5":{"47":1}}],["设查询向量指向某个方向",{"5":{"69":1}}],["设查询向量",{"5":{"69":1}}],["设共有个注意力头",{"5":{"69":1}}],["设共有",{"5":{"69":1}}],["设真实的数据生成过程涉及两个变量的交互效应",{"5":{"71":2}}],["设真实目标为",{"5":{"70":2}}],["设真实类别为",{"5":{"70":2}}],["设真实分布为",{"5":{"70":2}}],["设仅依赖于token嵌入而不包含位置信息",{"5":{"99":1,"101":1}}],["设填充后的序列长度为",{"5":{"99":2,"101":2}}],["设掩码位置集合为",{"5":{"99":2,"101":2}}],["设掩码span的长度服从某种分布",{"5":{"99":2,"101":2}}],["设分词器将原始文本映射为token序列",{"5":{"99":2,"101":2}}],["设词",{"5":{"99":2,"101":2}}],["在填充位置为",{"5":{"95":1}}],["在同一维度上的分量也较大",{"5":{"95":1}}],["在同一层内",{"5":{"45":2}}],["在某个方向上有负特征值",{"5":{"97":2}}],["在某个维度上的分量较大",{"5":{"95":1}}],["在某些变体设计中",{"5":{"94":2}}],["在某些实现中",{"5":{"91":2}}],["在某些任务上表现良好",{"5":{"86":2}}],["在某些正则化方法中",{"5":{"50":2}}],["在某些深度学习应用中",{"5":{"96":2}}],["在某些情况下",{"5":{"48":2,"87":2}}],["在更一般的形式中",{"5":{"95":2}}],["在非对角线上的元素接近0",{"5":{"95":2}}],["在非常深的网络中",{"5":{"41":2}}],["在flash",{"5":{"93":2}}],["在frobenius范数和谱范数意义下",{"5":{"50":2}}],["在设计多头注意力架构时",{"5":{"93":2}}],["在保持表达能力的同时降低计算成本",{"5":{"93":2}}],["在保持正区域良好梯度的同时",{"5":{"41":2}}],["在单一的注意力机制下",{"5":{"93":2}}],["在单层注意力中",{"5":{"92":2}}],["在各层都较高",{"5":{"92":1}}],["在各种任务上表现良好",{"5":{"90":2}}],["在注意力计算中",{"5":{"95":2}}],["在注意力中",{"5":{"92":2}}],["在注意力机制中",{"5":{"50":2,"70":6,"87":2,"92":6}}],["在相对位置编码中",{"5":{"92":2}}],["在相位调制中",{"5":{"88":2}}],["在卷积神经网络中",{"5":{"92":2,"93":2}}],["在卷积网络中的作用有异曲同工之妙",{"5":{"94":2}}],["在卷积网络中",{"5":{"41":2}}],["在rnn中",{"5":{"92":10}}],["在rope的语境中",{"5":{"88":4}}],["在rope的框架中",{"5":{"88":2}}],["在rope中",{"5":{"88":2}}],["在建模长程依赖时面临根本性的困难",{"5":{"92":2}}],["在建立了infonce与softmax的统一性后",{"5":{"69":2}}],["在句子",{"5":{"92":2}}],["在序列到序列",{"5":{"95":2}}],["在序列数据处理中",{"5":{"92":2}}],["在序列长度和维度都较大时可能成为计算瓶颈",{"5":{"88":2}}],["在解码器",{"5":{"91":2}}],["在机器翻译等基准任务上表现良好",{"5":{"91":2}}],["在机器学习的优化框架下",{"5":{"61":2}}],["在机器学习任务中",{"5":{"61":2}}],["在机器学习领域",{"5":{"61":2}}],["在机器学习与深度学习的庞大理论体系中",{"5":{"51":2}}],["在机器学习训练中",{"5":{"96":2}}],["在机器学习特别是大语言模型中有着广泛应用",{"5":{"96":2}}],["在原始transformer的实现中",{"5":{"91":2}}],["在原假设下所有排列是等可能的",{"5":{"96":2}}],["在频率域中",{"5":{"90":2}}],["在频率空间中是一个对角矩阵",{"5":{"90":1}}],["在频率空间中",{"5":{"90":2}}],["在傅里叶分析的语境下",{"5":{"90":2}}],["在长程依赖建模中发挥着关键作用",{"5":{"92":2}}],["在长度为",{"5":{"90":1}}],["在长度为的序列上",{"5":{"90":1}}],["在长序列场景下",{"5":{"89":2}}],["在复数形式下",{"5":{"90":2}}],["在基",{"5":{"89":1}}],["在矩阵分析部分",{"5":{"89":2}}],["在表达能力分析部分",{"5":{"89":2,"93":2}}],["在gpu上",{"5":{"89":2,"95":2}}],["在短序列场景下",{"5":{"89":2}}],["在标准的多头注意力中",{"5":{"93":4}}],["在标准的正弦余弦编码中",{"5":{"91":2}}],["在标准配置",{"5":{"93":2}}],["在标准配置下",{"5":{"87":2}}],["在标准transformer中",{"5":{"89":2}}],["在当前输入中不被使用",{"5":{"89":2}}],["在未来的大语言模型发展中",{"5":{"88":2}}],["在attention计算中隐式应用",{"5":{"88":1}}],["在attention计算前编码",{"5":{"88":1}}],["在attention计算前编码和在attention计算中隐式应用",{"5":{"88":1}}],["在attention分数上添加一个与相对距离成指数衰减的偏置",{"5":{"88":2}}],["在llama",{"5":{"88":2}}],["在群作用的语言下",{"5":{"88":2}}],["在传统的加性位置编码中",{"5":{"88":2}}],["在低秩结构分析部分",{"5":{"87":2}}],["在谱性质分析部分",{"5":{"87":2}}],["在有限样本下通常表现更好",{"5":{"87":2}}],["在足够深的transformer中",{"5":{"87":2}}],["在推理阶段",{"5":{"93":2}}],["在推理时",{"5":{"86":2,"88":2,"89":2}}],["在推荐系统和缺失数据插补中",{"5":{"50":2}}],["在滑动窗口的基础上引入膨胀因子",{"5":{"86":2}}],["在scaled",{"5":{"61":2,"69":1}}],["在sigmoid中",{"5":{"42":2}}],["在概率单纯形上",{"5":{"61":2,"70":6}}],["在优化视角下",{"5":{"61":2}}],["在优化算法和正则化方法中有着重要的应用",{"5":{"50":2}}],["在明确了熵和kl散度的定义后",{"5":{"61":2}}],["在离散分类任务中",{"5":{"61":2}}],["在分类任务中最小化交叉熵损失等价于最大化数据的似然函数",{"5":{"61":2}}],["在分类任务中",{"5":{"61":2,"69":1,"70":6}}],["在分类任务中的数值稳定性处理",{"5":{"70":2}}],["在分析梯度流和稳定性时",{"5":{"50":2}}],["在分析梯度饱和之前",{"5":{"41":2}}],["在分析神经网络梯度的分布时",{"5":{"96":2}}],["在融合框架下",{"5":{"61":2}}],["在求和之前提取最大值",{"5":{"61":2}}],["在上一节的梯度推导中",{"5":{"61":2}}],["在上一节中",{"5":{"46":2,"94":2}}],["在moe中",{"5":{"93":2}}],["在moe模型中",{"5":{"93":2}}],["在mae中",{"5":{"51":2}}],["在mse最小化的意义下",{"5":{"51":2}}],["在mlm中",{"5":{"99":2,"101":2}}],["在大多数情况下是凸函数",{"5":{"51":2}}],["在大语言模型",{"5":{"50":2}}],["在大语言模型评估中",{"5":{"96":2}}],["在大语言模型的理论框架中",{"5":{"96":2}}],["在大语言模型的理论基础中占据着不可替代的核心地位",{"5":{"50":2}}],["在大语言模型的架构设计中",{"5":{"50":2}}],["在大语言模型的实践中",{"5":{"50":2}}],["在大语言模型的训练中",{"5":{"96":1}}],["在大语言模型的在线评估中",{"5":{"96":2}}],["在大语言模型的开发和研究中",{"5":{"96":2}}],["在大语言模型的语境下",{"5":{"48":2,"50":2}}],["在大语言模型中即为词汇表的大小",{"5":{"96":2}}],["在大语言模型中",{"5":{"48":2,"50":18,"96":16,"97":2}}],["在固定的数据生成过程下",{"5":{"51":2}}],["在欧几里得范数下",{"5":{"51":2}}],["在维欧几里得空间中",{"5":{"51":1}}],["在误差较大时使用mae",{"5":{"51":2}}],["在特定迭代次数时按比例降低学习率",{"5":{"97":2}}],["在特定领域任务上",{"5":{"89":2}}],["在特定假设下",{"5":{"51":2}}],["在特征提取和表示学习中经常被使用",{"5":{"50":2}}],["在课程学习",{"5":{"51":2}}],["在批处理情况下变成",{"5":{"50":1}}],["在物理学和机器学习中都有重要应用",{"5":{"50":2}}],["在模型的前向传播中",{"5":{"89":2}}],["在模型的宽度维度上划分参数",{"5":{"50":2}}],["在模型并行化中",{"5":{"50":2}}],["在模型压缩和低秩近似中",{"5":{"50":2}}],["在模型压缩方面",{"5":{"50":2}}],["在对话系统中",{"5":{"96":2}}],["在对比学习中",{"5":{"50":2}}],["在对比学习",{"5":{"70":2}}],["在对齐阶段",{"5":{"99":2,"101":2}}],["在参数高效微调",{"5":{"50":2}}],["在参数化的概率分布族",{"5":{"70":1,"71":1}}],["在参数化的概率分布族上",{"5":{"70":1,"71":1}}],["在循环神经网络",{"5":{"50":2}}],["在主成分分析",{"5":{"50":2}}],["在变换下只发生缩放而不改变方向",{"5":{"50":2}}],["在变分推断中",{"5":{"96":4}}],["在变分自编码器和生成模型中",{"5":{"96":2}}],["在词嵌入研究中",{"5":{"50":2}}],["在现代深度学习框架中",{"5":{"50":2}}],["在观测数据",{"5":{"96":1}}],["在观测数据后",{"5":{"96":1}}],["在比较语言模型时",{"5":{"96":2}}],["在比较语言模型性能时",{"5":{"96":2}}],["在正则化中",{"5":{"50":2}}],["在正则化技术方面",{"5":{"96":2}}],["在正则条件下",{"5":{"96":2}}],["在知识蒸馏中",{"5":{"96":2}}],["在强化学习与语言模型的结合中",{"5":{"96":2}}],["在vae中",{"5":{"96":2}}],["在高层整合全局信息",{"5":{"92":2}}],["在高斯噪声假设下",{"5":{"51":2}}],["在高斯过程回归和二次优化中有广泛应用",{"5":{"50":2}}],["在高斯过程和贝叶斯优化中",{"5":{"96":2}}],["在高维度区域的热力图模式相似度较高",{"5":{"91":2}}],["在高维度",{"5":{"91":2}}],["在高维空间中",{"5":{"48":2,"51":2}}],["在高维输入空间中",{"5":{"71":2}}],["在输出层产生一个与词汇表大小相同的logits向量",{"5":{"96":2}}],["在输出建模方面",{"5":{"96":2}}],["在输入空间中形成一个",{"5":{"71":1}}],["在输入空间中定义了一个",{"5":{"71":1}}],["在权重初始化方面",{"5":{"96":2}}],["在统计学和机器学习中有着核心地位",{"5":{"96":2}}],["在语言模型评估中",{"5":{"96":2}}],["在语言模型训练中",{"5":{"96":2}}],["在语言模型的评估中",{"5":{"96":2}}],["在语言模型的某些应用中",{"5":{"96":2}}],["在语言模型研究中",{"5":{"96":2}}],["在语言模型中",{"5":{"96":12}}],["在函数",{"5":{"48":1}}],["在函数的等值面上",{"5":{"48":1}}],["在点",{"5":{"48":3}}],["在点​附近",{"5":{"48":1}}],["在适当的噪声水平下",{"5":{"48":2}}],["在凸优化问题中",{"5":{"48":2}}],["在后期精细调优",{"5":{"48":2}}],["在训练后期使用较低的温度",{"5":{"95":2}}],["在训练初期使用较高的温度",{"5":{"95":2}}],["在训练初期逐渐增加学习率",{"5":{"48":2,"97":2}}],["在训练动态分析部分",{"5":{"89":2}}],["在训练过程中通过梯度下降进行优化",{"5":{"89":2}}],["在训练过程中",{"5":{"87":2,"89":2,"94":2}}],["在训练良好的transformer中",{"5":{"87":2,"92":2}}],["在训练神经网络时",{"5":{"61":2}}],["在训练大语言模型中尤为重要",{"5":{"48":2}}],["在贝叶斯深度学习和变分推断中",{"5":{"48":2}}],["在最小二乘问题中特别有用",{"5":{"48":2}}],["在最小奇异值方向上可能产生相对较大的输出梯度",{"5":{"41":2}}],["在与梯度垂直的方向",{"5":{"48":2}}],["在与梯度相反的方向",{"5":{"48":2}}],["在计算复杂度上",{"5":{"92":2}}],["在计算注意力分数时",{"5":{"92":2}}],["在计算图中",{"5":{"48":2}}],["在计算损失和梯度时",{"5":{"47":2}}],["在计算机图形学和计算机视觉中",{"5":{"47":2}}],["在神经网络的稳定性分析和lipschitz约束中",{"5":{"50":2}}],["在神经网络的语境下",{"5":{"47":2}}],["在神经网络中",{"5":{"50":2,"71":2}}],["在此处按元素应用于向量",{"5":{"46":2}}],["在全连接层中",{"5":{"46":2}}],["在本节中",{"5":{"46":2}}],["在反向模式自动微分中",{"5":{"45":2}}],["在反向传播时重新计算被省略的激活值来节省内存",{"5":{"50":2}}],["在反向传播时重新计算其余激活值",{"5":{"48":2}}],["在反向传播算法中",{"5":{"42":2}}],["在反向传播中需要",{"5":{"41":1}}],["在反向传播中需要​",{"5":{"41":1}}],["在反向传播中",{"5":{"41":8,"42":6,"44":2,"71":2,"87":2}}],["在层网络中",{"5":{"45":1}}],["在每个子空间内计算相对简单的注意力模式",{"5":{"93":2}}],["在每个频率维度上",{"5":{"90":2}}],["在每个象限内",{"5":{"45":2}}],["在每一层提供必要的非线性变换",{"5":{"71":2}}],["在第",{"5":{"92":2}}],["在第层",{"5":{"92":1}}],["在第二和第三维度上的收缩产生一个",{"5":{"50":1}}],["在第2",{"5":{"45":2}}],["在第四章中",{"5":{"71":2}}],["在第四章的前三节中",{"5":{"69":2}}],["在第五章和第六章中",{"5":{"70":2}}],["在第六章中",{"5":{"70":2}}],["在混合精度训练中",{"5":{"44":2}}],["在获得各层误差信号后",{"5":{"44":2}}],["在2",{"5":{"44":2,"45":2}}],["在整个实数轴上都是正的",{"5":{"71":2}}],["在整个输入范围内应该接近1",{"5":{"41":2}}],["在自回归语言模型中",{"5":{"95":2}}],["在自注意力中",{"5":{"92":6}}],["在自注意力机制中",{"5":{"41":4,"69":2,"92":2}}],["在自然语言处理的语境下",{"5":{"94":2,"95":2}}],["在自然语言处理中",{"5":{"96":2}}],["在自然语言中",{"5":{"88":4}}],["在自监督学习中",{"5":{"71":2}}],["在任何输入下",{"5":{"71":2}}],["在需要从离散分布采样的场景中",{"5":{"71":2}}],["在多层之间共享同一个位置编码矩阵",{"5":{"89":2}}],["在多层神经网络中",{"5":{"44":2}}],["在多类分类中为one",{"5":{"96":2}}],["在多项式分布中",{"5":{"71":2}}],["在多头注意力中被广泛使用",{"5":{"93":2}}],["在多头注意力中",{"5":{"41":2,"50":2,"92":2,"93":4}}],["在多任务学习中",{"5":{"70":2,"99":4,"101":4}}],["在多任务",{"5":{"70":2}}],["在逻辑回归中",{"5":{"71":2}}],["在伯努利分布中",{"5":{"71":2}}],["在另一侧",{"5":{"71":2}}],["在其开创性论文",{"5":{"61":2}}],["在其他位置为0",{"5":{"95":2}}],["在其他类别上均匀分配",{"5":{"96":1}}],["在其他类别上均匀分配的概率",{"5":{"96":1}}],["在其他区域抑制",{"5":{"71":2}}],["在其存在的范围内",{"5":{"96":2}}],["在深入数学推导之前",{"5":{"93":2}}],["在深入数学细节之前",{"5":{"47":2}}],["在深度维度上划分层",{"5":{"50":2}}],["在深度学习序列建模的发展历程中",{"5":{"91":2}}],["在深度学习的优化过程中",{"5":{"61":2}}],["在深度学习的分类任务中",{"5":{"61":2}}],["在深度学习的反向传播算法中",{"5":{"48":2}}],["在深度学习优化中",{"5":{"48":2}}],["在深度学习中有着广泛的应用",{"5":{"50":2}}],["在深度学习中",{"5":{"48":4,"50":6,"96":4}}],["在深层网络中",{"5":{"41":2,"42":2}}],["在形式上非常相似",{"5":{"42":1}}],["在transformer中",{"5":{"41":4,"71":2,"92":4}}],["在transformer架构中扮演着关键角色",{"5":{"42":2}}],["在transformer架构中",{"5":{"94":4,"99":2,"101":2}}],["在transformer的训练中",{"5":{"97":2}}],["在transformer的自注意力机制中",{"5":{"71":2}}],["在transformer的语境下",{"5":{"69":2}}],["在处理不平衡数据时",{"5":{"51":2}}],["在处有",{"5":{"42":1}}],["在处有拐点",{"5":{"42":1}}],["在处",{"5":{"42":2}}],["在负无穷处趋向于饱和值0",{"5":{"42":2}}],["在许多场景下",{"5":{"41":2}}],["在pca中",{"5":{"94":2}}],["在pytorch中",{"5":{"50":2}}],["在post",{"5":{"41":2}}],["在pre",{"5":{"41":4}}],["在prefix",{"5":{"99":2,"101":2}}],["在prefixlm的实际实现中",{"5":{"99":2,"101":2}}],["在线性代数的学习路径中",{"5":{"50":2}}],["在线性模型中",{"5":{"71":2}}],["在线性区域",{"5":{"41":2}}],["在线学习",{"5":{"41":2}}],["在梯度传播过程中",{"5":{"41":2}}],["在极端情况下",{"5":{"41":2}}],["在数值上完全不可检测",{"5":{"41":2,"42":2}}],["在时间序列预测中",{"5":{"51":2}}],["在时间反向传播",{"5":{"41":2}}],["在时",{"5":{"41":2}}],["在",{"5":{"41":2,"42":8,"44":2,"45":2,"51":1,"61":4,"70":4,"96":1,"97":8}}],["在实践中通常有",{"5":{"95":2}}],["在实践中使用",{"5":{"41":1}}],["在实践中使用作为近似",{"5":{"41":1}}],["在实践中",{"5":{"41":4,"50":2,"61":2,"70":2,"71":2,"88":2,"92":2,"93":2,"94":2,"95":2,"97":4}}],["在实际中",{"5":{"97":2}}],["在实际编码中",{"5":{"90":2}}],["在实际深度学习框架中",{"5":{"61":2}}],["在实际计算中",{"5":{"61":2}}],["在实际计算中也具有重要的应用价值",{"5":{"42":2}}],["在实际实现中",{"5":{"50":2,"61":2,"88":2,"93":2,"96":2,"99":2,"101":2}}],["在实际实现反向传播时",{"5":{"44":2}}],["在实际训练中",{"5":{"41":2,"44":2}}],["在实际应用中",{"5":{"41":2,"47":2,"51":2,"70":2,"88":2,"91":2,"92":2,"94":2,"95":2,"96":4}}],["在实际的预训练数据处理中",{"5":{"99":2,"101":2}}],["在实现中",{"5":{"95":2}}],["在实现反向传播时",{"5":{"50":2,"61":2}}],["在实现注意力机制的梯度计算时",{"5":{"70":2}}],["在前五节中",{"5":{"97":2}}],["在前面两节中",{"5":{"93":2}}],["在前面几节中",{"5":{"87":2}}],["在前面的几节中",{"5":{"86":2}}],["在前向传播过程中",{"5":{"50":2}}],["在前向模式自动微分中",{"5":{"45":2}}],["在前两节中",{"5":{"70":2}}],["在前四节中",{"5":{"99":2,"101":2}}],["在一侧",{"5":{"71":2}}],["在一定的正则性条件下",{"5":{"69":2,"99":2,"101":2}}],["在介绍infonce之前",{"5":{"69":2}}],["在4",{"5":{"69":6}}],["在向量空间中",{"5":{"69":2}}],["在5",{"5":{"69":4}}],["在infonce中",{"5":{"69":2}}],["在候选集合中定义一个",{"5":{"69":2}}],["在候选集合",{"5":{"69":2}}],["在位置编码的语境下",{"5":{"90":4}}],["在位置的",{"5":{"69":1}}],["在位置",{"5":{"69":1}}],["在结构上高度相似",{"5":{"70":2}}],["在均方误差最小的意义下",{"5":{"51":2}}],["在均方误差中",{"5":{"70":2}}],["在局部线性化下",{"5":{"70":2}}],["在回归任务中",{"5":{"70":2}}],["在这个场景中",{"5":{"94":2}}],["在这个表达式中",{"5":{"92":2}}],["在这个推导中",{"5":{"51":2}}],["在这个假设下",{"5":{"51":2}}],["在这个度量下",{"5":{"70":2,"71":2}}],["在这个框架下",{"5":{"70":2,"71":2}}],["在这种情况下",{"5":{"70":2}}],["在几何上",{"5":{"88":2}}],["在几何上等价于先进行线性变换",{"5":{"47":1}}],["在几何上等价于线性变换",{"5":{"45":1}}],["在几何上称为仿射变换",{"5":{"47":2}}],["在几何上对仿射变换的结果执行了逐元素的非线性",{"5":{"45":1}}],["在几何结构上都与概率流形的几何性质紧密相关",{"5":{"70":2}}],["在条件分布",{"5":{"70":1}}],["在预测空间上的正交投影",{"5":{"51":1}}],["在预训练阶段",{"5":{"99":2,"101":2}}],["在监督微调阶段",{"5":{"99":2,"101":2}}],["在生成模型中",{"5":{"96":2}}],["在生成过程中",{"5":{"99":2,"101":2}}],["在置换下保持不变",{"5":{"99":1,"101":1}}],["hybrid",{"5":{"86":2,"88":2}}],["high",{"5":{"91":2}}],["hierarchical",{"5":{"50":2}}],["hidden",{"5":{"50":2}}],["hinton",{"5":{"44":2}}],["hilbert",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["householder变换和givens旋转",{"5":{"50":2}}],["homogeneous",{"5":{"46":2,"47":2}}],["homogeneity",{"5":{"71":2}}],["hot注意力",{"5":{"87":2}}],["hot",{"5":{"61":2,"87":2,"94":2}}],["hot向量",{"5":{"61":2,"70":2}}],["hot编码的真实分布",{"5":{"61":2}}],["hot编码时",{"5":{"61":2}}],["hot编码",{"5":{"96":8}}],["hot采样",{"5":{"71":4}}],["hot时",{"5":{"41":2}}],["hot分布时取等号",{"5":{"87":2}}],["hot分布时",{"5":{"41":2,"87":2}}],["hot分布",{"5":{"41":2,"69":2,"87":2,"95":2}}],["h",{"5":{"42":34,"46":6,"69":22,"86":5}}],["hessian的特征值表示函数在不同特征方向上的曲率",{"5":{"48":2}}],["hessian矩阵",{"5":{"97":2}}],["hessian矩阵的奇异值分解揭示了函数的局部曲率结构",{"5":{"48":2}}],["hessian矩阵决定了泰勒展开的二次项",{"5":{"48":2}}],["hessian矩阵在优化中的作用主要体现在以下几个方面",{"5":{"48":2}}],["hessian矩阵定义为",{"5":{"48":2}}],["hessian矩阵是多元函数的二阶导数矩阵",{"5":{"48":2}}],["hessian矩阵与曲率分析",{"2":{"70":1,"97":1},"5":{"70":1,"97":1}}],["hessian",{"5":{"70":24,"97":24}}],["he方差",{"5":{"41":1}}],["he初始化使用均值为0",{"5":{"96":2}}],["he初始化的推导考虑了relu的",{"5":{"41":2}}],["he初始化将权重方差设置为",{"5":{"41":1}}],["he初始化将权重方差设置为​",{"5":{"41":1}}],["he初始化将权重初始化为",{"5":{"41":2}}],["he初始化",{"5":{"41":2}}],["head",{"5":{"70":2,"93":6}}],["hartleys",{"5":{"61":2}}],["hadamard积",{"5":{"44":2}}],["hat",{"5":{"42":108,"46":2,"61":92,"97":14}}],["html",{"4":{"21":1,"41":1,"42":1,"44":1,"45":1,"46":1,"47":1,"48":1,"50":1,"51":1,"61":1,"69":1,"70":1,"71":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"98":1,"99":1,"100":1,"101":1},"5":{"21":20,"85":52}}],["href=",{"5":{"21":20,"85":52}}],["huber损失",{"5":{"51":2}}],["huber损失在误差较小时使用mse",{"5":{"51":2}}],["huber损失是mse和mae的组合",{"5":{"51":2}}],["human",{"5":{"99":2,"101":2}}],[">6",{"5":{"85":4}}],[">5",{"5":{"85":6}}],[">在大语言模型的训练中",{"5":{"96":1}}],[">期望",{"5":{"96":1}}],[">k",{"5":{"48":1}}],[">矩阵形式的链式法则在深度学习中尤为重要",{"5":{"48":1}}],[">数学上",{"5":{"47":1}}],[">从前向传播的数学描述到实际代码实现",{"5":{"45":1}}],[">",{"5":{"42":5,"44":1,"45":1,"47":1,"48":1,"50":2,"61":1,"96":1}}],[">3",{"5":{"21":3,"85":3}}],[">1",{"5":{"21":3,"85":3}}],[">2",{"5":{"21":4,"85":4}}],[">4",{"5":{"85":6}}],["<br><img",{"5":{"48":1}}],["<br><a",{"5":{"21":9,"85":25}}],["<br>",{"5":{"45":1,"47":1,"48":1,"50":1,"96":2}}],["<img",{"5":{"44":1,"45":2,"47":2,"48":2,"50":2,"61":1,"96":3}}],["<",{"5":{"42":2}}],["<a",{"5":{"21":1,"85":1}}],["大多数局部极小值在测试性能上相近",{"5":{"97":2}}],["大熊猫的主要食物是什么",{"5":{"94":4}}],["大时",{"5":{"91":1}}],["大的​",{"5":{"90":1}}],["大的",{"5":{"90":5,"91":2}}],["大部分方向上的变化很小",{"5":{"89":2}}],["大量实验表明",{"5":{"88":2}}],["大量这样的局部探测器通过线性组合",{"5":{"71":2}}],["大小为",{"5":{"86":2}}],["大小",{"5":{"50":2}}],["大语言模型由于其巨大的参数量和复杂架构",{"5":{"97":2}}],["大语言模型中的实际损失函数通常是非凸的",{"5":{"97":2}}],["大语言模型中的输入通常是一个三维张量",{"5":{"50":2}}],["大语言模型中的所有参数最终都以矩阵或张量的形式存储和计算",{"5":{"50":2}}],["大语言模型中的损失函数",{"0":{"99":1,"101":1},"4":{"98":1,"99":1,"100":1,"101":1},"5":{"85":4,"99":1,"101":1}}],["大语言模型中的损失函数<",{"5":{"85":1}}],["大语言模型中的典型多任务设置",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["大语言模型的某些扩展",{"5":{"96":2}}],["大语言模型的优化面临独特的挑战",{"5":{"48":2}}],["大语言模型的训练通常对学习率非常敏感",{"5":{"97":2}}],["大语言模型的训练本质上是一个优化问题",{"5":{"48":2}}],["大语言模型的训练涉及多个层面的损失函数设计",{"5":{"99":2,"101":2}}],["大语言模型通常使用子词分词器",{"5":{"99":2,"101":2}}],["大语言模型通常在多种任务上进行联合训练",{"5":{"99":2,"101":2}}],["大数定律和中心极限定理也指导着模型评估策略",{"5":{"96":2}}],["大数定律和中心极限定理是概率论中最重要的极限定理",{"5":{"96":2}}],["大数定律解释了为什么使用更大的批量",{"5":{"96":2}}],["大数定律",{"5":{"96":2}}],["大数定律与中心极限定理",{"2":{"96":1},"5":{"96":1}}],["大大提高了计算效率",{"5":{"47":2}}],["大模型中的数学",{"0":{"21":1,"85":1},"4":{"21":1},"5":{"21":1,"85":1}}],["大卫",{"2":{"21":1,"85":1},"5":{"21":1,"44":2,"85":1}}],["where或what",{"5":{"95":2}}],["what",{"5":{"95":2}}],["whole",{"5":{"99":4,"101":4}}],["wgmma",{"5":{"86":2}}],["wavelet",{"5":{"90":2,"91":2}}],["walter",{"5":{"47":2}}],["warmup策略在训练初期逐渐增加学习率",{"5":{"48":2}}],["warmup",{"5":{"48":4,"97":8}}],["warren",{"5":{"47":2}}],["warp",{"5":{"45":1,"93":2}}],["w^2",{"5":{"46":2}}],["weighted",{"5":{"51":2}}],["weight",{"5":{"50":2}}],["weierstrass定理推导出通用逼近能力",{"5":{"71":2}}],["weierstrass定理指出",{"5":{"71":2}}],["weierstrass逼近定理",{"5":{"71":2}}],["werden",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["w",{"5":{"42":6,"46":22,"69":6,"86":2,"97":4}}],["window",{"5":{"86":4}}],["wise",{"5":{"50":2,"91":2}}],["wissen",{"2":{"21":2,"85":2},"5":{"21":2,"85":2}}],["williams",{"5":{"44":2}}],["wir",{"2":{"21":2,"85":2},"5":{"21":2,"85":2}}],["word",{"5":{"94":2,"99":6,"101":6}}],["wordpiece",{"5":{"99":2,"101":2}}],["wwm",{"5":{"99":2,"101":2}}],["wwm策略确保如果一个词的一部分被掩码",{"5":{"99":2,"101":2}}],["wwmlm损失",{"5":{"99":2,"101":2}}],["wwm的优势在于",{"5":{"99":2,"101":2}}],["www",{"5":{"99":2,"101":2}}],["希尔伯特",{"2":{"21":1,"85":1},"5":{"21":1,"85":1}}],["前",{"5":{"89":1}}],["前个维度携带主要信息",{"5":{"89":1}}],["前几个奇异值较大",{"5":{"87":2}}],["前几个奇异值占据了大部分能量",{"5":{"87":2,"89":2}}],["前一层的输出作为后一层的输入",{"5":{"46":2}}],["前馈神经网络中的全连接层更是由多个权重矩阵堆叠而成",{"5":{"50":2}}],["前馈扩展维度4d=16384",{"5":{"45":2}}],["前馈网络",{"5":{"45":2,"71":4,"101":2}}],["前馈网络中的激活函数选择",{"2":{"71":1},"5":{"71":1}}],["前馈网络中的激活函数决定了信息如何被非线性变换",{"5":{"41":2}}],["前馈网络数学",{"2":{"21":1,"85":1},"4":{"44":1,"45":1,"46":1,"47":1},"5":{"21":9,"85":9}}],["前向和反向传播",{"5":{"97":2}}],["前向和反向方差稳定",{"5":{"41":1}}],["前向模式计算",{"5":{"45":1}}],["前向模式计算​",{"5":{"45":1}}],["前向模式与反向模式自动微分的比较",{"5":{"45":1}}],["前向模式与反向模式自动微分的比较理解前向传播的计算复杂度对于评估网络效率",{"5":{"45":1}}],["前向模式与反向模式的比较",{"5":{"44":1}}],["前向模式与反向模式的比较定义2",{"5":{"44":1}}],["前向模式的微分计算",{"5":{"45":2}}],["前向模式自动微分虽然计算效率较低",{"5":{"45":2}}],["前向模式自动微分",{"5":{"44":1}}],["前向模式",{"5":{"44":2,"45":1}}],["前向传播具有良好的并行性",{"5":{"45":2}}],["前向传播实现了从输入空间到输出空间的几何变换",{"5":{"45":2}}],["前向传播在数学上是一个复合函数的逐层求值过程",{"5":{"45":2}}],["前向传播对应于计算图的拓扑排序",{"5":{"45":2}}],["前向传播不仅是一个代数计算过程",{"5":{"45":2}}],["前向传播最本质的数学描述是复合函数",{"5":{"45":2}}],["前向传播是这些数学描述在实际运行时的具体执行过程",{"5":{"45":2}}],["前向传播本质上是一个复合函数的求值过程",{"5":{"45":2}}],["前向传播的本质是复合函数的求值",{"5":{"45":2}}],["前向传播的矩阵运算形式天然具有良好的并行性",{"5":{"45":2}}],["前向传播的核心数学问题可以概括为",{"5":{"45":2}}],["前向传播的意义远不止于",{"5":{"45":2}}],["前向传播的并行性层次",{"5":{"45":1}}],["前向传播的并行性层次本节从多个数学视角深入分析了前向传播的本质",{"5":{"45":1}}],["前向传播的并行性",{"2":{"45":1},"5":{"45":1}}],["前向传播的计算复杂度",{"2":{"45":1},"5":{"45":1}}],["前向传播的数学本质",{"0":{"45":1},"4":{"45":1},"5":{"21":4,"45":1,"85":4}}],["前向传播的数学本质<",{"5":{"21":1,"85":1}}],["前向传播与反向传播的flops比较",{"5":{"44":1}}],["前向传播与反向传播的flops比较推论2",{"5":{"44":1}}],["前向传播",{"5":{"44":3,"45":2}}],["前向传播保存的中间值",{"5":{"44":2}}],["前向传播只是完成了",{"5":{"44":2}}],["前层参数保持随机初始化状态",{"5":{"41":2}}],["前层的参数几乎无法接收到关于损失函数的信息",{"5":{"41":2}}],["前缀",{"5":{"99":2,"101":2}}],["前缀语言建模与填充token的数学处理",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["前缀语言建模损失",{"5":{"99":2,"101":2}}],["前缀语言建模损失仅对目标token计算损失",{"5":{"99":2,"101":2}}],["前缀token",{"5":{"99":2,"101":2}}],["前缀部分通常使用双向注意力",{"5":{"99":2,"101":2}}],["前缀需要",{"5":{"99":2,"101":2}}],["掩码机制等重要内容",{"5":{"95":2}}],["掩码将位置",{"5":{"91":1}}],["掩码将位置的注意力分数设为",{"5":{"91":1}}],["掩码操作与位置编码的结合有两种常见方式",{"5":{"91":2}}],["掩码语言建模",{"2":{"99":1,"101":1},"5":{"99":5,"101":5}}],["掩码语言建模的形式化定义",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["掩码语言建模损失使用被掩码位置的真实token作为正样本",{"5":{"101":2}}],["掩码语言建模损失",{"5":{"99":2,"101":2}}],["掩码策略的数学分析",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["掩码策略的信息论分析",{"5":{"99":2,"101":2}}],["掩码分布",{"5":{"99":2,"101":2}}],["掩码率与学习效率",{"5":{"99":2,"101":2}}],["掩码率的选择是一个关键的超参数",{"5":{"99":1,"101":1}}],["掩码率是一个经验性的折中",{"5":{"99":2,"101":2}}],["掩码率",{"5":{"99":1,"101":1}}],["隐式和显式正则化的强度",{"5":{"97":2}}],["隐式位置编码",{"5":{"88":2}}],["隐式应用",{"5":{"88":2}}],["隐式奖励与对齐机制",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["隐式奖励",{"5":{"99":2,"101":2}}],["隐藏维度12288",{"5":{"93":2}}],["隐藏维度为的标准自注意力配置",{"5":{"95":1}}],["隐藏维度为的输入",{"5":{"86":1}}],["隐藏维度为​",{"5":{"93":1}}],["隐藏维度为",{"5":{"86":1,"93":1,"95":1}}],["隐藏维度",{"5":{"50":1,"93":2}}],["隐藏维度通常在数千量级",{"5":{"50":1}}],["隐藏层误差信号传播",{"5":{"44":2}}],["序列顺序",{"5":{"92":2}}],["序列开头和结尾的位置在建模远距离依赖时可能面临额外的挑战",{"5":{"92":2}}],["序列中任意两个位置之间可以直接建立联系",{"5":{"92":2}}],["序列中两个位置之间的依赖关系可以用路径长度来度量",{"5":{"92":2}}],["序列中的每个元素依次被处理",{"5":{"91":2}}],["序列所需的最低频率",{"5":{"90":1}}],["序列",{"5":{"90":2}}],["序列各位置的信息会趋于",{"5":{"87":2}}],["序列逐步输入",{"5":{"86":2}}],["序列长度",{"5":{"50":2,"90":1}}],["序列长度和嵌入维度​共同决定了可用的频率范围",{"5":{"90":1}}],["序列长度和嵌入维度",{"5":{"50":2}}],["序列的联合分布与边际分布",{"5":{"69":2}}],["序列的联合分布可以分解为条件分布的乘积",{"5":{"99":2,"101":2}}],["序列通常被组织为",{"5":{"99":2,"101":2}}],["序列标注任务",{"5":{"99":2,"101":2}}],["序列生成任务",{"5":{"99":2,"101":2}}],["带有位置编码的输入表示为",{"5":{"89":2}}],["带置换矩阵",{"5":{"50":2}}],["带偏置的softmax",{"5":{"69":2}}],["带掩码的语言建模损失为",{"5":{"99":2,"101":2}}],["带kl约束的策略优化",{"5":{"99":2,"101":2}}],["带参考策略的对比学习",{"5":{"99":2,"101":2}}],["会影响模型学习位置模式的能力",{"5":{"101":2}}],["会变得非常小",{"5":{"95":1}}],["会压缩输入空间到低维流形",{"5":{"94":2}}],["会将所有头的信息混合起来",{"5":{"93":1}}],["会有更多不同尺度的频率成分",{"5":{"91":2}}],["会占据绝对主导地位",{"5":{"61":2}}],["会导致上下文信息过于稀疏",{"5":{"99":2,"101":2}}],["会导致监督信号不足",{"5":{"99":2,"101":2}}],["忽略非线性激活的影响",{"5":{"92":2}}],["忽略边界效应",{"5":{"87":2}}],["忽略",{"5":{"88":2,"99":2,"101":2}}],["预定义",{"5":{"90":2}}],["预测减真实",{"5":{"96":2}}],["预测第",{"5":{"61":2}}],["预测",{"5":{"51":1,"69":4,"99":2,"101":2}}],["预测空间是矩阵的列空间",{"5":{"51":1}}],["预测空间",{"5":{"51":3}}],["预测的最优问题",{"5":{"51":1}}],["预测向量为",{"5":{"51":2}}],["预测向量",{"5":{"51":4}}],["预测样本类别",{"5":{"69":1}}],["预测值与真实值的差异",{"5":{"70":2}}],["预测0",{"5":{"70":2}}],["预测概率与真实概率一致",{"5":{"70":2}}],["预测为",{"5":{"99":2,"101":2}}],["预训练阶段",{"5":{"99":2,"101":2}}],["策略进行稳定化",{"5":{"97":2}}],["策略和余弦退火调度是大语言模型训练的标准配置",{"5":{"97":2}}],["策略在大语言模型训练中尤为重要",{"5":{"97":2}}],["策略",{"5":{"97":2}}],["策略二",{"5":{"88":1}}],["策略一",{"5":{"88":2}}],["策略网络的离散动作选择",{"5":{"71":2}}],["策略正则化",{"5":{"99":2,"101":2}}],["策略优化阶段",{"5":{"99":2,"101":2}}],["策略优化的目标为",{"5":{"99":2,"101":2}}],["策略的logit是参考策略的logit加上奖励项的缩放",{"5":{"99":2,"101":2}}],["近距离关系",{"5":{"91":2}}],["近似误差为",{"5":{"89":2}}],["近似分布",{"5":{"61":2}}],["近似距离",{"5":{"70":2}}],["近端策略优化",{"5":{"99":2,"101":2}}],["核函数将数据映射到高维隐空间",{"5":{"87":2}}],["核矩阵的谱可以用傅里叶分析研究",{"5":{"87":2}}],["核矩阵的谱性质",{"5":{"87":2}}],["核矩阵是半正定矩阵",{"5":{"87":2}}],["核矩阵是该隐空间中的gram矩阵",{"5":{"87":2}}],["核矩阵",{"5":{"87":2}}],["核方法的理论工具可以应用于注意力机制的分析",{"5":{"87":2}}],["核方法",{"5":{"87":2}}],["核方法中的许多理论工具可以应用于注意力的分析",{"5":{"87":2}}],["核范数与矩阵的秩密切相关",{"5":{"87":2}}],["核范数正则化可以鼓励解的低秩性质",{"5":{"50":2}}],["核范数是frobenius范数和谱范数之间的折中",{"5":{"50":2}}],["核范数",{"5":{"50":2,"87":2}}],["核范数和1",{"5":{"50":2}}],["核心洞察是",{"5":{"69":2}}],["核心思想是",{"5":{"87":2}}],["核心思想",{"5":{"99":2,"101":2}}],["现在",{"5":{"92":2}}],["现在考虑自注意力中的query",{"5":{"88":2}}],["现在让我们完整地推演rope如何进入自注意力机制",{"5":{"88":2}}],["现在计算它们的内积",{"5":{"88":2}}],["现代gpu的大规模并行能力可以充分利用这一特性",{"5":{"93":2}}],["现代gpu的并行架构非常适合这种批处理计算",{"5":{"50":2}}],["现代gpu架构专门优化了张量运算",{"5":{"50":2}}],["现代gpu和tpu等硬件加速器擅长并行计算",{"5":{"45":2}}],["现代神经网络通常由以下类型的层组成",{"5":{"46":2}}],["现代transformer",{"5":{"71":2}}],["现代深度学习框架",{"5":{"93":4}}],["现代深度学习框架都针对这种矩阵运算进行了高度优化",{"5":{"51":2}}],["现代深度学习框架的优化器",{"5":{"46":2}}],["现代深度学习优化器",{"5":{"70":2}}],["现代大语言模型的预训练和微调通常涉及多种任务的组合",{"5":{"99":2,"101":2}}],["现象类似",{"5":{"93":2}}],["现象",{"5":{"90":2,"91":2,"92":2}}],["现象与注意力机制中的",{"5":{"70":2}}],["现象相关",{"5":{"70":2}}],["现有损失函数存在一些固有的局限性",{"5":{"99":2,"101":2}}],["未来的研究方向还包括损失函数与模型架构的协同设计",{"5":{"101":2}}],["未来的研究方向之一是设计自适应的损失函数",{"5":{"99":2,"101":2}}],["未缩放的点积",{"5":{"95":1}}],["未缩放的点积的方差为",{"5":{"95":1}}],["未见过的位置编码",{"5":{"88":2}}],["未饱和区域",{"5":{"42":2}}],["外推时可能不稳定",{"5":{"91":2}}],["外推能力等",{"5":{"90":2}}],["外推能力是指模型处理超出训练长度的序列的能力",{"5":{"89":2}}],["外推可能失败",{"5":{"89":2}}],["外推效果会较好",{"5":{"89":2}}],["外推困难等",{"5":{"89":2}}],["外推上的鲁棒性",{"5":{"88":1}}],["外推上的鲁棒性是第三个因素",{"5":{"88":1}}],["外推性能往往不佳",{"5":{"88":2}}],["外积计算",{"5":{"44":1}}],["外所有token的序列",{"5":{"99":1,"101":1}}],["视为被重复",{"5":{"50":1}}],["视为一个",{"5":{"69":2}}],["视为一个潜在的",{"5":{"69":1}}],["视为一个二分类问题",{"5":{"99":1,"101":1}}],["视为",{"5":{"69":3}}],["统计量的几何基础",{"5":{"51":1}}],["统计量来源",{"5":{"41":1}}],["统计学来源",{"5":{"51":2}}],["统计显著",{"5":{"96":2}}],["统计推断的思想贯穿于模型训练",{"5":{"96":2}}],["统计推断是利用样本数据对总体特征进行推断的过程",{"5":{"96":2}}],["统计推断与参数估计",{"2":{"96":1},"5":{"96":1}}],["统计意义",{"5":{"70":2}}],["统一的数学框架",{"2":{"69":1},"5":{"69":2}}],["统一的数学框架三个场景的共同结构是",{"5":{"69":1}}],["统一的优化框架",{"5":{"69":2}}],["噪声水平",{"5":{"97":2}}],["噪声过滤",{"5":{"89":2}}],["噪声的参数通过变分推断学习得到",{"5":{"96":2}}],["噪声对比估计",{"5":{"69":2}}],["噪声对比估计的核心思想",{"5":{"69":2}}],["噪声分布的选择对性能有显著影响",{"5":{"69":1}}],["噪声分布",{"5":{"69":1}}],["配对t检验可以判断两个模型性能的差异是否显著",{"5":{"96":2}}],["配对t检验是比较两个相关样本均值的常用方法",{"5":{"96":2}}],["配分函数",{"5":{"69":2}}],["区别在于",{"5":{"94":2}}],["区别仅在于输入向量的来源",{"5":{"70":2}}],["区间内",{"5":{"71":1,"95":2}}],["区间",{"5":{"41":1,"42":7,"71":1,"97":2}}],["区分相近频率的能力",{"5":{"90":2}}],["区分它来自分布",{"5":{"69":2}}],["区分性",{"5":{"70":1}}],["那么它们对最终点积的贡献就会更大",{"5":{"95":2}}],["那么​的影响需要经过步的传递才能到达",{"5":{"92":1}}],["那么",{"5":{"69":6,"92":1}}],["那么infonce的目标正是最大化观察到的正样本对的似然",{"5":{"69":2}}],["那么标准的infonce损失为",{"5":{"69":2}}],["那么优化过程会倾向于最大化每个位置与其他位置之间的互信息",{"5":{"69":2}}],["那么softmax操作本质上是在计算与每个的相似度",{"5":{"69":1}}],["那么softmax操作本质上是在计算",{"5":{"69":1}}],["那么注意力权重就是是的正样本的软概率",{"5":{"69":1}}],["那么注意力权重",{"5":{"69":1}}],["差向量的模为",{"5":{"90":2}}],["差向量的模约为",{"5":{"90":2}}],["差分正则化鼓励学习平滑的位置编码",{"5":{"89":2}}],["差分正则化惩罚相邻位置编码的差异",{"5":{"89":2}}],["差分正则化和谱正则化",{"5":{"89":2}}],["差异很大",{"5":{"61":2}}],["差异的信息论度量",{"5":{"71":2}}],["差异",{"5":{"70":2}}],["差值最小化",{"5":{"69":2}}],["影响编码的",{"5":{"89":2}}],["影响实际性能",{"5":{"86":2}}],["影响局部优化的收敛速度",{"5":{"48":2}}],["影响softmax分布的锐度",{"5":{"69":1}}],["影响softmax分布",{"5":{"69":1}}],["影响模型对不同尺度位置信息的利用程度",{"5":{"70":2}}],["防止编码向量过大",{"5":{"89":2}}],["防止表示崩溃",{"5":{"69":2}}],["防止策略发生剧烈变化",{"5":{"99":2,"101":2}}],["防止策略模型偏离初始策略太远",{"5":{"99":1,"101":1}}],["防止策略模型偏离初始策略",{"5":{"99":1,"101":1}}],["锐度",{"5":{"69":4,"95":2,"96":2}}],["锐化决策边界",{"5":{"69":2}}],["场景一",{"5":{"69":2}}],["场景二",{"5":{"69":2}}],["场景三",{"5":{"69":2}}],["场景",{"5":{"69":1}}],["集中在正样本索引上",{"5":{"69":2}}],["集中在正样本上",{"5":{"69":2}}],["尺度效应",{"5":{"69":2}}],["尺度参数",{"5":{"69":4}}],["尺度",{"5":{"70":2,"91":2}}],["内存访问模式是",{"5":{"92":2}}],["内存带宽与计算效率",{"2":{"92":1},"5":{"92":1}}],["内存布局优化方面",{"5":{"89":2}}],["内存效率不仅与参数数量有关",{"5":{"50":2}}],["内容驱动假设",{"5":{"92":2}}],["内容",{"5":{"91":4,"95":2}}],["内容向量",{"5":{"88":2}}],["内容与位置的交叉项",{"5":{"88":2}}],["内积的虚部为",{"5":{"90":2}}],["内积的值保持不变",{"5":{"88":2}}],["内积可以看作是一个",{"5":{"88":1}}],["内积可以看作是一个g",{"5":{"88":1}}],["内积严格等于原始query和key在一个旋转后的向量上的内积",{"5":{"88":2}}],["内积为",{"5":{"88":4}}],["内积不再只依赖于相对位置",{"5":{"88":2}}],["内积",{"5":{"69":2,"88":2}}],["广度",{"5":{"93":2}}],["广义内积",{"5":{"87":2}}],["广义infonce框架",{"5":{"69":2}}],["广播规则通常是从最后一个维度开始逐维比较",{"5":{"50":2}}],["广播是张量逐元素运算中的一种自动对齐机制",{"5":{"50":2}}],["广播",{"5":{"50":2}}],["拉伸",{"5":{"94":2}}],["拉普拉斯近似用于构建后验分布的高斯代理",{"5":{"48":2}}],["拉普拉斯近似利用hessian矩阵在峰值附近对概率分布进行高斯近似",{"5":{"48":2}}],["拉向查询向量",{"5":{"69":1}}],["位精度",{"5":{"89":1}}],["位置到位置",{"5":{"92":2}}],["位置到相位向量的映射是单射",{"5":{"90":2}}],["位置关注位置",{"5":{"92":2}}],["位置在第层的表示​是​",{"5":{"92":1}}],["位置头适合捕获需要考虑序列顺序的依赖关系",{"5":{"92":2}}],["位置可以直接",{"5":{"92":2}}],["位置可以通过不同频率的正弦波表示",{"5":{"91":2}}],["位置与自身的距离",{"5":{"92":2}}],["位置和位置之间的距离为",{"5":{"92":1}}],["位置和位置之间的相似度只取决于它们的距离",{"5":{"91":1}}],["位置三类交互项",{"5":{"91":2}}],["位置为行",{"5":{"91":2}}],["位置2的编码为",{"5":{"91":2}}],["位置2",{"5":{"91":2}}],["位置1与位置2的点积",{"5":{"91":2}}],["位置1的编码为",{"5":{"91":2}}],["位置1",{"5":{"91":2}}],["位置0与位置2的点积",{"5":{"91":2}}],["位置0与位置1的点积",{"5":{"91":2}}],["位置0的编码为",{"5":{"91":2}}],["位置0",{"5":{"91":2}}],["位置独立性",{"5":{"91":2}}],["位置投影",{"5":{"91":1}}],["位置投影的一个重要性质是",{"5":{"91":2}}],["位置投影​和​可以预先计算",{"5":{"91":1}}],["位置相关的依赖",{"5":{"91":2}}],["位置交互",{"5":{"91":2}}],["位置差",{"5":{"90":1}}],["位置差的编码分辨率与频率成反比",{"5":{"90":1}}],["位置变换算子",{"5":{"90":1}}],["位置变换算子在频率空间中是一个对角矩阵",{"5":{"90":1}}],["位置变化的主要模式可以用少数几个维度捕获",{"5":{"89":2}}],["位置对应圆群上的点",{"5":{"90":1}}],["位置区分精度",{"5":{"90":2}}],["位置域中时间分辨率",{"5":{"90":2}}],["位置域中的卷积对应于频率域中的乘积",{"5":{"90":2}}],["位置域中的平移对应于频率域中的相位偏移",{"5":{"90":2}}],["位置域",{"5":{"90":2}}],["位置表示",{"5":{"89":2}}],["位置依赖",{"5":{"89":2,"92":2}}],["位置索引对应旋转角度",{"5":{"88":2}}],["位置平移",{"5":{"88":2}}],["位置j的key",{"5":{"88":2}}],["位置i的query",{"5":{"88":2}}],["位置应该编码为相位",{"5":{"88":2}}],["位置的键",{"5":{"92":1}}],["位置的查询",{"5":{"92":1}}],["位置的最终表示保留了原始嵌入的直接成分",{"5":{"92":1}}],["位置的原始嵌入​通过残差连接直接参与到最终层的表示",{"5":{"92":1}}],["位置的值向量包含了位置从位置聚合的信息",{"5":{"92":1}}],["位置的值向量直接参与了位置的输出计算",{"5":{"92":1}}],["位置的注意力权重​可能很小",{"5":{"92":1}}],["位置的输出​是所有位置的值的加权平均",{"5":{"92":1}}],["位置的信息通过位置作为",{"5":{"92":1}}],["位置的信息就可以",{"5":{"92":1}}],["位置的信息对位置的贡献也较小",{"5":{"92":1}}],["位置的信息有多少能够传递到位置",{"5":{"92":1}}],["位置的信息",{"5":{"91":1}}],["位置的编码与位置的编码是对称的",{"5":{"91":1}}],["位置的编码向量​定义为",{"5":{"91":1}}],["位置的编码​可以视为频率域中的",{"5":{"90":1}}],["位置的编码​​在训练时从未见过",{"5":{"89":1}}],["位置的编码是初始编码经过各频率成分独立旋转次后的结果",{"5":{"90":1}}],["位置的编码可以表示为初始编码​经过次变换的结果",{"5":{"90":1}}],["位置的key向量",{"5":{"87":1}}],["位置的query向量",{"5":{"87":1}}],["位置",{"5":{"87":4,"88":2,"89":1,"90":4,"91":8,"92":26,"96":2}}],["位置感知的特征表示",{"5":{"71":2}}],["位置信息需要显式注入",{"5":{"92":2}}],["位置信息和内容信息共享投影矩阵",{"5":{"91":1}}],["位置信息和内容信息共享投影矩阵和​",{"5":{"91":1}}],["位置信息和token信息通过不同的权重矩阵投影后相加",{"5":{"71":2}}],["位置信息自然地编码在隐藏状态的更新过程中",{"5":{"91":2}}],["位置信息可以用低频到中频的成分表示",{"5":{"90":2}}],["位置信息完全由内容嵌入和后续学习提供",{"5":{"89":2}}],["位置信息没有与内容信息混淆",{"5":{"88":2}}],["位置信息以三种不同的方式混入内积计算",{"5":{"88":2}}],["位置信息到内容中",{"5":{"88":2}}],["位置信息能够被自注意力的内积运算自然地提取",{"5":{"88":2}}],["位置信息应该以什么方式编码到向量中",{"5":{"88":2}}],["位置信息",{"5":{"71":2,"88":2}}],["位置信息被缩放到合适的范围",{"5":{"41":2}}],["位置信息流形",{"5":{"70":2}}],["位置信息通过加法",{"5":{"88":2}}],["位置信息通过位置编码注入模型",{"5":{"99":2,"101":2}}],["位置编码等高级主题奠定坚实的理论基础",{"5":{"95":2}}],["位置编码假设",{"5":{"92":2}}],["位置编码提供的方向信息可以帮助模型捕获跨越长距离的关联",{"5":{"92":2}}],["位置编码仍然提供了绝对位置信息",{"5":{"91":2}}],["位置编码本身不参与掩码操作",{"5":{"91":2}}],["位置编码本身不会增加可训练参数的数量",{"5":{"88":2}}],["位置编码通过投影进入query和key的计算",{"5":{"91":2}}],["位置编码通过投影矩阵和​进入注意力计算",{"5":{"91":1}}],["位置编码通过上述四项影响最终的注意力权重分布",{"5":{"91":2}}],["位置编码通过影响query和key的计算来间接影响注意力权重",{"5":{"91":2}}],["位置编码为",{"5":{"91":2}}],["位置编码之间的相似度",{"5":{"91":2}}],["位置编码被",{"5":{"91":2}}],["位置编码需要与输入嵌入相结合",{"5":{"91":2}}],["位置编码需要为",{"5":{"90":1}}],["位置编码需要为每个位置生成唯一的表示",{"5":{"90":2}}],["位置编码需要为个位置编码约比特的信息",{"5":{"90":1}}],["位置编码应当能够表达位置之间的距离和相对关系",{"5":{"91":2}}],["位置编码点积作为相对位置",{"5":{"90":1}}],["位置编码点积作为相对位置的函数",{"5":{"90":1}}],["位置编码中频率参数",{"5":{"90":1}}],["位置编码中频率参数的几何直觉可以通过圆周运动来理解",{"5":{"90":1}}],["位置编码是双向的",{"5":{"91":2}}],["位置编码是这些平面上点的集合",{"5":{"90":2}}],["位置编码是transformer架构中最具数学美感的设计之一",{"5":{"88":2}}],["位置编码能够有效表示的序列长度范围与频率参数的配置密切相关",{"5":{"90":2}}],["位置编码不应干扰内容信息的表示",{"5":{"91":2}}],["位置编码不应破坏自注意力的核心计算特性",{"5":{"88":2}}],["位置编码不再是孤立的数值向量",{"5":{"90":2}}],["位置编码最多能够编码约",{"5":{"89":1}}],["位置编码最多能够编码约比特的位置信息",{"5":{"89":1}}],["位置编码矩阵为",{"5":{"91":2}}],["位置编码矩阵",{"5":{"89":2}}],["位置编码矩阵通常按行主序存储",{"5":{"89":2}}],["位置编码矩阵通常以半精度",{"5":{"89":1}}],["位置编码矩阵的秩是影响其性质的关键参数",{"5":{"89":1}}],["位置编码矩阵的其他行",{"5":{"89":2}}],["位置编码向量的质心为",{"5":{"89":2}}],["位置编码将继续扮演关键角色",{"5":{"88":2}}],["位置编码领域仍在快速发展",{"5":{"88":2}}],["位置编码技术可以分为几个主要类别",{"5":{"88":2}}],["位置编码就是位置编码",{"5":{"88":2}}],["位置编码问题可以形式化地表述为",{"5":{"88":2}}],["位置编码数学",{"2":{"85":1},"4":{"88":1,"89":1,"90":1,"91":1},"5":{"85":9}}],["位置编码的具体形式将在后续章节详细讨论",{"5":{"94":2}}],["位置编码的引入为注意力机制注入了位置感知能力",{"5":{"92":2}}],["位置编码的必要性",{"2":{"92":1},"5":{"92":1}}],["位置编码的基本要求是不同位置具有不同的编码向量",{"5":{"91":2}}],["位置编码的生成是确定性的",{"5":{"91":2}}],["位置编码的设计需要满足几个条件",{"5":{"92":2}}],["位置编码的设计需要满足几个基本的数学要求",{"5":{"91":2}}],["位置编码的设计将面临新的挑战",{"5":{"88":2}}],["位置编码的点积只依赖于位置差",{"5":{"91":4}}],["位置编码的点积只依赖于相对位置",{"5":{"90":1}}],["位置编码的点积",{"5":{"90":1}}],["位置编码的平滑性",{"5":{"90":2}}],["位置编码的能量集中在离散的频率点上",{"5":{"90":2}}],["位置编码的能量完全集中在有限的几个离散频率上",{"5":{"90":2}}],["位置编码的分辨率",{"5":{"90":2}}],["位置编码的频率结构引入了一种隐式的先验",{"5":{"90":2}}],["位置编码的频率结构对模型的正则化和泛化能力有重要影响",{"5":{"90":2}}],["位置编码的频率成分间隔为",{"5":{"90":2}}],["位置编码的频率分解意味着不同维度携带不同尺度的位置信息",{"5":{"71":2}}],["位置编码的频谱可以表示为冲激函数的叠加",{"5":{"90":2}}],["位置编码的各频率成分具有不同的",{"5":{"90":2}}],["位置编码的gram矩阵",{"5":{"89":1}}],["位置编码的gram矩阵的第个元素为",{"5":{"89":1}}],["位置编码的参数量约为",{"5":{"89":1}}],["位置编码的参数量约为个",{"5":{"89":1}}],["位置编码的学习率可以与模型其他部分相同",{"5":{"89":2}}],["位置编码的前向传播是简单的查表操作",{"5":{"89":2}}],["位置编码的正则化可以影响学习到的编码特性",{"5":{"89":2}}],["位置编码的优化景观通常是非凸的",{"5":{"89":2}}],["位置编码的质量可能下降",{"5":{"88":2}}],["位置编码的外推能力是指",{"5":{"88":2}}],["位置编码的计算是稳定且可预测的",{"5":{"88":2}}],["位置编码的使命",{"5":{"88":2}}],["位置编码的更广阔图景",{"2":{"88":1},"5":{"88":1}}],["位置编码的本质问题",{"2":{"88":1},"5":{"88":1}}],["位置编码的核心要求之一是能够表示相对位置",{"5":{"90":2}}],["位置编码的核心思想是为序列中的每个位置赋予一个独特的",{"5":{"70":2}}],["位置编码的核心作用是打破自注意力机制的",{"5":{"99":2,"101":2}}],["位置编码的数学形式为向原始嵌入添加一个与位置相关的编码向量",{"5":{"92":2}}],["位置编码的数学设计",{"5":{"71":2}}],["位置编码的数学结构虽然在表面上属于不同的主题",{"5":{"70":2}}],["位置编码的选择直接影响模型学习位置模式的能力",{"5":{"99":2,"101":2}}],["位置编码",{"5":{"41":2,"71":8,"91":3,"92":4,"99":2,"101":4}}],["位置编码与学习率调度存在相互作用",{"5":{"97":2}}],["位置编码与激活函数的结合决定了位置信息如何在网络中传递",{"5":{"71":2}}],["位置编码与特征工程的对应",{"2":{"70":1},"5":{"70":1}}],["位置编码可以视为对位置先验的一种建模",{"5":{"90":2}}],["位置编码可以视为定义在频率空间上的一组线性算子",{"5":{"90":2}}],["位置编码可以视为一个映射",{"5":{"89":2}}],["位置编码可以理解为",{"5":{"90":2}}],["位置编码可以理解为在",{"5":{"70":2}}],["位置编码可以被理解为对",{"5":{"71":2}}],["位置编码可以被视为特征工程",{"5":{"70":1}}],["位置编码可以被视为",{"5":{"70":1}}],["位置编码具有更好的数学性质",{"5":{"70":2}}],["位置编码对语言建模损失的必要性",{"5":{"99":2,"101":2}}],["位置编码在模型初始化时生成",{"5":{"91":2}}],["位置编码在这个聚合过程中扮演关键角色",{"5":{"99":2,"101":2}}],["位置无法关注位置",{"5":{"92":2}}],["位置无关性与注意力机制的数学关系",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["位置签名",{"5":{"99":2,"101":2}}],["位于",{"5":{"69":1}}],["散度关于",{"5":{"97":1}}],["散度关于是凸的",{"5":{"70":1}}],["散度",{"5":{"61":4,"70":4,"97":1}}],["散度的凸性直接推导",{"5":{"70":2}}],["散度不同",{"5":{"70":2}}],["散度可以视为两个分布在fisher信息度量下的",{"5":{"70":1}}],["散度在局部等价于fisher信息加权的二次形式",{"5":{"70":2}}],["散度与从到的",{"5":{"70":1}}],["散度与从",{"5":{"70":1}}],["散布在周围",{"5":{"69":1}}],["三个计算阶段的复杂度分别为",{"5":{"95":2}}],["三个场景的共同结构是",{"5":{"69":1}}],["三项",{"5":{"91":2}}],["三阶及更高阶的张量则可以表示更复杂的数据结构",{"5":{"50":2}}],["三阶导数",{"5":{"42":2}}],["三维情况下是一个等边三角形",{"5":{"70":2}}],["度量预测与真实标签之间的差异",{"5":{"44":1}}],["度量位置",{"5":{"69":1}}],["黎曼几何",{"5":{"61":1}}],["黎曼流形优化",{"2":{"70":1},"5":{"70":1}}],["黎曼度量由",{"5":{"70":2}}],["黎曼度量",{"5":{"70":1}}],["黎曼梯度下降考虑了不同方向上损失变化的",{"5":{"70":2}}],["黎曼梯度下降",{"5":{"70":1}}],["容易样本",{"5":{"70":2}}],["强度",{"5":{"89":2}}],["强调预测值与真实值之间的欧氏距离",{"5":{"70":2}}],["强调概率分布之间的信息差异",{"5":{"70":2}}],["强凸和",{"5":{"97":1}}],["强凸性是比凸性更强的性质",{"5":{"97":2}}],["强凸性与优化速度",{"5":{"97":2}}],["强凸性与条件数",{"5":{"70":2}}],["强凸性系数很小",{"5":{"70":2,"97":2}}],["强凸函数满足",{"5":{"70":1,"97":1}}],["强凸的",{"5":{"70":1}}],["倍",{"5":{"70":2}}],["抵消",{"5":{"70":4,"88":2}}],["了远距离位置之间的关联",{"5":{"92":2}}],["了频率成分",{"5":{"90":2}}],["了内容向量",{"5":{"88":2}}],["了分布",{"5":{"69":2}}],["了",{"5":{"70":2}}],["了均方误差中的项",{"5":{"70":1}}],["了均方误差中的",{"5":{"70":1}}],["恰好",{"5":{"70":2}}],["巧合",{"5":{"70":2}}],["惩罚力度",{"5":{"70":2}}],["错误",{"5":{"70":2}}],["错误程度",{"5":{"70":2}}],["严格包含",{"5":{"93":1}}],["严格包含​",{"5":{"93":1}}],["严格遵循rope的设计",{"5":{"88":2}}],["严格地只依赖于两个位置的相对距离",{"5":{"88":2}}],["严格来说不是真正的距离",{"5":{"96":2}}],["严格凸函数至多有一个全局最小值点",{"5":{"48":2}}],["严格凸函数是凸性更强的形式",{"5":{"48":2}}],["严重",{"5":{"97":1}}],["严重程度",{"5":{"70":2}}],["严重得多",{"5":{"70":1}}],["离散系统可能振荡甚至发散",{"5":{"97":2}}],["离散化的稳定性取决于学习率与损失函数曲率的关系",{"5":{"97":2}}],["离散化引入了数值误差",{"5":{"97":2}}],["离散梯度下降",{"5":{"97":2}}],["离散序列的傅里叶变换",{"5":{"90":2}}],["离散情况",{"5":{"90":2}}],["离散程度和相互关系",{"5":{"96":2}}],["离散随机变量只能取有限或可数无穷个值",{"5":{"96":2}}],["离谱",{"5":{"70":2}}],["身份标识",{"5":{"70":2}}],["手动调节",{"5":{"70":2}}],["帕累托最优性",{"5":{"70":2}}],["架构",{"5":{"70":2}}],["架构的设计哲学",{"5":{"70":2}}],["架构设计的数学一致性",{"5":{"70":2}}],["概率质量集中在一个或少数几个最大的输入上",{"5":{"95":2}}],["概率质量函数",{"5":{"96":2}}],["概率",{"5":{"61":2}}],["概率越小的事件发生时",{"5":{"61":2}}],["概率分布变得更加平坦",{"5":{"96":2}}],["概率分布被视为一个流形",{"5":{"70":2,"71":2}}],["概率分布的信息期望",{"2":{"61":1},"5":{"61":1}}],["概率分布的几何结构为理解注意力权重提供了类比框架",{"5":{"70":2}}],["概率密度最大的点",{"5":{"96":2}}],["概率由pdf在区间上的积分给出",{"5":{"96":2}}],["概率论和几何学的角度深入分析了各种损失函数的数学结构和理论基础",{"5":{"97":2}}],["概率论是研究随机现象规律性的数学分支",{"5":{"96":2}}],["概率论与统计",{"0":{"96":1},"4":{"96":1},"5":{"21":4,"85":4,"96":1}}],["概率论与统计<",{"5":{"21":1,"85":1}}],["概率视角下的神经元模型",{"2":{"47":1},"5":{"47":1}}],["概率视角下的激活函数",{"2":{"71":1},"5":{"71":1}}],["概率参数",{"5":{"71":1}}],["概率参数的对数几率为",{"5":{"71":1}}],["概率解释",{"5":{"61":1}}],["概率解释将logits转化为满足概率公理的分布",{"5":{"61":1}}],["概率解释框架",{"5":{"61":1}}],["概率解释为损失函数的设计提供了坚实的理论基础",{"5":{"71":2}}],["概率解释以及信息论意义",{"5":{"71":2}}],["概率解释的完整性",{"2":{"61":1},"5":{"61":1}}],["概率解释的一致性",{"5":{"69":2}}],["概率是归一化后的值",{"5":{"71":2}}],["概率本质",{"5":{"69":2}}],["概率单纯形是定义在",{"5":{"61":1}}],["概率单纯形是定义在中满足且的点集",{"5":{"61":1}}],["概率单纯形上的几何结构",{"2":{"70":1},"5":{"70":1}}],["概率单纯形退化为一条线段",{"5":{"70":2}}],["概率单纯形",{"5":{"70":1}}],["概率单纯形可以几何的理解为空间中由坐标轴截距为",{"5":{"70":1}}],["概率单纯形可以几何的理解为",{"5":{"70":1}}],["概率接近",{"5":{"70":2}}],["概念",{"5":{"70":1}}],["半负定矩阵",{"5":{"70":1}}],["半正定矩阵",{"5":{"70":1}}],["项可能超过1",{"5":{"51":2}}],["项",{"5":{"70":1,"90":1}}],["比如位置",{"5":{"92":1}}],["比如位置和位置之间的关联",{"5":{"92":1}}],["比特的信息",{"5":{"90":1}}],["比特的位置信息",{"5":{"89":1}}],["比特",{"5":{"89":1}}],["比",{"5":{"70":1}}],["校准",{"5":{"70":1}}],["人咬狗",{"5":{"88":2,"91":2,"92":2}}],["人类反馈强化学习损失",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["人类反馈强化学习",{"5":{"99":2,"101":2}}],["超平面",{"5":{"71":2}}],["超越softmax的归一化",{"2":{"99":1,"101":1},"5":{"99":1,"101":1}}],["详细解释了公式中每个组成部分的数学本质",{"5":{"95":2}}],["详细推导了其数学定义",{"5":{"93":2}}],["详细推导了二分类和多分类场景下的交叉熵损失",{"5":{"61":2}}],["详见第六章",{"5":{"99":2,"101":2}}],["详见6",{"5":{"99":2,"101":2}}],["排列不变性",{"5":{"99":2,"101":2}}],["仅保留top",{"5":{"69":2,"99":2,"101":2}}],["仅对有效位置",{"5":{"99":2,"101":2}}],["仅依赖于token嵌入而不包含位置信息",{"5":{"99":1,"101":1}}],["仅是位置",{"5":{"99":1,"101":1}}],["阻碍其他任务的学习",{"5":{"99":2,"101":2}}],["监督微调阶段",{"5":{"99":4,"101":4}}],["问题设定",{"5":{"44":2}}],["问题",{"5":{"41":4,"42":2,"44":1,"47":2,"71":2,"94":2,"95":2}}],["问答任务",{"5":{"99":2,"101":2}}],["问答任务需要关注问题中的关键实体",{"5":{"99":2,"101":2}}],["翻译任务",{"5":{"99":2,"101":2}}],["摘要生成任务",{"5":{"99":2,"101":2}}],["摘要任务需要关注文档的核心内容",{"5":{"99":2,"101":2}}],["奖励模型的误差可能传播到策略优化中",{"5":{"97":2}}],["奖励模型训练阶段",{"5":{"99":2,"101":2}}],["奖励模型可能学习到错误的偏好模式",{"5":{"99":2,"101":2}}],["收敛越慢",{"5":{"97":4}}],["收敛速度由强凸性系数",{"5":{"97":1}}],["收敛速度由强凸性系数与利普希茨常数的比值",{"5":{"97":1}}],["收敛速度的影响因素",{"5":{"97":2}}],["收敛速度分析",{"5":{"97":2}}],["收敛速度与最终性能",{"2":{"97":1},"5":{"97":1}}],["收敛行为可以精确分析",{"5":{"97":2}}],["收敛性质以及与模型泛化能力的联系",{"5":{"97":2}}],["收敛性和正则化对编码的影响",{"5":{"89":2}}],["收敛于真实参数值",{"5":{"96":2}}],["收集人类对不同模型输出的偏好比较数据",{"5":{"99":2,"101":2}}],["坏",{"5":{"99":2,"101":2}}],["真实类别",{"5":{"61":2}}],["真实",{"5":{"61":2,"69":4}}],["真实标签通常被视为确定性的",{"5":{"61":2}}],["真实标签为",{"5":{"61":4,"96":2}}],["真实标签",{"5":{"44":2}}],["真实分布是一个one",{"5":{"61":1}}],["真实分布被放在",{"5":{"61":2}}],["真实分布",{"5":{"61":5,"69":4,"71":1}}],["真实分布和预测分布",{"5":{"71":1}}],["真正依赖于位置",{"5":{"99":1,"101":1}}],["似然函数",{"5":{"61":2}}],["似然函数定义为",{"5":{"96":2}}],["似然的梯度",{"5":{"99":2,"101":2}}]],"serializationVersion":2}