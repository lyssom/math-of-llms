{"documentCount":56,"nextId":160,"documentIds":{"21":"大模型中的数学.html","41":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","42":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","44":"第2章-前馈网络数学/2.4-反向传播梯度推导.html","45":"第2章-前馈网络数学/2.3-前向传播的数学本质.html","46":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","47":"第2章-前馈网络数学/2.1-神经元的数学模型.html","48":"第1章-数学基础/1.3-微积分与优化基础.html","50":"第1章-数学基础/1.1-线性代数与张量运算.html","51":"第4章-损失函数数学/4.1-均方误差（mse）的数学基础与几何解释.html","61":"第4章-损失函数数学/4.2-交叉熵的概率论推导.html","69":"第4章-损失函数数学/4.4-infonce与注意力的softmax统一.html","70":"第4章-损失函数数学/4.3-损失函数数学结构对比.html","71":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","86":"第5章-注意力机制数学/5.6-注意力机制的变体.html","87":"第5章-注意力机制数学/5.5-注意力矩阵的谱性质与低秩结构.html","88":"第6章-位置编码数学/6.4-rotary-position-embedding（rope）的复数、群论完整推导.html","89":"第6章-位置编码数学/6.3-可学习位置编码的矩阵性质.html","90":"第6章-位置编码数学/6.2-频率空间的数学分析.html","91":"第6章-位置编码数学/6.1-正弦余弦位置编码的数学定义与推导.html","92":"第5章-注意力机制数学/5.4-注意力如何建模长程依赖.html","93":"第5章-注意力机制数学/5.3-多头注意力的矩阵推导与表达能力分析.html","94":"第5章-注意力机制数学/5.2-query、key、value的矩阵表示与变换.html","95":"第5章-注意力机制数学/5.1-scaled-dot-product-attention-的数学公式.html","96":"第1章-数学基础/1.2-概率论与统计.html","97":"第4章-损失函数数学/4.6-损失函数的优化性质.html","101":"第4章-损失函数数学/4.5-大语言模型中的损失函数.html","131":"index.html","132":"第14章-信息论视角/14.3-注意力机制与信息流量.html","133":"第14章-信息论视角/14.2-正则化与信息瓶颈.html","134":"第13章-动态系统与训练稳定性/13.1-离散时间动态系统的数学基础.html","135":"第13章-动态系统与训练稳定性/13.2-jacobian、hessian-的稳定性分析.html","136":"第13章-动态系统与训练稳定性/13.3-收敛性、震荡、周期行为.html","137":"第14章-信息论视角/14.1-信息熵与互信息.html","138":"第12章-概率视角下的大模型/12.3-标度律.html","139":"第12章-概率视角下的大模型/12.2-最大似然与交叉熵的详细推导.html","140":"第12章-概率视角下的大模型/12.1-自回归公式（链式法则）.html","141":"第11章-矩阵与张量分解/11.3-注意力矩阵的低秩结构.html","142":"第11章-矩阵与张量分解/11.2-大模型中低秩近似的数学依据.html","143":"第11章-矩阵与张量分解/11.1-矩阵与张量分解：svd、tucker、cp分解.html","144":"第10章-正则化与归一化数学/10.3-正则化对梯度与损失的影响.html","145":"第10章-正则化与归一化数学/10.2-dropout的期望保持性质.html","146":"第10章-正则化与归一化数学/10.1-常见归一化方法的数学表述.html","147":"第9章-梯度流与优化数学/9.3-梯度裁剪与正则化.html","148":"第9章-梯度流与优化数学/9.2-梯度消失与梯度爆炸的数学条件.html","149":"第9章-梯度流与优化数学/9.1-梯度协方差矩阵.html","150":"第5章-注意力机制数学/5.4-残差连接与归一化.html","151":"第5章-注意力机制数学/5.7-注意力机制的变体.html","152":"第5章-注意力机制数学/5.6-注意力矩阵的谱性质与低秩结构.html","153":"第5章-注意力机制数学/5.5-注意力如何建模长程依赖.html","154":"第8章-强化学习/8.3-ppo算法与大模型训练.html","155":"第8章-强化学习/8.2-策略梯度与actor-critic方法.html","156":"第8章-强化学习/8.1-强化学习基础与马尔可夫决策过程.html","157":"第7章-条件计算与稀疏模型：混合专家方法/7.3-训练与推理优化.html","158":"第7章-条件计算与稀疏模型：混合专家方法/7.2-专家网络与负载均衡.html","159":"第7章-条件计算与稀疏模型：混合专家方法/7.1-moe概述与门控机制.html"},"fieldIds":{"title":0,"aliases":1,"headers":2,"tags":3,"path":4,"content":5},"fieldLength":{"21":[1,1,17,1,2,64],"41":[2,1,35,1,5,900],"42":[3,1,14,1,6,433],"44":[3,1,19,1,6,513],"45":[3,1,15,1,6,611],"46":[2,1,7,1,5,305],"47":[3,1,14,1,6,542],"48":[3,1,6,1,6,500],"50":[2,1,26,1,5,871],"51":[5,1,32,1,8,665],"61":[3,1,44,1,6,872],"69":[2,1,21,1,5,731],"70":[3,1,27,1,6,808],"71":[3,1,28,1,6,858],"86":[3,1,27,1,6,541],"87":[2,1,48,1,5,910],"88":[8,1,22,1,11,920],"89":[3,1,14,1,6,725],"90":[3,1,13,1,6,694],"91":[3,1,12,1,6,707],"92":[3,1,50,1,6,1075],"93":[3,1,38,1,6,812],"94":[5,1,26,1,8,437],"95":[7,1,28,1,10,573],"96":[3,1,17,1,6,990],"97":[3,1,20,1,6,540],"101":[3,1,37,1,6,693],"131":[1,1,40,1,2,245],"132":[3,1,21,1,6,918],"133":[3,1,17,1,6,833],"134":[3,1,19,1,6,441],"135":[5,1,47,1,8,551],"136":[5,1,16,1,8,949],"137":[3,1,16,1,6,733],"138":[3,1,7,1,6,463],"139":[3,1,34,1,6,180],"140":[5,1,27,1,7,451],"141":[3,1,12,1,6,274],"142":[3,1,17,1,6,296],"143":[6,1,23,1,8,590],"144":[3,1,29,1,6,506],"145":[3,1,38,1,6,615],"146":[3,1,28,1,6,682],"147":[3,1,26,1,6,490],"148":[3,1,30,1,6,654],"149":[3,1,16,1,6,487],"150":[3,1,14,1,6,412],"151":[3,1,27,1,6,541],"152":[3,1,48,1,6,910],"153":[2,1,49,1,5,1074],"154":[3,1,57,1,6,426],"155":[4,1,62,1,7,350],"156":[3,1,67,1,6,219],"157":[3,1,7,1,7,386],"158":[3,1,14,1,7,493],"159":[3,1,16,1,7,679]},"averageFieldLength":[3.2321428571428577,1,26.446428571428577,1,6.178571428571428,609.6071428571429],"storedFields":{"21":{"title":"大模型中的数学","aliases":[],"headers":["Wir müssen wissen, wir werden wissen --David Hilbert","我们必须知道，我们终将知道 --大卫·希尔伯特","第一章 线性代数基础","第二章 前馈网络数学","第三章 激活函数与非线性数学"],"tags":[],"path":"大模型中的数学.html"},"41":{"title":"3.3 梯度饱和与梯度爆炸的数学根源","aliases":[],"headers":["3.3.1 梯度饱和的数学机制","饱和现象的数学定义","Sigmoid函数的饱和区域分析","Tanh函数的饱和特性","饱和效应的累积放大机制","3.3.2 梯度爆炸的数学机制","梯度爆炸的现象描述","简单RNN的梯度分析","Transformer中的梯度爆炸机制","残差连接的数学作用","梯度爆炸的阈值与裁剪策略","3.3.3 梯度流与信息保持","有效秩与信息传递","奇异值与梯度范数分析","初始化与梯度流","与注意力机制的梯度流","3.3.4 归一化技术与梯度稳定","批归一化的数学原理","层归一化的数学原理","层归一化与Transformer的配合","RMSNorm与计算优化","归一化技术的梯度稳定机制","3.3.5 激活函数设计的数学原则","避免梯度饱和区域","保持梯度流稳定性","提供足够的非线性表达能力","计算效率","与下游任务的契合","与注意力机制的设计协同","3.3.6本节小结"],"tags":[],"path":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html"},"42":{"title":"3.2 导数推导与梯度特性","aliases":[],"headers":["3.2.1 Sigmoid函数的导数与性质","Sigmoid函数的数学定义","导数推导与自化性质","高阶导数与泰勒展开","几何解释与梯度特性","3.2.2 Tanh函数的导数与性质","Tanh函数的数学定义与性质","与Sigmoid的数学关系","导数推导与性质分析","3.2.3 ReLU函数族的导数与性质","ReLU函数的定义与导数"],"tags":[],"path":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html"},"44":{"title":"2.4 反向传播梯度推导","aliases":[],"headers":["2.4.1 引言：为什么需要反向传播","2.4.2 链式法则的矩阵形式","2.4.3 误差信号的传播","2.4.4 参数梯度的完整推导","2.4.5 批量处理的梯度计算","2.4.6 计算复杂度分析","2.4.7 反向传播与自动微分的关系","2.4.8 数值稳定性与实现细节","2.4.9 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.4-反向传播梯度推导.html"},"45":{"title":"2.3 前向传播的数学本质","aliases":[],"headers":["2.3.1 引言：从计算到函数逼近","2.3.2 复合函数视角下的前向传播","2.3.3 信息流动的几何描述","2.3.4 计算图的表示与实现","2.3.5 前向传播的计算复杂度","2.3.6 前向传播的并行性","2.3.7 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.3-前向传播的数学本质.html"},"46":{"title":"2.2 神经网络的矩阵形式","aliases":[],"headers":["2.2.1 引言：从神经元到层","2.2.2 单层网络的矩阵表示","2.2.3 多层网络的矩阵形式"],"tags":[],"path":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html"},"47":{"title":"2.1 神经元的数学模型","aliases":[],"headers":["2.1.1 从生物神经元到数学抽象","2.1.2 感知机与仿射变换","2.1.3 激活函数的数学角色","2.1.4 神经元的几何解释","2.1.5 概率视角下的神经元模型","2.1.6 神经元模型的矩阵表示","2.1.7 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.1-神经元的数学模型.html"},"48":{"title":"1.3 微积分与优化基础","aliases":[],"headers":["1.3.1 偏导数与链式法则","1.3.2 多元函数梯度与Hessian","1.3.3 梯度下降与凸优化基础"],"tags":[],"path":"第1章-数学基础/1.3-微积分与优化基础.html"},"50":{"title":"1.1 线性代数与张量运算","aliases":[],"headers":["1.1.1 线性代数的核心地位","1.1.2 基本运算与运算规则","1.1.3 张量的表示与运算","1.1.4 特征空间的几何直觉","1.1.5 特征值与特征向量","1.1.6 奇异值与低秩近似","1.1.7 正交性与正交矩阵","1.1.8 矩阵范数与度量","1.1.9 矩阵分解的深化理解","1.1.10 Kronecker积与向量化","1.1.11 张量网络与高维运算","1.1.12 批处理与并行计算的张量视图","1.1.13 计算复杂度与内存效率"],"tags":[],"path":"第1章-数学基础/1.1-线性代数与张量运算.html"},"51":{"title":"4.1 均方误差（MSE）的数学基础与几何解释","aliases":[],"headers":["4.1.1 基本定义与符号约定","4.1.2 加权均方误差的推广","4.1.3 总体MSE与样本MSE","4.1.4 条件期望与最优预测","4.1.5 高斯噪声假设与最大似然估计","4.1.6 MSE与其他损失函数的联系","4.1.7 向量空间中的MSE","4.1.8 正交投影与最优预测","4.1.9 几何图示与直观理解","4.1.10 期望MSE的分解","4.1.11 各分解项的数学意义","4.1.12 偏差-方差权衡的可视化","4.1.13 凸性与优化特性","4.1.14 对异常值的敏感性","4.1.15 MSE的梯度特性","本节小结"],"tags":[],"path":"第4章-损失函数数学/4.1-均方误差（mse）的数学基础与几何解释.html"},"61":{"title":"4.2 交叉熵的概率论推导","aliases":[],"headers":["4.2.1 信息论基础：熵与散度的数学定义","自信息的概率论定义","香农熵：概率分布的信息期望","Kullback-Leibler散度：分布间差异的信息度量","4.2.2 交叉熵与KL散度的数学关系","交叉熵的定义与推导","交叉熵与KL散度的等价性分析","交叉熵的数学性质","4.2.3 交叉熵在分类任务中的完整推导","二分类任务的交叉熵损失","多分类任务与Softmax函数","多分类交叉熵损失的完整推导","从Logits到概率的完整流程","4.2.4 Softmax与交叉熵的梯度计算","交叉熵梯度的链式法则","雅可比矩阵的结构分析","4.2.5 数值稳定性问题与LogSumExp技巧","Softmax数值不稳定的根源","Log-Subtraction问题 除了Softmax","LogSumExp技巧的数学原理","融合计算：Softmax-CrossEntropy联合优化","融合梯度计算","4.2.6 交叉熵与最大似然估计的统一视角","最大似然估计的基本原理","分类任务的最大似然推导","从KL散度视角的再解释","概率解释的完整性","泛化性讨论","4.2.7 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.2-交叉熵的概率论推导.html"},"69":{"title":"4.4 InfoNCE与注意力的Softmax统一","aliases":[],"headers":["4.4.1 InfoNCE损失函数的数学基础","从NCE到InfoNCE的理论演进","InfoNCE的数学定义与推导","InfoNCE的互信息下界性质","4.4.2 InfoNCE的梯度结构与优化动力学","梯度的显式计算","正负样本对比机制的几何解释","温度参数的作用与影响","4.4.3 Softmax：统一的数学框架","Softmax操作的几何与概率本质","Softmax在三大场景中的统一性","从Softmax统一性看Transformer的设计哲学","4.4.4 从InfoNCE到注意力的数学桥梁","注意力作为广义对比学习","互信息最大化的视角","多头注意力的数学结构"],"tags":[],"path":"第4章-损失函数数学/4.4-infonce与注意力的softmax统一.html"},"70":{"title":"4.3 损失函数数学结构对比","aliases":[],"headers":["4.3.1 均方误差与交叉熵的数学结构对比","结构定义与基本形式","梯度结构的对比分析","Hessian矩阵与曲率分析","4.3.2 损失函数数学性质的系统性对比","凸性分析","梯度饱和特性","边界行为分析","4.3.3 几何视角下的损失函数分析","概率单纯形上的几何结构","信息几何视角","黎曼流形优化","4.3.4 任务适配性与损失函数选择","回归任务与分类任务的本质差异","损失函数选择的原则","多任务学习中的损失函数组合","4.3.5 与注意力机制的数学联系","Softmax结构的统一性","注意力权重与概率分布的类比","位置编码与特征工程的对应","4.3.6 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.3-损失函数数学结构对比.html"},"71":{"title":"3.1 激活函数的数学角色","aliases":[],"headers":["3.1.1 非线性的数学必要性","线性映射的数学结构","线性模型表达能力的根本局限","非线性变换打破表达瓶颈","3.1.2 激活函数作为函数逼近的基石","通用逼近定理的数学表述","局部基函数与函数重构","深度与宽度的数学权衡","3.1.3 概率视角下的激活函数","Sigmoid函数与伯努利分布","Softmax函数与多项式分布","Gumbel-Softmax与离散分布采样","3.1.4 信息论视角与激活函数","激活函数的信息压缩作用","激活函数与互信息最大化","激活函数的信息几何视角","3.1.5 激活函数与Transformer架构的数学协同","前馈网络中的激活函数选择","注意力机制与激活函数的协同","激活函数与位置编码的配合","3.1.6 本节小结"],"tags":[],"path":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html"},"86":{"title":"5.6 注意力机制的变体","aliases":[],"headers":["5.6.1 稀疏注意力的数学框架","5.6.2 稀疏模式的学习与优化","5.6.3 稀疏注意力的复杂度分析","5.6.4 线性注意力的数学定义","5.6.5 特征映射函数的设计","5.6.6 线性注意力的表达能力分析","5.6.7 递归形式的线性注意力","5.6.8 Flash注意力的数学原理","5.6.9 Flash注意力的复杂度与性能分析","5.6.10 Flash注意力-2与Flash注意力-3","5.6.11 多查询注意力的数学定义","5.6.12 分组查询注意力的平衡设计","5.6.15 门控与注意力的交互分析"],"tags":[],"path":"第5章-注意力机制数学/5.6-注意力机制的变体.html"},"87":{"title":"5.5 注意力矩阵的谱性质与低秩结构","aliases":[],"headers":["5.5.1 注意力矩阵的数学定义","5.5.2 注意力矩阵与Gram矩阵的关系","5.5.3 注意力矩阵的范数性质","5.5.4 注意力矩阵的特征值分析","5.5.5 谱半径与收敛性","5.5.6 特征向量的数学结构","5.5.7 条件数与数值稳定性","5.5.8 低秩结构的定义与直观解释","5.5.9 Softmax变换对秩的影响","5.5.10 实证分析：有效秩与衰减奇异值","5.5.11 低秩分解与计算优化","5.5.12 低秩结构与信息瓶颈","5.5.13 与卷积矩阵的比较","5.5.14 与Gram矩阵的比较","5.5.15 与核矩阵的比较","5.5.16 表达能力的理论界限","5.5.7 泛化能力的理论分析","5.5.18 计算效率的理论分析","5.5.19 奇异值分布的实证分析","5.5.20 注意力矩阵的病态性问题","5.5.21 线性注意力的谱分析","5.5.22 稀疏注意力的谱性质","5.5.23 注意力矩阵的低秩近似与模型压缩","5.5.24 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.5-注意力矩阵的谱性质与低秩结构.html"},"88":{"title":"6.4 Rotary Position Embedding（RoPE）的复数、群论完整推导","aliases":[],"headers":["6.4.1 位置编码的本质问题","6.4.2 RoPE的核心思想：从幅度到相位的范式转换","6.4.3 RoPE的数学形式化","6.4.4 群论视角：RoPE的深层结构","6.4.5 RoPE与正弦位置编码的对比分析","6.4.6 RoPE的工程实现","6.4.7 RoPE的变体与扩展","6.4.8 RoPE在大语言模型中的应用","6.4.9 位置编码的更广阔图景","6.4.10 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.4-rotary-position-embedding（rope）的复数、群论完整推导.html"},"89":{"title":"6.3 可学习位置编码的矩阵性质","aliases":[],"headers":["6.3.1 可学习位置编码的定义与参数化","6.3.2 表达能力分析","6.3.3 矩阵秩与奇异值分析","6.3.4 初始化策略与训练动态","6.3.5 与正弦编码的比较分析","6.3.6 参数效率与实践考量","6.3.7 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.3-可学习位置编码的矩阵性质.html"},"90":{"title":"6.2 频率空间的数学分析","aliases":[],"headers":["6.2.1傅里叶分析基础","6.2.2 正弦余弦编码的频谱结构","6.2.3 频率选择与编码分辨率","6.2.4 相对位置的频率表示","6.2.5 复数形式的统一分析","6.2.6 频率空间与位置空间的对偶性","6.2.7 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.2-频率空间的数学分析.html"},"91":{"title":"6.1 正弦余弦位置编码的数学定义与推导","aliases":[],"headers":["6.1.1 正弦余弦位置编码的数学定义","6.1.2 编码的唯一性与可辨识性","6.1.3 与注意力分数的交互分析","6.1.4 数值示例与可视化","6.1.5 正弦余弦编码与其他编码的比较","6.1.6 本节小结"],"tags":[],"path":"第6章-位置编码数学/6.1-正弦余弦位置编码的数学定义与推导.html"},"92":{"title":"5.4 注意力如何建模长程依赖","aliases":[],"headers":["5.4.1 循环神经网络的梯度流问题","5.4.2 路径长度与依赖建模","5.4.3 依赖跨度的数学度量","5.4.4 全连接结构与信息直达","5.4.5 注意力权重的归一化与信息聚合","5.4.6 相对位置与内容无关的交互","5.4.7 信息传递路径的数学分析","5.4.8 长序列中的效率优势","5.4.9 内存带宽与计算效率","5.4.10 注意力权重的分布特性","5.4.11 局部注意力与全局注意力","5.4.12 注意力头的专业化分工","5.4.13 层级化的依赖建模","5.4.14 依赖传递的数学模型","5.4.15 残差连接与梯度流动","5.4.16 层级特征与依赖跨度","5.4.17 位置编码的必要性","5.4.18 正弦位置编码的数学性质","5.4.19 旋转位置编码与相对位置建模","5.4.20 远程依赖的边界效应","5.4.21 注意力机制的函数空间","5.4.22 表达能力与计算能力界限","5.4.23 依赖建模的计算复杂度下界","5.4.24 注意力机制的归纳偏置","5.4.25 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.4-注意力如何建模长程依赖.html"},"93":{"title":"5.3 多头注意力的矩阵推导与表达能力分析","aliases":[],"headers":["5.3.1 从单头到多头的动机","5.3.2 多头注意力的数学定义","5.3.3 参数数量与维度配置","5.3.4 投影矩阵的并行计算","5.3.5 多头注意力的张量计算形式","5.3.6 注意力头的重参数化视角","5.3.7 多头机制与子空间学习","5.3.8 与混合专家模型的类比","5.3.9 注意力头的分工与专业化","5.3.10 头数与头维度的权衡","5.3.11 多头注意力的复杂度","5.3.12 并行计算与硬件加速","5.3.13 注意力头的稀疏激活","5.3.14 注意力头的分组与跨头交互","5.3.15 多查询注意力与分组查询注意力","5.3.16 多头注意力的函数逼近视角","5.3.17 头间的信息流动分析","5.3.18 多头注意力与卷积运算的关系","5.3.19 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.3-多头注意力的矩阵推导与表达能力分析.html"},"94":{"title":"5.2 Query、Key、Value的矩阵表示与变换","aliases":[],"headers":["5.2.1 输入嵌入矩阵的数学表示","5.2.2 线性投影的矩阵形式","5.2.3 维度设计与参数数量","5.2.4 投影矩阵的行列式分析","5.2.5 投影矩阵的奇异值分解","5.2.6 正交投影与投影矩阵的初始化","5.2.7 查询空间、键空间与值空间的语义分离","5.2.8 投影的几何视角：空间变换与特征提取","5.2.9 注意力分数的矩阵分解表示","5.2.10 反向传播中的梯度计算","5.2.11 缩放对梯度稳定性的影响","5.2.12 多层堆叠中的表示变换"],"tags":[],"path":"第5章-注意力机制数学/5.2-query、key、value的矩阵表示与变换.html"},"95":{"title":"5.1 Scaled Dot-Product Attention 的数学公式","aliases":[],"headers":["5.1.1 注意力机制的核心思想","5.1.2 缩放点积注意力的数学定义","5.1.3 注意力分数矩阵的几何意义","5.1.4 缩放因子的统计学原理","5.1.5 Softmax函数的饱和行为分析","5.1.6 缩放因子的数学推导与作用","5.1.7 注意力权重的概率解释","5.1.8 Softmax的梯度流分析","5.1.9 注意力权重的温度参数推广","5.1.10 注意力计算的矩阵运算流程","5.1.11 计算复杂度分析","5.1.12 因果掩码的数学形式","5.1.13 填充掩码的实现","5.1.14 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.1-scaled-dot-product-attention-的数学公式.html"},"96":{"title":"1.2 概率论与统计","aliases":[],"headers":["1.2.1 随机变量与概率分布","1.2.2 期望、方差与协方差","1.2.3 高斯分布与多元高斯","1.2.4 KL散度与交叉熵","1.2.5 统计推断与参数估计","1.2.6 大数定律与中心极限定理","1.2.7 假设检验与模型评估","1.2.8 Logits与概率分布的生成"],"tags":[],"path":"第1章-数学基础/1.2-概率论与统计.html"},"97":{"title":"4.6 损失函数的优化性质","aliases":[],"headers":["4.6.1 优化景观的几何分析","凸性与全局最优性","Hessian矩阵与曲率分析","鞍点与局部极小值的景观分析","4.6.2 梯度下降动力学与收敛分析","梯度流与离散动力系统","学习率调度与收敛性","自适应优化算法的收敛性质","梯度裁剪与爆炸抑制","混合精度训练与损失缩放","4.6.5 损失函数优化性质的比较分析","不同损失函数的优化难度比较","学习率敏感性分析","收敛速度与最终性能","4.6.6 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.6-损失函数的优化性质.html"},"101":{"title":"4.5 大语言模型中的损失函数","aliases":[],"headers":["4.5.1 语言建模损失：下一个Token预测的数学框架","自回归语言建模的形式化定义","位置无关性与注意力机制的数学关系","前缀语言建模与填充Token的数学处理","4.5.2 掩码语言建模：完形填空任务的数学分析","掩码语言建模的形式化定义","掩码策略的数学分析","分词器对损失函数的影响","4.5.3 多任务学习中的损失函数组合","多任务学习的数学框架","梯度归一化与帕累托优化","大语言模型中的典型多任务设置","4.5.4 人类反馈强化学习损失（RLHF）","RLHF的数学框架","KL散度约束的数学分析","PPO在RLHF中的应用","4.5.5 直接偏好优化（DPO）","DPO的数学推导","DPO的梯度分析","隐式奖励与对齐机制","4.5.6 损失函数前沿与未来方向","损失函数的理论极限","自适应损失函数","超越Softmax的归一化","损失函数与模型架构的协同设计","4.5.7 本节小结"],"tags":[],"path":"第4章-损失函数数学/4.5-大语言模型中的损失函数.html"},"131":{"title":"大模型中的数学","aliases":[],"headers":["Wir müssen wissen, wir werden wissen --David Hilbert","我们必须知道，我们终将知道 --大卫·希尔伯特","第一章 线性代数基础","第二章 前馈网络数学","第三章 激活函数与非线性数学","第四章 损失函数数学","第五章 注意力机制数学","第六章 位置编码数学","第七章 条件计算与稀疏模型：混合专家方法","第八章 强化学习","第九章 梯度流与优化数学","第十章 正则化与归一化数学","第十一章 矩阵与张量分解","第十二章 概率视角下的大模型","第十三章 动态系统与训练稳定性","第十四章 信息论视角"],"tags":[],"path":"index.html"},"132":{"title":"14.3 注意力机制与信息流量","aliases":[],"headers":["14.3.1 注意力机制的信息论基础","14.3.2 自注意力机制的信息流分析","14.3.3 Softmax 注意力的信息论特性","14.3.4 注意力机制中的互信息分析","14.3.5 Transformer 中的信息流动","14.3.6 长程依赖的信息论分析","14.3.7 注意力机制的优化与正则化","14.3.8 注意力机制的扩展与前沿","14.3.9 本节小结"],"tags":[],"path":"第14章-信息论视角/14.3-注意力机制与信息流量.html"},"133":{"title":"14.2 正则化与信息瓶颈","aliases":[],"headers":["14.2.1 信息瓶颈理论基础","14.2.2 信息论正则化框架","14.2.3 信息论泛化界","14.2.4 条件互信息与多任务学习","14.2.5 信息瓶颈的扩展理论","14.2.6 实践中的信息瓶颈优化","14.2.7 信息瓶颈的局限性与发展方向","14.2.8 本节小结"],"tags":[],"path":"第14章-信息论视角/14.2-正则化与信息瓶颈.html"},"134":{"title":"13.1 离散时间动态系统的数学基础","aliases":[],"headers":["13.1.1 离散时间动态系统的数学基础","13.1.2 稳定性理论基础","13.1.3 梯度流动的连续时间极限","13.1.4 学习率与系统动力学","13.1.5 随机梯度下降的随机动力学","13.1.6 动量方法的动力学分析","13.1.7 自适应方法的动态特性","13.1.8 训练曲线的动力学解释","13.1.9 本节小结"],"tags":[],"path":"第13章-动态系统与训练稳定性/13.1-离散时间动态系统的数学基础.html"},"135":{"title":"13.2 Jacobian、Hessian 的稳定性分析","aliases":[],"headers":["13.2.1 Jacobian 矩阵的理论框架","Jacobian 矩阵的定义与结构","反向传播的 Jacobian 结构","Jacobian 的谱性质","13.2.2 Hessian 矩阵的理论框架","Hessian 矩阵的定义与性质","正定性与曲率","广义特征值问题","13.2.3 线性化稳定性分析","局部线性化定理","梯度下降的 Jacobian 分析","动量方法的 Jacobian 分析","自适应方法的 Hessian 近似","13.2.4 梯度消失与梯度爆炸的数学分析","Jacobian 范数与梯度流","残差连接的 Jacobian 分析","初始化与 Jacobian 谱","13.2.5 Hessian 谱分析与优化困难","病态 Hessian 的特征","鞍点的逃逸动力学","平坦最小值的 Hessian 特征","13.2.6 大模型中的特殊现象","特征值分布的尺度律","损失 landscape 的低秩结构","训练动态的尺度不变性","13.2.7 稳定性分析的数值方法","特征值估计","谱范数的幂迭代","13.2.8 本节小结"],"tags":[],"path":"第13章-动态系统与训练稳定性/13.2-jacobian、hessian-的稳定性分析.html"},"136":{"title":"13.3 收敛性、震荡、周期行为","aliases":[],"headers":["13.3.1 收敛性理论基础","13.3.2 训练震荡的数学分析","13.3.3 极限环与周期行为","13.3.5 混沌理论的视角","13.3.6 收敛性、震荡与泛化的关联","13.3.7 实际应用中的行为控制","13.3.8 本节小结"],"tags":[],"path":"第13章-动态系统与训练稳定性/13.3-收敛性、震荡、周期行为.html"},"137":{"title":"14.1 信息熵与互信息","aliases":[],"headers":["14.1.1 信息熵的数学基础","14.1.2 KL 散度的数学理论","14.1.3 互信息的深层理论","14.1.4 信息论与优化的关联","14.1.5 信息论度量的计算方法","14.1.6 大模型中的信息论前沿","14.1.7 本节小结"],"tags":[],"path":"第14章-信息论视角/14.1-信息熵与互信息.html"},"138":{"title":"12.3 标度律","aliases":[],"headers":["12.3.1 幂律关系的数学形式","12.3.2 计算最优与数据最优","12.3.3 涌现能力的概率解释"],"tags":[],"path":"第12章-概率视角下的大模型/12.3-标度律.html"},"139":{"title":"12.2 最大似然与交叉熵的详细推导","aliases":[],"headers":["12.2.1 最大似然估计的数学原理","似然函数的数学定义","最大似然估计的存在性条件","大模型中的似然函数形式","12.2.2 交叉熵的数学推导","信息论基础","交叉熵的推导","最大似然与交叉熵的等价性","12.2.3 梯度计算与优化","似然梯度的数学表达","Fisher信息矩阵","12.2.4 在深度学习中的实现","负对数似然损失函数","数值稳定性考虑","12.2.5 理论性质分析","偏差-方差权衡","一致性和渐近正态性","12.2.6 与信息论的深层联系","互信息的视角","最小描述长度原理","12.2.7 实际应用中的考虑","批量大小对梯度估计的影响","学习率调度的理论指导","12.2.8 总结"],"tags":[],"path":"第12章-概率视角下的大模型/12.2-最大似然与交叉熵的详细推导.html"},"140":{"title":"12.1 自回归公式（链式法则）","aliases":[],"headers":["12.1.1 语言建模的数学定义","词表与序列空间的建立","联合概率分布的建模目标","序列生成的条件概率框架","12.1.2 概率链式法则与自回归分解","链式法则的数学基础","自回归模型的数学表述","计算复杂度的维度分析","温度参数与概率分布控制","12.1.3 参数化与似然函数","神经网络的概率参数化","似然函数的构建","对数似然与损失函数","最大似然估计的理论性质","12.1.4 案例分析：从二元语法到GPT","N-gram模型的马尔可夫假设","Transformer的自注意力突破","训练与推理的一致性","12.1.5 本节小结"],"tags":[],"path":"第12章-概率视角下的大模型/12.1-自回归公式（链式法则）.html"},"141":{"title":"11.3 注意力矩阵的低秩结构","aliases":[],"headers":["11.3.1 注意力矩阵的谱分析与奇异值分布","标准自注意力的数学定义","理论上的秩上界","几何直观解释","11.3.3 理论界限：Johnson-Lindenstrauss引理的应用","JL引理的数学陈述","Linformer的数学框架"],"tags":[],"path":"第11章-矩阵与张量分解/11.3-注意力矩阵的低秩结构.html"},"142":{"title":"11.2 大模型中低秩近似的数学依据","aliases":[],"headers":["11.2.1 深度网络权重的低秩特性","经验观察：权重矩阵的谱分布","损失曲面的几何与低秩解","11.2.4 大语言模型中的低秩适应（LoRA）","LoRA的数学框架","LoRA的优化视角","LoRA的变体与扩展","11.2.5 近似误差与泛化保证","低秩近似的误差界","泛化保证的理论分析"],"tags":[],"path":"第11章-矩阵与张量分解/11.2-大模型中低秩近似的数学依据.html"},"143":{"title":"11.1 矩阵与张量分解：SVD、Tucker、CP分解","aliases":[],"headers":["11.1.1 从主成分分析到张量分解","11.1.2 奇异值分解：矩阵分解的基石","SVD的数学定义","SVD的几何解释","截断SVD与低秩近似","11.1.4 CP分解：秩一张量分解","CP分解的数学形式","Tucker分解与SVD的关系","Tucker分解的计算方法","CP分解与Tucker分解的比较","11.1.6 深度学习中的张量分解应用","卷积神经网络的张量分解","注意力机制的张量视角","张量回归层","11.1.7 本节小结"],"tags":[],"path":"第11章-矩阵与张量分解/11.1-矩阵与张量分解：svd、tucker、cp分解.html"},"144":{"title":"10.3 正则化对梯度与损失的影响","aliases":[],"headers":["10.3.1 L2正则化与权重衰减的梯度分析","L2正则化的梯度表达式","10.3.3 正则化与损失曲面的几何变化","等高线的变形","极小值位置的移动","曲率变化与优化轨迹","10.3.4 正则化的隐式梯度效应","Dropout的隐式正则化","BatchNorm的平滑效应","标签平滑的正则化效应","10.3.5 谱正则化与Hessian特征值","权重矩阵的谱性质","谱范数与 Lipschitz 常数","Frobenius范数正则化","核范数与低秩正则化","10.3.6 正则化与优化算法的相互作用","学习率调度中的正则化效应","早停作为正则化","梯度裁剪与正则化的协同","10.3.7 本节小结"],"tags":[],"path":"第10章-正则化与归一化数学/10.3-正则化对梯度与损失的影响.html"},"145":{"title":"10.2 Dropout的期望保持性质","aliases":[],"headers":["10.2.1 Dropout 的数学建模","随机掩码的引入","有放缩与无放缩的实现差异","10.2.2 期望保持性质的数学证明","期望保持性质的严格表述","线性层后的期望保持","均值场近似的期望分析","10.2.3 Dropout 的方差分析","输出方差的数学推导","向量输入的协方差矩阵","方差与保留概率的关系","10.2.4 Dropout 与 L2 正则化的等价性","理论推导","直观解释","10.2.5 变分Dropout与高斯Dropout","变分Dropout的数学框架","高斯Dropout的数学性质","10.2.6 Dropout 在不同网络结构中的应用","全连接网络中的Dropout","卷积网络中的Dropout","循环神经网络中的Dropout","10.2.7 期望保持性质的实践意义","训练与推理的一致性","Dropout概率的选择","Dropout与其他正则化技术的组合","10.2.8 本节小结"],"tags":[],"path":"第10章-正则化与归一化数学/10.2-dropout的期望保持性质.html"},"146":{"title":"10.1 常见归一化方法的数学表述","aliases":[],"headers":["10.1.1 归一化的统一范式","10.1.2 批归一化（Batch Normalization）","训练阶段的数学推导","推理阶段的数学处理","批归一化的局限性","10.1.3 层归一化（Layer Normalization）","数学公式推导","层归一化在Transformer中的应用","层归一化的数学特性","10.1.4 均方根归一化（RMSNorm）","数学公式推导","计算复杂度分析","RMSNorm的理论基础","10.1.5 三种归一化方法的维度对比与总结","张量维度处理对比","公式对比汇总","适用场景分析","10.1.6 本节小结"],"tags":[],"path":"第10章-正则化与归一化数学/10.1-常见归一化方法的数学表述.html"},"147":{"title":"9.3 梯度裁剪与正则化","aliases":[],"headers":["9.3.1 梯度裁剪的数学框架","梯度爆炸的数学表征","范数裁剪的数学形式","值裁剪与逐元素裁剪","全局裁剪与分层裁剪","9.3.2 梯度裁剪的理论分析","裁剪对收敛性的影响","裁剪与平滑 Lipschitz 条件","裁剪动力学的相图分析","9.3.3 正则化的数学基础","显式正则化与优化景观","隐式正则化与优化诱导的偏差","正则化与裁剪的协同效应","9.3.4 梯度裁剪的实践考量","阈值选择策略","裁剪与其他技术的组合","裁剪对泛化的影响","9.3.5 本节小结"],"tags":[],"path":"第9章-梯度流与优化数学/9.3-梯度裁剪与正则化.html"},"148":{"title":"9.2 梯度消失与梯度爆炸的数学条件","aliases":[],"headers":["9.2.1 问题引入与现象定义","9.2.2 反向传播的链式法则与雅可比矩阵","9.2.3 标量情形的直观分析","9.2.4 矩阵范数与谱半径：一般情形","谱半径与收敛性","奇异值分解视角","9.2.5 激活函数的几何性质","Sigmoid函数：梯度消失的典型案例","Tanh函数：改进但未解决","ReLU函数：分段线性的突破","9.2.6 深度累积效应与梯度流","9.2.7 权重初始化的数学启示","Xavier初始化","He初始化","初始化策略总结","9.2.8 归一化技术与梯度流","9.2.9 本节小结"],"tags":[],"path":"第9章-梯度流与优化数学/9.2-梯度消失与梯度爆炸的数学条件.html"},"149":{"title":"9.1 梯度协方差矩阵","aliases":[],"headers":["9.1.1 SGD中的噪声模型","9.1.2 数学定义与基本性质","9.1.3 频谱特征与各向异性","9.1.4 与Hessian矩阵的联系","9.1.5 对优化动力学的影响","逃离鞍点的动力学机制","平坦极小值的隐式偏好","与朗之万动力学的联系","9.1.6 本节小结与延伸讨论"],"tags":[],"path":"第9章-梯度流与优化数学/9.1-梯度协方差矩阵.html"},"150":{"title":"5.4 残差连接与归一化","aliases":[],"headers":["5.4.1 残差连接的数学原理","5.4.2 Highway Connection（HC）","5.4.3 Multi-Scale Highway Connection（MHC）","5.4.4 Layer Normalization"],"tags":[],"path":"第5章-注意力机制数学/5.4-残差连接与归一化.html"},"151":{"title":"5.7 注意力机制的变体","aliases":[],"headers":["5.7.1 稀疏注意力的数学框架","5.7.2 稀疏模式的学习与优化","5.7.3 稀疏注意力的复杂度分析","5.7.4 线性注意力的数学定义","5.7.5 特征映射函数的设计","5.7.6 线性注意力的表达能力分析","5.7.7 递归形式的线性注意力","5.7.8 Flash注意力的数学原理","5.7.9 Flash注意力的复杂度与性能分析","5.7.10 Flash注意力-2与Flash注意力-3","5.7.11 多查询注意力的数学定义","5.7.12 分组查询注意力的平衡设计","5.7.15 门控与注意力的交互分析"],"tags":[],"path":"第5章-注意力机制数学/5.7-注意力机制的变体.html"},"152":{"title":"5.6 注意力矩阵的谱性质与低秩结构","aliases":[],"headers":["5.6.1 注意力矩阵的数学定义","5.6.2 注意力矩阵与Gram矩阵的关系","5.6.3 注意力矩阵的范数性质","5.6.4 注意力矩阵的特征值分析","5.6.5 谱半径与收敛性","5.6.6 特征向量的数学结构","5.6.7 条件数与数值稳定性","5.6.8 低秩结构的定义与直观解释","5.6.9 Softmax变换对秩的影响","5.6.10 实证分析：有效秩与衰减奇异值","5.6.11 低秩分解与计算优化","5.6.12 低秩结构与信息瓶颈","5.6.13 与卷积矩阵的比较","5.6.14 与Gram矩阵的比较","5.6.15 与核矩阵的比较","5.6.16 表达能力的理论界限","5.6.7 泛化能力的理论分析","5.6.18 计算效率的理论分析","5.6.19 奇异值分布的实证分析","5.6.20 注意力矩阵的病态性问题","5.6.21 线性注意力的谱分析","5.6.22 稀疏注意力的谱性质","5.6.23 注意力矩阵的低秩近似与模型压缩","5.6.24 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.6-注意力矩阵的谱性质与低秩结构.html"},"153":{"title":"5.5 注意力如何建模长程依赖","aliases":[],"headers":["5.5.1 循环神经网络的梯度流问题","5.5.2 路径长度与依赖建模","5.5.3 依赖跨度的数学度量","5.5.5 全连接结构与信息直达","5.5.5 注意力权重的归一化与信息聚合","5.5.6 相对位置与内容无关的交互","5.5.7 信息传递路径的数学分析","5.5.8 长序列中的效率优势","5.5.9 内存带宽与计算效率","5.5.10 注意力权重的分布特性","5.5.11 局部注意力与全局注意力","5.5.12 注意力头的专业化分工","5.5.13 层级化的依赖建模","5.5.14 依赖传递的数学模型","5.5.15 残差连接与梯度流动","5.5.16 层级特征与依赖跨度","5.5.17 位置编码的必要性","5.5.18 正弦位置编码的数学性质","5.5.19 旋转位置编码与相对位置建模","5.5.20 远程依赖的边界效应","5.5.21 注意力机制的函数空间","5.5.22 表达能力与计算能力界限","5.5.23 依赖建模的计算复杂度下界","5.5.24 注意力机制的归纳偏置","5.5.25 本节小结"],"tags":[],"path":"第5章-注意力机制数学/5.5-注意力如何建模长程依赖.html"},"154":{"title":"8.3 PPO算法与大模型训练","aliases":[],"headers":["8.3.1 PPO核心思想：限制策略更新幅度","8.3.2 重要性采样与比率 <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-msub><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45F TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: -0.15em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D461 TEX-I\"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D703 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math></mjx-container></span>","8.3.3 PPO-Clip目标函数","8.3.4 KL散度约束与自适应惩罚","8.3.5 RLHF中的奖励模型与策略优化"],"tags":[],"path":"第8章-强化学习/8.3-ppo算法与大模型训练.html"},"155":{"title":"8.2 策略梯度与Actor-Critic方法","aliases":[],"headers":["8.2.1 策略梯度定理","8.2.2 REINFORCE算法","8.2.3 Baseline与方差缩减","8.2.4 Actor-Critic框架：V(s)作为Critic","8.2.5 优势函数 <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D434 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D70B TEX-I\"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D460 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2C\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"2\"><mjx-c class=\"mjx-c1D44E TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 的深入理解"],"tags":[],"path":"第8章-强化学习/8.2-策略梯度与actor-critic方法.html"},"156":{"title":"8.1 强化学习基础与马尔可夫决策过程","aliases":[],"headers":["8.1.1 强化学习基本框架","8.1.2 马尔可夫决策过程（MDP）的数学定义","8.1.3 状态价值函数 <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D449 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em; margin-left: 0.059em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D70B TEX-I\"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D460 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math></mjx-container></span> 与动作价值函数 <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-msup><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D444 TEX-I\"></mjx-c></mjx-mi><mjx-script style=\"vertical-align: 0.363em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D70B TEX-I\"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D460 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2C\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"2\"><mjx-c class=\"mjx-c1D44E TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math></mjx-container></span>","8.1.4 贝尔曼方程与最优策略","8.1.5 折扣因子 <span class=\"math math-inline is-loaded\"><mjx-container class=\"MathJax\" jax=\"CHTML\"><mjx-math class=\"MJX-TEX\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D6FE TEX-I\"></mjx-c></mjx-mi></mjx-math></mjx-container></span> 的数学意义"],"tags":[],"path":"第8章-强化学习/8.1-强化学习基础与马尔可夫决策过程.html"},"157":{"title":"7.3 训练与推理优化","aliases":[],"headers":["7.3.1 MoE端到端训练梯度计算","7.3.2 专家并行策略","7.3.3 显存优化与通信开销"],"tags":[],"path":"第7章-条件计算与稀疏模型：混合专家方法/7.3-训练与推理优化.html"},"158":{"title":"7.2 专家网络与负载均衡","aliases":[],"headers":["7.2.1 专家网络的数学表示","7.2.2 加权组合输出的数学表达","7.2.3 路由概率与专家分配","7.2.4 负载均衡辅助损失","7.2.5 防止专家坍缩的数学原理","7.2.6 Importance Weight机制"],"tags":[],"path":"第7章-条件计算与稀疏模型：混合专家方法/7.2-专家网络与负载均衡.html"},"159":{"title":"7.1 MoE概述与门控机制","aliases":[],"headers":["7.1.1 MoE核心思想：条件计算","7.1.2 稠密模型与MoE的对比分析","7.1.3 缩放定律的数学解释","7.1.4 稀疏激活的效率优势","7.1.5 Softmax门控函数","7.1.6 Top-K稀疏路由","7.1.7 噪声加入与均匀负载"],"tags":[],"path":"第7章-条件计算与稀疏模型：混合专家方法/7.1-moe概述与门控机制.html"}},"dirtCount":33,"index":[["辅助损失函数可以定义为分配比例与均匀分布的偏离程度",{"5":{"159":2}}],["牺牲参数存储和通信效率",{"5":{"159":2}}],["稠密模型",{"5":{"159":1}}],["稠密模型与moe模型的核心差异可以总结为下表",{"5":{"159":2}}],["稠密模型与moe的对比分析",{"2":{"159":1},"5":{"159":1}}],["稠密模型和moe模型遵循截然不同的规模缩放规律",{"5":{"159":2}}],["稠密模型和moe模型对参数的使用方式存在本质区别",{"5":{"159":2}}],["迅速达到硬件能力的瓶颈",{"5":{"159":2}}],["失去了moe架构的优势",{"5":{"159":2}}],["失去了moe架构通过分工提升表达能力的初衷",{"5":{"158":2}}],["失去动量加速效果",{"5":{"136":2}}],["才会产生非零的更新梯度",{"5":{"158":2}}],["才能保证收敛",{"5":{"134":2}}],["才能进行这种分解",{"5":{"50":2}}],["虚拟专家",{"5":{"158":2}}],["虚部包含正弦项",{"5":{"90":2}}],["向目标值靠拢",{"5":{"158":2}}],["向",{"5":{"158":2}}],["向量输入的协方差矩阵",{"2":{"145":1},"5":{"145":1}}],["向量",{"5":{"50":2,"88":1,"143":3,"148":2}}],["向量链式法则",{"5":{"44":2}}],["向量是线性空间中最基本的元素",{"5":{"50":2}}],["向量距离和相似度是度量特征空间中样本关系的基本工具",{"5":{"50":2}}],["向量化",{"5":{"50":2}}],["向量化操作与kronecker积结合可以简化矩阵方程的表示",{"5":{"50":2}}],["向量乘积",{"5":{"50":2}}],["向量加法是逐分量",{"5":{"50":2}}],["向量加法满足交换律和结合律",{"5":{"50":1}}],["向量空间中的mse",{"2":{"51":1},"5":{"51":1}}],["向量的乘积需要",{"5":{"50":1}}],["向量的模长对所有位置都保持不变",{"5":{"88":1}}],["向量的模长",{"5":{"88":1}}],["浪费了大部分模型容量",{"5":{"158":2}}],["浪费了模型的容量潜力",{"5":{"158":2}}],["浪费计算资源",{"5":{"42":2}}],["积",{"5":{"158":2}}],["积分解",{"5":{"135":2}}],["破坏流水线并行的效率",{"5":{"157":2}}],["检查点策略可以应用于专家网络",{"5":{"157":2}}],["检查点",{"5":{"157":2}}],["继续上例",{"5":{"157":4}}],["百万参数",{"5":{"157":2}}],["系数2表示发送和接收各一次",{"5":{"157":2}}],["系统趋于稳定在局部极小值",{"5":{"144":2}}],["系统趋于收敛",{"5":{"136":2}}],["系统有更多能量跨越能量壁垒",{"5":{"144":2}}],["系统有两个不动点",{"5":{"136":2}}],["系统地建立了自回归语言模型的数学框架",{"5":{"140":2}}],["系统地推导自回归模型的数学表达",{"5":{"140":2}}],["系统地分析正弦余弦位置编码的频谱结构",{"5":{"90":2}}],["系统的响应函数",{"5":{"138":2}}],["系统的定性行为会发生突变",{"5":{"136":2}}],["系统对初始条件敏感",{"5":{"136":2}}],["系统是收缩的",{"5":{"136":2}}],["系统不能分解为两个互不相交的不变子系统的并",{"5":{"136":2}}],["系统无平衡点",{"5":{"136":2}}],["系统处于临界状态",{"5":{"136":2}}],["系统处于临界阻尼状态",{"5":{"136":2}}],["系统处于过阻尼状态",{"5":{"136":2}}],["系统处于欠阻尼状态",{"5":{"136":2}}],["系统呈现振荡发散行为",{"5":{"136":2}}],["系统呈现振荡衰减行为",{"5":{"136":2}}],["系统更新方程为",{"5":{"136":2}}],["系统发生周期倍增分岔",{"5":{"136":2}}],["系统发散",{"5":{"134":2,"135":2}}],["系统进入发散状态",{"5":{"136":2}}],["系统将在不动点附近震荡而非收敛或发散",{"5":{"136":2}}],["系统根据其收敛行为可以分为以下几类",{"5":{"136":2}}],["系统建立收敛性的理论基础",{"5":{"136":2}}],["系统可从鞍点逃逸",{"5":{"135":2}}],["系统渐近稳定的充要条件是",{"5":{"135":2}}],["系统行为",{"5":{"134":1}}],["系统矩阵",{"5":{"134":2}}],["系统收敛到新的周期2轨道",{"5":{"136":2}}],["系统收敛到不动点",{"5":{"136":2}}],["系统收敛缓慢但稳定",{"5":{"134":2}}],["系统收敛",{"5":{"134":2}}],["系统在最小值附近进行精细搜索",{"5":{"136":2}}],["系统在原点有半稳定不动点",{"5":{"136":2}}],["系统在不动点附近",{"5":{"136":2}}],["系统在临界点稳定的条件为",{"5":{"135":2}}],["系统在小扰动下收敛",{"5":{"135":2}}],["系统在小邻域内的局部拓扑结构由",{"5":{"134":2}}],["系统在该点附近呈收敛行为",{"5":{"134":2}}],["系统保持初始状态不变",{"5":{"134":2}}],["系统所有可能状态的集合",{"5":{"134":2}}],["系统阐述了信息熵",{"5":{"137":2}}],["系统阐述反向传播算法的完整理论框架",{"5":{"44":2}}],["系统阐述神经元从生物原型到数学抽象的演化过程",{"5":{"47":2}}],["系统性质发生突变",{"5":{"138":2}}],["系统性地构建这一统一框架",{"5":{"69":2}}],["系统性地推导交叉熵的数学定义",{"5":{"61":2}}],["系统性地推导了交叉熵损失函数的数学定义",{"5":{"61":2}}],["系统性地剖析rope的设计哲学",{"5":{"88":2}}],["系统性地探讨损失函数的优化性质",{"5":{"97":2}}],["系统性",{"5":{"51":2}}],["系统分析注意力矩阵的谱性质",{"5":{"87":2,"152":2}}],["系统探讨可学习位置编码的数学原理和性质",{"5":{"89":2}}],["系统推导mse的统计学基础",{"5":{"51":2}}],["系统推导scaled",{"5":{"95":2}}],["华丽但不一定连贯的句子",{"5":{"156":2}}],["权衡即时与未来奖励",{"5":{"156":2}}],["权重共享与参数效率",{"5":{"150":1}}],["权重共享与参数效率是mhc设计中的一个重要考量",{"5":{"150":1}}],["权重独立于输入",{"5":{"148":2}}],["权重收缩",{"5":{"144":1}}],["权重初始化策略",{"5":{"148":2}}],["权重初始化策略的设计直接源于对梯度稳定性的数学分析",{"5":{"148":2}}],["权重初始化和激活函数对训练稳定性的影响机制",{"5":{"135":2}}],["权重初始化的核心目标就是使",{"5":{"148":1}}],["权重初始化的核心目标就是使尽可能接近1",{"5":{"148":1}}],["权重初始化的数学启示",{"2":{"148":1},"5":{"148":1}}],["权重初始化的一个核心目标是确保各层的前向激活和反向梯度都具有稳定的方差",{"5":{"41":2}}],["权重初始化的尺度选择",{"5":{"96":2}}],["权重衰减",{"5":{"147":4}}],["权重衰减项",{"5":{"135":2}}],["权重衰减的归一化效应",{"5":{"135":2}}],["权重衰减和标签平滑的作用机制",{"5":{"97":2}}],["权重缩放下的",{"5":{"135":2}}],["权重加权聚合",{"5":{"132":2}}],["权重决定了每个输入位置的信息贡献比例",{"5":{"132":1}}],["权重较大",{"5":{"101":2}}],["权重较小",{"5":{"101":2}}],["权重较低的任务可能被",{"5":{"101":2}}],["权重较高的任务主导学习过程",{"5":{"101":2}}],["权重的选择是一个开放问题",{"5":{"70":2}}],["权重的乘积",{"5":{"71":1}}],["权重梯度的范数受到以下三个因素的共同影响",{"5":{"148":2}}],["权重梯度与误差向量的关系为",{"5":{"148":2}}],["权重梯度",{"5":{"44":2}}],["权重梯度为",{"5":{"44":2}}],["权重节点",{"5":{"45":2}}],["权重",{"5":{"45":2,"46":6,"132":1,"158":2,"159":4}}],["权重矩阵性质与网络深度如何共同决定梯度的命运",{"5":{"148":2}}],["权重矩阵",{"5":{"46":1,"47":1,"159":2}}],["权重矩阵决定了不同特征之间的线性组合方式",{"5":{"47":1}}],["权重矩阵无处不在",{"5":{"50":2}}],["权重矩阵的谱半径",{"5":{"148":2}}],["权重矩阵的谱半径也会发生变化",{"5":{"148":2}}],["权重矩阵的谱半径为",{"5":{"148":2}}],["权重矩阵的谱性质",{"2":{"144":1},"5":{"144":1}}],["权重矩阵的谱分布",{"2":{"142":1},"5":{"142":1}}],["权重矩阵的元素含义",{"5":{"46":2}}],["权重矩阵的元素​表示第个输入特征与第个输出神经元之间的连接强度",{"5":{"46":1}}],["权重矩阵的列空间决定了该层能够表示的特征空间的范围",{"5":{"50":2}}],["权重矩阵的低秩近似可以显著减少参数量和计算量",{"5":{"50":2}}],["权重矩阵与",{"5":{"50":1}}],["权重和阈值需要人工设定",{"5":{"47":2}}],["权重分布更均匀",{"5":{"87":2,"152":2}}],["权重序列​与正弦序列和余弦序列正交",{"5":{"90":1}}],["权重序列",{"5":{"90":1}}],["权重由注意力分数隐式决定",{"5":{"92":2,"153":2}}],["权重会集中到这些位置上",{"5":{"92":2,"153":2}}],["权重依赖于输入数据",{"5":{"93":2}}],["遵循策略",{"5":{"156":6}}],["智能体变得",{"5":{"156":4}}],["智能体能获得的期望即时奖励",{"5":{"156":2}}],["智能体的目标就是学习一个策略",{"5":{"156":2}}],["智能体观察到当前状态",{"5":{"156":2}}],["智能体在状态",{"5":{"156":4}}],["智能体交互的外部世界",{"5":{"156":2}}],["智能体就是语言模型本身",{"5":{"156":2}}],["智能体",{"5":{"156":2}}],["智能体必须通过试错",{"5":{"156":2}}],["贝尔曼最优方程是非线性的",{"5":{"156":2}}],["贝尔曼最优方程",{"5":{"156":2}}],["贝尔曼期望方程",{"5":{"156":2}}],["贝尔曼方程",{"5":{"156":2}}],["贝尔曼方程与最优策略",{"2":{"156":1},"5":{"156":1}}],["贝叶斯估计和期望传播等统计推断方法",{"5":{"96":2}}],["贝叶斯估计是另一种参数估计方法",{"5":{"96":2}}],["贝叶斯估计使用后验分布进行预测",{"5":{"96":2}}],["贝叶斯方法可用于模型选择",{"5":{"96":2}}],["环境接收到动作",{"5":{"156":2}}],["环境在下一个时间步",{"5":{"156":2}}],["环境的完整描述",{"5":{"156":2}}],["环境可以是用户",{"5":{"156":2}}],["环境",{"5":{"156":2}}],["环境是极其复杂的对话系统或用户交互环境",{"5":{"155":2}}],["环境模型已知",{"5":{"155":2}}],["价值函数近似器",{"5":{"155":1}}],["价值函数通常通过最小化td误差来训练",{"5":{"154":2}}],["价值函数损失项",{"5":{"154":2}}],["奖励函数",{"5":{"156":2}}],["奖励是评估动作好坏的直接标准",{"5":{"156":2}}],["奖励",{"5":{"155":2,"156":2}}],["奖励序列组成",{"5":{"155":2}}],["奖励缩放等",{"5":{"154":2}}],["奖励黑客攻击",{"5":{"154":2}}],["奖励项",{"5":{"154":2}}],["奖励模型",{"5":{"154":1}}],["奖励模型对这些分布外输出的评分可能不可靠",{"5":{"154":2}}],["奖励模型的训练通常采用",{"5":{"154":1}}],["奖励模型的训练通常采用排序损失",{"5":{"154":1}}],["奖励模型的误差可能传播到策略优化中",{"5":{"97":2}}],["奖励模型训练",{"5":{"154":4}}],["奖励模型训练阶段",{"5":{"101":2}}],["奖励模型可能学习到错误的偏好模式",{"5":{"101":2}}],["新旧策略之间的kl散度太大",{"5":{"154":2}}],["新策略选择动作",{"5":{"154":2}}],["新的不动点",{"5":{"136":2}}],["新的不动点变为稳定的",{"5":{"136":1}}],["新的不动点是不稳定的",{"5":{"136":1}}],["协变量漂移问题",{"5":{"150":2}}],["协方差",{"5":{"96":2}}],["协方差定义为",{"5":{"96":2}}],["协方差可以展开为",{"5":{"96":2}}],["协方差矩阵为fisher信息矩阵逆的正态分布",{"5":{"140":2}}],["协方差矩阵是多维随机变量的二阶中心矩描述",{"5":{"96":2}}],["协方差矩阵的特征值表示编码在不同方向上的分散程度",{"5":{"89":2}}],["协方差矩阵的对角线元素是各随机变量的方差",{"5":{"96":2}}],["协方差矩阵在大语言模型中的应用包括",{"5":{"96":2}}],["细节的信息",{"5":{"150":2}}],["细尺度的变换具有较小的感受野",{"5":{"150":2}}],["细胞体对这些输入信号进行整合处理",{"5":{"47":2}}],["粗尺度的变换具有较大的感受野",{"5":{"150":2}}],["意外",{"5":{"150":2}}],["意味着更保守的更新",{"5":{"154":2}}],["意味着所有编码向量线性无关",{"5":{"89":2}}],["意味着位置编码的参数是高度结构化的",{"5":{"90":2}}],["意味着均方误差的曲率在整个参数空间中是不变的",{"5":{"97":2}}],["门的动态特性",{"5":{"150":1}}],["门的动态特性是highway",{"5":{"150":1}}],["门函数",{"5":{"150":2}}],["门函数本身由神经网络参数",{"5":{"150":2}}],["门控的表达能力越强",{"5":{"159":2}}],["门控的信息论作用",{"5":{"132":2}}],["门控权重不仅取决于自身得分的梯度",{"5":{"159":2}}],["门控权重向量为",{"5":{"158":2}}],["门控函数",{"5":{"159":2}}],["门控函数的输出虽然覆盖所有",{"5":{"159":2}}],["门控函数通常在序列维度上独立地为每个位置计算权重",{"5":{"158":2}}],["门控函数为",{"5":{"158":2}}],["门控机制是moe架构的核心决策模块",{"5":{"159":2}}],["门控机制",{"5":{"159":5}}],["门控机制根据输入内容从中选择合适的函数进行计算",{"5":{"158":2}}],["门控机制与注意力计算的交互可以增强模型对不同输入模式的适应性",{"5":{"86":2,"151":2}}],["门控阶段和专家计算阶段",{"5":{"157":2}}],["门控",{"5":{"132":1}}],["门控网络的训练面临一个独特的优化挑战",{"5":{"159":2}}],["门控网络的参数",{"5":{"158":2}}],["门控网络通过第一个公式学习更好的路由决策",{"5":{"158":2}}],["门控网络参数",{"5":{"157":2}}],["门控网络",{"5":{"132":2}}],["门控值",{"5":{"132":2}}],["门控控制信息通过率",{"5":{"132":1}}],["门控注意力机制",{"5":{"132":2}}],["门控注意力引入门控机制控制信息流",{"5":{"132":2}}],["门控注意力",{"5":{"132":2}}],["门控注意力等",{"5":{"86":2,"151":2}}],["门控与注意力的交互分析",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["门控线性注意力",{"5":{"86":2,"151":2}}],["门控交叉注意力",{"5":{"86":2,"151":2}}],["门控softmax",{"5":{"101":2}}],["恒等路径的信息流是固定比例的",{"5":{"150":2}}],["恒等路径",{"5":{"150":2}}],["恒等路径仍然保证了信息的传递",{"5":{"150":2}}],["恒等激活",{"5":{"44":2}}],["巧妙地绕过了这一难题",{"5":{"150":2}}],["巧合",{"5":{"70":2}}],["尤其在transformer等大语言模型中发挥着不可或缺的作用",{"5":{"150":2}}],["尤其是当策略由深度神经网络参数化时",{"5":{"154":2}}],["尤其是当网络层数较多时",{"5":{"150":2}}],["尤其是在损失函数曲率高度各向异性的情况下",{"5":{"70":2}}],["尤其是gpu",{"5":{"86":2,"151":2}}],["尤其是transformer架构的核心创新之一",{"5":{"95":2}}],["扩散张量",{"5":{"149":1}}],["扩展为条件函数族",{"5":{"159":2}}],["扩展为齐次坐标",{"5":{"47":1}}],["扩展",{"5":{"158":2}}],["扩展到",{"5":{"135":2,"136":1}}],["朗之万动力学",{"5":{"149":1}}],["马尔可夫性质",{"5":{"156":1}}],["马尔可夫性质极大地简化了强化学习问题的建模和求解",{"5":{"156":2}}],["马尔可夫决策过程",{"2":{"156":1},"5":{"156":3}}],["马尔可夫近似",{"5":{"149":2}}],["马尔可夫假设在以下方面存在不足",{"5":{"140":2}}],["马尔可夫假设的局限性",{"5":{"140":2}}],["马尔可夫假设要求",{"5":{"140":2}}],["危险",{"5":{"149":2}}],["山脊",{"5":{"149":2}}],["纯梯度下降在鞍点处会停滞",{"5":{"149":2}}],["纯注意力机制是置换不变的",{"5":{"91":2,"92":2,"153":2}}],["逃离鞍点",{"5":{"149":2}}],["逃离鞍点的动力学机制",{"2":{"149":1},"5":{"149":1}}],["抖动",{"5":{"149":2}}],["爆炸",{"5":{"148":1}}],["消失",{"5":{"148":1}}],["消除输入分布的偏移",{"5":{"146":2}}],["消除了分别处理正弦和余弦的复杂性",{"5":{"90":2}}],["⁡",{"5":{"148":1}}],["⁡​",{"5":{"148":1}}],["监督微调",{"5":{"154":4}}],["监督微调阶段",{"5":{"101":4}}],["监控梯度范数的变化",{"5":{"148":2}}],["死亡",{"5":{"148":2}}],["死神经元",{"5":{"41":4,"71":2}}],["死神经元问题",{"5":{"41":1}}],["信号可以无损地传递",{"5":{"148":2}}],["信息容量衡量了网络在给定约束下能够编码的最大信息量",{"5":{"133":2}}],["信息容量",{"5":{"133":2}}],["信息容量或功能特性方面有所改进",{"5":{"132":2}}],["信息占主导",{"5":{"133":2}}],["信息平面分析为理解深度学习提供了可视化工具",{"5":{"133":2}}],["信息平面是二维坐标系",{"5":{"133":2}}],["信息平面",{"5":{"133":2}}],["信息保持",{"5":{"132":1}}],["信息熵",{"5":{"139":2}}],["信息熵的量化意义",{"5":{"137":2}}],["信息熵的数学基础",{"2":{"137":1},"5":{"137":1}}],["信息熵是量化随机变量不确定性的核心概念",{"5":{"137":2}}],["信息熵之间的数学联系",{"5":{"132":2}}],["信息熵与互信息<",{"5":{"131":1}}],["信息熵与互信息",{"0":{"137":1},"4":{"137":1},"5":{"131":4,"137":1}}],["信息分析报告",{"5":{"132":2}}],["信息注入效率",{"5":{"132":2}}],["信息互补",{"5":{"132":2}}],["信息损失的累积效应减小",{"5":{"132":2}}],["信息损失的风险也相应降低",{"5":{"132":2}}],["信息传递受到距离的严重限制",{"5":{"132":2}}],["信息传递效率随距离的衰减规律为理解注意力机制的长程依赖能力提供了量化工具",{"5":{"132":2}}],["信息传递效率",{"5":{"132":2}}],["信息传递一步到位",{"5":{"132":2}}],["信息传递路径的数学分析",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["信息传递路径必须沿着序列顺序",{"5":{"92":2,"153":2}}],["信息沿序列逐步传播",{"5":{"132":2}}],["信息覆盖范围",{"5":{"132":2}}],["信息路径长度",{"5":{"132":2}}],["信息路径长度大大缩短",{"5":{"132":2}}],["信息路由机制",{"5":{"132":2}}],["信息聚合",{"5":{"132":2}}],["信息的具体内容",{"5":{"132":2}}],["信息内容三个角色来理解",{"5":{"132":2}}],["信息内容",{"5":{"132":2}}],["信息标签",{"5":{"132":4}}],["信息标识",{"5":{"94":2}}],["信息请求",{"5":{"132":2}}],["信息角色",{"5":{"132":2}}],["信息选择算子",{"5":{"132":2}}],["信息流量特性",{"5":{"132":2}}],["信息流动",{"5":{"132":2}}],["信息流动效率",{"5":{"132":2}}],["信息流动的几何描述",{"2":{"45":1},"5":{"45":1}}],["信息流动矩阵为",{"5":{"87":2,"152":2}}],["信息较小的方向上",{"5":{"70":2}}],["信息较大的方向上",{"5":{"70":2}}],["信息加权的距离",{"5":{"70":2}}],["信息",{"5":{"70":2,"90":2}}],["信息矩阵为自然梯度提供了几何解释",{"5":{"137":2}}],["信息矩阵衡量了分布对参数变化的敏感程度",{"5":{"137":2}}],["信息矩阵给出",{"5":{"70":2}}],["信息矩阵定义的距离度量下进行等速移动",{"5":{"137":2}}],["信息矩阵定义了概率流形上的",{"5":{"70":1}}],["信息矩阵定义了概率流形上的黎曼度量",{"5":{"70":1}}],["信息矩阵定义为",{"5":{"70":2}}],["信息矩阵",{"5":{"70":2,"97":2,"137":4}}],["信息矩阵的性质和数据的内在维度决定",{"5":{"135":2}}],["信息矩阵的近似",{"5":{"135":2}}],["信息矩阵的作用",{"5":{"133":2}}],["信息矩阵的结构",{"5":{"70":2}}],["信息矩阵的形式",{"5":{"70":2}}],["信息贡献",{"5":{"69":2}}],["信息论不仅为深度学习提供了丰富的数学工具和概念框架",{"5":{"137":2}}],["信息论为预测和控制模型行为提供了有力工具",{"5":{"137":2}}],["信息论为理解大模型的特性提供了独特的视角",{"5":{"137":2}}],["信息论为理解深度学习提供了深刻的数学框架",{"5":{"137":2}}],["信息论为理解深度学习的泛化性能提供了独特的视角",{"5":{"133":2}}],["信息论度量",{"5":{"137":2}}],["信息论度量的计算方法",{"2":{"137":1},"5":{"137":1}}],["信息论与优化的桥梁",{"5":{"137":2}}],["信息论与优化的关联",{"2":{"137":1},"5":{"137":1}}],["信息论与优化理论之间存在深刻的联系",{"5":{"137":2}}],["信息论约束决定了网络宽度的最小要求",{"5":{"133":2}}],["信息论正则化方法",{"5":{"133":2}}],["信息论正则化直接在信息层面进行约束",{"5":{"133":2}}],["信息论正则化框架",{"2":{"133":1},"5":{"133":1}}],["信息论泛化界",{"2":{"133":1},"5":{"133":1}}],["信息论上",{"5":{"132":2}}],["信息论视角将继续发挥重要作用",{"5":{"137":2}}],["信息论视角",{"2":{"131":1},"4":{"132":1,"133":1,"137":1},"5":{"131":7}}],["信息论视角与激活函数",{"2":{"71":1},"5":{"71":1}}],["信息论提供了量化信息",{"5":{"71":2}}],["信息论基础",{"2":{"61":1,"139":1},"5":{"61":1,"69":2,"139":1}}],["信息论",{"5":{"61":2}}],["信息论的引入不仅为深度学习提供了严格的数学语言",{"5":{"137":2}}],["信息论的基本限制",{"5":{"132":2}}],["信息论的基本概念始于熵",{"5":{"96":2}}],["信息论的概念",{"5":{"70":2}}],["信息论的核心概念之一是自信息",{"5":{"61":1}}],["信息论的核心概念之一是",{"5":{"61":1}}],["信息经过次非线性变换后到达输出层",{"5":{"45":1}}],["信息经过",{"5":{"45":1}}],["信息经过变换后丢失了一些细节",{"5":{"92":2,"153":2}}],["信息丰富程度",{"5":{"61":2}}],["信息编码",{"5":{"61":2}}],["信息几何视角",{"2":{"70":1},"5":{"70":1}}],["信息几何是研究概率分布空间几何性质的理论框架",{"5":{"71":2}}],["信息几何",{"5":{"61":2,"70":4}}],["信息会收敛到平稳分布",{"5":{"87":2,"152":2}}],["信息压缩最严重的位置",{"5":{"132":2}}],["信息压缩",{"5":{"87":2,"152":2}}],["信息瓶颈和自监督学习提供了统一的理论框架",{"5":{"137":2}}],["信息瓶颈超参数的自适应调整策略",{"5":{"133":2}}],["信息瓶颈提供了分析框架",{"5":{"133":2}}],["信息瓶颈最优解",{"5":{"133":1}}],["信息瓶颈最优解满足以下上界约束",{"5":{"133":2}}],["信息瓶颈最优解满足以下自洽方程",{"5":{"133":1}}],["信息瓶颈最优性",{"5":{"133":2}}],["信息瓶颈层是满足信息论约束的网络层",{"5":{"133":2}}],["信息瓶颈层",{"5":{"133":2}}],["信息瓶颈存在闭式解",{"5":{"133":2}}],["信息瓶颈目标直接权衡了拟合能力",{"5":{"133":1}}],["信息瓶颈目标直接权衡了拟合能力和复杂度",{"5":{"133":1}}],["信息瓶颈目标修改为同时优化鲁棒性",{"5":{"133":2}}],["信息瓶颈目标驱动网络首先学习预测能力",{"5":{"133":2}}],["信息瓶颈目标可推广为",{"5":{"137":2}}],["信息瓶颈目标可近似为",{"5":{"133":2}}],["信息瓶颈目标可以表示为期望形式",{"5":{"133":2}}],["信息瓶颈目标可以改写为",{"5":{"133":2}}],["信息瓶颈目标可以等价地表示为最小化以下互信息差",{"5":{"133":2}}],["信息瓶颈目标的等价形式",{"5":{"133":2}}],["信息瓶颈问题寻求最优编码",{"5":{"133":2}}],["信息瓶颈问题",{"5":{"133":2}}],["信息瓶颈",{"5":{"133":2}}],["信息瓶颈的深度扩展",{"5":{"137":2}}],["信息瓶颈的深度上界",{"5":{"133":2}}],["信息瓶颈的理论工具将继续发挥重要作用",{"5":{"133":2}}],["信息瓶颈的理论基础",{"5":{"133":2}}],["信息瓶颈的局限",{"5":{"133":2}}],["信息瓶颈的局限性与发展方向",{"2":{"133":1},"5":{"133":1}}],["信息瓶颈的变分形式",{"5":{"133":2}}],["信息瓶颈的扩展理论",{"2":{"133":1},"5":{"133":1}}],["信息瓶颈的目标函数为",{"5":{"87":2,"152":2}}],["信息瓶颈理论",{"5":{"137":2}}],["信息瓶颈理论为理解深度学习的核心机制提供了统一的框架",{"5":{"133":2}}],["信息瓶颈理论为理解深度学习的表示学习提供了统一的框架",{"5":{"133":2}}],["信息瓶颈理论提供了一种优雅的正则化框架",{"5":{"133":2}}],["信息瓶颈理论由",{"5":{"133":2}}],["信息瓶颈理论的核心洞见在于",{"5":{"133":2}}],["信息瓶颈理论基础",{"2":{"133":1},"5":{"133":1}}],["信息瓶颈理论认为",{"5":{"71":2,"87":2,"152":2}}],["信息瓶颈与解码器",{"5":{"132":2}}],["信息到不同维度",{"5":{"87":2,"152":2}}],["信息被编码在载波信号的相位变化中",{"5":{"88":2}}],["信息量",{"5":{"89":2,"133":2}}],["信息必须沿着序列逐步传递",{"5":{"91":2}}],["信息必须经过所有中间位置的隐藏状态才能传递",{"5":{"92":2,"153":2}}],["信息必须逐步传递",{"5":{"92":2,"153":2}}],["信息只能从相邻位置逐步传递",{"5":{"92":2,"153":2}}],["信息中继",{"5":{"92":2,"153":2}}],["信息需要经过路径长度与依赖跨度成正比的逐步传递",{"5":{"92":2,"153":2}}],["信息需要沿着序列逐步传递",{"5":{"92":2,"153":2}}],["信息需要沿着序列依次传递",{"5":{"95":2}}],["信息需求",{"5":{"94":2}}],["放大能力",{"5":{"148":2}}],["放大或保持稳定",{"5":{"41":2}}],["情形三",{"5":{"148":2}}],["情形二",{"5":{"148":2}}],["情形一",{"5":{"148":2}}],["情况更为复杂",{"5":{"147":2}}],["情况稍微复杂一些",{"5":{"48":2}}],["断裂",{"5":{"148":2}}],["跳入参数空间中远离当前区域的点",{"5":{"147":1}}],["跳过10个词",{"5":{"92":2,"153":2}}],["跳过20个词",{"5":{"92":2,"153":2}}],["飞出",{"5":{"147":2}}],["轴移动",{"5":{"147":2}}],["轴突",{"5":{"47":2}}],["符合梯度方向作为最速下降方向的本质",{"5":{"147":2}}],["符号",{"5":{"143":1}}],["符号有所不同",{"5":{"90":2}}],["属性",{"5":{"146":1}}],["属于模型族",{"5":{"140":1}}],["属于类别0",{"5":{"47":1}}],["属于类别1",{"5":{"47":1}}],["属于正类",{"5":{"61":1}}],["属于组",{"5":{"86":1,"151":1}}],["替代批量统计量",{"5":{"146":1}}],["替换为",{"5":{"137":1}}],["替换为软标签",{"5":{"96":2}}],["替换softmax",{"5":{"87":1,"152":1}}],["旨在解决批归一化在循环神经网络中的不适用性问题",{"5":{"146":2}}],["旨在在保持长程依赖建模能力的同时降低计算复杂度",{"5":{"92":2,"153":2}}],["持续学习",{"5":{"146":2}}],["认为归一化的主要好处可能来自损失景观的光滑化",{"5":{"146":2}}],["认识到其在灵活性与泛化性之间的权衡",{"5":{"89":2}}],["恢复模型的表示能力",{"5":{"146":2}}],["快捷路径",{"5":{"150":2}}],["快照集成",{"5":{"145":2}}],["快速下降",{"5":{"134":2}}],["快速增长",{"5":{"133":3}}],["快速学习阶段",{"5":{"133":2}}],["快速衰减",{"5":{"87":4,"152":4}}],["快速衰减分布",{"5":{"89":2}}],["快速衰减的奇异值分布对应于",{"5":{"89":2}}],["快速傅里叶变换",{"5":{"90":2}}],["快速收敛不一定带来更好的最终性能",{"5":{"97":2}}],["端点处",{"5":{"145":2}}],["伯努利掩码的独立性",{"5":{"145":2}}],["伯努利分布与sigmoid激活",{"5":{"47":2}}],["伯努利分布",{"5":{"96":2}}],["伯努利分布的期望为",{"5":{"96":2}}],["伯努利分布可用于描述二元随机事件",{"5":{"96":2}}],["丢弃概率",{"5":{"145":2}}],["丢弃",{"5":{"145":2}}],["丢失了输入的空间结构信息",{"5":{"143":2}}],["丢失长依赖",{"5":{"140":1}}],["惩罚对象",{"5":{"144":1}}],["惩罚力度",{"5":{"70":2}}],["恰恰是噪声最强的方向",{"5":{"149":2}}],["恰恰相反",{"5":{"145":2}}],["恰当的正则化策略能够显著改善模型的收敛速度和泛化性能",{"5":{"144":2}}],["恰好",{"5":{"70":2}}],["移除了向量在",{"5":{"146":1}}],["移除了向量在方向上的投影",{"5":{"146":1}}],["移除了平移参数",{"5":{"146":2}}],["移除平移参数的合理性",{"5":{"146":2}}],["移除输入的尺度信息",{"5":{"132":2}}],["移动平均计算的",{"5":{"146":1}}],["移动平均计算的和",{"5":{"146":1}}],["移动平均",{"5":{"146":2}}],["移动平均的无偏性修正",{"5":{"146":2}}],["移动极小值位置和改变hessian矩阵特征值来改善优化条件",{"5":{"144":2}}],["岭回归解在所有方向上都比ols解更接近原点",{"5":{"144":2}}],["岭回归解",{"5":{"144":2}}],["岭回归",{"5":{"144":2}}],["峡谷",{"5":{"144":2}}],["圆角化",{"5":{"144":6}}],["诱导稀疏性以及与优化算法相互作用",{"5":{"144":2}}],["改进但未解决",{"2":{"148":1},"5":{"148":1}}],["改变损失函数的几何景观",{"5":{"146":2}}],["改变损失曲面的形状",{"5":{"144":2}}],["改善条件数的作用",{"5":{"135":2}}],["改善条件数",{"5":{"134":2}}],["早停通过限制训练步数引入偏差",{"5":{"144":2}}],["早停的等价正则化形式",{"5":{"144":2}}],["早停的偏差",{"5":{"144":2}}],["早停的正则化等价性",{"5":{"144":2}}],["早停等价于在原始损失函数上添加一个隐式的正则化项",{"5":{"144":2}}],["早停",{"5":{"144":2}}],["早停作为正则化",{"2":{"144":1},"5":{"144":1}}],["早期层",{"5":{"92":2,"153":2}}],["四维卷积核",{"5":{"143":1}}],["四维卷积核可以看作一个四阶张量",{"5":{"143":1}}],["明确的秩一成分",{"5":{"143":1}}],["明确了语言建模的核心目标",{"5":{"140":2}}],["侧向和正面切片是正交的",{"5":{"143":2}}],["管心张量",{"5":{"143":2}}],["短视",{"5":{"156":2}}],["短轴为",{"5":{"143":1}}],["短轴为的椭圆",{"5":{"143":1}}],["短语边界",{"5":{"88":2}}],["椭圆等高线",{"5":{"144":2}}],["椭圆的主轴分别缩放",{"5":{"143":1}}],["椭圆的主轴分别缩放和倍",{"5":{"143":1}}],["椭圆的方向发生改变",{"5":{"143":2}}],["椭球拉伸严重",{"5":{"135":2}}],["冗余性",{"5":{"143":2}}],["冗余度",{"5":{"132":2}}],["冗余度高",{"5":{"132":2}}],["施加稀疏正则化",{"5":{"142":1}}],["幅值与方向分离",{"5":{"142":1}}],["幅度",{"5":{"88":1}}],["资源敏感场景",{"5":{"142":1}}],["截断在区间",{"5":{"154":2}}],["截断",{"5":{"147":2}}],["截断svd的近似误差可以用被丢弃的奇异值来界",{"5":{"143":2}}],["截断svd",{"5":{"143":4}}],["截断svd与低秩近似",{"2":{"143":1},"5":{"143":1}}],["截断svd近似为",{"5":{"142":2}}],["截距为",{"5":{"138":2}}],["寻找最相似的",{"5":{"141":1}}],["寻找最优的编码方案",{"5":{"61":2}}],["种可能",{"5":{"141":1}}],["using",{"5":{"145":2}}],["u",{"5":{"141":2}}],["unsqueeze",{"5":{"132":4}}],["unit",{"5":{"42":2,"86":2,"151":2}}],["uniform",{"5":{"96":2}}],["顺序处理",{"5":{"140":1}}],["顺序的",{"5":{"92":2,"153":2}}],["及其变体占据了核心地位",{"5":{"149":2}}],["及其概率分布",{"5":{"96":1}}],["及时发现并处理梯度不稳定问题",{"5":{"148":2}}],["及之后的键",{"5":{"140":1}}],["元组构成的集合",{"5":{"140":1}}],["元素为正且和为1",{"5":{"140":2}}],["元素非负且和为1",{"5":{"140":2}}],["元素级函数",{"5":{"48":2}}],["元素的元组",{"5":{"88":1}}],["贪心解码",{"5":{"140":2}}],["历史越长",{"5":{"140":2}}],["历史上下文的重要性",{"5":{"140":2}}],["历史上",{"5":{"71":2}}],["子词",{"5":{"140":2}}],["子空间的性质和",{"5":{"135":2}}],["子空间是特征空间中的重要结构",{"5":{"50":2}}],["阐释链式法则在语言建模中的核心作用",{"5":{"140":2}}],["阐明了震荡产生的数学条件",{"5":{"136":2}}],["案例分析",{"2":{"140":1},"5":{"140":1}}],["紧致性确保连续函数在紧致集上达到最大值",{"5":{"139":2}}],["紧凑",{"5":{"87":2,"152":2}}],["甚至导致主任务性能下降",{"5":{"158":2}}],["甚至导致收敛到次优解",{"5":{"147":2}}],["甚至引发训练崩溃",{"5":{"154":4}}],["甚至可能是有害的",{"5":{"146":2}}],["甚至可能下降",{"5":{"138":2}}],["甚至对理解其正则化机制更为关键",{"5":{"145":2}}],["甚至更长",{"5":{"92":2,"153":2}}],["罕见",{"5":{"138":2}}],["亿时",{"5":{"138":2}}],["亿的模型几乎无法给出正确答案",{"5":{"138":2}}],["亿条低质量数据更有效",{"5":{"138":2}}],["亿条高质量数据可能比",{"5":{"138":2}}],["涌现",{"5":{"138":6}}],["涌现能力出现的难易程度与任务所需依赖关系的复杂度和训练数据中相关模式的频率密切相关",{"5":{"138":2}}],["涌现能力通常被定义为",{"5":{"138":2}}],["涌现能力可以通过统计学习的框架来理解",{"5":{"138":2}}],["涌现能力",{"5":{"138":2}}],["涌现能力的定义与观察",{"5":{"138":1}}],["涌现能力的定义与观察首先需要明确其统计本质",{"5":{"138":1}}],["涌现能力的概率解释",{"2":{"138":1},"5":{"138":1}}],["涌现能力的信息论阈值",{"5":{"137":2}}],["浮点运算次数",{"5":{"138":2}}],["浮点运算数flops",{"5":{"45":2}}],["浮点运算数",{"5":{"45":2}}],["何时应该增加数据量",{"5":{"138":2}}],["欠训练",{"5":{"138":2}}],["欠拟合风险增加",{"5":{"133":2}}],["之后应用dropout",{"5":{"145":1}}],["之前的所有词元",{"5":{"140":1}}],["之前的许多模型实际上是",{"5":{"138":2}}],["之间存在幂律关系",{"5":{"159":2}}],["之间存在某种关联",{"5":{"93":2}}],["之间共享的信息",{"5":{"137":1}}],["之间的关系被打破",{"5":{"159":2}}],["之间的关系",{"5":{"147":2}}],["之间的关联",{"5":{"92":2,"153":2}}],["之间的关联强度依赖于某种复杂的非线性函数",{"5":{"93":1}}],["之间的kl散度定义为",{"5":{"139":1}}],["之间的条件互信息定义为在已知",{"5":{"137":1}}],["之间的互补性定义为联合信息与各自信息之和的差",{"5":{"132":1}}],["之间的互信息为",{"5":{"133":1}}],["之间的互信息结构",{"5":{"132":2}}],["之间的互信息结构是理解注意力机制信息流动的关键",{"5":{"132":2}}],["之间的互信息关系可表示为多变量互信息的组合",{"5":{"132":2}}],["之间的互信息定义为联合分布与边缘分布乘积之间的",{"5":{"137":1}}],["之间的互信息定义为",{"5":{"69":1,"71":1,"139":2}}],["之间的信息流动遵循信息论的基本不等式",{"5":{"132":2}}],["之间的差值成正比",{"5":{"69":2}}],["之间的差异",{"5":{"48":1,"61":1}}],["之间的交叉熵",{"5":{"69":4}}],["之间的数学协同关系",{"5":{"71":2}}],["之间的乘积交互",{"5":{"71":1}}],["之间的依赖程度",{"5":{"69":1,"71":1}}],["之间的夹角",{"5":{"48":1}}],["之间的深刻联系",{"5":{"61":2}}],["之间的统一性提供了必要的背景知识",{"5":{"61":2}}],["之间的",{"5":{"61":2,"137":3}}],["之间的余弦相似度",{"5":{"87":1,"152":1}}],["之间的相似度度量为信息相关性的度量",{"5":{"132":1}}],["之间的相似度分数",{"5":{"69":2}}],["之间的相似度只取决于它们的距离",{"5":{"91":1}}],["之间的距离为",{"5":{"92":1,"153":1}}],["之间",{"5":{"45":1,"138":2}}],["之间取得平衡",{"5":{"89":2}}],["他们的研究表明",{"5":{"138":2}}],["他们的研究结果表明",{"5":{"138":2}}],["围绕这一问题",{"5":{"138":2}}],["万倍",{"5":{"138":2}}],["万有逼近定理",{"5":{"71":2}}],["背后的数学规律",{"5":{"138":2}}],["背后蕴含着深刻的数学直觉和实践经验",{"5":{"91":2}}],["规模即力量",{"5":{"138":4}}],["规模的不断扩大带来了能力的质的飞跃",{"5":{"138":2}}],["规律性",{"5":{"90":2}}],["获得",{"5":{"137":1}}],["获得后关于的信息量",{"5":{"137":1}}],["做出更准确的预测",{"5":{"150":2}}],["做",{"5":{"137":1}}],["视频的时空序列",{"5":{"143":2}}],["视角看",{"5":{"137":2}}],["视为",{"5":{"69":3}}],["视为一个二分类问题",{"5":{"101":1}}],["视为一个潜在的",{"5":{"69":1}}],["视为一个",{"5":{"69":2}}],["视为被重复",{"5":{"50":1}}],["剩余的不确定性",{"5":{"137":1}}],["打乱",{"5":{"137":2}}],["打破了",{"5":{"132":2}}],["打破了神经网络的线性瓶颈",{"5":{"47":2}}],["往往具有很大的条件数",{"5":{"149":1}}],["往往与显式正则化技术协同作用",{"5":{"144":2}}],["往往无法直接计算",{"5":{"137":2}}],["往往表现出更多的局部注意力",{"5":{"92":2,"153":2}}],["往往表现出更多的全局注意力",{"5":{"92":2,"153":2}}],["准确率可能突然跃升至",{"5":{"138":2}}],["准则与贝叶斯推断中的证据下界密切相关",{"5":{"137":2}}],["准三角不等式",{"5":{"137":2}}],["链式思考提示的涌现机制",{"5":{"138":1}}],["链式思考提示的涌现机制提供了另一个典型案例",{"5":{"138":1}}],["链式思考",{"5":{"138":2}}],["链式法则给出",{"5":{"150":2}}],["链式法则给出了多个随机变量的联合熵分解",{"5":{"96":2}}],["链式法则方法",{"5":{"140":2}}],["链式法则为自回归模型提供了严格的数学基础",{"5":{"140":2}}],["链式法则可以简洁地表示为",{"5":{"140":2}}],["链式法则",{"0":{"140":1},"4":{"140":1},"5":{"45":4,"48":4,"131":5,"137":2,"140":1}}],["链式法则在反向传播算法中扮演着核心角色",{"5":{"45":2}}],["链式法则是分析这一复合过程的核心数学工具",{"5":{"45":2}}],["链式法则将在下文详细讨论",{"5":{"48":2}}],["链式法则使得我们能够计算损失函数相对于任意深层参数的梯度",{"5":{"48":2}}],["链式法则的分解不仅在数学上优雅",{"5":{"140":2}}],["链式法则的数学基础",{"2":{"140":1},"5":{"140":1}}],["链式法则的证明利用概率分解",{"5":{"137":2}}],["链式法则的矩阵形式",{"2":{"44":1},"5":{"44":1,"135":2}}],["链式法则的基本形式可以这样表述",{"5":{"48":2}}],["链式法则的应用更加系统化",{"5":{"48":2}}],["链式法则的应用更加直观和系统",{"5":{"48":2}}],["链",{"5":{"137":2}}],["希望近似后验接近先验分布",{"5":{"137":2}}],["希望重构准确",{"5":{"137":2}}],["希尔伯特",{"2":{"21":1,"131":1},"5":{"21":1,"131":1}}],["见引理",{"5":{"137":4}}],["赋予了它新的概率论含义",{"5":{"137":2}}],["赋予神经网络万能逼近能力",{"5":{"47":2}}],["年建立的信息论基础概念",{"5":{"137":2}}],["年提出",{"5":{"133":2}}],["ℝⁿ",{"5":{"136":2}}],["∈",{"5":{"136":2}}],["校正步骤使得动量方向更准确地指向最优解",{"5":{"136":2}}],["校准",{"5":{"70":1}}],["≥",{"5":{"136":2}}],["ℓ",{"5":{"136":2}}],["ℒ",{"5":{"136":16}}],["衰减速率满足",{"5":{"136":2}}],["衰减因子",{"5":{"135":4}}],["衰减因子为",{"5":{"134":6}}],["邻域内出发的轨道都渐近趋近于",{"5":{"136":1}}],["邻近轨道趋于汇聚",{"5":{"136":2}}],["邻居",{"5":{"92":2,"153":2}}],["交换",{"5":{"136":2}}],["交叉性",{"5":{"136":2}}],["交叉熵对应于",{"5":{"70":1}}],["交叉熵对应于kl散度",{"5":{"70":1}}],["交叉熵则源于信息论中的",{"5":{"70":1}}],["交叉熵则源于信息论中的相对熵概念",{"5":{"70":1}}],["交叉熵是更好的选择",{"5":{"70":2}}],["交叉熵从",{"5":{"70":2}}],["交叉熵梯度",{"5":{"70":2}}],["交叉熵梯度的链式法则",{"2":{"61":1},"5":{"61":1}}],["交叉熵损失定义为",{"5":{"140":2}}],["交叉熵损失对于",{"5":{"101":2}}],["交叉熵损失可以解释为",{"5":{"137":2}}],["交叉熵损失可以写为",{"5":{"61":2}}],["交叉熵损失可能无法捕获数据分布的所有方面",{"5":{"101":2}}],["交叉熵损失假设预测分布与真实分布的差异可以通过kl散度度量",{"5":{"101":2}}],["交叉熵损失是",{"5":{"70":2}}],["交叉熵损失是最常用的损失函数",{"5":{"96":2}}],["交叉熵损失关于的梯度是",{"5":{"70":1}}],["交叉熵损失为",{"5":{"69":2}}],["交叉熵损失以及它们的数学结构对比",{"5":{"69":2}}],["交叉熵损失具有清晰的概率解释",{"5":{"71":2}}],["交叉熵损失",{"5":{"44":4,"70":1,"71":2}}],["交叉熵损失函数通常实现为负对数似然",{"5":{"139":2}}],["交叉熵损失函数关于参数",{"5":{"70":1,"97":1}}],["交叉熵损失函数关于参数是凸函数",{"5":{"70":1,"97":1}}],["交叉熵损失函数是最为广泛采用的优化目标",{"5":{"61":2}}],["交叉熵损失函数源自信息论的基本原理",{"5":{"61":2}}],["交叉熵损失函数的优化过程可以被理解为在信息几何框架下",{"5":{"61":2}}],["交叉熵损失中的",{"5":{"61":2}}],["交叉熵损失的信息论解释",{"5":{"137":2}}],["交叉熵损失的信息论意义",{"5":{"137":2}}],["交叉熵损失的经验风险为",{"5":{"61":2}}],["交叉熵损失的梯度特性使得基于梯度优化的训练过程具有良好的数值性质",{"5":{"61":1}}],["交叉熵损失的",{"5":{"61":1}}],["交叉熵损失的优势在于它直接与对数似然相关",{"5":{"96":2}}],["交叉熵损失与最大似然估计的联系为我们提供了一个一致的概率解释框架",{"5":{"61":1}}],["交叉熵损失与最大似然估计的联系为我们提供了一个一致的",{"5":{"61":1}}],["交叉熵损失相对于模型输出",{"5":{"96":2}}],["交叉熵损失在概率接近",{"5":{"61":2}}],["交叉熵损失在混合精度训练中需要特别处理",{"5":{"97":2}}],["交叉熵损失虽然也是凸函数",{"5":{"97":2}}],["交叉熵定义为使用",{"5":{"137":1}}],["交叉熵定义为使用编码来自分布的样本所需的平均编码长度",{"5":{"137":1}}],["交叉熵定义为",{"5":{"61":2,"96":2}}],["交叉熵与",{"5":{"137":4}}],["交叉熵与最大似然估计的统一视角",{"2":{"61":1},"5":{"61":1}}],["交叉熵与kl散度满足以下关系",{"5":{"139":2}}],["交叉熵与kl散度的关系",{"5":{"139":2}}],["交叉熵与kl散度的数学关系",{"2":{"61":1},"5":{"61":1}}],["交叉熵与kl散度的等价性分析",{"2":{"61":1},"5":{"61":1}}],["交叉熵与kl散度的等价性可以形式化地表述",{"5":{"61":2}}],["交叉熵与kl散度之间存在着简洁而深刻的数学关系",{"5":{"61":2}}],["交叉熵与kl散度密切相关",{"5":{"96":2}}],["交叉熵的推导",{"2":{"139":1},"5":{"139":1}}],["交叉熵的理论极限",{"5":{"101":2}}],["交叉熵的核心结构",{"5":{"101":2}}],["交叉熵的对数增长对应于",{"5":{"70":1}}],["交叉熵的对数增长对应于范数的某种极端形式",{"5":{"70":1}}],["交叉熵的这种无上界特性反映了其对错误预测的",{"5":{"70":2}}],["交叉熵的强凸性取决于数据的",{"5":{"70":2,"97":2}}],["交叉熵的变曲率意味着优化空间在不同区域的形状是不同的",{"5":{"70":2}}],["交叉熵的",{"5":{"70":4,"97":2}}],["交叉熵的梯度不受",{"5":{"70":2}}],["交叉熵的梯度与预测概率与真实标签的差值",{"5":{"69":1}}],["交叉熵的梯度与预测概率与真实标签的差值成正比",{"5":{"69":1}}],["交叉熵的梯度形式",{"5":{"97":2}}],["交叉熵的数学推导",{"2":{"139":1},"5":{"139":1}}],["交叉熵的数学结构同样具有鲜明的特征",{"5":{"70":2}}],["交叉熵的数学性质",{"2":{"61":1},"5":{"61":1}}],["交叉熵的概率论推导<",{"5":{"131":1}}],["交叉熵的概率论推导",{"0":{"61":1},"4":{"61":1},"5":{"61":1,"131":4}}],["交叉熵的概念便自然浮现",{"5":{"61":2}}],["交叉熵的定义与推导",{"2":{"61":1},"5":{"61":1}}],["交叉熵的定义可以分解为两部分",{"5":{"61":2}}],["交叉熵的学习率敏感性中等",{"5":{"97":2}}],["交叉熵的框架之上",{"5":{"101":2}}],["交叉熵",{"5":{"61":6,"69":2,"70":4,"97":1,"137":2,"139":2}}],["交叉熵关于模型分布参数",{"5":{"61":2}}],["交叉熵中的",{"5":{"61":2}}],["交叉熵在分类任务中通常比",{"5":{"70":2}}],["交叉熵在分类任务中的完整推导",{"2":{"61":1},"5":{"61":1}}],["交叉熵在这些方面的优势使其成为分类任务的首选",{"5":{"61":2}}],["交叉熵可以分解为熵与kl散度之和",{"5":{"96":2}}],["交叉gram矩阵",{"5":{"87":2,"152":2}}],["交叉协方差",{"5":{"87":2,"94":2,"152":2}}],["鞍结分岔的正规形",{"5":{"136":2}}],["鞍结分岔的定义",{"5":{"136":2}}],["鞍结分岔是不动点出现或消失的分岔",{"5":{"136":2}}],["鞍结分岔",{"5":{"136":2}}],["鞍点发散方向",{"5":{"149":2}}],["鞍点方向",{"5":{"149":2}}],["鞍点的数量远多于局部极小值",{"5":{"149":2}}],["鞍点的",{"5":{"135":2}}],["鞍点的逃逸动力学",{"2":{"135":1},"5":{"135":3}}],["鞍点的存在是普遍的",{"5":{"97":2}}],["鞍点是优化过程中的重要障碍之一",{"5":{"149":2}}],["鞍点是",{"5":{"134":2}}],["鞍点和平坦区域",{"5":{"48":2}}],["鞍点逃离是大规模非凸优化中的一个重要问题",{"5":{"48":2}}],["鞍点比局部最小值更为常见",{"5":{"48":2}}],["鞍点与局部极小值的景观分析",{"2":{"97":1},"5":{"97":1}}],["鞍点",{"5":{"97":2,"134":3}}],["鞍点附近的曲率特性决定了优化算法逃离鞍点的速度",{"5":{"97":2}}],["判据",{"5":{"136":2}}],["判别器参数",{"5":{"133":2}}],["判别器",{"5":{"133":2}}],["映射并分析其不动点稳定性可得",{"5":{"136":2}}],["映射到query",{"5":{"69":1}}],["映射到概率单纯形",{"5":{"69":1}}],["映射到输出",{"5":{"47":1}}],["映射到新的特征空间",{"5":{"47":1}}],["映射到",{"5":{"61":2}}],["映射到一个",{"5":{"93":1}}],["穿过单位圆导致稳定性改变",{"5":{"136":2}}],["穿越速率",{"5":{"136":2}}],["穿越条件",{"5":{"136":2}}],["穿行",{"5":{"48":2}}],["孤立性",{"5":{"136":2}}],["吸引域通常较大",{"5":{"136":2}}],["吸引域是初始条件集合",{"5":{"136":2}}],["吸引域内轨迹收敛",{"5":{"134":1}}],["吸引性",{"5":{"136":2}}],["轨道",{"5":{"136":1}}],["轨道是闭合的",{"5":{"136":2}}],["轨迹发散",{"5":{"134":1}}],["轨迹",{"5":{"134":2}}],["闭轨性",{"5":{"136":2}}],["惯性",{"5":{"136":2}}],["停滞",{"5":{"136":2}}],["稳健",{"5":{"145":2}}],["稳态震荡的振幅与学习率满足以下关系",{"5":{"136":2}}],["稳定的结点",{"5":{"136":1}}],["稳定的结点和不稳定的鞍点",{"5":{"136":1}}],["稳定收敛",{"5":{"136":2}}],["稳定训练任务",{"5":{"147":1}}],["稳定训练",{"5":{"142":1}}],["稳定训练区域的数学刻画",{"5":{"136":2}}],["稳定训练的关键",{"5":{"132":2}}],["稳定和不稳定不动点成对出现或消失",{"5":{"136":2}}],["稳定方向",{"5":{"135":2}}],["稳定且存在",{"5":{"134":2}}],["稳定",{"5":{"134":4,"147":1}}],["稳定性要求",{"5":{"135":2}}],["稳定性意义",{"5":{"135":2}}],["稳定性分析的基础上",{"5":{"136":2}}],["稳定性分析的数值方法",{"2":{"135":1},"5":{"135":1}}],["稳定性分析",{"5":{"135":2}}],["稳定性分析工具",{"5":{"134":2}}],["稳定性和线性化分析",{"5":{"134":2}}],["稳定性和最终性能",{"5":{"42":2}}],["稳定性理论",{"5":{"134":2}}],["稳定性理论基础",{"2":{"134":1},"5":{"134":1}}],["稳定性",{"5":{"134":2,"136":2}}],["阻尼比是判断系统震荡特性的关键参数",{"5":{"136":2}}],["阻尼比与震荡行为",{"5":{"136":2}}],["阻尼比越小",{"5":{"136":2}}],["阻尼比",{"5":{"136":2}}],["阻碍其他任务的学习",{"5":{"101":2}}],["振幅趋于无穷大",{"5":{"136":2}}],["振幅",{"5":{"136":2}}],["振幅为1",{"5":{"90":2}}],["典型例子包括",{"5":{"136":1}}],["典型例子包括或",{"5":{"136":1}}],["典型的方法包括",{"5":{"147":2}}],["典型的操作包括矩阵乘法",{"5":{"48":2}}],["典型的低秩适配方法如lora",{"5":{"50":2}}],["典型的衰减曲线类似于幂律分布或指数分布",{"5":{"87":2,"152":2}}],["混沌训练行为可能导致",{"5":{"136":2}}],["混沌训练的实际影响",{"5":{"136":2}}],["混沌吸引子的特征",{"5":{"136":2}}],["混沌理论提供了分析这种复杂行为的工具",{"5":{"136":2}}],["混沌理论的视角",{"2":{"136":1},"5":{"136":1}}],["混沌区",{"5":{"136":2}}],["混沌等",{"5":{"136":2}}],["混合并行",{"5":{"157":1}}],["混合",{"5":{"41":2,"144":2}}],["混合积性质",{"5":{"50":2}}],["混合精度的显存节省约为50",{"5":{"157":2}}],["混合精度训练需要注意数值溢出问题",{"5":{"157":2}}],["混合精度训练是降低显存和通信开销的另一重要技术",{"5":{"157":2}}],["混合精度训练是另一种重要的效率优化技术",{"5":{"50":2}}],["混合精度训练中的梯度缩放",{"5":{"44":2}}],["混合精度训练",{"5":{"44":3,"97":4}}],["混合精度训练可以将内存消耗减半",{"5":{"50":2}}],["混合精度训练与损失缩放",{"2":{"97":1},"5":{"97":1}}],["混合精度训练使用半精度浮点数进行计算以节省内存和加速",{"5":{"48":2}}],["混合精度训练使用两种数值精度",{"5":{"97":2}}],["混合精度利用了现代gpu对低精度运算的专门优化",{"5":{"50":2}}],["混合精度计算",{"5":{"93":2}}],["混合位置编码",{"5":{"88":2}}],["混合位置编码结合了可学习编码和正弦编码的优点",{"5":{"89":2}}],["混合编码的函数空间是正弦编码函数空间与可学习编码函数空间的和集",{"5":{"89":2}}],["混合编码的训练策略可以是",{"5":{"89":2}}],["混合编码的秩最多为​",{"5":{"89":1}}],["混合编码的秩最多为",{"5":{"89":1}}],["混合专家方法",{"2":{"131":1},"4":{"106":1,"107":1,"108":1,"157":1,"158":1,"159":1},"5":{"131":7}}],["混合专家",{"5":{"93":2,"159":2}}],["极小值的位置变化",{"5":{"144":2}}],["极小值位置的移动",{"2":{"144":1},"5":{"144":1}}],["极小值",{"5":{"143":2,"149":4}}],["极致压缩",{"5":{"142":1}}],["极限为0",{"5":{"158":2}}],["极限环",{"5":{"136":1}}],["极限环可能在以下场景中出现",{"5":{"136":2}}],["极限环的振幅与",{"5":{"136":1}}],["极限环的振幅与成正比",{"5":{"136":1}}],["极限环的存在性需要更复杂的判据",{"5":{"136":2}}],["极限环的数学定义",{"5":{"136":2}}],["极限环附近不存在其他同类闭轨道",{"5":{"136":2}}],["极限环具有以下特性",{"5":{"136":2}}],["极限环对应于参数在一定范围内的周期性振荡",{"5":{"136":2}}],["极限环是动态系统中的重要概念",{"5":{"136":2}}],["极限环与周期行为",{"2":{"136":1},"5":{"136":1}}],["极限点为不动点",{"5":{"136":2}}],["极大",{"5":{"147":2}}],["极大地促进了离散分布表示学习的发展",{"5":{"71":2}}],["极大值或鞍点",{"5":{"48":2}}],["介绍了特征值估计和谱范数计算的实用算法",{"5":{"135":2}}],["介于",{"5":{"45":1}}],["ρ",{"5":{"135":6}}],["幂律关系通常在一定范围内成立",{"5":{"138":2}}],["幂律关系的数学形式",{"2":{"138":1},"5":{"138":1}}],["幂律函数",{"5":{"138":2}}],["幂律分布的数学特性",{"5":{"138":1}}],["幂律分布的数学特性决定了标度律在预测方面的可靠性",{"5":{"138":1}}],["幂律分布的性质",{"5":{"135":2}}],["幂律分布具有以下特性",{"5":{"135":2}}],["幂迭代的收敛性",{"5":{"135":2}}],["幂迭代估计谱半径",{"5":{"135":2}}],["构建两层门控网络",{"5":{"159":2}}],["构建三对角矩阵",{"5":{"135":2}}],["构成了现代神经网络激活函数的主流选择",{"5":{"42":2}}],["构成的函数族在适当条件下可以满足这些要求",{"5":{"71":1}}],["构成标准正交基",{"5":{"50":2}}],["α",{"5":{"135":4}}],["迭代映射变为线性的",{"5":{"147":2}}],["迭代序列",{"5":{"136":1}}],["迭代序列收敛到某个极限",{"5":{"136":1}}],["迭代次数",{"5":{"135":4}}],["迭代过程的互信息满足以下衰减关系",{"5":{"133":2}}],["起到归一化特征值",{"5":{"135":2}}],["起到正则化作用并降低过拟合风险",{"5":{"136":2}}],["起到正则化作用",{"5":{"132":2}}],["起到正则化的作用",{"5":{"90":2}}],["块得到",{"5":{"135":2}}],["块稀疏将序列划分为固定大小的块",{"5":{"86":2,"151":2}}],["临界状态",{"5":{"148":1}}],["临界状态与初始化目标",{"5":{"148":1}}],["临界状态与初始化目标是理想的临界状态",{"5":{"148":1}}],["临界状态是理想的",{"5":{"148":1}}],["临界稳定",{"5":{"148":2}}],["临界现象的特点是",{"5":{"138":2}}],["临界点",{"5":{"136":2}}],["临界点的二阶判别法",{"5":{"135":2}}],["临界学习率",{"5":{"134":2}}],["汇",{"5":{"134":1}}],["借助",{"5":{"134":2,"135":2,"136":2}}],["宽而平坦的最小值区域对应更稳定的动力学特性",{"5":{"134":2}}],["宽度对信息容量的影响",{"5":{"133":2}}],["宽度为的全连接网络",{"5":{"71":1}}],["宽度为",{"5":{"71":1}}],["搜索减速",{"5":{"134":2}}],["物理意义",{"5":{"135":2}}],["物理解释",{"5":{"134":2}}],["物理类比",{"5":{"134":2}}],["病态系统的优化困难",{"5":{"135":2}}],["病态程度",{"5":{"135":2}}],["病态程度等因素直接影响收敛速度",{"5":{"134":2}}],["病态",{"2":{"135":1},"5":{"135":1}}],["病态问题",{"5":{"134":2}}],["病态问题的困难",{"5":{"134":2}}],["病态条件数的影响",{"5":{"148":2}}],["病态条件数导致",{"5":{"135":2}}],["病态条件",{"5":{"134":2}}],["病态性通常发生在注意力权重分布极端不均匀的情况下",{"5":{"87":2,"152":2}}],["滚球",{"5":{"134":2}}],["□",{"5":{"134":2}}],["渐近服从均值为0",{"5":{"140":2}}],["渐近正态性",{"5":{"139":2,"140":2}}],["渐近正态的",{"5":{"96":2}}],["渐近稳定且存在",{"5":{"134":2}}],["渐近稳定",{"5":{"134":5}}],["状态的价值等于在该状态下",{"5":{"156":2}}],["状态转移概率函数",{"5":{"156":2}}],["状态转移函数",{"5":{"134":4}}],["状态可以包括当前的对话历史",{"5":{"156":2}}],["状态",{"5":{"156":2}}],["状态价值函数",{"2":{"156":1},"5":{"155":1,"156":3}}],["状态单调变化",{"5":{"136":2}}],["状态在有限时间内返回起点",{"5":{"136":2}}],["状态更新函数",{"5":{"134":2}}],["状态变量",{"5":{"134":2}}],["状态空间大小为",{"5":{"140":2}}],["状态空间的别称",{"5":{"134":2}}],["状态空间",{"5":{"134":4,"156":2}}],["促进实际分配频率均衡",{"5":{"158":2}}],["促进路由概率均衡",{"5":{"158":2}}],["促进专家使用的均衡化",{"5":{"158":2}}],["促进专家分化",{"5":{"158":2,"159":2}}],["促进探索",{"5":{"157":2}}],["促进表示压缩",{"5":{"133":1}}],["促进学习更具泛化能力的表示",{"5":{"133":2}}],["受限于计算预算",{"5":{"159":1}}],["受输入维度限制",{"5":{"133":1}}],["受最低频率限制",{"5":{"90":2}}],["静态分析",{"5":{"133":2}}],["静态损失缩放使用固定不变的缩放因子",{"5":{"97":2}}],["剪枝后的模型",{"5":{"133":2}}],["剪枝率",{"5":{"133":2}}],["剪切则是一种保持体积但改变形状的变换",{"5":{"50":2}}],["蒸馏效率由以下信息论因素共同决定",{"5":{"133":2}}],["蒸馏效率的信息论分析",{"5":{"133":2}}],["知识蒸馏利用",{"5":{"137":2}}],["知识蒸馏",{"5":{"137":2}}],["知识蒸馏和正则化方法的核心工具",{"5":{"137":2}}],["知识蒸馏和正则化方法的核心",{"5":{"137":2}}],["知识蒸馏可视为教师模型到学生模型的信息传输过程",{"5":{"133":2}}],["知识蒸馏的信息瓶颈视角",{"5":{"133":2}}],["γ",{"5":{"133":2,"146":1}}],["退化为标准",{"5":{"134":2}}],["退化为均匀分布",{"5":{"132":2}}],["退火因子",{"5":{"133":2}}],["退火策略可以表示为",{"5":{"159":2}}],["退火策略",{"5":{"133":2}}],["估计相对稳定",{"5":{"149":2}}],["估计值",{"5":{"137":2}}],["估计量逐渐收敛到真实性能",{"5":{"138":2}}],["估计量的方差很大",{"5":{"138":2}}],["估计量是互信息的下界",{"5":{"137":2}}],["估计量一致收敛于真实的互信息",{"5":{"133":2}}],["估计准确",{"5":{"133":2}}],["估计保守",{"5":{"133":2}}],["估计和最大化互信息",{"5":{"71":2}}],["ψ",{"5":{"133":18}}],["拓扑传递性",{"5":{"136":2}}],["拓扑保持的必要性",{"5":{"133":2}}],["拓扑信息瓶颈目标在标准信息瓶颈基础上增加拓扑保持项",{"5":{"133":2}}],["拓扑信息瓶颈",{"5":{"133":2}}],["拓扑排序保证每个节点在其所有前驱节点之后",{"5":{"45":2}}],["τ",{"5":{"133":4}}],["框架可推导出此上界",{"5":{"137":2}}],["框架和信息论不等式",{"5":{"133":2}}],["框架会自动处理",{"5":{"61":2}}],["锚点",{"5":{"133":2}}],["←",{"5":{"133":16,"135":30,"136":34,"137":34}}],["ŷ",{"5":{"133":2}}],["ε",{"5":{"133":4,"135":2,"136":6}}],["σ²",{"5":{"145":1}}],["σ^2",{"5":{"133":4}}],["σ",{"5":{"133":6,"137":8}}],["μ^2",{"5":{"133":2}}],["μ",{"5":{"133":4}}],["β",{"5":{"133":32,"135":10,"146":1}}],["θ",{"5":{"133":22,"136":14}}],["φ",{"5":{"133":16,"137":18}}],["驱动",{"5":{"133":2}}],["驱动下降",{"5":{"133":1}}],["驱动快速增长",{"5":{"133":1}}],["拟合阶段",{"5":{"133":4}}],["纵轴为状态变量",{"5":{"136":2}}],["纵轴为",{"5":{"133":2}}],["横轴为分岔参数",{"5":{"136":2}}],["横轴为",{"5":{"133":2}}],["抽象层次的信息瓶颈",{"5":{"132":2}}],["排序损失",{"5":{"154":1}}],["排序并建议剪枝",{"5":{"132":2}}],["排列不变性",{"5":{"101":2}}],["∇ℒ",{"5":{"133":2,"136":2}}],["∇",{"5":{"132":2,"133":10,"137":2}}],["η",{"5":{"132":2,"133":6,"136":18,"137":4}}],["λ3",{"5":{"132":2}}],["λ2",{"5":{"132":2}}],["λ1",{"5":{"132":2}}],["λ",{"5":{"132":2,"135":22}}],["采取动作的期望收益",{"5":{"155":2}}],["采样得到的",{"5":{"155":2}}],["采样的数据来估计新策略",{"5":{"154":2}}],["采样",{"5":{"132":2,"155":4}}],["采用了rope的变体",{"5":{"88":1}}],["~",{"5":{"132":2,"133":2}}],["缓慢增加",{"5":{"133":2}}],["缓解了深层网络的梯度消失问题",{"5":{"132":2}}],["缓存的空间需求为",{"5":{"93":2}}],["π",{"5":{"132":2}}],["调节信息流量",{"5":{"132":2}}],["调节温度可以控制注意力的分散程度",{"5":{"132":2}}],["调度参数",{"5":{"132":2}}],["调整参数",{"5":{"45":1}}],["调整参数以使逼近目标函数",{"5":{"45":1}}],["调整位置",{"5":{"71":2}}],["调整多少超参数",{"5":{"71":2}}],["调整",{"5":{"89":2,"133":2}}],["贡献小的头优先剪枝",{"5":{"132":2}}],["贡献内容",{"5":{"91":2}}],["净信息贡献",{"5":{"132":2}}],["≠",{"5":{"132":4,"137":2}}],["容易得多",{"5":{"150":2}}],["容易样本",{"5":{"70":2}}],["容量呈指数增长",{"5":{"133":2}}],["容量受输入维度限制",{"5":{"133":2}}],["容量受值维度限制",{"5":{"132":2}}],["容量与维度线性相关",{"5":{"132":2}}],["容量限制",{"5":{"132":2}}],["体现了条件计算范式的核心理念",{"5":{"158":2}}],["体现了智能体对未来的重视程度",{"5":{"156":2}}],["体现了统计力学与信息论的深刻联系",{"5":{"132":2}}],["体现了信息匹配的数学本质",{"5":{"132":2}}],["体现在参数层面",{"5":{"93":2}}],["震荡机制分析",{"5":{"136":2}}],["震荡幅度随之衰减",{"5":{"136":2}}],["震荡与泛化性能之间的关系是优化深度学习模型的关键",{"5":{"136":2}}],["震荡与泛化的关联",{"2":{"136":1},"5":{"136":1}}],["震荡持续时间越长",{"5":{"136":2}}],["震荡的特征量",{"5":{"136":2}}],["震荡产生的临界学习率条件",{"5":{"136":2}}],["震荡行为的数学定义",{"5":{"136":2}}],["震荡和周期行为之间的数学关系",{"5":{"136":2}}],["震荡和周期行为的数学角度深入分析了神经网络训练的动力学特性",{"5":{"136":2}}],["震荡和周期行为",{"5":{"136":2}}],["震荡",{"0":{"136":1},"4":{"136":1},"5":{"131":5,"136":3}}],["似然梯度的数学表达",{"2":{"139":1},"5":{"139":1}}],["似然的梯度",{"5":{"101":2}}],["似然函数具有特殊的乘积结构",{"5":{"140":2}}],["似然函数的分解性",{"5":{"140":2}}],["似然函数的构建",{"2":{"140":1},"5":{"140":1}}],["似然函数的数学定义",{"2":{"139":1},"5":{"139":1}}],["似然函数定义为",{"5":{"96":2,"139":2}}],["似然函数",{"5":{"61":2,"139":2}}],["真正依赖于位置",{"5":{"101":1}}],["真实梯度会试图将参数推回鞍点",{"5":{"149":2}}],["真实梯度",{"5":{"149":2}}],["真实分布属于模型族",{"5":{"140":2}}],["真实分布和预测分布",{"5":{"71":1}}],["真实分布",{"5":{"61":5,"69":4,"71":1}}],["真实分布被放在",{"5":{"61":2}}],["真实分布是一个one",{"5":{"61":1}}],["真实标签的负对数似然",{"5":{"144":2}}],["真实标签",{"5":{"44":2}}],["真实标签为",{"5":{"61":4,"96":2}}],["真实标签通常被视为确定性的",{"5":{"61":2}}],["真实",{"5":{"61":2,"69":4}}],["真实类别",{"5":{"61":2}}],["坏",{"5":{"101":2,"154":2}}],["收集一批状态",{"5":{"155":2}}],["收集",{"5":{"155":2}}],["收集人类对不同模型输出的偏好比较数据",{"5":{"101":2}}],["收缩较少",{"5":{"144":2}}],["收敛于均匀分布",{"5":{"157":2}}],["收敛于one",{"5":{"157":2}}],["收敛于真实参数值",{"5":{"96":2}}],["收敛阶段",{"5":{"137":2}}],["收敛界和强凸函数的线性收敛速率",{"5":{"136":1}}],["收敛变慢",{"5":{"136":2}}],["收敛阈值",{"5":{"136":2}}],["收敛过程中",{"5":{"136":2}}],["收敛路径与平坦最小值",{"5":{"136":2}}],["收敛区",{"5":{"136":2}}],["收敛",{"5":{"136":6}}],["收敛速率取决于条件数",{"5":{"136":2}}],["收敛速度又会太慢",{"5":{"155":2}}],["收敛速度更快",{"5":{"146":2}}],["收敛速度趋于零",{"5":{"136":2}}],["收敛速度是衡量序列趋近极限点速率的重要指标",{"5":{"136":2}}],["收敛速度的类型划分",{"5":{"136":2}}],["收敛速度的影响因素",{"5":{"97":2}}],["收敛速度极慢",{"5":{"135":2}}],["收敛速度比",{"5":{"135":2}}],["收敛速度",{"5":{"134":2}}],["收敛速度逐渐减缓",{"5":{"134":2}}],["收敛速度接近",{"5":{"134":2}}],["收敛速度与最终性能",{"2":{"97":1},"5":{"97":1}}],["收敛速度分析",{"5":{"97":2}}],["收敛速度由强凸性系数与利普希茨常数的比值",{"5":{"97":1}}],["收敛速度由强凸性系数",{"5":{"97":1}}],["收敛序列的基本性质",{"5":{"136":2}}],["收敛到平坦极小",{"5":{"149":2}}],["收敛到某个极限",{"5":{"136":1}}],["收敛到临界点",{"5":{"136":2}}],["收敛到临界点的充分条件",{"5":{"136":2}}],["收敛到",{"5":{"136":3}}],["收敛条件满足",{"5":{"135":2}}],["收敛慢",{"5":{"135":2}}],["收敛快",{"5":{"135":2,"146":1}}],["收敛迅速",{"5":{"134":2}}],["收敛极其缓慢",{"5":{"134":2}}],["收敛极慢",{"5":{"134":2}}],["收敛要求",{"5":{"134":2}}],["收敛的充要条件是",{"5":{"134":2}}],["收敛机制",{"5":{"134":2}}],["收敛性理论体系",{"5":{"136":2}}],["收敛性理论基础",{"2":{"136":1},"5":{"136":1}}],["收敛性导向的自适应学习率调度",{"5":{"136":2}}],["收敛性的基本定义",{"5":{"136":2}}],["收敛性是动态系统最重要的定性性质之一",{"5":{"136":2}}],["收敛性",{"0":{"136":1},"2":{"136":1},"4":{"136":1},"5":{"131":5,"136":2}}],["收敛性和正则化对编码的影响",{"5":{"89":2}}],["收敛性质以及与模型泛化能力的联系",{"5":{"97":2}}],["收敛行为可以精确分析",{"5":{"97":2}}],["收敛越慢",{"5":{"97":4,"136":2}}],["摘要任务需要关注文档的核心内容",{"5":{"101":2}}],["摘要生成任务",{"5":{"101":2}}],["翻译任务",{"5":{"101":2}}],["问答任务需要关注问题中的关键实体",{"5":{"101":2}}],["问答任务",{"5":{"101":2}}],["问题是其中最突出的一个",{"5":{"159":2}}],["问题引入与现象定义",{"2":{"148":1},"5":{"148":1}}],["问题",{"5":{"41":4,"42":2,"44":1,"47":2,"71":2,"94":2,"95":2,"155":2}}],["问题设定",{"5":{"44":2}}],["仅激活部分参数",{"5":{"159":1}}],["仅在输入",{"5":{"150":2}}],["仅有零均值并不能完全描述噪声的特性",{"5":{"149":2}}],["仅缩放其范数至阈值",{"5":{"147":2}}],["仅包含平衡点",{"5":{"134":2}}],["仅是位置",{"5":{"101":1}}],["仅依赖于token嵌入而不包含位置信息",{"5":{"101":1}}],["仅对有效位置",{"5":{"101":2}}],["仅保留top",{"5":{"69":2,"101":2}}],["详见6",{"5":{"101":2}}],["详见第六章",{"5":{"101":2}}],["详细推导了二分类和多分类场景下的交叉熵损失",{"5":{"61":2}}],["详细推导了其数学定义",{"5":{"93":2}}],["详细解释了公式中每个组成部分的数学本质",{"5":{"95":2}}],["超出容量的样本被重新路由",{"5":{"158":2}}],["超出此边界后",{"5":{"136":2}}],["超大规模训练",{"5":{"157":1}}],["超参数",{"5":{"154":2}}],["超过margin",{"5":{"158":2}}],["超过了目标kl散度",{"5":{"154":2}}],["超过临界值",{"5":{"138":2}}],["超过此值系统发散",{"5":{"134":2}}],["超临界",{"5":{"136":2}}],["超线性收敛",{"5":{"136":2}}],["超越softmax的归一化",{"2":{"101":1},"5":{"101":1}}],["超平面",{"5":{"71":2}}],["人类反馈强化学习",{"5":{"101":2}}],["人类反馈强化学习损失",{"2":{"101":1},"5":{"101":1}}],["人咬狗",{"5":{"88":2,"91":2,"92":2,"153":2}}],["比遵循当前策略的平均表现好多少",{"5":{"155":2}}],["比率的数学性质",{"5":{"154":1}}],["比率的数学性质也值得深入分析",{"5":{"154":1}}],["比率增大意味着目标函数值减小",{"5":{"154":2}}],["比率",{"5":{"154":2}}],["比较",{"5":{"145":2}}],["比单层瓶颈更准确地描述深度网络的训练动态",{"5":{"133":2}}],["比",{"5":{"70":1}}],["比特",{"5":{"89":1}}],["比特的位置信息",{"5":{"89":1}}],["比特的信息",{"5":{"90":1}}],["比如位置和位置之间的关联",{"5":{"92":1,"153":1}}],["比如位置",{"5":{"92":1,"153":1}}],["项增大",{"5":{"158":2}}],["项限制了编码分布的自由度",{"5":{"133":2}}],["项对参数施加了软约束",{"5":{"133":2}}],["项",{"5":{"70":1,"90":1}}],["项可能超过1",{"5":{"51":2}}],["半精度",{"5":{"157":4}}],["半径为",{"5":{"146":2,"147":2}}],["半负定",{"5":{"135":2}}],["半负定矩阵",{"5":{"70":1}}],["半正定",{"5":{"135":2}}],["半正定矩阵",{"5":{"70":1}}],["半群性质",{"5":{"134":2}}],["概念",{"5":{"70":1}}],["概率形式",{"5":{"140":1}}],["概率链式法则",{"5":{"140":2}}],["概率链式法则与自回归分解",{"2":{"140":1},"5":{"140":1}}],["概率编码在",{"5":{"133":2}}],["概率信息瓶颈定义为对条件分布的直接优化",{"5":{"133":2}}],["概率信息瓶颈",{"5":{"133":2}}],["概率接近",{"5":{"70":2}}],["概率单纯形可以几何的理解为",{"5":{"70":1}}],["概率单纯形可以几何的理解为空间中由坐标轴截距为",{"5":{"70":1}}],["概率单纯形",{"5":{"70":1}}],["概率单纯形退化为一条线段",{"5":{"70":2}}],["概率单纯形上的几何结构",{"2":{"70":1},"5":{"70":1}}],["概率单纯形是定义在中满足且的点集",{"5":{"61":1}}],["概率单纯形是定义在",{"5":{"61":1}}],["概率本质",{"5":{"69":2}}],["概率是归一化后的值",{"5":{"71":2}}],["概率解释的一致性",{"5":{"69":2}}],["概率解释的完整性",{"2":{"61":1},"5":{"61":1}}],["概率解释以及信息论意义",{"5":{"71":2}}],["概率解释为损失函数的设计提供了坚实的理论基础",{"5":{"71":2}}],["概率解释框架",{"5":{"61":1}}],["概率解释将logits转化为满足概率公理的分布",{"5":{"61":1}}],["概率解释",{"5":{"61":1}}],["概率参数的对数几率为",{"5":{"71":1}}],["概率参数",{"5":{"71":1}}],["概率视角下的大模型",{"2":{"131":1},"4":{"138":1,"139":1,"140":1},"5":{"131":7}}],["概率视角下的激活函数",{"2":{"71":1},"5":{"71":1}}],["概率视角下的神经元模型",{"2":{"47":1},"5":{"47":1}}],["概率论视角下的涌现机制",{"5":{"138":1}}],["概率论视角下的涌现机制可以从模型学习的条件概率分布来理解",{"5":{"138":1}}],["概率论与统计<",{"5":{"21":1,"131":1}}],["概率论与统计",{"0":{"96":1},"4":{"96":1},"5":{"21":4,"96":1,"131":4}}],["概率论是研究随机现象规律性的数学分支",{"5":{"96":2}}],["概率论和几何学的角度深入分析了各种损失函数的数学结构和理论基础",{"5":{"97":2}}],["概率由pdf在区间上的积分给出",{"5":{"96":2}}],["概率密度最大的点",{"5":{"96":2}}],["概率分布为",{"5":{"137":2}}],["概率分布的几何结构为理解注意力权重提供了类比框架",{"5":{"70":2}}],["概率分布的信息期望",{"2":{"61":1},"5":{"61":1}}],["概率分布被视为一个流形",{"5":{"70":2,"71":2}}],["概率分布变得更加平坦",{"5":{"96":2}}],["概率越小的事件发生时",{"5":{"61":2}}],["概率",{"5":{"61":2}}],["概率质量函数",{"5":{"96":2}}],["概率质量集中在一个或少数几个最大的输入上",{"5":{"95":2}}],["架构稳定训练的信息论基础",{"5":{"132":2}}],["架构可以从信息请求",{"5":{"132":2}}],["架构通过精心设计的信息流动机制实现了高效的训练和强大的表示能力",{"5":{"132":2}}],["架构设计的数学一致性",{"5":{"70":2}}],["架构的设计哲学",{"5":{"70":2}}],["架构",{"5":{"70":2}}],["帕累托最优性",{"5":{"70":2}}],["手动调节",{"5":{"70":2}}],["身份标识",{"5":{"70":2}}],["离谱",{"5":{"70":2}}],["离散系统的",{"5":{"136":2}}],["离散系统可能振荡甚至发散",{"5":{"97":2}}],["离散优化的连续时间极限提供了理解算法行为的另一视角",{"5":{"134":2}}],["离散",{"5":{"134":2}}],["离散时间动态系统可等价地用递推关系表示",{"5":{"134":2}}],["离散时间动态系统",{"5":{"134":2}}],["离散时间动态系统特指时间变量取整数值的情形",{"5":{"134":2}}],["离散时间动态系统的数学基础<",{"5":{"131":1}}],["离散时间动态系统的数学基础",{"0":{"134":1},"2":{"134":1},"4":{"134":1},"5":{"131":4,"134":2}}],["离散假设",{"5":{"133":2}}],["离散随机变量只能取有限或可数无穷个值",{"5":{"96":2}}],["离散程度和相互关系",{"5":{"96":2}}],["离散情况",{"5":{"90":2}}],["离散序列的傅里叶变换",{"5":{"90":2}}],["离散梯度下降",{"5":{"97":2}}],["离散化引入了数值误差",{"5":{"97":2}}],["离散化的稳定性取决于学习率与损失函数曲率的关系",{"5":{"97":2}}],["严重梯度消失",{"5":{"148":1}}],["严重得多",{"5":{"70":1}}],["严重程度",{"5":{"70":2}}],["严重",{"5":{"97":1}}],["严格来说是相对熵",{"5":{"154":2}}],["严格来说不是真正的距离",{"5":{"96":2}}],["严格定义梯度协方差矩阵",{"5":{"149":2}}],["严格地表述这一性质",{"5":{"145":2}}],["严格地只依赖于两个位置的相对距离",{"5":{"88":2}}],["严格凹性确保全局最大值唯一",{"5":{"139":2}}],["严格凸函数是凸性更强的形式",{"5":{"48":2}}],["严格凸函数至多有一个全局最小值点",{"5":{"48":2}}],["严格遵循rope的设计",{"5":{"88":2}}],["严格包含​",{"5":{"93":1}}],["严格包含",{"5":{"93":1}}],["错误程度",{"5":{"70":2}}],["错误",{"5":{"70":2}}],["了某些方向的梯度信息",{"5":{"148":2}}],["了均方误差中的",{"5":{"70":1}}],["了均方误差中的项",{"5":{"70":1}}],["了",{"5":{"70":2,"148":2}}],["了分布",{"5":{"69":2}}],["了内容向量",{"5":{"88":2}}],["了频率成分",{"5":{"90":2}}],["了远距离位置之间的关联",{"5":{"92":2,"153":2}}],["抵消",{"5":{"70":4,"88":2}}],["倍重计算开销",{"5":{"157":2}}],["倍以内",{"5":{"154":2}}],["倍周期分岔序列",{"5":{"136":2}}],["倍",{"5":{"70":2,"138":2,"142":1,"143":3,"154":4,"158":4,"159":10}}],["强压缩",{"5":{"148":1}}],["强化学习的目标是找到一个",{"5":{"156":1}}],["强化学习的目标是找到一个最优策略",{"5":{"156":1}}],["强化学习的核心框架可以抽象为智能体与环境之间的持续交互循环",{"5":{"156":2}}],["强化学习没有现成的",{"5":{"156":2}}],["强化学习是机器学习的一个重要分支",{"5":{"156":2}}],["强化学习基本框架",{"2":{"156":1},"5":{"156":1}}],["强化学习基础与马尔可夫决策过程<",{"5":{"131":1}}],["强化学习基础与马尔可夫决策过程",{"0":{"156":1},"4":{"109":1,"156":1},"5":{"131":4,"156":1}}],["强化学习",{"2":{"131":1},"4":{"109":1,"110":1,"111":1,"154":1,"155":1,"156":1},"5":{"131":7}}],["强凸函数的线性收敛",{"5":{"136":2}}],["强凸函数满足",{"5":{"70":1,"97":1}}],["强凸的",{"5":{"70":1,"136":2}}],["强凸性系数很小",{"5":{"70":2,"97":2}}],["强凸性与条件数",{"5":{"70":2}}],["强凸性与优化速度",{"5":{"97":2}}],["强凸性是比凸性更强的性质",{"5":{"97":2}}],["强凸和",{"5":{"97":1}}],["强调概率分布之间的信息差异",{"5":{"70":2}}],["强调预测值与真实值之间的欧氏距离",{"5":{"70":2}}],["强度",{"5":{"89":2}}],["黎曼梯度下降",{"5":{"70":1}}],["黎曼梯度下降考虑了不同方向上损失变化的",{"5":{"70":2}}],["黎曼度量",{"5":{"70":1}}],["黎曼度量由",{"5":{"70":2}}],["黎曼流形优化",{"2":{"70":1},"5":{"70":1}}],["黎曼几何",{"5":{"61":1}}],["度量了损失",{"5":{"135":2}}],["度量位置",{"5":{"69":1}}],["度量预测与真实标签之间的差异",{"5":{"44":1}}],["三种方法的本质区别在于",{"5":{"146":1}}],["三种方法的本质区别在于统计量的计算轴不同",{"5":{"146":1}}],["三种归一化方法各有其最佳适用场景",{"5":{"146":2}}],["三种归一化方法的维度对比",{"5":{"146":1}}],["三种归一化方法的维度对比为便于查阅和比较",{"5":{"146":1}}],["三种归一化方法的维度对比与总结",{"2":{"146":1},"5":{"146":1}}],["三种归一化方法的统计量计算轴",{"5":{"146":2}}],["三种dropout变体的对比",{"5":{"145":2}}],["三元组各自承担不同的信息角色",{"5":{"132":2}}],["三维情况下是一个等边三角形",{"5":{"70":2}}],["三阶张量",{"5":{"143":3}}],["三阶导数",{"5":{"42":2}}],["三阶及更高阶的张量则可以表示更复杂的数据结构",{"5":{"50":2}}],["三项",{"5":{"91":2}}],["三个场景的共同结构是",{"5":{"69":1}}],["三个计算阶段的复杂度分别为",{"5":{"95":2}}],["散布在周围",{"5":{"69":1}}],["散度作为分布间差异的度量",{"5":{"137":2}}],["散度和互信息在深度学习中的核心作用",{"5":{"137":2}}],["散度满足以下关系",{"5":{"137":2}}],["散度将大模型",{"5":{"137":2}}],["散度形式",{"5":{"137":2}}],["散度诱导的黎曼几何中的测地线",{"5":{"137":2}}],["散度在",{"5":{"137":2}}],["散度在处最小",{"5":{"137":1}}],["散度在处的二阶近似为",{"5":{"137":1}}],["散度在局部等价于fisher信息加权的二次形式",{"5":{"70":2}}],["散度不满足三角不等式",{"5":{"137":2}}],["散度不满足对称性",{"5":{"137":2}}],["散度不同",{"5":{"70":2}}],["散度定义为积分形式",{"5":{"137":2}}],["散度定义为",{"5":{"137":2}}],["散度是变分推断",{"5":{"137":2}}],["散度等已经成为分析和设计神经网络的核心工具",{"5":{"137":2}}],["散度的优化视角",{"5":{"137":2}}],["散度的关系",{"5":{"137":2}}],["散度的二阶近似",{"5":{"137":2}}],["散度的三角不等式不成立",{"5":{"137":2}}],["散度的非对称性",{"5":{"137":2}}],["散度的非负性",{"5":{"137":6}}],["散度的数学理论",{"2":{"137":1},"5":{"137":1}}],["散度的凸性直接推导",{"5":{"70":2}}],["散度下界",{"5":{"133":2}}],["散度方法使用判别器来估计互信息的下界",{"5":{"133":2}}],["散度估计互信息",{"5":{"133":2}}],["散度有闭式解",{"5":{"133":2}}],["散度与从",{"5":{"70":1}}],["散度与从到的",{"5":{"70":1}}],["散度可以视为两个分布在fisher信息度量下的",{"5":{"70":1}}],["散度",{"5":{"61":4,"70":4,"97":1,"133":4,"137":12}}],["散度关于是凸的",{"5":{"70":1}}],["散度关于",{"5":{"97":1}}],["位关于",{"5":{"133":1}}],["位于",{"5":{"69":1}}],["位置间信息传递效率",{"5":{"132":2}}],["位置签名",{"5":{"101":2}}],["位置无关性与注意力机制的数学关系",{"2":{"101":1},"5":{"101":1}}],["位置无法关注位置",{"5":{"92":2,"153":2}}],["位置编码维度需满足以下信息论下界",{"5":{"132":2}}],["位置编码需编码以下两类位置信息",{"5":{"132":1}}],["位置编码需要为个位置编码约比特的信息",{"5":{"90":1}}],["位置编码需要为每个位置生成唯一的表示",{"5":{"90":2}}],["位置编码需要为",{"5":{"90":1}}],["位置编码需要与输入嵌入相结合",{"5":{"91":2}}],["位置编码在这个聚合过程中扮演关键角色",{"5":{"101":2}}],["位置编码在模型初始化时生成",{"5":{"91":2}}],["位置编码对语言建模损失的必要性",{"5":{"101":2}}],["位置编码具有更好的数学性质",{"5":{"70":2}}],["位置编码可以被视为",{"5":{"70":1}}],["位置编码可以被视为特征工程",{"5":{"70":1}}],["位置编码可以被理解为对",{"5":{"71":2}}],["位置编码可以理解为在",{"5":{"70":2}}],["位置编码可以理解为",{"5":{"90":2}}],["位置编码可以视为一个映射",{"5":{"89":2}}],["位置编码可以视为定义在频率空间上的一组线性算子",{"5":{"90":2}}],["位置编码可以视为对位置先验的一种建模",{"5":{"90":2}}],["位置编码与特征工程的对应",{"2":{"70":1},"5":{"70":1}}],["位置编码与激活函数的结合决定了位置信息如何在网络中传递",{"5":{"71":2}}],["位置编码与学习率调度存在相互作用",{"5":{"97":2}}],["位置编码",{"5":{"41":2,"71":8,"91":3,"92":4,"101":4,"132":1,"140":2,"153":4}}],["位置编码的信息论下界",{"5":{"132":2}}],["位置编码的信息论框架",{"5":{"132":2}}],["位置编码的选择直接影响模型学习位置模式的能力",{"5":{"101":2}}],["位置编码的数学结构虽然在表面上属于不同的主题",{"5":{"70":2}}],["位置编码的数学设计",{"5":{"71":2}}],["位置编码的数学形式为向原始嵌入添加一个与位置相关的编码向量",{"5":{"92":2,"153":2}}],["位置编码的核心作用是打破自注意力机制的",{"5":{"101":2}}],["位置编码的核心思想是为序列中的每个位置赋予一个独特的",{"5":{"70":2}}],["位置编码的核心要求之一是能够表示相对位置",{"5":{"90":2}}],["位置编码的本质问题",{"2":{"88":1},"5":{"88":1}}],["位置编码的更广阔图景",{"2":{"88":1},"5":{"88":1}}],["位置编码的使命",{"5":{"88":2}}],["位置编码的计算是稳定且可预测的",{"5":{"88":2}}],["位置编码的外推能力是指",{"5":{"88":2}}],["位置编码的质量可能下降",{"5":{"88":2}}],["位置编码的优化景观通常是非凸的",{"5":{"89":2}}],["位置编码的正则化可以影响学习到的编码特性",{"5":{"89":2}}],["位置编码的前向传播是简单的查表操作",{"5":{"89":2}}],["位置编码的学习率可以与模型其他部分相同",{"5":{"89":2}}],["位置编码的参数量约为个",{"5":{"89":1}}],["位置编码的参数量约为",{"5":{"89":1}}],["位置编码的gram矩阵的第个元素为",{"5":{"89":1}}],["位置编码的gram矩阵",{"5":{"89":1}}],["位置编码的各频率成分具有不同的",{"5":{"90":2}}],["位置编码的频谱可以表示为冲激函数的叠加",{"5":{"90":2}}],["位置编码的频率分解意味着不同维度携带不同尺度的位置信息",{"5":{"71":2}}],["位置编码的频率成分间隔为",{"5":{"90":2}}],["位置编码的频率结构对模型的正则化和泛化能力有重要影响",{"5":{"90":2}}],["位置编码的频率结构引入了一种隐式的先验",{"5":{"90":2}}],["位置编码的分辨率",{"5":{"90":2}}],["位置编码的能量完全集中在有限的几个离散频率上",{"5":{"90":2}}],["位置编码的能量集中在离散的频率点上",{"5":{"90":2}}],["位置编码的平滑性",{"5":{"90":2}}],["位置编码的点积",{"5":{"90":1}}],["位置编码的点积只依赖于相对位置",{"5":{"90":1}}],["位置编码的点积只依赖于位置差",{"5":{"91":4}}],["位置编码的设计将面临新的挑战",{"5":{"88":2}}],["位置编码的设计需要满足几个基本的数学要求",{"5":{"91":2}}],["位置编码的设计需要满足几个条件",{"5":{"92":2,"153":2}}],["位置编码的生成是确定性的",{"5":{"91":2}}],["位置编码的基本要求是不同位置具有不同的编码向量",{"5":{"91":2}}],["位置编码的必要性",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["位置编码的引入为注意力机制注入了位置感知能力",{"5":{"92":2,"153":2}}],["位置编码的具体形式将在后续章节详细讨论",{"5":{"94":2}}],["位置编码数学",{"2":{"131":1},"4":{"88":1,"89":1,"90":1,"91":1},"5":{"131":9}}],["位置编码问题可以形式化地表述为",{"5":{"88":2}}],["位置编码就是位置编码",{"5":{"88":2}}],["位置编码技术可以分为几个主要类别",{"5":{"88":2}}],["位置编码领域仍在快速发展",{"5":{"88":2}}],["位置编码将继续扮演关键角色",{"5":{"88":2}}],["位置编码向量的质心为",{"5":{"89":2}}],["位置编码矩阵的其他行",{"5":{"89":2}}],["位置编码矩阵的秩是影响其性质的关键参数",{"5":{"89":1}}],["位置编码矩阵通常以半精度",{"5":{"89":1}}],["位置编码矩阵通常按行主序存储",{"5":{"89":2}}],["位置编码矩阵",{"5":{"89":2}}],["位置编码矩阵为",{"5":{"91":2}}],["位置编码最多能够编码约比特的位置信息",{"5":{"89":1}}],["位置编码最多能够编码约",{"5":{"89":1}}],["位置编码不再是孤立的数值向量",{"5":{"90":2}}],["位置编码不应破坏自注意力的核心计算特性",{"5":{"88":2}}],["位置编码不应干扰内容信息的表示",{"5":{"91":2}}],["位置编码能够有效表示的序列长度范围与频率参数的配置密切相关",{"5":{"90":2}}],["位置编码是transformer架构中最具数学美感的设计之一",{"5":{"88":2}}],["位置编码是这些平面上点的集合",{"5":{"90":2}}],["位置编码是双向的",{"5":{"91":2}}],["位置编码中频率参数的几何直觉可以通过圆周运动来理解",{"5":{"90":1}}],["位置编码中频率参数",{"5":{"90":1}}],["位置编码点积作为相对位置的函数",{"5":{"90":1}}],["位置编码点积作为相对位置",{"5":{"90":1}}],["位置编码应当能够表达位置之间的距离和相对关系",{"5":{"91":2}}],["位置编码被",{"5":{"91":2}}],["位置编码之间的相似度",{"5":{"91":2}}],["位置编码为",{"5":{"91":2}}],["位置编码通过影响query和key的计算来间接影响注意力权重",{"5":{"91":2}}],["位置编码通过上述四项影响最终的注意力权重分布",{"5":{"91":2}}],["位置编码通过投影矩阵和​进入注意力计算",{"5":{"91":1}}],["位置编码通过投影进入query和key的计算",{"5":{"91":2}}],["位置编码本身不会增加可训练参数的数量",{"5":{"88":2}}],["位置编码本身不参与掩码操作",{"5":{"91":2}}],["位置编码仍然提供了绝对位置信息",{"5":{"91":2}}],["位置编码提供的方向信息可以帮助模型捕获跨越长距离的关联",{"5":{"92":2,"153":2}}],["位置编码假设",{"5":{"92":2,"153":2}}],["位置编码等高级主题奠定坚实的理论基础",{"5":{"95":2}}],["位置信息通过位置编码注入模型",{"5":{"101":2}}],["位置信息通过加法",{"5":{"88":2}}],["位置信息流形",{"5":{"70":2}}],["位置信息被缩放到合适的范围",{"5":{"41":2}}],["位置信息",{"5":{"71":2,"88":2}}],["位置信息应该以什么方式编码到向量中",{"5":{"88":2}}],["位置信息能够被自注意力的内积运算自然地提取",{"5":{"88":2}}],["位置信息到内容中",{"5":{"88":2}}],["位置信息以三种不同的方式混入内积计算",{"5":{"88":2}}],["位置信息没有与内容信息混淆",{"5":{"88":2}}],["位置信息完全由内容嵌入和后续学习提供",{"5":{"89":2}}],["位置信息可以用低频到中频的成分表示",{"5":{"90":2}}],["位置信息自然地编码在隐藏状态的更新过程中",{"5":{"91":2}}],["位置信息和token信息通过不同的权重矩阵投影后相加",{"5":{"71":2}}],["位置信息和内容信息共享投影矩阵和​",{"5":{"91":1}}],["位置信息和内容信息共享投影矩阵",{"5":{"91":1}}],["位置信息需要显式注入",{"5":{"92":2,"153":2}}],["位置感知的特征表示",{"5":{"71":2}}],["位置",{"5":{"87":4,"88":2,"89":1,"90":4,"91":8,"92":26,"96":2,"132":1,"152":4,"153":26}}],["位置的query向量",{"5":{"87":1,"152":1}}],["位置的key向量",{"5":{"87":1,"152":1}}],["位置的编码可以表示为初始编码​经过次变换的结果",{"5":{"90":1}}],["位置的编码是初始编码经过各频率成分独立旋转次后的结果",{"5":{"90":1}}],["位置的编码​​在训练时从未见过",{"5":{"89":1}}],["位置的编码​可以视为频率域中的",{"5":{"90":1}}],["位置的编码向量​定义为",{"5":{"91":1}}],["位置的编码与位置的编码是对称的",{"5":{"91":1}}],["位置的信息",{"5":{"91":1}}],["位置的信息有多少能够传递到位置",{"5":{"92":1,"153":1}}],["位置的信息对位置的贡献也较小",{"5":{"92":1,"153":1}}],["位置的信息就可以",{"5":{"92":1,"153":1}}],["位置的信息通过位置作为",{"5":{"92":1,"153":1}}],["位置的输出​是所有位置的值的加权平均",{"5":{"92":1,"153":1}}],["位置的注意力权重​可能很小",{"5":{"92":1,"153":1}}],["位置的值向量直接参与了位置的输出计算",{"5":{"92":1,"153":1}}],["位置的值向量包含了位置从位置聚合的信息",{"5":{"92":1,"153":1}}],["位置的原始嵌入​通过残差连接直接参与到最终层的表示",{"5":{"92":1,"153":1}}],["位置的最终表示保留了原始嵌入的直接成分",{"5":{"92":1,"153":1}}],["位置的查询",{"5":{"92":1,"153":1}}],["位置的键",{"5":{"92":1,"153":1}}],["位置应该编码为相位",{"5":{"88":2}}],["位置i的query",{"5":{"88":2}}],["位置j的key",{"5":{"88":2}}],["位置平移",{"5":{"88":2}}],["位置索引对应旋转角度",{"5":{"88":2}}],["位置依赖",{"5":{"89":2,"92":2,"153":2}}],["位置表示",{"5":{"89":2}}],["位置域",{"5":{"90":2}}],["位置域中的平移对应于频率域中的相位偏移",{"5":{"90":2}}],["位置域中的卷积对应于频率域中的乘积",{"5":{"90":2}}],["位置域中时间分辨率",{"5":{"90":2}}],["位置区分精度",{"5":{"90":2}}],["位置对应圆群上的点",{"5":{"90":1}}],["位置变量",{"5":{"136":2}}],["位置变化的主要模式可以用少数几个维度捕获",{"5":{"89":2}}],["位置变换算子在频率空间中是一个对角矩阵",{"5":{"90":1}}],["位置变换算子",{"5":{"90":1}}],["位置差的编码分辨率与频率成反比",{"5":{"90":1}}],["位置差",{"5":{"90":1}}],["位置交互",{"5":{"91":2}}],["位置相关的依赖",{"5":{"91":2}}],["位置投影​和​可以预先计算",{"5":{"91":1}}],["位置投影的一个重要性质是",{"5":{"91":2}}],["位置投影",{"5":{"91":1}}],["位置独立性",{"5":{"91":2}}],["位置0",{"5":{"91":2}}],["位置0的编码为",{"5":{"91":2}}],["位置0与位置1的点积",{"5":{"91":2}}],["位置0与位置2的点积",{"5":{"91":2}}],["位置1",{"5":{"91":2}}],["位置1的编码为",{"5":{"91":2}}],["位置1与位置2的点积",{"5":{"91":2}}],["位置2",{"5":{"91":2}}],["位置2的编码为",{"5":{"91":2}}],["位置为行",{"5":{"91":2}}],["位置三类交互项",{"5":{"91":2}}],["位置和位置之间的相似度只取决于它们的距离",{"5":{"91":1}}],["位置和位置之间的距离为",{"5":{"92":1,"153":1}}],["位置与自身的距离",{"5":{"92":2,"153":2}}],["位置可以通过不同频率的正弦波表示",{"5":{"91":2}}],["位置可以直接",{"5":{"92":2,"153":2}}],["位置头适合捕获需要考虑序列顺序的依赖关系",{"5":{"92":2,"153":2}}],["位置在第层的表示​是​",{"5":{"92":1,"153":1}}],["位置关注位置",{"5":{"92":2,"153":2}}],["位置到相位向量的映射是单射",{"5":{"90":2}}],["位置到位置的信息传递效率定义为互信息与自信息之比",{"5":{"132":1}}],["位置到位置",{"5":{"92":2,"153":2}}],["位精度",{"5":{"89":1}}],["拉向查询向量",{"5":{"69":1}}],["拉普拉斯近似利用hessian矩阵在峰值附近对概率分布进行高斯近似",{"5":{"48":2}}],["拉普拉斯近似用于构建后验分布的高斯代理",{"5":{"48":2}}],["拉伸",{"5":{"94":2}}],["广播",{"5":{"50":2}}],["广播是张量逐元素运算中的一种自动对齐机制",{"5":{"50":2}}],["广播规则通常是从最后一个维度开始逐维比较",{"5":{"50":2}}],["广义优势估计",{"5":{"155":2}}],["广义特征值问题定义为",{"5":{"135":2}}],["广义特征值问题",{"2":{"135":1},"5":{"135":3}}],["广义特征值",{"5":{"133":1}}],["广义特征值对应于保留信息与压缩程度的权衡",{"5":{"133":1}}],["广义infonce框架",{"5":{"69":2}}],["广义内积",{"5":{"87":2,"152":2}}],["广度",{"5":{"93":2}}],["内",{"5":{"154":2}}],["内部维度必须匹配",{"5":{"143":1}}],["内呈现震荡行为",{"5":{"136":1}}],["内积与互信息成正比",{"5":{"132":2}}],["内积",{"5":{"69":2,"88":2}}],["内积不再只依赖于相对位置",{"5":{"88":2}}],["内积为",{"5":{"88":4}}],["内积严格等于原始query和key在一个旋转后的向量上的内积",{"5":{"88":2}}],["内积可以看作是一个g",{"5":{"88":1}}],["内积可以看作是一个",{"5":{"88":1}}],["内积的值保持不变",{"5":{"88":2}}],["内积的虚部为",{"5":{"90":2}}],["内容与位置的交叉项",{"5":{"88":2}}],["内容向量",{"5":{"88":2}}],["内容",{"5":{"91":4,"95":2}}],["内容驱动假设",{"5":{"92":2,"153":2}}],["内存访问量与激活的参数量成正比",{"5":{"159":2}}],["内存访问模式是",{"5":{"92":2,"153":2}}],["内存带宽效率相应提升了",{"5":{"159":2}}],["内存带宽和硬件利用率等多个角度进行分析",{"5":{"159":2}}],["内存带宽与计算效率",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["内存效率不仅与参数数量有关",{"5":{"50":2}}],["内存布局优化方面",{"5":{"89":2}}],["尺度",{"5":{"70":2,"91":2,"146":2}}],["尺度参数",{"5":{"69":4}}],["尺度效应",{"5":{"69":2}}],["集中在正样本上",{"5":{"69":2}}],["集中在正样本索引上",{"5":{"69":2}}],["场景",{"5":{"69":1}}],["场景三",{"5":{"69":2}}],["场景二",{"5":{"69":2}}],["场景一",{"5":{"69":2}}],["锐化决策边界",{"5":{"69":2}}],["锐度与平坦极小值",{"5":{"142":2}}],["锐度",{"5":{"69":4,"95":2,"96":2,"132":2}}],["防止专家坍缩的其他策略包括",{"5":{"158":2}}],["防止专家坍缩的数学原理",{"2":{"158":1},"5":{"158":1}}],["防止因数值精度问题导致的梯度不稳定",{"5":{"146":2}}],["防止掩码分布过于偏离先验分布",{"5":{"145":2}}],["防止过拟合的核心技术",{"5":{"147":2}}],["防止过拟合到单一位置",{"5":{"132":2}}],["防止过度自信的预测",{"5":{"137":2}}],["防止表示过拟合训练数据",{"5":{"133":2}}],["防止表示崩溃",{"5":{"69":2}}],["防止了当",{"5":{"132":1}}],["防止了当较大时分数差异过大导致的熵过低",{"5":{"132":1}}],["防止分数差异过大",{"5":{"132":1}}],["防止策略模型偏离初始策略",{"5":{"101":1}}],["防止策略模型偏离初始策略太远",{"5":{"101":1}}],["防止策略发生剧烈变化",{"5":{"101":2}}],["防止编码向量过大",{"5":{"89":2}}],["影响训练的稳定性",{"5":{"146":2}}],["影响训练稳定性",{"5":{"136":2}}],["影响互信息",{"5":{"137":1}}],["影响优化的随机性",{"5":{"134":2}}],["影响模型对不同尺度位置信息的利用程度",{"5":{"70":2}}],["影响softmax分布",{"5":{"69":1}}],["影响softmax分布的锐度",{"5":{"69":1}}],["影响局部优化的收敛速度",{"5":{"48":2}}],["影响实际性能",{"5":{"86":2,"151":2}}],["影响编码的",{"5":{"89":2}}],["差值最小化",{"5":{"69":2}}],["差异",{"5":{"70":2}}],["差异的信息论度量",{"5":{"71":2}}],["差异很大",{"5":{"61":2}}],["差分正则化和谱正则化",{"5":{"89":2}}],["差分正则化惩罚相邻位置编码的差异",{"5":{"89":2}}],["差分正则化鼓励学习平滑的位置编码",{"5":{"89":2}}],["差向量的模约为",{"5":{"90":2}}],["差向量的模为",{"5":{"90":2}}],["那么回报",{"5":{"156":2}}],["那么执行策略梯度更新后",{"5":{"155":2}}],["那么我们应该增加在该状态下选择该动作的概率",{"5":{"155":2}}],["那么一次采样数据的影响会被放大100倍",{"5":{"154":2}}],["那么重要性采样估计的方差会变得非常高",{"5":{"154":2}}],["那么策略就会持续稳定地改进",{"5":{"154":2}}],["那么学习残差",{"5":{"150":2}}],["那么增加更多层就不会导致性能下降",{"5":{"150":2}}],["那么在极限",{"5":{"141":1}}],["那么在极限下",{"5":{"141":1}}],["那么对应的",{"5":{"141":1}}],["那么对应的行也保持相关",{"5":{"141":1}}],["那么模型学习到该模式的难度与",{"5":{"138":2}}],["那么为了将损失减半",{"5":{"138":2}}],["那么注意力权重",{"5":{"69":1}}],["那么注意力权重就是是的正样本的软概率",{"5":{"69":1}}],["那么softmax操作本质上是在计算",{"5":{"69":1}}],["那么softmax操作本质上是在计算与每个的相似度",{"5":{"69":1}}],["那么优化过程会倾向于最大化每个位置与其他位置之间的互信息",{"5":{"69":2}}],["那么标准的infonce损失为",{"5":{"69":2}}],["那么infonce的目标正是最大化观察到的正样本对的似然",{"5":{"69":2}}],["那么",{"5":{"69":6,"92":1,"153":1}}],["那么​的影响需要经过步的传递才能到达",{"5":{"92":1,"153":1}}],["那么它们对最终点积的贡献就会更大",{"5":{"95":2}}],["区分了凸函数和非凸函数的收敛条件",{"5":{"136":2}}],["区分性",{"5":{"70":1}}],["区分它来自分布",{"5":{"69":2}}],["区分相近频率的能力",{"5":{"90":2}}],["区间的函数",{"5":{"150":2}}],["区间",{"5":{"41":1,"42":7,"71":1,"97":2}}],["区间内连续变化",{"5":{"145":2}}],["区间内",{"5":{"71":1,"95":2,"154":2}}],["区别仅在于输入向量的来源",{"5":{"70":2}}],["区别在于",{"5":{"94":2}}],["配分函数",{"5":{"69":2}}],["配对t检验是比较两个相关样本均值的常用方法",{"5":{"96":2}}],["配对t检验可以判断两个模型性能的差异是否显著",{"5":{"96":2}}],["噪声会主导门控信号",{"5":{"159":2}}],["噪声标准差",{"5":{"159":2}}],["噪声加入仅在训练阶段使用",{"5":{"159":2}}],["噪声加入相当于在门控网络中引入了正则化项",{"5":{"159":2}}],["噪声加入的核心思想是通过在门控得分中注入随机性来打破这种正反馈循环",{"5":{"159":2}}],["噪声加入与均匀负载",{"2":{"159":1},"5":{"159":1}}],["噪声加入",{"5":{"158":2,"159":2}}],["噪声的调节作用有限",{"5":{"159":2}}],["噪声的引入解决了门控网络可能陷入局部最优的问题",{"5":{"159":2}}],["噪声的强度与曲率成正比",{"5":{"149":1}}],["噪声的参数通过变分推断学习得到",{"5":{"96":2}}],["噪声向量",{"5":{"149":1}}],["噪声向量近似服从一个均值向量为零的多元高斯分布",{"5":{"149":1}}],["噪声结构的动态演化",{"5":{"149":2}}],["噪声越小",{"5":{"149":2}}],["噪声",{"5":{"149":2}}],["噪声最强的方向恰好是曲率最负的方向",{"5":{"149":2}}],["噪声椭球",{"5":{"149":2}}],["噪声协方差",{"5":{"149":2}}],["噪声方差系数",{"5":{"145":1}}],["噪声注入作为正则化",{"5":{"145":2}}],["噪声熵",{"5":{"138":2}}],["噪声更小",{"5":{"134":2}}],["噪声分布",{"5":{"69":1}}],["噪声分布的选择对性能有显著影响",{"5":{"69":1}}],["噪声对比估计的一致性",{"5":{"133":2}}],["噪声对比估计的核心思想",{"5":{"69":2}}],["噪声对比估计",{"5":{"69":2}}],["噪声过滤",{"5":{"89":2}}],["噪声水平",{"5":{"97":2}}],["统一框架",{"5":{"134":2}}],["统一分析框架",{"5":{"134":2}}],["统一的优化框架",{"5":{"69":2}}],["统一的数学框架三个场景的共同结构是",{"5":{"69":1}}],["统一的数学框架",{"2":{"69":1},"5":{"69":2}}],["统计特性的不变性",{"5":{"150":1}}],["统计特性的不变性是layer",{"5":{"150":1}}],["统计阈值",{"5":{"147":1}}],["统计网络",{"5":{"137":2}}],["统计频率",{"5":{"137":2}}],["统计意义",{"5":{"70":2}}],["统计推断与参数估计",{"2":{"96":1},"5":{"96":1}}],["统计推断是利用样本数据对总体特征进行推断的过程",{"5":{"96":2}}],["统计推断的思想贯穿于模型训练",{"5":{"96":2}}],["统计显著",{"5":{"96":2}}],["统计学来源",{"5":{"51":2}}],["统计量",{"5":{"146":1}}],["统计量维度为",{"5":{"146":8}}],["统计量的计算轴不同",{"5":{"146":1}}],["统计量的估计方差增大",{"5":{"146":2}}],["统计量的几何基础",{"5":{"51":1}}],["统计量计算方式",{"5":{"146":2}}],["统计量来源",{"5":{"41":1}}],["外所有token的序列",{"5":{"101":1}}],["外积计算",{"5":{"44":1}}],["外推性能往往不佳",{"5":{"88":2}}],["外推上的鲁棒性是第三个因素",{"5":{"88":1}}],["外推上的鲁棒性",{"5":{"88":1}}],["外推困难等",{"5":{"89":2}}],["外推效果会较好",{"5":{"89":2}}],["外推可能失败",{"5":{"89":2}}],["外推能力是指模型处理超出训练长度的序列的能力",{"5":{"89":2}}],["外推能力等",{"5":{"90":2}}],["外推时可能不稳定",{"5":{"91":2}}],["未被选中的专家的权重根本不需要被加载到计算单元中",{"5":{"159":2}}],["未被选中的专家不会接收到任何梯度",{"5":{"157":2}}],["未被激活的专家的",{"5":{"158":2}}],["未选中位置为0",{"5":{"157":2}}],["未来的奖励因为任务可能终止而自然地被打了折扣",{"5":{"156":2}}],["未来的研究方向之一是设计自适应的损失函数",{"5":{"101":2}}],["未来的研究方向还包括损失函数与模型架构的协同设计",{"5":{"101":2}}],["未来只与现在有关",{"5":{"156":2}}],["未加约束的梯度下降在高维非凸损失曲面上可能表现出混沌行为",{"5":{"147":2}}],["未饱和区域",{"5":{"42":2}}],["未见过的位置编码",{"5":{"88":2}}],["未缩放的点积的方差为",{"5":{"95":1}}],["未缩放的点积",{"5":{"95":1}}],["现实世界中的高维数据通常不是均匀分布在整个高维空间中的",{"5":{"143":2}}],["现有损失函数存在一些固有的局限性",{"5":{"101":2}}],["现象是低秩结构产生的根本原因",{"5":{"141":2}}],["现象相关",{"5":{"70":2}}],["现象与注意力机制中的",{"5":{"70":2}}],["现象",{"5":{"90":2,"91":2,"92":2,"138":2,"148":2,"153":2,"159":2}}],["现象类似",{"5":{"93":2}}],["现代处理器的性能瓶颈往往不在于计算单元的算力",{"5":{"159":2}}],["现代强化学习算法通常采用",{"5":{"155":1}}],["现代强化学习算法通常采用actor",{"5":{"155":1}}],["现代大语言模型",{"5":{"140":2}}],["现代大语言模型的预训练和微调通常涉及多种任务的组合",{"5":{"101":2}}],["现代深度学习模型",{"5":{"143":2}}],["现代深度学习优化器",{"5":{"70":2}}],["现代深度学习框架提供了高度优化的top",{"5":{"159":2}}],["现代深度学习框架的优化器",{"5":{"46":2}}],["现代深度学习框架都针对这种矩阵运算进行了高度优化",{"5":{"51":2}}],["现代深度学习框架",{"5":{"93":4}}],["现代transformer",{"5":{"71":2}}],["现代神经网络通常由以下类型的层组成",{"5":{"46":2}}],["现代gpu和tpu针对稠密矩阵运算进行了高度优化",{"5":{"159":2}}],["现代gpu和tpu等硬件加速器擅长并行计算",{"5":{"45":2}}],["现代gpu架构专门优化了张量运算",{"5":{"50":2}}],["现代gpu的并行架构非常适合这种批处理计算",{"5":{"50":2}}],["现代gpu的大规模并行能力可以充分利用这一特性",{"5":{"93":2}}],["现在考虑一般情形",{"5":{"148":2}}],["现在考虑softmax变换",{"5":{"141":2}}],["现在考虑自注意力中的query",{"5":{"88":2}}],["现在计算它们的内积",{"5":{"88":2}}],["现在让我们完整地推演rope如何进入自注意力机制",{"5":{"88":2}}],["现在",{"5":{"92":2,"153":2}}],["核张量表示成分间交互",{"5":{"143":1}}],["核张量与因子矩阵",{"5":{"143":1}}],["核张量满足所谓的",{"5":{"143":1}}],["核张量",{"5":{"143":5}}],["核密度估计",{"5":{"137":2}}],["核函数",{"5":{"137":2}}],["核函数将数据映射到高维隐空间",{"5":{"87":2,"152":2}}],["核心目标",{"5":{"143":1}}],["核心意义",{"5":{"135":4}}],["核心结论",{"5":{"134":2,"135":2,"148":2}}],["核心思想",{"5":{"101":2,"134":2}}],["核心思想是",{"5":{"87":2,"152":2}}],["核心洞察是",{"5":{"69":2}}],["核范数正则化则展示了如何通过奇异值之和的惩罚来诱导低秩结构",{"5":{"144":2}}],["核范数正则化在推荐系统和矩阵补全任务中尤为重要",{"5":{"144":2}}],["核范数正则化的最优解满足",{"5":{"144":2}}],["核范数正则化是l1范数在矩阵空间的推广",{"5":{"144":2}}],["核范数正则化可以鼓励解的低秩性质",{"5":{"50":2}}],["核范数与低秩逼近",{"5":{"144":2}}],["核范数与低秩正则化",{"2":{"144":1},"5":{"144":1}}],["核范数与矩阵的秩密切相关",{"5":{"87":2,"152":2}}],["核范数和1",{"5":{"50":2}}],["核范数",{"5":{"50":2,"87":2,"144":3,"152":2}}],["核范数是frobenius范数和谱范数之间的折中",{"5":{"50":2}}],["核方法中的许多理论工具可以应用于注意力的分析",{"5":{"87":2,"152":2}}],["核方法",{"5":{"87":2,"152":2}}],["核方法的理论工具可以应用于注意力机制的分析",{"5":{"87":2,"152":2}}],["核矩阵",{"5":{"87":2,"152":2}}],["核矩阵是该隐空间中的gram矩阵",{"5":{"87":2,"152":2}}],["核矩阵是半正定矩阵",{"5":{"87":2,"152":2}}],["核矩阵的谱性质",{"5":{"87":2,"152":2}}],["核矩阵的谱可以用傅里叶分析研究",{"5":{"87":2,"152":2}}],["近年来",{"5":{"146":4}}],["近端策略优化",{"5":{"101":2,"154":2}}],["近似硬top",{"5":{"159":2}}],["近似straight",{"5":{"157":2}}],["近似服从一个均值向量为零的多元高斯分布",{"5":{"149":1}}],["近似关系需要修正",{"5":{"149":2}}],["近似为",{"5":{"142":2}}],["近似误差界",{"5":{"143":2}}],["近似误差",{"5":{"142":2}}],["近似误差与泛化保证",{"2":{"142":1},"5":{"142":1}}],["近似误差为",{"5":{"89":2}}],["近似最大特征值",{"5":{"135":2}}],["近似的理论基础",{"5":{"135":2}}],["近似",{"2":{"135":1},"5":{"135":1}}],["近似距离",{"5":{"70":2}}],["近似分布",{"5":{"61":2}}],["近距离关系",{"5":{"91":2}}],["策略可能会在接收到高回报的随机波动后过度更新",{"5":{"155":2}}],["策略可以改进",{"5":{"154":2}}],["策略都会发生变化",{"5":{"155":2}}],["策略本身通常由神经网络参数化表示",{"5":{"155":2}}],["策略梯度的更新方向是平衡的",{"5":{"155":2}}],["策略梯度为",{"5":{"155":2}}],["策略梯度方法的核心是直接优化策略参数",{"5":{"155":2}}],["策略梯度方法正是为解决这一问题而生的",{"5":{"155":2}}],["策略梯度定理",{"2":{"155":1},"5":{"155":3}}],["策略梯度与actor",{"0":{"155":1},"4":{"110":1,"155":1},"5":{"131":5,"155":1}}],["策略不断更新",{"5":{"154":2}}],["策略比率的进一步减小不会带来额外的目标函数提升",{"5":{"154":2}}],["策略在每次更新后都会发生变化",{"5":{"154":2}}],["策略在大语言模型训练中尤为重要",{"5":{"97":2}}],["策略参数通过",{"5":{"154":2}}],["策略网络参数的大幅更新可能导致新策略与旧策略差异过大",{"5":{"154":2}}],["策略网络的离散动作选择",{"5":{"71":2}}],["策略的logit是参考策略的logit加上奖励项的缩放",{"5":{"101":2}}],["策略优化",{"5":{"154":2}}],["策略优化的目标为",{"5":{"101":2}}],["策略优化阶段",{"5":{"101":2}}],["策略正则化",{"5":{"101":2}}],["策略一",{"5":{"88":2}}],["策略二",{"5":{"88":1}}],["策略",{"5":{"97":2,"136":2,"147":1,"154":2,"156":2,"159":2}}],["策略和余弦退火调度是大语言模型训练的标准配置",{"5":{"97":2}}],["策略进行稳定化",{"5":{"97":2}}],["预处理",{"5":{"134":2}}],["预训练技术等高级主题奠定了基础",{"5":{"140":2}}],["预训练模型的权重矩阵",{"5":{"142":1}}],["预训练模型的权重矩阵包含大量冗余信息",{"5":{"142":1}}],["预训练模型",{"5":{"133":2}}],["预训练阶段",{"5":{"101":2}}],["预测涌现能力的挑战与进展",{"5":{"138":1}}],["预测涌现能力的挑战与进展是当前研究的热点问题",{"5":{"138":1}}],["预测收敛行为",{"5":{"135":2}}],["预测损失",{"5":{"133":2}}],["预测为",{"5":{"101":2}}],["预测概率与真实概率一致",{"5":{"70":2}}],["预测0",{"5":{"70":2}}],["预测值与真实值的差异",{"5":{"70":2}}],["预测样本类别",{"5":{"69":1}}],["预测向量",{"5":{"51":4}}],["预测向量为",{"5":{"51":2}}],["预测的最优问题",{"5":{"51":1}}],["预测空间",{"5":{"51":3}}],["预测空间是矩阵的列空间",{"5":{"51":1}}],["预测",{"5":{"51":1,"69":4,"101":2}}],["预测第",{"5":{"61":2}}],["预测减真实",{"5":{"96":2}}],["预定义",{"5":{"90":2}}],["忽略偏置项的小量贡献",{"5":{"158":2}}],["忽略偏置和激活函数",{"5":{"148":2}}],["忽略的影响",{"5":{"146":1}}],["忽略常数项",{"5":{"145":2}}],["忽略了局部结构和重要性差异",{"5":{"133":2}}],["忽略",{"5":{"88":2,"101":2,"146":3}}],["忽略边界效应",{"5":{"87":2,"152":2}}],["忽略非线性激活的影响",{"5":{"92":2,"153":2}}],["会被实际激活并参与计算",{"5":{"159":2}}],["会转移到一个新的状态",{"5":{"156":2}}],["会提高目标函数值",{"5":{"154":2}}],["会产生较小的梯度",{"5":{"144":2}}],["会发散或急剧变化",{"5":{"138":2}}],["会导致大部分专家参数得不到充分训练",{"5":{"158":2}}],["会导致严重的负载不均衡和通信热点",{"5":{"157":2}}],["会导致监督信号不足",{"5":{"101":2}}],["会导致上下文信息过于稀疏",{"5":{"101":2}}],["会占据绝对主导地位",{"5":{"61":2}}],["会有更多不同尺度的频率成分",{"5":{"91":2}}],["会将所有头的信息混合起来",{"5":{"93":1}}],["会压缩输入空间到低维流形",{"5":{"94":2}}],["会变得非常小",{"5":{"95":1}}],["会影响模型学习位置模式的能力",{"5":{"101":2}}],["带梯度协方差噪声的随机优化可以形式化为",{"5":{"149":1}}],["带梯度协方差噪声的随机优化可以形式化为朗之万动力学",{"5":{"149":1}}],["带宽",{"5":{"137":2}}],["带动量的",{"5":{"134":2}}],["带来额外近似误差",{"5":{"133":2}}],["带来多方面的训练优势",{"5":{"132":2}}],["带参考策略的对比学习",{"5":{"101":2}}],["带kl约束的策略优化",{"5":{"101":2}}],["带掩码的语言建模损失为",{"5":{"101":2}}],["带偏置的softmax",{"5":{"69":2}}],["带置换矩阵",{"5":{"50":2}}],["带有位置编码的输入表示为",{"5":{"89":2}}],["序列空间的基数为",{"5":{"140":1}}],["序列空间的基数",{"5":{"140":2}}],["序列空间记为",{"5":{"140":2}}],["序列空间",{"5":{"140":3}}],["序列生成的条件概率框架",{"2":{"140":1},"5":{"140":1}}],["序列生成任务",{"5":{"101":2}}],["序列的联合概率为",{"5":{"140":2}}],["序列的联合概率分布为",{"5":{"139":2}}],["序列的联合分布可以分解为条件分布的乘积",{"5":{"101":2}}],["序列的联合分布与边际分布",{"5":{"69":2}}],["序列有界",{"5":{"136":1}}],["序列都收敛",{"5":{"136":2}}],["序列标注任务",{"5":{"101":2}}],["序列通常被组织为",{"5":{"101":2}}],["序列长度和嵌入维度",{"5":{"50":2}}],["序列长度和嵌入维度​共同决定了可用的频率范围",{"5":{"90":1}}],["序列长度",{"5":{"50":2,"90":1}}],["序列逐步输入",{"5":{"86":2,"151":2}}],["序列各位置的信息会趋于",{"5":{"87":2,"152":2}}],["序列",{"5":{"90":2,"133":2,"136":1}}],["序列所需的最低频率",{"5":{"90":1}}],["序列中的每个元素依次被处理",{"5":{"91":2}}],["序列中两个位置之间的依赖关系可以用路径长度来度量",{"5":{"92":2,"153":2}}],["序列中任意两个位置之间可以直接建立联系",{"5":{"92":2,"153":2}}],["序列开头和结尾的位置在建模远距离依赖时可能面临额外的挑战",{"5":{"92":2,"153":2}}],["序列顺序",{"5":{"92":2,"153":2}}],["隐藏层到隐藏层的连接",{"5":{"145":2}}],["隐藏层误差信号传播",{"5":{"44":2}}],["隐藏维度通常在数千量级",{"5":{"50":1}}],["隐藏维度",{"5":{"50":1,"93":2,"159":2}}],["隐藏维度为",{"5":{"86":1,"93":1,"95":1,"151":1}}],["隐藏维度为​",{"5":{"93":1}}],["隐藏维度为的输入",{"5":{"86":1,"151":1}}],["隐藏维度为的标准自注意力配置",{"5":{"95":1}}],["隐藏维度12288",{"5":{"93":2}}],["隐式正则化等关键现象",{"5":{"149":2}}],["隐式正则化越弱",{"5":{"149":2}}],["隐式正则化越强",{"5":{"144":2,"149":2}}],["隐式正则化",{"5":{"147":2}}],["隐式正则化的角度",{"5":{"147":2}}],["隐式正则化与优化诱导的偏差",{"2":{"147":1},"5":{"147":1}}],["隐式奖励",{"5":{"101":2}}],["隐式奖励与对齐机制",{"2":{"101":1},"5":{"101":1}}],["隐式应用",{"5":{"88":2}}],["隐式位置编码",{"5":{"88":2}}],["隐式和显式正则化的强度",{"5":{"97":2}}],["掩码向量",{"5":{"145":2}}],["掩码向量与输入向量的逐元素乘法定义了dropout操作",{"5":{"145":1}}],["掩码向量的每个元素独立地服从伯努利分布",{"5":{"145":1}}],["掩码",{"5":{"132":1,"145":2,"159":4}}],["掩码决定了哪些位置对可以被注意到",{"5":{"132":1}}],["掩码率",{"5":{"101":1}}],["掩码率是一个经验性的折中",{"5":{"101":2}}],["掩码率的选择是一个关键的超参数",{"5":{"101":1}}],["掩码率与学习效率",{"5":{"101":2}}],["掩码分布",{"5":{"101":2,"145":1}}],["掩码策略的信息论分析",{"5":{"101":2}}],["掩码策略的数学分析",{"2":{"101":1},"5":{"101":1}}],["掩码语言建模损失",{"5":{"101":2}}],["掩码语言建模损失使用被掩码位置的真实token作为正样本",{"5":{"101":2}}],["掩码语言建模的形式化定义",{"2":{"101":1},"5":{"101":1}}],["掩码语言建模",{"2":{"101":1},"5":{"101":5}}],["掩码操作与位置编码的结合有两种常见方式",{"5":{"91":2}}],["掩码将位置的注意力分数设为",{"5":{"91":1}}],["掩码将位置",{"5":{"91":1}}],["掩码机制等重要内容",{"5":{"95":2}}],["前两个专家承担几乎所有负载",{"5":{"158":2}}],["前端权重的更新幅度会变得极大",{"5":{"148":2}}],["前个奇异值通常能够捕获权重矩阵的绝大部分",{"5":{"142":1}}],["前个维度携带主要信息",{"5":{"89":1}}],["前沿理论与应用",{"5":{"137":2}}],["前缀需要",{"5":{"101":2}}],["前缀部分通常使用双向注意力",{"5":{"101":2}}],["前缀token",{"5":{"101":2}}],["前缀语言建模损失仅对目标token计算损失",{"5":{"101":2}}],["前缀语言建模损失",{"5":{"101":2}}],["前缀语言建模与填充token的数学处理",{"2":{"101":1},"5":{"101":1}}],["前缀",{"5":{"101":2}}],["前层的参数几乎无法接收到关于损失函数的信息",{"5":{"41":2}}],["前层参数保持随机初始化状态",{"5":{"41":2}}],["前向传播为",{"5":{"148":2}}],["前向传播只是完成了",{"5":{"44":2}}],["前向传播保存的中间值",{"5":{"44":2}}],["前向传播",{"5":{"44":3,"45":2,"133":2}}],["前向传播与反向传播的flops比较推论2",{"5":{"44":1}}],["前向传播与反向传播的flops比较",{"5":{"44":1}}],["前向传播的数学本质<",{"5":{"21":1,"131":1}}],["前向传播的数学本质",{"0":{"45":1},"4":{"45":1},"5":{"21":4,"45":1,"131":4}}],["前向传播的计算复杂度",{"2":{"45":1},"5":{"45":1}}],["前向传播的并行性",{"2":{"45":1},"5":{"45":1}}],["前向传播的并行性层次本节从多个数学视角深入分析了前向传播的本质",{"5":{"45":1}}],["前向传播的并行性层次",{"5":{"45":1}}],["前向传播的意义远不止于",{"5":{"45":2}}],["前向传播的核心数学问题可以概括为",{"5":{"45":2}}],["前向传播的矩阵运算形式天然具有良好的并行性",{"5":{"45":2}}],["前向传播的本质是复合函数的求值",{"5":{"45":2}}],["前向传播本质上是一个复合函数的求值过程",{"5":{"45":2}}],["前向传播是这些数学描述在实际运行时的具体执行过程",{"5":{"45":2}}],["前向传播最本质的数学描述是复合函数",{"5":{"45":2}}],["前向传播不仅是一个代数计算过程",{"5":{"45":2}}],["前向传播对应于计算图的拓扑排序",{"5":{"45":2}}],["前向传播在数学上是一个复合函数的逐层求值过程",{"5":{"45":2}}],["前向传播实现了从输入空间到输出空间的几何变换",{"5":{"45":2}}],["前向传播具有良好的并行性",{"5":{"45":2}}],["前向模式",{"5":{"44":2,"45":1}}],["前向模式自动微分",{"5":{"44":1}}],["前向模式自动微分虽然计算效率较低",{"5":{"45":2}}],["前向模式的微分计算",{"5":{"45":2}}],["前向模式与反向模式的比较定义2",{"5":{"44":1}}],["前向模式与反向模式的比较",{"5":{"44":1}}],["前向模式与反向模式自动微分的比较理解前向传播的计算复杂度对于评估网络效率",{"5":{"45":1}}],["前向模式与反向模式自动微分的比较",{"5":{"45":1}}],["前向模式计算​",{"5":{"45":1}}],["前向模式计算",{"5":{"45":1}}],["前向和反向方差稳定",{"5":{"41":1}}],["前向和反向传播使用fp16",{"5":{"157":2}}],["前向和反向传播",{"5":{"97":2}}],["前馈层被替换为moe层",{"5":{"159":2}}],["前馈网络的中间维度为",{"5":{"159":2}}],["前馈网络数学",{"2":{"21":1,"131":1},"4":{"44":1,"45":1,"46":1,"47":1},"5":{"21":9,"131":9}}],["前馈网络中的激活函数决定了信息如何被非线性变换",{"5":{"41":2}}],["前馈网络中的激活函数选择",{"2":{"71":1},"5":{"71":1}}],["前馈网络",{"5":{"45":2,"71":4,"101":2}}],["前馈扩展维度4d=16384",{"5":{"45":2}}],["前馈神经网络中的全连接层更是由多个权重矩阵堆叠而成",{"5":{"50":2}}],["前一层的输出作为后一层的输入",{"5":{"46":2}}],["前几个奇异值就能解释大部分方差",{"5":{"143":2}}],["前几个奇异值占据了大部分能量",{"5":{"87":2,"89":2,"152":2}}],["前几个奇异值较大",{"5":{"87":2,"152":2}}],["前",{"5":{"89":1,"142":1}}],["www",{"5":{"101":2}}],["wwm的优势在于",{"5":{"101":2}}],["wwmlm损失",{"5":{"101":2}}],["wwm策略确保如果一个词的一部分被掩码",{"5":{"101":2}}],["wwm",{"5":{"101":2}}],["wordpiece",{"5":{"101":2}}],["word",{"5":{"94":2,"101":6}}],["within",{"5":{"146":2}}],["wir",{"2":{"21":2,"131":2},"5":{"21":2,"131":2}}],["williams",{"5":{"44":2}}],["wissen",{"2":{"21":2,"131":2},"5":{"21":2,"131":2}}],["wise",{"5":{"50":2,"91":2,"147":4}}],["window",{"5":{"86":4,"151":4}}],["w",{"5":{"42":6,"46":22,"69":6,"86":2,"97":4,"132":16,"135":10,"142":36,"151":2}}],["werden",{"2":{"21":1,"131":1},"5":{"21":1,"131":1}}],["weierstrass逼近定理",{"5":{"71":2}}],["weierstrass定理指出",{"5":{"71":2}}],["weierstrass定理推导出通用逼近能力",{"5":{"71":2}}],["weight变体引入了一个margin参数",{"5":{"158":2}}],["weight损失通常与其他负载均衡策略结合使用",{"5":{"158":2}}],["weight损失关于门控网络参数",{"5":{"158":2}}],["weight损失直接以方差为目标",{"5":{"158":2}}],["weight机制的一个优势是其目标函数是光滑的",{"5":{"158":2}}],["weight机制从统计方差的角度来量化负载不均衡",{"5":{"158":2}}],["weight机制",{"2":{"158":1},"5":{"158":1}}],["weights",{"5":{"132":16}}],["weight",{"5":{"50":2,"157":2,"158":6}}],["weighted",{"5":{"51":2}}],["w^2",{"5":{"46":2}}],["way",{"5":{"145":2}}],["warp",{"5":{"45":1,"93":2}}],["warren",{"5":{"47":2}}],["warmup",{"5":{"48":4,"97":8}}],["warmup策略在训练初期逐渐增加学习率",{"5":{"48":2}}],["walter",{"5":{"47":2}}],["wavelet",{"5":{"90":2,"91":2}}],["wgmma",{"5":{"86":2,"151":2}}],["whole",{"5":{"101":4}}],["what",{"5":{"95":2}}],["where或what",{"5":{"95":2}}],["大约一半的神经元输出为零",{"5":{"148":2}}],["大多数键向量与给定查询向量的相似性都很低",{"5":{"141":2}}],["大多数局部极小值在测试性能上相近",{"5":{"97":2}}],["大规模分布式训练中的噪声结构",{"5":{"149":2}}],["大规模llm",{"5":{"146":1}}],["大规模预训练",{"5":{"140":1}}],["大规模神经网络的",{"5":{"135":2}}],["大到使得",{"5":{"138":2}}],["大学习率可能起到隐式正则化作用",{"5":{"136":2}}],["大学习率的隐式正则化效应",{"5":{"136":2}}],["大特征值数量较少",{"5":{"135":2}}],["大",{"5":{"135":2}}],["大梯度主导",{"5":{"134":2}}],["大批量训练的模型往往泛化性能较差",{"5":{"149":2}}],["大批量cnn",{"5":{"146":1}}],["大批量",{"5":{"134":2}}],["大模型有足够的容量分离学习这两个分布",{"5":{"138":2}}],["大模型参数量",{"5":{"137":1}}],["大模型参数量与数据量的比值影响互信息",{"5":{"137":1}}],["大模型的特殊考虑",{"5":{"140":2}}],["大模型的涌现能力可用信息论临界点描述",{"5":{"137":2}}],["大模型的互信息特性",{"5":{"137":2}}],["大模型特殊现象",{"5":{"135":2}}],["大模型剪枝的信息论准则",{"5":{"133":2}}],["大模型中的似然函数形式",{"2":{"139":1},"5":{"139":1}}],["大模型中的信息论前沿",{"2":{"137":1},"5":{"137":1}}],["大模型中的特殊现象",{"2":{"135":1},"5":{"135":1}}],["大模型中的数学",{"0":{"21":1,"131":1},"4":{"21":1},"5":{"21":1,"131":1}}],["大模型中低秩近似的数学依据<",{"5":{"131":1}}],["大模型中低秩近似的数学依据",{"0":{"142":1},"4":{"142":1},"5":{"131":4,"142":1}}],["大卫",{"2":{"21":1,"131":1},"5":{"21":1,"44":2,"131":1}}],["大大提高了计算效率",{"5":{"47":2}}],["大数定律与中心极限定理",{"2":{"96":1},"5":{"96":1}}],["大数定律",{"5":{"96":2}}],["大数定律解释了为什么使用更大的批量",{"5":{"96":2}}],["大数定律和中心极限定理是概率论中最重要的极限定理",{"5":{"96":2}}],["大数定律和中心极限定理也指导着模型评估策略",{"5":{"96":2}}],["大语言模型最引人注目的特征之一是",{"5":{"138":2}}],["大语言模型通常在多种任务上进行联合训练",{"5":{"101":2}}],["大语言模型通常使用子词分词器",{"5":{"101":2}}],["大语言模型的参数量通常在数十亿到数千亿规模",{"5":{"154":2}}],["大语言模型的涌现也可以被理解为一种",{"5":{"138":2}}],["大语言模型的崛起标志着人工智能领域进入了一个新的纪元",{"5":{"138":2}}],["大语言模型的训练涉及多个层面的损失函数设计",{"5":{"101":2}}],["大语言模型的训练本质上是一个优化问题",{"5":{"48":2}}],["大语言模型的训练通常对学习率非常敏感",{"5":{"97":2}}],["大语言模型的优化面临独特的挑战",{"5":{"48":2}}],["大语言模型的某些扩展",{"5":{"96":2}}],["大语言模型中的低秩适应",{"2":{"142":1},"5":{"142":1}}],["大语言模型中的典型多任务设置",{"2":{"101":1},"5":{"101":1}}],["大语言模型中的损失函数<",{"5":{"131":1}}],["大语言模型中的损失函数",{"0":{"101":1},"4":{"101":1},"5":{"101":1,"131":4}}],["大语言模型中的所有参数最终都以矩阵或张量的形式存储和计算",{"5":{"50":2}}],["大语言模型中的输入通常是一个三维张量",{"5":{"50":2}}],["大语言模型中的实际损失函数通常是非凸的",{"5":{"97":2}}],["大语言模型由于其巨大的参数量和复杂架构",{"5":{"97":2}}],["大小",{"5":{"50":2}}],["大小为",{"5":{"86":2,"151":2}}],["大量这样的局部探测器通过线性组合",{"5":{"71":2}}],["大量实验表明",{"5":{"88":2}}],["大部分方向上的变化很小",{"5":{"89":2}}],["大的学习率跳过尖锐最小值区域",{"5":{"136":2}}],["大的特征值",{"5":{"135":2}}],["大的",{"5":{"90":5,"91":2}}],["大的​",{"5":{"90":1}}],["大时",{"5":{"91":1}}],["大熊猫的主要食物是什么",{"5":{"94":4}}],["<span",{"2":{"154":1,"155":1,"156":3}}],["<a",{"5":{"21":1,"131":1}}],["<",{"5":{"42":2,"131":1,"133":2,"136":3,"144":2}}],["<img",{"5":{"44":1,"45":2,"47":2,"48":2,"50":2,"61":1,"96":3,"143":1,"148":2}}],["<br>",{"5":{"45":1,"47":1,"48":1,"50":1,"96":2,"148":1}}],["<br><a",{"5":{"21":9,"131":50}}],["<br><img",{"5":{"48":1}}],["><",{"2":{"154":5,"155":7,"156":13}}],["><mjx",{"2":{"154":9,"155":11,"156":24}}],[">双曲正切函数定义为​",{"5":{"148":1}}],[">9",{"5":{"131":3}}],[">8",{"5":{"131":3}}],[">7",{"5":{"131":3}}],[">4",{"5":{"131":6}}],[">2",{"5":{"21":4,"131":4}}],[">14",{"5":{"131":3}}],[">13",{"5":{"131":3}}],[">12",{"5":{"131":3}}],[">11",{"5":{"131":3}}],[">10",{"5":{"131":3}}],[">1",{"5":{"21":3,"131":3}}],[">3",{"5":{"21":3,"131":3}}],[">",{"5":{"42":5,"44":1,"45":1,"47":1,"48":1,"50":2,"61":1,"96":1,"132":1,"133":1,"135":1,"136":3,"143":1,"144":3,"148":1}}],[">从前向传播的数学描述到实际代码实现",{"5":{"45":1}}],[">数学上",{"5":{"47":1}}],[">矩阵形式的链式法则在深度学习中尤为重要",{"5":{"48":1}}],[">k",{"5":{"48":1}}],[">期望",{"5":{"96":1}}],[">在大语言模型的训练中",{"5":{"96":1}}],[">5",{"5":{"131":7}}],[">6",{"5":{"131":4}}],["hc",{"2":{"150":1},"5":{"150":1}}],["human",{"5":{"101":2,"154":2}}],["huber损失是mse和mae的组合",{"5":{"51":2}}],["huber损失在误差较小时使用mse",{"5":{"51":2}}],["huber损失",{"5":{"51":2}}],["href=",{"5":{"21":20,"131":102}}],["html",{"4":{"21":1,"41":1,"42":1,"44":1,"45":1,"46":1,"47":1,"48":1,"50":1,"51":1,"61":1,"69":1,"70":1,"71":1,"86":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"94":1,"95":1,"96":1,"97":1,"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"131":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"156":1,"157":1,"158":1,"159":1},"5":{"21":20,"131":102}}],["hadamard",{"5":{"158":2}}],["hadamard积",{"5":{"44":2,"146":2}}],["hacking",{"5":{"154":2}}],["hard",{"5":{"144":2}}],["hartman",{"5":{"134":2,"135":4}}],["hartleys",{"5":{"61":2}}],["hat",{"5":{"42":108,"46":2,"61":92,"97":14}}],["he",{"5":{"135":4,"148":3}}],["heavy",{"5":{"134":2}}],["heads",{"5":{"132":28}}],["head",{"5":{"70":2,"93":6,"132":20}}],["he初始化确实能够显著改善梯度流",{"5":{"148":2}}],["he初始化的权重方差更大",{"5":{"148":2}}],["he初始化的推导考虑了relu的",{"5":{"41":2}}],["he初始化",{"2":{"148":1},"5":{"41":2,"148":3}}],["he初始化将权重初始化为",{"5":{"41":2}}],["he初始化将权重方差设置为​",{"5":{"41":1}}],["he初始化将权重方差设置为",{"5":{"41":1}}],["he初始化使用均值为0",{"5":{"96":2}}],["he方差",{"5":{"41":1}}],["hessian特征值小",{"5":{"149":2}}],["hessian特征值大",{"5":{"149":2}}],["hessian接近单位矩阵的倍数",{"5":{"144":2}}],["hessian",{"0":{"135":1},"2":{"135":6},"4":{"135":1},"5":{"70":24,"97":24,"131":5,"134":22,"135":75,"136":12}}],["hessian矩阵之间的深刻联系",{"5":{"149":2}}],["hessian矩阵的所有特征值都增加了",{"5":{"144":2}}],["hessian矩阵的奇异值分解揭示了函数的局部曲率结构",{"5":{"48":2}}],["hessian矩阵与曲率分析",{"2":{"70":1,"97":1},"5":{"70":1,"97":1}}],["hessian矩阵是多元函数的二阶导数矩阵",{"5":{"48":2}}],["hessian矩阵定义为",{"5":{"48":2}}],["hessian矩阵在优化中的作用主要体现在以下几个方面",{"5":{"48":2}}],["hessian矩阵决定了泰勒展开的二次项",{"5":{"48":2}}],["hessian矩阵",{"5":{"97":2}}],["hessian的特征值表示函数在不同特征方向上的曲率",{"5":{"48":2}}],["h",{"5":{"42":34,"46":6,"69":22,"86":5,"132":62,"135":4,"137":16,"151":5}}],["hooi迭代",{"5":{"143":1}}],["hooi算法通过交替更新各模的因子矩阵来优化tucker分解的最小二乘目标",{"5":{"143":2}}],["hooi算法框架",{"5":{"143":2}}],["hooi的收敛性",{"5":{"143":2}}],["hooi",{"5":{"143":2}}],["hosvd分解中的因子矩阵",{"5":{"143":1}}],["hosvd分解中的因子矩阵是正交的",{"5":{"143":1}}],["hosvd和hooi算法为其计算提供了有效手段",{"5":{"143":2}}],["hosvd提供了一种计算tucker分解的启发式方法",{"5":{"143":2}}],["hosvd与最佳低秩tucker分解",{"5":{"143":2}}],["hosvd的性质",{"5":{"143":2}}],["hosvd",{"5":{"143":2}}],["hopf",{"5":{"136":16}}],["hot极限的秩分析",{"5":{"141":2}}],["hot分布",{"5":{"41":2,"69":2,"87":2,"95":2,"140":2,"152":2,"157":2,"159":2}}],["hot分布时",{"5":{"41":2,"87":2,"152":2}}],["hot分布时取等号",{"5":{"87":2,"152":2}}],["hot时",{"5":{"41":2}}],["hot采样",{"5":{"71":4}}],["hot编码",{"5":{"96":8,"139":2}}],["hot编码时",{"5":{"61":2}}],["hot编码的真实分布",{"5":{"61":2}}],["hot向量",{"5":{"61":2,"70":2,"141":2}}],["hot",{"5":{"61":2,"87":2,"94":2,"137":2,"152":2}}],["hot注意力",{"5":{"87":2,"152":2}}],["homogeneity",{"5":{"71":2}}],["homogeneous",{"5":{"46":2,"47":2}}],["householder变换和givens旋转",{"5":{"50":2}}],["history",{"5":{"136":6}}],["hilbert",{"2":{"21":1,"131":1},"5":{"21":1,"131":1}}],["hinton",{"5":{"44":2}}],["hidden",{"5":{"50":2}}],["hierarchical",{"5":{"50":2}}],["highway块近似为恒等映射",{"5":{"150":2}}],["highway块主要执行变换",{"5":{"150":2}}],["highway",{"2":{"150":2},"5":{"150":18}}],["high",{"5":{"91":2}}],["hybrid",{"5":{"86":2,"88":2,"151":2}}],["在专家并行策略下",{"5":{"159":2}}],["在专家坍缩状态下",{"5":{"158":2,"159":2}}],["在连续的计算中依次处理不同专家的相同位置",{"5":{"159":2}}],["在稠密模型中",{"5":{"159":2}}],["在语言建模等任务上",{"5":{"159":2}}],["在语言模型中",{"5":{"96":12}}],["在语言模型研究中",{"5":{"96":2}}],["在语言模型的某些应用中",{"5":{"96":2}}],["在语言模型的评估中",{"5":{"96":2}}],["在语言模型训练中",{"5":{"96":2}}],["在语言模型评估中",{"5":{"96":2}}],["在传统的神经网络中",{"5":{"159":2}}],["在传统的加性位置编码中",{"5":{"88":2}}],["在门控得分中注入高斯噪声",{"5":{"158":2}}],["在负载均衡损失",{"5":{"158":2}}],["在负无穷处趋向于饱和值0",{"5":{"42":2}}],["在后期减小",{"5":{"159":2}}],["在后期训练阶段减小",{"5":{"158":2}}],["在后期精细调优",{"5":{"48":2}}],["在早期训练阶段使用较大的",{"5":{"158":2}}],["在随机初始化或期望意义上",{"5":{"158":2}}],["在随机梯度下降中",{"5":{"135":2}}],["在top",{"5":{"158":2}}],["在transformer架构的具体实现中",{"5":{"158":2}}],["在transformer架构中",{"5":{"94":4,"101":2}}],["在transformer架构中扮演着关键角色",{"5":{"42":2}}],["在transformer的语境下",{"5":{"69":2}}],["在transformer的自注意力机制中",{"5":{"71":2}}],["在transformer的训练中",{"5":{"97":2}}],["在transformer中的应用",{"5":{"150":1}}],["在transformer中的应用是layer",{"5":{"150":1}}],["在transformer中",{"5":{"41":4,"71":2,"92":4,"146":2,"153":4}}],["在稀疏激活的top",{"5":{"158":2}}],["在本设备上的专家",{"5":{"157":2}}],["在本节中",{"5":{"46":2}}],["在胖树拓扑中",{"5":{"157":2}}],["在环形拓扑中",{"5":{"157":2}}],["在持续不断的任务中",{"5":{"156":2}}],["在当前批次中的分配比例为",{"5":{"159":2}}],["在当前批次中的平均路由概率",{"5":{"158":2}}],["在当前批次中从未被激活",{"5":{"158":2}}],["在当前批次中被选中的频率",{"5":{"158":2}}],["在当前状态",{"5":{"156":4}}],["在当前输入中不被使用",{"5":{"89":2}}],["在理解了马尔可夫决策过程的基本框架后",{"5":{"155":2}}],["在理想的均衡状态下",{"5":{"159":2}}],["在理想的条件下",{"5":{"140":2}}],["在理想情况下",{"5":{"132":4,"140":2}}],["在于它衡量了两个概率分布之间的",{"5":{"154":1}}],["在于它们允许模型恢复归一化操作可能丢失的表示能力",{"5":{"150":1}}],["在actor",{"5":{"154":2,"155":2}}],["在attention分数上添加一个与相对距离成指数衰减的偏置",{"5":{"88":2}}],["在attention计算前编码和在attention计算中隐式应用",{"5":{"88":1}}],["在attention计算前编码",{"5":{"88":1}}],["在attention计算中隐式应用",{"5":{"88":1}}],["在状态",{"5":{"154":2,"155":4}}],["在强化学习中扮演着至关重要的角色",{"5":{"156":2}}],["在强化学习的上下文中",{"5":{"154":2}}],["在强化学习与语言模型的结合中",{"5":{"96":2}}],["在残差连接之前应用layer",{"5":{"150":2}}],["在残差连接之后应用layer",{"5":{"150":2}}],["在残差学习的框架下",{"5":{"150":2}}],["在没有残差连接的网络中",{"5":{"150":2}}],["在鞍点方向上",{"5":{"149":2}}],["在鞍点处",{"5":{"149":2}}],["在hessian特征值为负的方向",{"5":{"149":2}}],["在曲率较陡的方向",{"5":{"149":2}}],["在曲率小的方向上更弱",{"5":{"149":2}}],["在远离最小值点的区域",{"5":{"149":2}}],["在远离原点的区域",{"5":{"144":2}}],["在平坦方向",{"5":{"149":4}}],["在平衡点",{"5":{"134":4}}],["在上述条件下",{"5":{"149":2}}],["在上一节中",{"5":{"46":2,"94":2,"154":2}}],["在上一节的梯度推导中",{"5":{"61":2}}],["在以下条件下",{"5":{"149":2}}],["在以下意义上具有最优的泛化性能",{"5":{"133":1}}],["在活跃区域",{"5":{"148":1}}],["在使用relu激活的深层网络中",{"5":{"148":2}}],["在使用梯度裁剪时",{"5":{"144":2}}],["在另一些方向有负特征值",{"5":{"149":2}}],["在另一些方向上压缩信号",{"5":{"148":2}}],["在另一侧",{"5":{"71":2}}],["在附近不会压缩梯度",{"5":{"148":1}}],["在外部区域",{"5":{"147":2}}],["在内部区域",{"5":{"147":2}}],["在gpt",{"5":{"146":2}}],["在gpu上",{"5":{"89":2,"95":2}}],["在cnn中",{"5":{"146":2}}],["在批量维度",{"5":{"146":1}}],["在批量维度上求和",{"5":{"146":1}}],["在批处理情况下变成",{"5":{"50":1}}],["在层归一化中",{"5":{"146":2}}],["在层网络中",{"5":{"45":1}}],["在归一化公式的分母中",{"5":{"146":2}}],["在有限步数后收敛到的解与最小范数解有某种等价性",{"5":{"147":2}}],["在有限样本下通常表现更好",{"5":{"87":2,"152":2}}],["在有放缩版本中",{"5":{"145":2}}],["在全连接网络中",{"5":{"148":2}}],["在全连接神经网络中",{"5":{"145":2}}],["在全连接层中",{"5":{"46":2}}],["在均值场近似下",{"5":{"145":2}}],["在均方误差中",{"5":{"70":2}}],["在均方误差最小的意义下",{"5":{"51":2}}],["在不同网络结构中的应用",{"2":{"145":1},"5":{"145":1}}],["在不动点处取值",{"5":{"136":2}}],["在损失曲面几何方面",{"5":{"144":2}}],["在数学上",{"5":{"156":2,"158":2}}],["在数学上更加易于处理",{"5":{"144":2}}],["在数据并行或模型并行训练中",{"5":{"149":2}}],["在数值上完全不可检测",{"5":{"41":2,"42":2}}],["在张量代数部分",{"5":{"143":2}}],["在许多任务中",{"5":{"146":2}}],["在许多情况下是不必要的",{"5":{"146":2}}],["在许多实际应用中",{"5":{"143":2}}],["在许多场景下",{"5":{"41":2}}],["在流形",{"5":{"142":1}}],["在流形上的梯度下降等价于直接优化和的梯度更新",{"5":{"142":1}}],["在较小的情况下",{"5":{"142":1}}],["在经过充分训练的深度网络中",{"5":{"142":2}}],["在jl引理的条件下",{"5":{"141":2}}],["在最优参数",{"5":{"139":2}}],["在最小奇异值方向上可能产生相对较大的输出梯度",{"5":{"41":2}}],["在最小二乘问题中特别有用",{"5":{"48":2}}],["在物理学中",{"5":{"138":2}}],["在物理学和机器学习中都有重要应用",{"5":{"50":2}}],["在规模",{"5":{"138":2}}],["在小模型中完全不存在",{"5":{"138":2}}],["在保留关于输入",{"5":{"137":1}}],["在保持高效学习的同时确保训练稳定性",{"5":{"154":2}}],["在保持信息的同时实现归一化",{"5":{"132":2}}],["在保持正区域良好梯度的同时",{"5":{"41":2}}],["在保持表达能力的同时降低计算成本",{"5":{"93":2}}],["在给定固定计算预算的条件下",{"5":{"138":2}}],["在给定",{"5":{"137":1}}],["在或处取得最小值",{"5":{"137":1}}],["在学习率",{"5":{"136":2}}],["在动态系统中",{"5":{"135":2}}],["在统计学习理论中",{"5":{"140":2}}],["在统计学和机器学习中有着核心地位",{"5":{"96":2}}],["在统计效果上等价",{"5":{"134":2}}],["在适当假设下",{"5":{"134":4}}],["在适当的条件下",{"5":{"149":2}}],["在适当的参数重缩放下",{"5":{"145":2}}],["在适当的光滑性假设下",{"5":{"135":2}}],["在适当的学习率下",{"5":{"134":2}}],["在适当的归一化假设下",{"5":{"132":2}}],["在适当的噪声水平下",{"5":{"48":2}}],["在临界点附近",{"5":{"138":2}}],["在临界点",{"5":{"134":2,"135":2}}],["在梯度下降中",{"5":{"144":2}}],["在梯度下降系统中",{"5":{"134":2,"136":2}}],["在梯度传播过程中",{"5":{"41":2}}],["在该平面上的位置",{"5":{"133":1}}],["在过参数化",{"5":{"133":2}}],["在信息瓶颈框架中",{"5":{"133":2,"137":4}}],["在压缩与预测之间取得平衡",{"5":{"133":2}}],["在压缩约束下尽可能保留预测所需的信息",{"5":{"133":2}}],["在压缩输入信息与保留输出信息之间取得最优权衡",{"5":{"133":2}}],["在编码器",{"5":{"132":2}}],["在置换下保持不变",{"5":{"101":1}}],["在生成序列时",{"5":{"140":2}}],["在生成过程中",{"5":{"101":2}}],["在生成模型中",{"5":{"96":2}}],["在监督微调阶段",{"5":{"101":2}}],["在预训练阶段",{"5":{"101":2}}],["在预测空间上的正交投影",{"5":{"51":1}}],["在条件分布",{"5":{"70":1}}],["在几何结构上都与概率流形的几何性质紧密相关",{"5":{"70":2}}],["在几何上对仿射变换的结果执行了逐元素的非线性",{"5":{"45":1}}],["在几何上称为仿射变换",{"5":{"47":2}}],["在几何上等价于线性变换",{"5":{"45":1}}],["在几何上等价于先进行线性变换",{"5":{"47":1}}],["在几何上",{"5":{"88":2}}],["在这些场景中",{"5":{"146":2}}],["在这种对应关系下",{"5":{"149":2}}],["在这种简化下",{"5":{"148":2}}],["在这种分岔中",{"5":{"136":2}}],["在这种情况下",{"5":{"70":2,"143":2}}],["在这个标准的前馈层中",{"5":{"159":2}}],["在这个框架中",{"5":{"155":2}}],["在这个框架下",{"5":{"70":2,"71":2}}],["在这个阶段",{"5":{"154":2}}],["在这个度量下",{"5":{"70":2,"71":2}}],["在这个假设下",{"5":{"51":2}}],["在这个推导中",{"5":{"51":2}}],["在这个表达式中",{"5":{"92":2,"153":2}}],["在这个场景中",{"5":{"94":2}}],["在回归任务中",{"5":{"70":2}}],["在局部线性化下",{"5":{"70":2}}],["在结构上高度相似",{"5":{"70":2}}],["在位置",{"5":{"69":1}}],["在位置的",{"5":{"69":1}}],["在位置编码的语境下",{"5":{"90":4}}],["在候选集合",{"5":{"69":2}}],["在候选集合中定义一个",{"5":{"69":2}}],["在infonce中",{"5":{"69":2}}],["在5",{"5":{"69":4}}],["在向量空间中",{"5":{"69":2}}],["在4",{"5":{"69":6}}],["在介绍infonce之前",{"5":{"69":2}}],["在一些mhc的实现中",{"5":{"150":2}}],["在一般情况下成立",{"5":{"137":2}}],["在一定的正则性条件下",{"5":{"69":2,"101":2}}],["在一侧",{"5":{"71":2}}],["在前两节建立了动态系统框架和",{"5":{"136":2}}],["在前两节中",{"5":{"70":2}}],["在前一节将神经网络训练建模为离散时间动态系统的基础上",{"5":{"135":2}}],["在前四节中",{"5":{"101":2}}],["在前向传播时使用硬top",{"5":{"159":2}}],["在前向传播时只保存部分层的激活",{"5":{"157":2}}],["在前向传播中",{"5":{"148":2,"157":4}}],["在前向传播过程中",{"5":{"50":2}}],["在前向模式自动微分中",{"5":{"45":2}}],["在前面的几节中",{"5":{"86":2,"151":2}}],["在前面几节中",{"5":{"87":2,"152":2}}],["在前面两节中",{"5":{"93":2}}],["在前五节中",{"5":{"97":2}}],["在实现注意力机制的梯度计算时",{"5":{"70":2}}],["在实现反向传播时",{"5":{"50":2,"61":2}}],["在实现中",{"5":{"95":2,"159":2}}],["在实际的大规模moe实现中",{"5":{"159":2}}],["在实际的神经网络中",{"5":{"145":2}}],["在实际的深度学习框架中",{"5":{"145":2}}],["在实际的预训练数据处理中",{"5":{"101":2}}],["在实际优化中",{"5":{"140":2}}],["在实际深度学习训练中",{"5":{"136":2}}],["在实际深度学习框架中",{"5":{"61":2,"147":2}}],["在实际应用中",{"5":{"41":2,"47":2,"51":2,"70":2,"88":2,"91":2,"92":2,"94":2,"95":2,"96":4,"136":2,"137":2,"140":4,"146":2,"148":2,"153":2,"154":2,"155":6,"158":2,"159":2}}],["在实际训练中",{"5":{"41":2,"44":2,"138":2,"158":2,"159":2}}],["在实际实现反向传播时",{"5":{"44":2}}],["在实际实现中",{"5":{"50":2,"61":2,"88":2,"93":2,"96":2,"101":2,"154":2,"158":2}}],["在实际计算中也具有重要的应用价值",{"5":{"42":2}}],["在实际计算中",{"5":{"61":2}}],["在实际编码中",{"5":{"90":2}}],["在实际中",{"5":{"97":2}}],["在实践中需要仔细权衡负载均衡与专家专业化之间的关系",{"5":{"158":2}}],["在实践中",{"5":{"41":4,"50":2,"61":2,"70":2,"71":2,"88":2,"92":2,"93":2,"94":2,"95":2,"97":4,"138":2,"147":2,"149":2,"153":2,"155":4,"159":4}}],["在实践中使用作为近似",{"5":{"41":1}}],["在实践中使用",{"5":{"41":1}}],["在实践中通常有",{"5":{"95":2}}],["在",{"5":{"41":2,"42":8,"44":2,"45":2,"51":1,"61":4,"70":4,"96":1,"97":8,"135":2,"136":2,"137":5,"142":1,"145":2,"147":6,"148":1,"149":1,"157":2,"158":6}}],["在时",{"5":{"41":2}}],["在时间步",{"5":{"156":2}}],["在时间反向传播",{"5":{"41":2}}],["在时间序列预测中",{"5":{"51":2}}],["在极端情况下",{"5":{"41":2}}],["在线学习能力",{"5":{"155":2}}],["在线学习",{"5":{"41":2}}],["在线性模型和均方损失下",{"5":{"145":2}}],["在线性模型中",{"5":{"71":2}}],["在线性区域",{"5":{"41":2}}],["在线性代数的学习路径中",{"5":{"50":2}}],["在ppo训练过程中",{"5":{"154":2}}],["在ppo",{"5":{"154":2}}],["在prefixlm的实际实现中",{"5":{"101":2}}],["在prefix",{"5":{"101":2}}],["在pre",{"5":{"41":4,"146":2}}],["在post",{"5":{"41":2}}],["在pytorch中",{"5":{"50":2}}],["在pca中",{"5":{"94":2}}],["在处理一段包含代码和自然语言的混合文本时",{"5":{"158":2}}],["在处理变长序列时",{"5":{"150":2}}],["在处理不平衡数据时",{"5":{"51":2}}],["在处的有效扩散张量为",{"5":{"149":1}}],["在处对做",{"5":{"137":1}}],["在处",{"5":{"42":2}}],["在处有拐点",{"5":{"42":1}}],["在处有",{"5":{"42":1}}],["在形式上非常相似",{"5":{"42":1}}],["在深入矩阵情形之前",{"5":{"148":2}}],["在深入各种具体归一化方法之前",{"5":{"146":2}}],["在深入探讨大语言模型的数学原理之前",{"5":{"140":2}}],["在深入数学细节之前",{"5":{"47":2}}],["在深入数学推导之前",{"5":{"93":2}}],["在深层网络中",{"5":{"41":2,"42":2,"150":2}}],["在深度神经网络训练过程中",{"5":{"147":2}}],["在深度神经网络中",{"5":{"146":2,"149":2,"150":2}}],["在深度网络中",{"5":{"142":2}}],["在深度网络的损失",{"5":{"134":2}}],["在深度学习实践中",{"5":{"147":2}}],["在深度学习分类任务中",{"5":{"137":4}}],["在深度学习训练中",{"5":{"136":2}}],["在深度学习训练过程中",{"5":{"136":2}}],["在深度学习中的实现",{"2":{"139":1},"5":{"139":1}}],["在深度学习中用于分析模型预测的不确定性",{"5":{"137":2}}],["在深度学习中相比",{"5":{"134":2}}],["在深度学习中",{"5":{"48":4,"50":6,"96":4,"137":6,"139":2}}],["在深度学习中有着广泛的应用",{"5":{"50":2}}],["在深度学习优化中",{"5":{"48":2}}],["在深度学习的发展历程中",{"5":{"150":2}}],["在深度学习的实际情况下",{"5":{"149":2}}],["在深度学习的实践中",{"5":{"144":2}}],["在深度学习的优化理论中",{"5":{"149":2}}],["在深度学习的优化过程中",{"5":{"61":2}}],["在深度学习的数学基础中",{"5":{"143":2}}],["在深度学习的反向传播算法中",{"5":{"48":2}}],["在深度学习的分类任务中",{"5":{"61":2}}],["在深度学习序列建模的发展历程中",{"5":{"91":2}}],["在深度维度上划分层",{"5":{"50":2}}],["在其存在的范围内",{"5":{"96":2}}],["在其他区域抑制",{"5":{"71":2}}],["在其他类别上均匀分配的概率",{"5":{"96":1}}],["在其他类别上均匀分配",{"5":{"96":1}}],["在其他位置为0",{"5":{"95":2}}],["在其开创性论文",{"5":{"61":2,"137":2}}],["在伯努利分布中",{"5":{"71":2}}],["在逻辑回归中",{"5":{"71":2}}],["在多模态大模型中",{"5":{"137":2}}],["在多任务",{"5":{"70":2}}],["在多任务学习中",{"5":{"70":2,"101":4,"133":4}}],["在多头注意力中",{"5":{"41":2,"50":2,"92":2,"93":4,"153":2}}],["在多头注意力中被广泛使用",{"5":{"93":2}}],["在多项式分布中",{"5":{"71":2}}],["在多类分类中为one",{"5":{"96":2}}],["在多层神经网络中",{"5":{"44":2}}],["在多层之间共享同一个位置编码矩阵",{"5":{"89":2}}],["在需要从离散分布采样的场景中",{"5":{"71":2}}],["在任何输入下",{"5":{"71":2}}],["在自监督学习中",{"5":{"71":2,"137":2}}],["在自然语言中",{"5":{"88":4}}],["在自然语言处理中",{"5":{"96":2}}],["在自然语言处理的语境下",{"5":{"94":2,"95":2}}],["在自注意力机制中",{"5":{"41":4,"69":2,"92":2,"153":2}}],["在自注意力中",{"5":{"92":6,"153":6}}],["在自回归语言模型中",{"5":{"95":2}}],["在整个输入范围内应该接近1",{"5":{"41":2}}],["在整个实数轴上都是正的",{"5":{"71":2}}],["在2020年的开创性工作中",{"5":{"138":2}}],["在2",{"5":{"44":2,"45":2}}],["在获得各层误差信号后",{"5":{"44":2}}],["在混合精度训练中",{"5":{"44":2,"157":2}}],["在第10",{"5":{"144":2}}],["在第六章中",{"5":{"70":2}}],["在第五章和第六章中",{"5":{"70":2}}],["在第四章的前三节中",{"5":{"69":2}}],["在第四章中",{"5":{"71":2}}],["在第2",{"5":{"45":2}}],["在第二和第三维度上的收缩产生一个",{"5":{"50":1}}],["在第层",{"5":{"92":1,"153":1}}],["在第",{"5":{"92":2,"153":2}}],["在每一层使用相同的保留概率",{"5":{"145":2}}],["在每一层提供必要的非线性变换",{"5":{"71":2}}],["在每点处的几何形状",{"5":{"135":2}}],["在每个时间步",{"5":{"156":2}}],["在每个特征通道",{"5":{"146":2}}],["在每个象限内",{"5":{"45":2}}],["在每个频率维度上",{"5":{"90":2}}],["在每个子空间内计算相对简单的注意力模式",{"5":{"93":2}}],["在反向传播过程中",{"5":{"148":2}}],["在反向传播时使用软梯度近似",{"5":{"159":2}}],["在反向传播时",{"5":{"145":2,"157":2}}],["在反向传播时重新计算被省略的层的激活",{"5":{"157":2}}],["在反向传播时重新计算被省略的激活值来节省内存",{"5":{"50":2}}],["在反向传播时重新计算其余激活值",{"5":{"48":2}}],["在反向传播中",{"5":{"41":8,"42":6,"44":2,"71":2,"87":2,"144":2,"148":6,"152":2,"157":4}}],["在反向传播中需要​",{"5":{"41":1}}],["在反向传播中需要",{"5":{"41":1}}],["在反向传播算法中",{"5":{"42":2}}],["在反向模式自动微分中",{"5":{"45":2}}],["在此处按元素应用于向量",{"5":{"46":2}}],["在神经网络训练中",{"5":{"136":4}}],["在神经网络训练的背景下",{"5":{"136":2}}],["在神经网络中",{"5":{"50":2,"71":2,"135":2}}],["在神经网络的语境下",{"5":{"47":2}}],["在神经网络的稳定性分析和lipschitz约束中",{"5":{"50":2}}],["在计算时",{"5":{"157":2}}],["在计算上也是高效的",{"5":{"140":2}}],["在计算最优配置下",{"5":{"138":2}}],["在计算机图形学和计算机视觉中",{"5":{"47":2}}],["在计算损失和梯度时",{"5":{"47":2}}],["在计算图中",{"5":{"48":2}}],["在计算注意力分数时",{"5":{"92":2,"153":2}}],["在计算复杂度上",{"5":{"92":2,"153":2}}],["在与梯度相反的方向",{"5":{"48":2}}],["在与梯度垂直的方向",{"5":{"48":2}}],["在贝叶斯深度学习和变分推断中",{"5":{"48":2}}],["在训练时就进行缩放",{"5":{"145":1}}],["在训练时直接应用dropout掩码",{"5":{"145":1}}],["在训练时使用teacher",{"5":{"140":2}}],["在训练阶段",{"5":{"140":4}}],["在训练集上表现良好但在测试集上表现差",{"5":{"138":2}}],["在训练后期",{"5":{"144":2}}],["在训练后期主导",{"5":{"135":2}}],["在训练后期使用较低的温度",{"5":{"95":2}}],["在训练早期就快速收敛到极端不均衡的状态",{"5":{"158":2}}],["在训练早期",{"5":{"133":2}}],["在训练大语言模型中尤为重要",{"5":{"48":2}}],["在训练神经网络时",{"5":{"61":2}}],["在训练良好的transformer中",{"5":{"87":2,"92":2,"152":2,"153":2}}],["在训练过程中不断调整",{"5":{"158":2}}],["在训练过程中动态调整温度以平衡探索与利用",{"5":{"132":2}}],["在训练过程中",{"5":{"87":2,"89":2,"94":2,"133":2,"138":2,"149":2,"152":2,"158":2}}],["在训练过程中通过梯度下降进行优化",{"5":{"89":2}}],["在训练动态分析部分",{"5":{"89":2}}],["在训练初期使用较大的噪声促进探索",{"5":{"159":2}}],["在训练初期使用较高的温度",{"5":{"95":2}}],["在训练初期",{"5":{"144":2,"150":2}}],["在训练初期可能显著",{"5":{"135":2}}],["在训练初期逐渐增加学习率",{"5":{"48":2,"97":2}}],["在凸优化问题中",{"5":{"48":2}}],["在点​附近",{"5":{"48":1}}],["在点",{"5":{"48":3,"135":2}}],["在函数的等值面上",{"5":{"48":1}}],["在函数",{"5":{"48":1}}],["在权重初始化方面",{"5":{"96":2}}],["在输入空间中定义了一个",{"5":{"71":1}}],["在输入空间中形成一个",{"5":{"71":1}}],["在输出建模方面",{"5":{"96":2}}],["在输出层产生一个与词汇表大小相同的logits向量",{"5":{"96":2}}],["在高维参数空间中",{"5":{"149":2}}],["在高维输入空间中",{"5":{"71":2}}],["在高维空间中",{"5":{"48":2,"51":2,"141":2}}],["在高维度",{"5":{"91":2}}],["在高维度区域的热力图模式相似度较高",{"5":{"91":2}}],["在高斯过程和贝叶斯优化中",{"5":{"96":2}}],["在高斯过程回归和二次优化中有广泛应用",{"5":{"50":2}}],["在高斯噪声假设下",{"5":{"51":2}}],["在高层整合全局信息",{"5":{"92":2,"153":2}}],["在vae中",{"5":{"96":2}}],["在知识蒸馏中",{"5":{"96":2}}],["在正则性条件下",{"5":{"140":2}}],["在正则条件下",{"5":{"96":2,"139":8}}],["在正则化技术方面",{"5":{"96":2}}],["在正则化中",{"5":{"50":2}}],["在比较语言模型性能时",{"5":{"96":2}}],["在比较语言模型时",{"5":{"96":2}}],["在观测数据后",{"5":{"96":1}}],["在观测数据",{"5":{"96":1}}],["在现代深度学习框架中",{"5":{"50":2}}],["在词嵌入研究中",{"5":{"50":2}}],["在变分dropout中",{"5":{"145":2}}],["在变分自编码器",{"5":{"137":2}}],["在变分自编码器和生成模型中",{"5":{"96":2}}],["在变分推断中有重要应用",{"5":{"137":2}}],["在变分推断中",{"5":{"96":4}}],["在变换下只发生缩放而不改变方向",{"5":{"50":2}}],["在主成分分析",{"5":{"50":2}}],["在循环神经网络",{"5":{"50":2,"145":2}}],["在参数化部分",{"5":{"140":2}}],["在参数化的概率分布族上",{"5":{"70":1,"71":1}}],["在参数化的概率分布族",{"5":{"70":1,"71":1}}],["在参数空间中",{"5":{"136":1}}],["在参数高效微调",{"5":{"50":2}}],["在对话中",{"5":{"156":2}}],["在对话系统中",{"5":{"96":2}}],["在对抗学习场景下",{"5":{"133":2}}],["在对齐阶段",{"5":{"101":2}}],["在对比学习",{"5":{"70":2}}],["在对比学习中",{"5":{"50":2}}],["在模型压缩方面",{"5":{"50":2}}],["在模型压缩和低秩近似中",{"5":{"50":2}}],["在模型并行化中",{"5":{"50":2}}],["在模型的宽度维度上划分参数",{"5":{"50":2}}],["在模型的前向传播中",{"5":{"89":2}}],["在课程学习",{"5":{"51":2}}],["在特征维度",{"5":{"146":1}}],["在特征维度上求和",{"5":{"146":1}}],["在特征方向上的分量",{"5":{"135":2}}],["在特征提取和表示学习中经常被使用",{"5":{"50":2}}],["在特定假设下",{"5":{"51":2}}],["在特定领域任务上",{"5":{"89":2}}],["在特定迭代次数时按比例降低学习率",{"5":{"97":2}}],["在误差较大时使用mae",{"5":{"51":2}}],["在维欧几里得空间中",{"5":{"51":1}}],["在欧几里得范数下",{"5":{"51":2}}],["在固定的数据生成过程下",{"5":{"51":2}}],["在大规模应用中可能比较昂贵",{"5":{"154":1}}],["在大多数任务上表现稳定",{"5":{"154":2}}],["在大多数情况下是凸函数",{"5":{"51":2}}],["在大样本下收敛于真实值",{"5":{"133":2}}],["在大模型时代",{"5":{"132":2,"133":2,"137":2,"139":2}}],["在大语言模型和深度神经网络日益庞大的今天",{"5":{"142":2}}],["在大语言模型中",{"5":{"48":2,"50":18,"96":16,"97":2}}],["在大语言模型中即为词汇表的大小",{"5":{"96":2}}],["在大语言模型的应用场景中",{"5":{"155":2}}],["在大语言模型的rlhf实践中",{"5":{"154":2}}],["在大语言模型的rlhf训练中",{"5":{"154":2,"155":2}}],["在大语言模型的语境下",{"5":{"48":2,"50":2,"138":2,"140":2,"156":2}}],["在大语言模型的开发和研究中",{"5":{"96":2}}],["在大语言模型的在线评估中",{"5":{"96":2}}],["在大语言模型的训练中",{"5":{"96":1}}],["在大语言模型的实践中",{"5":{"50":2}}],["在大语言模型的架构设计中",{"5":{"50":2}}],["在大语言模型的理论基础中占据着不可替代的核心地位",{"5":{"50":2}}],["在大语言模型的理论框架中",{"5":{"96":2}}],["在大语言模型评估中",{"5":{"96":2}}],["在大语言模型",{"5":{"50":2}}],["在mhc中",{"5":{"150":2}}],["在mle点",{"5":{"149":1}}],["在mle点处",{"5":{"149":1}}],["在mlm中",{"5":{"101":2}}],["在mse最小化的意义下",{"5":{"51":2}}],["在mae中",{"5":{"51":2}}],["在moe的稀疏激活模式下",{"5":{"159":2}}],["在moe架构中",{"5":{"159":2}}],["在moe模型中",{"5":{"93":2,"159":4}}],["在moe中",{"5":{"93":2}}],["在求和之前提取最大值",{"5":{"61":2}}],["在融合框架下",{"5":{"61":2}}],["在分配不均衡时增大",{"5":{"159":2}}],["在分布式环境中",{"5":{"157":2}}],["在分析包含dropout的深度网络时",{"5":{"145":2}}],["在分析神经网络梯度的分布时",{"5":{"96":2}}],["在分析梯度裁剪之前",{"5":{"147":2}}],["在分析梯度饱和之前",{"5":{"41":2}}],["在分析梯度流和稳定性时",{"5":{"50":2}}],["在分类任务中的数值稳定性处理",{"5":{"70":2}}],["在分类任务中",{"5":{"61":2,"69":1,"70":6}}],["在分类任务中最小化交叉熵损失等价于最大化数据的似然函数",{"5":{"61":2}}],["在离散分类任务中",{"5":{"61":2}}],["在明确了熵和kl散度的定义后",{"5":{"61":2}}],["在优化算法和正则化方法中有着重要的应用",{"5":{"50":2}}],["在优化视角下",{"5":{"61":2}}],["在概率单纯形上",{"5":{"61":2,"70":6}}],["在sigmoid中",{"5":{"42":2}}],["在scaled",{"5":{"61":2,"69":1}}],["在滑动窗口的基础上引入膨胀因子",{"5":{"86":2,"151":2}}],["在推荐系统和缺失数据插补中",{"5":{"50":2}}],["在推理时进行自回归生成",{"5":{"140":2}}],["在推理时",{"5":{"86":2,"88":2,"89":2,"151":2}}],["在推理阶段",{"5":{"93":2,"140":2}}],["在足够深的transformer中",{"5":{"87":2,"152":2}}],["在谱性质分析部分",{"5":{"87":2,"152":2}}],["在低秩结构分析部分",{"5":{"87":2,"152":2}}],["在群作用的语言下",{"5":{"88":2}}],["在llama",{"5":{"88":2}}],["在未来的大语言模型发展中",{"5":{"88":2}}],["在标准监督训练中",{"5":{"137":2}}],["在标准损失",{"5":{"133":1}}],["在标准损失上添加互信息正则化项",{"5":{"133":1}}],["在标准transformer中",{"5":{"89":2}}],["在标准配置下",{"5":{"87":2,"152":2}}],["在标准配置",{"5":{"93":2}}],["在标准的moe设置中",{"5":{"159":2}}],["在标准的前馈网络中",{"5":{"157":2}}],["在标准的策略梯度方法中",{"5":{"154":2}}],["在标准的残差网络中",{"5":{"150":2}}],["在标准的dropout实现中",{"5":{"145":2}}],["在标准的transformer训练中",{"5":{"141":2}}],["在标准的",{"5":{"132":2}}],["在标准的正弦余弦编码中",{"5":{"91":2}}],["在标准的多头注意力中",{"5":{"93":4}}],["在短序列场景下",{"5":{"89":2}}],["在表达能力分析部分",{"5":{"89":2,"93":2}}],["在矩阵分析部分",{"5":{"89":2}}],["在基",{"5":{"89":1}}],["在复数形式下",{"5":{"90":2}}],["在长序列场景下",{"5":{"89":2}}],["在长度为的序列上",{"5":{"90":1}}],["在长度为",{"5":{"90":1}}],["在长程依赖建模中发挥着关键作用",{"5":{"92":2,"153":2}}],["在傅里叶分析的语境下",{"5":{"90":2}}],["在频率空间中",{"5":{"90":2}}],["在频率空间中是一个对角矩阵",{"5":{"90":1}}],["在频率域中",{"5":{"90":2}}],["在原始的transformer论文",{"5":{"146":2}}],["在原始transformer的实现中",{"5":{"91":2}}],["在原点附近",{"5":{"144":2}}],["在原假设下所有排列是等可能的",{"5":{"96":2}}],["在机器学习特别是大语言模型中有着广泛应用",{"5":{"96":2}}],["在机器学习训练中",{"5":{"96":2}}],["在机器学习与深度学习的庞大理论体系中",{"5":{"51":2}}],["在机器学习领域",{"5":{"61":2}}],["在机器学习任务中",{"5":{"61":2}}],["在机器学习的优化框架下",{"5":{"61":2}}],["在机器翻译等基准任务上表现良好",{"5":{"91":2}}],["在解码器",{"5":{"91":2}}],["在序列长度和维度都较大时可能成为计算瓶颈",{"5":{"88":2}}],["在序列数据处理中",{"5":{"92":2,"153":2}}],["在序列到序列",{"5":{"95":2}}],["在句子",{"5":{"92":2,"153":2}}],["在建立了infonce与softmax的统一性后",{"5":{"69":2}}],["在建模长程依赖时面临根本性的困难",{"5":{"92":2,"153":2}}],["在rlhf中",{"5":{"156":2}}],["在rmsnorm中",{"5":{"146":2}}],["在rope中",{"5":{"88":2}}],["在rope的框架中",{"5":{"88":2}}],["在rope的语境中",{"5":{"88":4}}],["在rnn中",{"5":{"92":10,"146":2,"153":10}}],["在卷积神经网络",{"5":{"145":2}}],["在卷积神经网络中",{"5":{"92":2,"93":2,"153":2}}],["在卷积网络中",{"5":{"41":2}}],["在卷积网络中的作用有异曲同工之妙",{"5":{"94":2}}],["在相同的计算预算下",{"5":{"159":2}}],["在相同的计算预算",{"5":{"159":2}}],["在相位调制中",{"5":{"88":2}}],["在相对位置编码中",{"5":{"92":2,"153":2}}],["在注意力机制中",{"5":{"50":2,"70":6,"87":2,"92":6,"152":2,"153":6}}],["在注意力中",{"5":{"92":2,"153":2}}],["在注意力计算中",{"5":{"95":2}}],["在各种任务上表现良好",{"5":{"90":2}}],["在各层都较高",{"5":{"92":1,"153":1}}],["在单步td学习中",{"5":{"155":2}}],["在单层注意力中",{"5":{"92":2,"153":2}}],["在单一的注意力机制下",{"5":{"93":2}}],["在设计多头注意力架构时",{"5":{"93":2}}],["在frobenius范数或谱范数意义下的最佳",{"5":{"143":1}}],["在frobenius范数和谱范数意义下",{"5":{"50":2}}],["在flash",{"5":{"93":2}}],["在非线性神经网络中",{"5":{"145":2}}],["在非常深的网络中",{"5":{"41":2}}],["在非对角线上的元素接近0",{"5":{"95":2}}],["在更一般的形式中",{"5":{"95":2}}],["在某些层",{"5":{"158":2}}],["在某些解释中",{"5":{"156":2}}],["在某些方向有正特征值",{"5":{"149":2}}],["在某些方向上放大信号",{"5":{"148":1}}],["在某些正则性条件下",{"5":{"149":2}}],["在某些正则化方法中",{"5":{"50":2}}],["在某些文献中",{"5":{"149":2}}],["在某些初始化条件下",{"5":{"148":2}}],["在某些任务中",{"5":{"146":2}}],["在某些任务上表现良好",{"5":{"86":2,"151":2}}],["在某些推理任务上",{"5":{"138":2}}],["在某些情况下",{"5":{"48":2,"87":2,"152":2}}],["在某些深度学习应用中",{"5":{"96":2}}],["在某些实现中",{"5":{"91":2}}],["在某些变体设计中",{"5":{"94":2}}],["在某个特定的维度轴上计算统计量",{"5":{"146":2}}],["在某个维度上的分量较大",{"5":{"95":1}}],["在某个方向上有负特征值",{"5":{"97":2}}],["在同一网络的训练过程中",{"5":{"148":2}}],["在同一层内",{"5":{"45":2}}],["在同一维度上的分量也较大",{"5":{"95":1}}],["在填充位置为",{"5":{"95":1}}],["设moe模型有",{"5":{"159":4}}],["设mini",{"5":{"41":2}}],["设top",{"5":{"158":2}}],["设transformer层的输入为",{"5":{"158":2}}],["设transformer有层",{"5":{"89":1}}],["设transformer有",{"5":{"89":1}}],["设备间通信",{"5":{"159":2}}],["设备利用率越高",{"5":{"157":2}}],["设备",{"5":{"157":2}}],["设备可以在发送激活数据的同时执行已经接收到的激活的计算",{"5":{"157":2}}],["设备可以在发送激活数据的同时计算已经接收到的激活",{"5":{"157":2}}],["设检查点数量为",{"5":{"157":2}}],["设专家数量为",{"5":{"159":2}}],["设专家分配概率的平均值为",{"5":{"158":2}}],["设专家权重矩阵的集合为",{"5":{"158":2}}],["设专家集合为",{"5":{"158":2}}],["设专家",{"5":{"157":6,"158":2,"159":2}}],["设每个专家的参数量为",{"5":{"157":2}}],["设每个头的query",{"5":{"93":2}}],["设通信时间为",{"5":{"157":4}}],["设设备",{"5":{"157":4}}],["设总共有",{"5":{"157":2,"159":2}}],["设批次输入为",{"5":{"158":2}}],["设批次大小为",{"5":{"157":4}}],["设批量大小为",{"5":{"146":2}}],["设批量输入",{"5":{"44":2}}],["设策略由参数化函数",{"5":{"155":2}}],["设语言模型的策略为",{"5":{"154":2}}],["设门控网络的原始得分为",{"5":{"158":2}}],["设门控网络的原始输出为",{"5":{"158":2}}],["设门控网络参数为",{"5":{"157":2}}],["设门控函数为",{"5":{"93":2}}],["设门的输出为",{"5":{"150":2}}],["设数据由",{"5":{"149":1}}],["设数据由的最小值附近的高斯噪声观测产生",{"5":{"149":1}}],["设数据的有效维度",{"5":{"149":2}}],["设从数据集中均匀随机采样的批量大小为",{"5":{"149":2}}],["设矩阵",{"5":{"148":1}}],["设矩阵的特征值为",{"5":{"148":1}}],["设所有权重相同且为",{"5":{"148":2}}],["设裁剪阈值为",{"5":{"147":2}}],["设梯度裁剪阈值为",{"5":{"147":2}}],["设rnn的循环单元为",{"5":{"145":2}}],["设神经网络中某一层的输入向量为",{"5":{"145":2}}],["设该层的输入为",{"5":{"144":2}}],["设个输入独立",{"5":{"148":1}}],["设个注意力头的输出为",{"5":{"143":1}}],["设个头的输出拼接为",{"5":{"41":1}}],["设原始损失为",{"5":{"159":2}}],["设原始损失函数为",{"5":{"144":2,"147":2}}],["设原始门控得分为",{"5":{"159":2}}],["设原始softmax输出为",{"5":{"159":2}}],["设原始第",{"5":{"145":2}}],["设原始梯度为",{"5":{"144":2}}],["设原始卷积层参数量为",{"5":{"143":2}}],["设原始注意力计算为",{"5":{"141":2}}],["设原始注意力矩阵为",{"5":{"86":2,"151":2}}],["设原始注意力分数矩阵为",{"5":{"92":2,"153":2}}],["设卷积核",{"5":{"143":1}}],["设卷积核进行cp分解",{"5":{"143":1}}],["设卷积输出的形状为",{"5":{"41":2}}],["设前",{"5":{"142":1}}],["设前个奇异值的累积能量比为",{"5":{"142":1}}],["设奇异值按降序排列为",{"5":{"142":2}}],["设任务要求模型学习一个复杂的条件概率分布",{"5":{"138":2}}],["设任务为序列分类或语言建模",{"5":{"89":2}}],["设训练",{"5":{"144":2}}],["设训练数据为",{"5":{"137":2}}],["设训练时使用的最大位置为​",{"5":{"89":1}}],["设训练时使用的最大位置为",{"5":{"89":1}}],["设损失函数为",{"5":{"158":2}}],["设损失函数",{"5":{"136":1}}],["设损失函数是凸函数",{"5":{"136":1}}],["设平衡点",{"5":{"136":1}}],["设平衡点处的",{"5":{"136":1}}],["设平均每层的梯度传递因子为",{"5":{"41":2}}],["设平均每层的梯度衰减因子为",{"5":{"42":2}}],["设最小值在原点",{"5":{"134":2}}],["设最后一层的输出为",{"5":{"51":2}}],["设网络宽度为",{"5":{"133":2}}],["设网络的前向传播为",{"5":{"148":2}}],["设网络的一层为",{"5":{"41":2}}],["设网络的原始输出为",{"5":{"71":2}}],["设到的信息传递效率为互信息之比",{"5":{"132":1}}],["设分数差异为",{"5":{"132":2}}],["设分词器将原始文本映射为token序列",{"5":{"101":2}}],["设线性投影矩阵为",{"5":{"132":2}}],["设词",{"5":{"101":2}}],["设掩码",{"5":{"145":2}}],["设掩码span的长度服从某种分布",{"5":{"101":2}}],["设掩码位置集合为",{"5":{"101":2}}],["设填充后的序列长度为",{"5":{"101":2}}],["设仅依赖于token嵌入而不包含位置信息",{"5":{"101":1}}],["设真实标签分布为",{"5":{"144":2}}],["设真实分布为",{"5":{"70":2}}],["设真实类别为",{"5":{"70":2}}],["设真实目标为",{"5":{"70":2}}],["设真实的数据生成过程涉及两个变量的交互效应",{"5":{"71":2}}],["设共有",{"5":{"69":1,"159":2}}],["设共有个注意力头",{"5":{"69":1}}],["设查询向量",{"5":{"69":1}}],["设查询向量指向某个方向",{"5":{"69":1}}],["设为条件数",{"5":{"149":1}}],["设为参数处的随机梯度",{"5":{"149":1}}],["设为dropout层的输入向量",{"5":{"145":1}}],["设为数据矩阵",{"5":{"143":1}}],["设为神经网络中某一层的权重矩阵",{"5":{"142":1}}],["设为词汇表",{"5":{"140":1}}],["设为单点分布",{"5":{"137":1}}],["设为凸函数",{"5":{"137":1}}],["设为类别标签",{"5":{"47":1}}],["设为饱和阈值",{"5":{"41":1}}],["设为损失函数",{"5":{"41":1}}],["设为标量损失函数",{"5":{"44":1}}],["设为第层的输出",{"5":{"45":1}}],["设为输入向量",{"5":{"47":1}}],["设为个类别之一",{"5":{"47":1}}],["设为注意力权重矩阵",{"5":{"92":2,"153":2}}],["设为注意力输出",{"5":{"94":1}}],["设为离散随机变量的状态空间大小",{"5":{"96":1}}],["设为预测概率",{"5":{"96":1}}],["设",{"5":{"41":10,"42":2,"44":6,"45":10,"47":8,"48":7,"51":2,"61":7,"71":4,"86":1,"87":10,"88":4,"89":5,"90":5,"91":2,"92":5,"93":3,"94":3,"95":4,"96":12,"101":6,"132":1,"133":1,"134":2,"135":14,"136":4,"137":5,"140":1,"142":4,"143":9,"144":2,"145":1,"146":2,"147":10,"148":15,"149":4,"150":2,"151":1,"152":10,"153":5,"154":2,"157":6,"158":14,"159":10}}],["设​",{"5":{"41":1,"87":1,"152":1}}],["设​为第层的误差信号",{"5":{"44":1}}],["设​为后向传播的中间值",{"5":{"45":1}}],["设​为的第行",{"5":{"95":1}}],["设​的奇异值分解为",{"5":{"89":1}}],["设​是的最大奇异值",{"5":{"92":1,"153":1}}],["设​是单头注意力能够表示的函数类",{"5":{"93":1}}],["设​和​是位置和的编码",{"5":{"90":1}}],["设​和分别是两个头的输出",{"5":{"93":1}}],["设​​为缩放后的注意力分数矩阵",{"5":{"95":1}}],["设第层使用sigmoid激活函数",{"5":{"148":1}}],["设第层的激活向量为​",{"5":{"148":1}}],["设第层的权重矩阵为",{"5":{"148":1}}],["设第层的近似误差为​",{"5":{"142":1}}],["设第层的输入为",{"5":{"45":1,"92":1,"153":1}}],["设第行的softmax输出为",{"5":{"141":1}}],["设第头的注意力权重为",{"5":{"132":1}}],["设第",{"5":{"45":1,"61":2,"92":1,"132":1,"141":1,"142":1,"148":3,"150":2,"153":1}}],["设第一层的注意力权重为",{"5":{"92":2,"153":2}}],["设第一层的输出为",{"5":{"93":2}}],["设第一层的输入为",{"5":{"94":2}}],["设输入空间中存在两个不同类型的输入簇",{"5":{"159":2}}],["设输入空间为",{"5":{"71":2}}],["设输入激活为",{"5":{"157":2}}],["设输入",{"5":{"150":2}}],["设输入信号为",{"5":{"150":2}}],["设输入张量为",{"5":{"146":8}}],["设输入张量",{"5":{"143":2}}],["设输入的绝对值较大时",{"5":{"42":2}}],["设输入向量为",{"5":{"46":2,"71":2,"94":2,"158":2,"159":2}}],["设输入为",{"5":{"46":2,"71":2}}],["设输入矩阵包含个样本",{"5":{"47":1}}],["设输入矩阵",{"5":{"47":1}}],["设输入嵌入矩阵为",{"5":{"87":2,"91":2,"152":2}}],["设输入嵌入为",{"5":{"87":2,"91":2,"152":2}}],["设输入嵌入张量为",{"5":{"93":2}}],["设输入是一个二值序列",{"5":{"92":2,"153":2}}],["设输入序列为",{"5":{"92":2,"101":4,"132":2,"141":2,"153":2}}],["设输入序列的长度为",{"5":{"93":2}}],["设是秩不超过的权重矩阵集合",{"5":{"142":1}}],["设是损失函数的一个局部极小值",{"5":{"142":1}}],["设是参数化概率族",{"5":{"137":1}}],["设是稳定的极限环",{"5":{"136":1}}],["设是状态空间中的一个目标点",{"5":{"136":1}}],["设是信息瓶颈最优解",{"5":{"133":1}}],["设是任务的梯度向量",{"5":{"101":1}}],["设是任意非常数的有界连续函数",{"5":{"71":1}}],["设是位置被掩码的概率分布",{"5":{"101":1}}],["设是位置的token嵌入经过多层自注意力计算后的隐藏状态",{"5":{"101":1}}],["设是位置编码矩阵",{"5":{"71":1}}],["设是隐藏状态",{"5":{"41":1}}],["设是隐藏状态向量",{"5":{"41":1}}],["设是",{"5":{"71":1}}],["设是一个多元函数",{"5":{"48":1}}],["设是一个可微函数",{"5":{"48":1}}],["设是一个单位向量",{"5":{"48":1}}],["设是一个维随机向量",{"5":{"96":1}}],["设是一个离散序列",{"5":{"90":1}}],["设是一个有向图",{"5":{"92":1,"153":1}}],["设是第层的表示向量",{"5":{"71":1}}],["设是第个类别的logit",{"5":{"96":1}}],["设是独立同分布的随机变量",{"5":{"96":2}}],["设是预测空间",{"5":{"51":1}}],["设是模型预测的期望",{"5":{"51":1}}],["设是特征映射对应的核函数",{"5":{"86":1,"151":1}}],["设是的任意特征值",{"5":{"87":1,"152":1}}],["设是某层某头的注意力矩阵",{"5":{"87":1,"152":1}}],["设是学习到的位置编码矩阵",{"5":{"89":1}}],["设是周期为的周期函数",{"5":{"90":1}}],["设激活函数逐元素应用于向量",{"5":{"71":1}}],["设激活函数",{"5":{"71":1}}],["设标量函数",{"5":{"44":2}}],["设计新算法和分析泛化性能提供了深刻的洞见",{"5":{"137":2}}],["设计新的编码方案",{"5":{"91":2}}],["设计学习率调度策略",{"5":{"136":2}}],["设计意义",{"5":{"135":2}}],["设计哲学",{"5":{"134":2}}],["设计哲学和工程实践",{"5":{"88":2}}],["设计模型架构和优化计算资源至关重要",{"5":{"45":2}}],["设计权重和偏置如下",{"5":{"46":2}}],["设计优化算法和理解泛化性质时非常有用",{"5":{"96":2}}],["设计改进方案都具有重要意义",{"5":{"95":2}}],["设计专门的激活函数",{"5":{"41":2}}],["设计专门的模型架构",{"5":{"101":2}}],["设计",{"5":{"101":2}}],["设隐藏层的输出为",{"5":{"71":2}}],["设隐藏维度",{"5":{"45":2}}],["设权重矩阵为",{"5":{"144":2}}],["设权重矩阵",{"5":{"135":2}}],["设权重",{"5":{"47":6,"135":2}}],["设权重向量",{"5":{"51":2}}],["设有个任务",{"5":{"101":1}}],["设有个神经元的层",{"5":{"47":1}}],["设有一个",{"5":{"44":1,"45":1,"46":1,"47":1}}],["设有一个层前馈网络",{"5":{"44":1}}],["设有一个层的前馈神经网络",{"5":{"45":1,"46":1}}],["设有一个层的前馈网络",{"5":{"47":1}}],["设有一个三阶张量",{"5":{"50":2}}],["设有一组神经元",{"5":{"46":2}}],["设有",{"5":{"47":1,"101":1}}],["设有两个维向量",{"5":{"50":1}}],["设有两个",{"5":{"50":1}}],["设的奇异值为",{"5":{"142":1}}],["设的奇异值分解为",{"5":{"89":1}}],["设的伴随值为",{"5":{"45":1}}],["设的第行为",{"5":{"47":1}}],["设的秩为",{"5":{"87":1,"143":1,"152":1}}],["设随机变量",{"5":{"96":3}}],["设随机变量在个类别上服从类别分布",{"5":{"96":1}}],["设参数与训练数据之间的互信息为",{"5":{"133":1}}],["设参数矩阵的奇异值分解为",{"5":{"41":1}}],["设参数矩阵",{"5":{"41":1}}],["设参数",{"5":{"96":2,"133":1}}],["设连续随机变量的pdf为",{"5":{"96":1}}],["设连续随机变量",{"5":{"96":1}}],["设单个样本的特征表示是形状为的二维张量",{"5":{"50":1}}],["设单个样本的特征表示是形状为",{"5":{"50":1}}],["设目标向量",{"5":{"51":2}}],["设对角权重矩阵",{"5":{"51":2}}],["设正常样本的误差为",{"5":{"51":2}}],["设相同的数据",{"5":{"51":2}}],["设和为两个映射",{"5":{"45":1}}],["设和是定义在同一概率空间上的两个离散概率分布",{"5":{"61":1}}],["设我们有观测数据",{"5":{"96":2}}],["设我们有一个独立同分布的样本集",{"5":{"61":2}}],["设样本量",{"5":{"51":2}}],["设样本属于正类",{"5":{"61":1}}],["设样本的真实类别为",{"5":{"61":2}}],["设样本",{"5":{"61":1}}],["设模型的隐藏层维度为",{"5":{"159":2}}],["设模型在任务",{"5":{"138":2}}],["设模型族定义了所有可能的预测向量集合",{"5":{"51":1}}],["设模型族",{"5":{"51":1}}],["设模型输出为概率分布",{"5":{"149":2}}],["设模型输出为",{"5":{"61":2}}],["设块大小为",{"5":{"86":2,"151":2}}],["设query矩阵",{"5":{"87":2,"152":2}}],["设核函数为",{"5":{"87":2,"152":2}}],["设一个神经网络层的输入为",{"5":{"150":2}}],["设一个残差块的输入为",{"5":{"150":2}}],["设一个深度为",{"5":{"71":2}}],["设一维卷积核为",{"5":{"87":2,"152":2}}],["设稀疏模式的参数为",{"5":{"86":2,"151":2}}],["设稀疏模式由掩码矩阵给出",{"5":{"87":1,"152":1}}],["设稀疏模式由掩码矩阵",{"5":{"87":1,"152":1}}],["设保留前个奇异值",{"5":{"87":1,"152":1}}],["设保留前",{"5":{"87":1,"152":1}}],["设序列长度为",{"5":{"86":2,"91":2,"92":4,"93":2,"94":2,"151":2,"153":4,"157":2}}],["设序列的最大长度为",{"5":{"89":2}}],["设序列的dft为",{"5":{"90":1}}],["设序列",{"5":{"90":1}}],["设非零奇异值为",{"5":{"89":2}}],["设去均值化的编码为",{"5":{"89":2}}],["设编码矩阵的秩为",{"5":{"89":2}}],["设编码向量的维度为​",{"5":{"89":1}}],["设编码向量的维度为",{"5":{"89":1}}],["设编码向量为",{"5":{"90":2}}],["设注意力头数量为",{"5":{"132":2}}],["设注意力分数矩阵为",{"5":{"86":2,"151":2}}],["设注意力输出为",{"5":{"89":2}}],["设注意力权重矩阵",{"5":{"93":2}}],["设离散随机变量的概率分布为",{"5":{"61":1}}],["设离散随机变量",{"5":{"61":1}}],["设离散序列",{"5":{"90":2}}],["设嵌入维度",{"5":{"91":2}}],["设头的边际贡献为",{"5":{"132":2}}],["设头的注意力权重矩阵为",{"5":{"92":1,"153":1}}],["设头",{"5":{"92":1,"153":1}}],["设头数被分为组",{"5":{"93":1}}],["设头数",{"5":{"93":1}}],["设多头注意力的头数为",{"5":{"93":2,"94":2}}],["设当前矩阵形状为",{"5":{"93":2}}],["设表示第层位置的表示与原始位置嵌入之间的互信息",{"5":{"92":1,"153":1}}],["设表示熵",{"5":{"93":1}}],["设位置距离",{"5":{"132":2}}],["设位置投影为",{"5":{"91":2}}],["设位置编码矩阵为",{"5":{"94":2}}],["pqr",{"5":{"143":2}}],["p=1",{"5":{"143":2}}],["pmatrix",{"5":{"143":8}}],["pmf",{"5":{"96":2}}],["pmf必须满足两个基本性质",{"5":{"96":2}}],["p̂",{"5":{"137":4}}],["pythagorean",{"5":{"137":2}}],["pytorch",{"5":{"50":2,"61":2}}],["p",{"5":{"42":98,"46":2,"61":126,"69":3,"97":8,"132":4,"133":14,"137":6,"141":4,"142":2,"143":2,"145":2}}],["pib",{"5":{"133":4}}],["pi",{"5":{"42":2,"61":6}}],["pitts",{"5":{"47":4}}],["pipeline",{"5":{"50":2}}],["penalty的比较",{"5":{"154":1}}],["penalty的比较反映了强化学习中两种不同的约束策略更新的思路",{"5":{"154":1}}],["penalty通过软约束允许策略有更大的灵活性",{"5":{"154":2}}],["penalty中",{"5":{"154":2}}],["penalty则通过kl散度惩罚项约束策略更新的幅度",{"5":{"154":2}}],["penalty",{"5":{"132":2,"154":4}}],["pe",{"5":{"42":4}}],["perm",{"5":{"137":4}}],["permutation",{"5":{"91":2,"92":2,"96":2,"153":2}}],["periodic",{"5":{"134":2}}],["pereira",{"5":{"133":2}}],["per",{"5":{"132":6,"157":4}}],["perceptron",{"5":{"47":2}}],["perplexity",{"5":{"96":2}}],["peft",{"5":{"50":2}}],["pair",{"5":{"137":2}}],["pac",{"5":{"133":6,"137":2}}],["padding",{"5":{"95":2,"101":2}}],["partial",{"5":{"42":68,"50":2,"61":52,"144":12}}],["parafac分解",{"5":{"143":2}}],["parafac",{"5":{"50":2}}],["param",{"5":{"157":4}}],["params",{"5":{"46":4}}],["parameter",{"5":{"50":2}}],["parallelism",{"5":{"50":6,"157":2}}],["path",{"5":{"41":2}}],["pattern",{"5":{"86":2,"151":2}}],["palm等",{"5":{"88":2}}],["palm等现代大语言模型都采用了rope或其变体",{"5":{"92":2,"153":2}}],["palm",{"5":{"88":2}}],["prior",{"5":{"145":2}}],["pruned",{"5":{"133":2}}],["prune",{"5":{"132":4}}],["preservation",{"5":{"145":2}}],["prevent",{"5":{"145":2}}],["prev|",{"5":{"136":2}}],["prev",{"5":{"136":4}}],["preference",{"5":{"101":2}}],["prefixlm",{"5":{"101":2}}],["prediction",{"5":{"101":2}}],["predictive",{"5":{"69":2}}],["pre",{"5":{"41":8,"146":6,"148":2,"150":4}}],["prelu",{"5":{"42":2}}],["precision",{"5":{"97":2}}],["process",{"5":{"156":2}}],["processing",{"5":{"47":2,"50":2}}],["prompt",{"5":{"154":2}}],["proximal",{"5":{"101":2,"154":2,"155":2}}],["probability",{"5":{"61":2,"70":2,"96":4,"140":2,"145":4,"156":2}}],["product",{"0":{"95":1},"4":{"95":1},"5":{"61":8,"69":4,"71":2,"86":2,"87":2,"88":2,"93":4,"94":2,"95":17,"131":5,"132":2,"151":2,"152":2}}],["propagation",{"5":{"45":2,"96":2}}],["property",{"5":{"42":2,"145":2,"156":2}}],["properties",{"5":{"87":2,"152":2}}],["png",{"5":{"44":4,"45":8,"47":8,"48":12,"50":8,"61":4,"96":12,"143":4,"148":8}}],["poincaré",{"5":{"136":2}}],["point",{"5":{"45":2,"48":2,"134":4}}],["policy",{"5":{"101":2,"154":2,"155":4,"156":4}}],["power",{"5":{"50":2,"138":2}}],["population",{"5":{"51":2}}],["pos",{"5":{"133":4,"137":10}}],["post",{"5":{"41":3,"146":4,"150":2}}],["position",{"0":{"88":1},"4":{"88":1},"5":{"88":5,"92":2,"131":5,"153":2}}],["positional",{"5":{"88":12,"89":2,"91":12,"92":8,"94":2,"153":8}}],["positive",{"5":{"61":2,"86":2,"137":2,"151":2}}],["positivity",{"5":{"86":2,"151":2}}],["pdf",{"5":{"96":2}}],["pdf在单点的取值不代表概率",{"5":{"96":2}}],["pdf同样满足非负性和归一性",{"5":{"96":1}}],["pdf同样满足非负性",{"5":{"96":1}}],["ppo目标函数为",{"5":{"154":2}}],["ppo还有另一种重要的变体",{"5":{"154":2}}],["ppo鼓励策略减少选择该动作的概率",{"5":{"154":2}}],["ppo鼓励策略增加选择该动作的概率",{"5":{"154":2}}],["ppo正是通过限制",{"5":{"154":2}}],["ppo通过限制策略变化的程度",{"5":{"154":2}}],["ppo通过在目标函数中引入约束机制来解决这个问题",{"5":{"154":2}}],["ppo论文提出了两种主要变体",{"5":{"154":2}}],["ppo设计了一种特殊的目标函数",{"5":{"154":2}}],["ppo已成为当前大语言模型rlhf训练中最主流的策略优化算法",{"5":{"154":2}}],["ppo核心思想",{"2":{"154":1},"5":{"154":1}}],["ppo算法与大模型训练<",{"5":{"131":1}}],["ppo算法与大模型训练",{"0":{"154":1},"4":{"111":1,"154":1},"5":{"131":4,"154":1}}],["ppo中的",{"5":{"101":2}}],["ppo的核心思想可以概括为",{"5":{"154":2}}],["ppo的核心思想是",{"5":{"101":2}}],["ppo的裁剪是基于策略比率的几何约束",{"5":{"101":2}}],["ppo的目标函数为",{"5":{"101":2}}],["ppo损失",{"5":{"101":2}}],["ppo在rlhf中的应用",{"2":{"101":1},"5":{"101":1}}],["ppo",{"2":{"154":1},"5":{"96":2,"154":31}}],["p值",{"5":{"96":2}}],["p值受样本量影响很大",{"5":{"96":2}}],["pca将数据矩阵分解为",{"5":{"143":2}}],["pca只能处理二维矩阵形式的数据",{"5":{"143":2}}],["pca通过寻找数据方差最大的方向",{"5":{"143":2}}],["pca",{"5":{"50":6,"91":2,"94":2,"143":3}}],["pca是一个无监督的降维方法",{"5":{"94":2}}],["phi",{"5":{"42":6,"46":2,"141":12}}],["phase",{"5":{"88":2,"134":2,"145":2}}],["p采样等解码策略也都是在概率分布层面进行操作的各种启发式方法",{"5":{"96":2}}],["它控制着探索与利用之间的权衡",{"5":{"159":2}}],["它控制着相似度分数的",{"5":{"69":2}}],["它鼓励门控网络在噪声较大时",{"5":{"159":2}}],["它接收输入",{"5":{"159":2}}],["它接收提示",{"5":{"154":2}}],["它也存在潜在的问题",{"5":{"158":2}}],["它给出了价值函数之间的递归关系",{"5":{"156":2}}],["它定义了智能体在状态",{"5":{"156":2}}],["它定义了在状态",{"5":{"156":4}}],["它能获得的累积奖励最大化",{"5":{"156":2}}],["它能够生成连贯",{"5":{"154":2}}],["它能够从稀疏观测中恢复低秩的完整矩阵",{"5":{"144":2}}],["它能够确保优化过程收敛到帕累托前沿上的某个点",{"5":{"101":2}}],["它能够并行处理序列中的所有位置",{"5":{"88":2}}],["它研究智能体",{"5":{"156":2}}],["它量化了在特定状态下采取特定动作相对于",{"5":{"155":2}}],["它负责评估当前策略的价值",{"5":{"155":2}}],["它负责选择动作",{"5":{"155":2}}],["它可能无法完美地捕捉人类对",{"5":{"154":2}}],["它可以被解释为一种",{"5":{"70":2}}],["它可以被理解为",{"5":{"71":2}}],["它可以显著简化代码",{"5":{"50":2}}],["它可以看作是矩阵展开为向量后的l2范数",{"5":{"50":2}}],["它可以直接解释为对数似然比的期望",{"5":{"61":2}}],["它可以视为复平面上单位圆上的一个点",{"5":{"90":2}}],["它都被限制在",{"5":{"154":2}}],["它会导致学习效率急剧下降",{"5":{"154":2}}],["它会趋向于",{"5":{"95":2}}],["它引入多尺度的信息处理机制",{"5":{"150":2}}],["它引入了更通用的门控机制来控制信息流动",{"5":{"150":2}}],["它帮助我们理解为什么批量大小的选择如此重要",{"5":{"149":2}}],["它精确地描述了噪声在不同参数方向上的各向异性结构",{"5":{"149":2}}],["它揭示了sgd中噪声并非简单的各向同性白噪声",{"5":{"149":2}}],["它揭示了神经网络如何通过层次化的非线性变换逼近复杂的函数关系",{"5":{"45":2}}],["它具有轻微的平滑效应",{"5":{"148":2}}],["它降低了层间耦合",{"5":{"148":2}}],["它的价值在于防止训练崩溃",{"5":{"147":2}}],["它的梯度关于误差",{"5":{"70":2}}],["它确保了dropout操作在统计意义上不改变输出的期望值",{"5":{"145":2}}],["它确保注意力权重构成有效的概率分布",{"5":{"92":2,"153":2}}],["它创造了一种独特的随机化正则化框架",{"5":{"145":2}}],["它倾向于产生低秩解",{"5":{"144":2}}],["它深刻地改变了优化过程中的梯度流动和损失曲面的几何结构",{"5":{"144":2}}],["它深刻揭示了熵作为",{"5":{"61":2}}],["它实际上是在",{"5":{"141":1}}],["它实际上是在空间中寻找最近的邻居",{"5":{"141":1}}],["它实现了",{"5":{"101":2}}],["它反映了模型在足够规模的训练数据下学习到复杂条件依赖关系的过程",{"5":{"138":2}}],["它反映了概率分布空间的内在曲率",{"5":{"61":2}}],["它消除了传统相关性度量的线性假设",{"5":{"137":2}}],["它本质上是一个自适应信息选择",{"5":{"132":2}}],["它本质上实现了从输入空间到输出空间的函数映射",{"5":{"45":2}}],["它根据输入内容动态调整信息流动的路径和权重",{"5":{"132":2}}],["它绕过了显式的奖励模型",{"5":{"101":2}}],["它迫使模型基于完整的词级上下文进行预测",{"5":{"101":2}}],["它意味着均方误差的优化问题是一个",{"5":{"70":1}}],["它意味着均方误差的优化问题是一个线性最小二乘问题",{"5":{"70":1}}],["它为注意力矩阵的低秩近似提供了理论保证",{"5":{"141":2}}],["它为注意力机制注入了位置信息",{"5":{"92":2,"153":2}}],["它为每个位置提供唯一的身份标识",{"5":{"101":2}}],["它为模型提供了关于序列结构的先验信息",{"5":{"70":2}}],["它不再求期望",{"5":{"156":2}}],["它不是凭空设计的",{"5":{"70":2}}],["它不是对称的",{"5":{"70":2}}],["它不是为每个位置独立编码",{"5":{"90":2}}],["它不仅仅是在区分正负样本",{"5":{"69":2}}],["它不仅告诉我们差异是否显著",{"5":{"96":2}}],["它不仅在实践中取得了显著的性能提升",{"5":{"93":2}}],["它不仅选择概率最高的类别",{"5":{"96":2}}],["它不改变向量的总体强度",{"5":{"88":2}}],["它不随输入序列的内容变化而变化",{"5":{"91":2}}],["它不假设依赖关系只存在于邻近位置",{"5":{"92":2,"153":2}}],["它测量的是两个概率向量之间的直线距离",{"5":{"70":2}}],["它对前向传播没有贡献",{"5":{"148":2}}],["它对大误差的敏感度更高",{"5":{"70":2}}],["它对所有输入token一视同仁",{"5":{"88":2}}],["它对应复数",{"5":{"88":2}}],["它对应于从到的线性变换中最大的拉伸因子",{"5":{"50":1}}],["它对应于从",{"5":{"50":1}}],["它对应于的第一个维度",{"5":{"94":1}}],["它对应于",{"5":{"94":1}}],["它对于理解优化的收敛行为和设计自适应优化算法至关重要",{"5":{"97":2}}],["它有一个下界",{"5":{"70":2}}],["它关于预测值",{"5":{"70":1}}],["它关于预测值是凸函数",{"5":{"70":1}}],["它关于",{"5":{"70":2}}],["它包含一个归一化操作",{"5":{"70":2}}],["它",{"5":{"69":2,"92":1,"153":1}}],["它表示在状态",{"5":{"156":2}}],["它表示从状态",{"5":{"156":2}}],["它表示损失函数对第",{"5":{"148":1}}],["它表示损失函数对第层净输入的敏感度",{"5":{"148":1}}],["它表示损失函数对各层净输入的梯度",{"5":{"44":2}}],["它表示使用基于分布的编码方案来编码来自分布的信息时",{"5":{"61":1}}],["它表示使用基于分布",{"5":{"61":1}}],["它表明infonce本质上是交叉熵在对比学习场景下的应用",{"5":{"69":2}}],["它表明",{"5":{"88":2}}],["它表达的是",{"5":{"94":4}}],["它通过一个巧妙的截断机制来限制策略更新的幅度",{"5":{"154":2}}],["它通过巧妙的机制限制策略更新的幅度",{"5":{"154":2}}],["它通过软化标签分布来防止模型对训练样本过度自信",{"5":{"144":2}}],["它通过将较小的奇异值方向收缩更多来稳定估计",{"5":{"144":2}}],["它通过将权重矩阵除以其谱范数来约束网络函数的lipschitz常数",{"5":{"50":2}}],["它通过计算序列中每个位置与其他所有位置的相关性来实现信息聚合",{"5":{"141":2}}],["它通过最大化观测数据的似然函数来寻找最优参数",{"5":{"139":2}}],["它通过限制梯度的范数来防止过度更新",{"5":{"41":2}}],["它通过对激活值进行归一化来稳定训练过程",{"5":{"41":2}}],["它通过匹配矩",{"5":{"96":2}}],["它通过调整频率参数来改善模型在超出训练长度时的表现",{"5":{"88":2}}],["它通过绝对位置编码的形式",{"5":{"91":2}}],["它通过在验证集性能开始下降时终止训练来实现正则化效果",{"5":{"144":2}}],["它通过在前向传播时只保存部分层的激活值",{"5":{"50":2}}],["它通过在查询和键向量上应用旋转操作来编码位置信息",{"5":{"92":2,"153":2}}],["它们可能分别处理不同类型的输入",{"5":{"158":2}}],["它们可能在处理相似的输入类型",{"5":{"158":2}}],["它们能获得高奖励分数",{"5":{"154":2}}],["它们能够建立的远距离依赖关系较少",{"5":{"92":2,"153":2}}],["它们各有优缺点",{"5":{"143":2}}],["它们各自定义的子空间和可能相交",{"5":{"93":1}}],["它们各自定义的子空间",{"5":{"93":1}}],["它们建立了概率建模",{"5":{"139":2}}],["它们决定了周期轨道附近的线性化行为",{"5":{"136":2}}],["它们决定了信息如何被编码和传递",{"5":{"71":2}}],["它们分别描述了系统在状态空间中的切线性变换和损失函数的曲率特性",{"5":{"135":2}}],["它们分别承担不同的信息角色",{"5":{"132":2}}],["它们直接决定了神经网络能否有效学习",{"5":{"41":2}}],["它们通过规范化激活值或梯度的分布来稳定训练过程",{"5":{"41":2}}],["它们定义了从输入空间到表示空间的映射",{"5":{"71":2}}],["它们的效应相互交织",{"5":{"147":2}}],["它们的效果最终都可以追溯到logits向量的相对结构和softmax变换的特性",{"5":{"96":2}}],["它们的外积定义了一个三阶张量",{"5":{"143":2}}],["它们的联合结构使得损失景观具有特定的几何形状",{"5":{"71":2}}],["它们的复合定义为",{"5":{"45":1}}],["它们的复合",{"5":{"45":1}}],["它们的均值",{"5":{"96":2}}],["它们的kronecker积定义为",{"5":{"50":2}}],["它们的比值也可能是不稳定的",{"5":{"61":2}}],["它们的rope编码向量分别为",{"5":{"88":2}}],["它们的差向量为",{"5":{"90":2}}],["它们的内积",{"5":{"90":2}}],["它们的内积为零",{"5":{"51":2}}],["它们的内积为",{"5":{"92":2,"153":2}}],["它们的编码向量分别为​和​",{"5":{"90":1}}],["它们的编码向量分别为",{"5":{"90":1}}],["它们的组合形成了一个能够唯一表示序列位置的编码系统",{"5":{"90":2}}],["它们的组合可以表示任意相位的同频率振动",{"5":{"91":2}}],["它们的点积为",{"5":{"91":2,"92":2,"153":2}}],["它们的输出表示会高度相关",{"5":{"93":2}}],["它们的学习目标是最小化预测损失",{"5":{"94":2}}],["它们共享相同的输入",{"5":{"46":2}}],["它们刻画了随机变量的集中趋势",{"5":{"96":2}}],["它们用于衡量两个概率分布之间的差异",{"5":{"96":2}}],["它们描述了大量随机变量之和",{"5":{"96":2}}],["它们揭示了线性变换的本质特征",{"5":{"50":2}}],["它们都控制着softmax分布的锐度",{"5":{"69":2}}],["它们都可以作为通用逼近器的核心组件",{"5":{"71":2}}],["它们都是将",{"5":{"61":2}}],["它们都通过控制激活值的分布来稳定训练动态",{"5":{"94":2}}],["它们都建立在softmax",{"5":{"101":2}}],["它们打破了注意力机制本应具有的平移等变性",{"5":{"88":2}}],["它们包含了注意力机制需要学习的全部参数",{"5":{"94":2}}],["它们是方阵变换",{"5":{"94":2}}],["它们在数学结构上存在本质的差异",{"5":{"70":2}}],["它们在数学性质和实际表现上有显著差异",{"5":{"91":2}}],["它们在测试集上的性能相近",{"5":{"97":2}}],["它将不同的专家分配到不同的设备上",{"5":{"157":2}}],["它将复杂的期望累积奖励的梯度问题",{"5":{"155":2}}],["它将每层的输入分布稳定在均值为零",{"5":{"148":2}}],["它将确定性的输入",{"5":{"145":1}}],["它将确定性的输入转换为随机输出",{"5":{"145":1}}],["它将信息瓶颈目标转化为标准的变分学习框架",{"5":{"133":2}}],["它将实数分数转换为概率分布",{"5":{"132":2}}],["它将实数映射到",{"5":{"42":3}}],["它将实数映射到区间",{"5":{"42":1}}],["它将难以计算的kl散度最小化问题转化为一个可计算的二分类问题",{"5":{"69":2}}],["它将概率值转换回实数域",{"5":{"42":2}}],["它将维向量映射到概率单纯形",{"5":{"71":1}}],["它将",{"5":{"71":1}}],["它将数学表达式显式地表示为节点和边的有向无环图",{"5":{"45":2}}],["它将仿射变换统一为线性变换的形式",{"5":{"46":2}}],["它将输入数据依次通过层非线性变换",{"5":{"45":1}}],["它将输入数据依次通过",{"5":{"45":1}}],["它将输入空间划分为两个半空间",{"5":{"47":2}}],["它将输入映射到一个特征空间后计算相似度",{"5":{"93":2}}],["它将hessian矩阵近似为两个小矩阵的kronecker积",{"5":{"48":2}}],["它将随机试验的结果数值化",{"5":{"96":2}}],["它将硬标签",{"5":{"96":2}}],["它将后验分布的推断转化为优化问题",{"5":{"96":2}}],["它将某一子层的输入与该子层的输出直接相加",{"5":{"50":2}}],["它将标量",{"5":{"50":2}}],["它将权重矩阵的谱范数归一化为1",{"5":{"50":2}}],["它将矩阵分解为下三角矩阵与其转置的乘积",{"5":{"50":2}}],["它将一个的矩阵与一个的矩阵组合成一个的大矩阵",{"5":{"50":1}}],["它将一个",{"5":{"50":1}}],["它将query头分为组",{"5":{"86":1,"151":1}}],["它将query头分为",{"5":{"86":1,"151":1}}],["它将任意周期函数表示为不同频率正弦和余弦函数的叠加",{"5":{"90":2}}],["它将正样本键",{"5":{"69":1}}],["它将正样本键拉向查询向量",{"5":{"69":1}}],["它将正弦和余弦统一为一个复指数运算",{"5":{"90":2}}],["它将位置信息从隐式",{"5":{"71":2}}],["它将位置信息视为一种",{"5":{"88":2}}],["它将位置信息编码为一种",{"5":{"88":2}}],["它将位置的编码映射到位置的编码",{"5":{"90":1}}],["它将位置",{"5":{"90":1}}],["它将原始的注意力分数转换为概率分布",{"5":{"95":2}}],["它将注意力机制纳入到概率图模型的框架中",{"5":{"95":2}}],["它将参数视为随机变量并使用贝叶斯公式进行推断",{"5":{"96":2}}],["它将参数更新视为在损失函数的负梯度方向上连续流动",{"5":{"97":2}}],["它减少了内部协变量偏移",{"5":{"41":2}}],["它移除了均值归一化",{"5":{"41":2}}],["它无法表示复杂的曲线",{"5":{"71":2}}],["它从根本上重新思考了模型容量与计算效率之间的关系",{"5":{"159":2}}],["它从根本上改变了神经网络的训练方式",{"5":{"146":2}}],["它从根本上改变了神经网络处理信息的方式",{"5":{"132":2}}],["它从根本上改变了梯度传播的路径",{"5":{"41":2}}],["它从数据压缩和相关性提取的角度解释深度神经网络的学习机制",{"5":{"133":2}}],["它从数学上证明了深度学习的表达能力基础",{"5":{"71":2}}],["它指的是在训练过程中梯度值变得非常小",{"5":{"41":2}}],["它指出",{"5":{"71":2}}],["它指出大量独立同分布随机变量之和",{"5":{"96":2}}],["它正是交叉熵损失的核心组成部分",{"5":{"71":2}}],["它是半正定对称矩阵",{"5":{"143":2}}],["它是梯度",{"5":{"135":2}}],["它是负对数似然的期望",{"5":{"71":2}}],["它是一个",{"5":{"70":2}}],["它是一个对数函数的组合",{"5":{"70":1}}],["它是一个二次函数",{"5":{"70":1}}],["它是一个二分类方法",{"5":{"69":2}}],["它是一个无界的实数",{"5":{"71":2}}],["它是一个复合函数的逐层求值过程",{"5":{"45":2}}],["它是一个非负矩阵",{"5":{"87":2,"152":2}}],["它是离散的二值模型",{"5":{"47":2}}],["它是连接神经网络的线性输出与概率分布的关键桥梁",{"5":{"96":2}}],["它提供了一定的正则化效果",{"5":{"41":2}}],["它提供了数值稳定性",{"5":{"71":2}}],["它度量了和之间的依赖程度",{"5":{"71":1}}],["它度量了",{"5":{"71":1}}],["它度量了模型分布与真实分布之间的差异",{"5":{"61":1}}],["它度量了模型分布",{"5":{"61":1}}],["它告诉我们给定当前参数下网络的输出是什么",{"5":{"44":2}}],["它告诉我们",{"5":{"44":2}}],["它告诉我们如何计算复合函数的导数",{"5":{"48":2}}],["它保证了策略更新的单调性改进性质",{"5":{"154":2}}],["它保证了优化问题的",{"5":{"51":2}}],["它保留输入的相对顺序",{"5":{"69":2}}],["它保留正信息",{"5":{"71":2}}],["它保持了空间的平行性和比例关系",{"5":{"47":2}}],["它保持了向量空间的代数结构",{"5":{"88":2}}],["它描述了系统状态随时间演化的长期行为",{"5":{"136":2}}],["它描述了状态转移函数的切线性变换",{"5":{"135":2}}],["它描述了当其他变量保持不变时",{"5":{"48":2}}],["它描述了矩阵所能表示的所有输出",{"5":{"50":2}}],["它描述了输入数据如何逐层变换",{"5":{"45":2}}],["它描述了输入中的冗余信息",{"5":{"50":2}}],["它利用链式法则自动计算复杂函数的导数",{"5":{"48":2}}],["它利用负梯度方向作为搜索方向来迭代地最小化目标函数",{"5":{"48":2}}],["它利用中心极限定理的原理来稳定训练过程",{"5":{"96":2}}],["它累积历史梯度来平滑参数更新",{"5":{"48":2}}],["它在统计估计中对应于最小二乘估计的优良性质",{"5":{"70":2}}],["它在输入空间的某个区域激活",{"5":{"71":2}}],["它在更新之前先对梯度进行校正",{"5":{"48":2}}],["它在梯度计算中同样发挥着重要作用",{"5":{"94":2}}],["它结合了动量方法和rmsprop的优点",{"5":{"48":2}}],["它唯一确定了随机变量的概率分布",{"5":{"96":2}}],["它总是存在的",{"5":{"96":2}}],["它考虑了各变量之间的相关性",{"5":{"96":2}}],["它用于权重归一化以稳定训练过程",{"5":{"50":2}}],["它只旋转或反射空间而不改变向量的长度",{"5":{"50":2}}],["它只关注真实类别对应的预测概率的对数值",{"5":{"61":2}}],["它只能学习输入空间的线性划分",{"5":{"159":2}}],["它只能处理线性可分的问题",{"5":{"47":2}}],["它只能建模线性的距离衰减",{"5":{"88":2}}],["它只能在同一组语义空间中计算输入序列各位置之间的关联强度",{"5":{"93":2}}],["它允许我们直接在参数空间中通过梯度上升来优化策略",{"5":{"155":2}}],["它允许我们使用旧策略采样的数据来估计新策略的梯度",{"5":{"154":2}}],["它允许我们将任何联合概率分布无损地分解为一系列条件概率的乘积",{"5":{"140":2}}],["它允许我们用期望的形式表达聚合操作",{"5":{"95":2}}],["它允许使用更大的学习率",{"5":{"41":2}}],["它允许同时处理多个样本以提高计算效率",{"5":{"50":2}}],["它允许序列中任意两个位置之间直接建立联系",{"5":{"92":2,"153":2}}],["它使用温度参数控制的软化top",{"5":{"157":2}}],["它使用完整的轨迹回报来更新策略",{"5":{"155":2}}],["它使用半精度浮点数",{"5":{"50":2}}],["它使得在相同的计算预算下",{"5":{"159":2}}],["它使得期望运算可以分解为各维度的独立期望乘积",{"5":{"145":2}}],["它使得大规模神经网络的端到端训练成为可能",{"5":{"44":2}}],["它使得我们能够高效地计算复合函数对底层变量的梯度",{"5":{"45":2}}],["它代表了给定输入下目标变量的",{"5":{"51":2}}],["它衡量的是一个随机事件发生所带来的",{"5":{"61":2}}],["它衡量的是按照该概率分布采样时",{"5":{"61":2}}],["它同样使用了",{"5":{"61":2}}],["它仅取决于真实分布本身",{"5":{"61":2}}],["它比简单的欧氏距离更适合描述概率分布间的差异",{"5":{"61":2}}],["它注意到rope的旋转性质可以在attention计算中直接利用",{"5":{"88":2}}],["它采用的是加性注入方式",{"5":{"88":2}}],["它采用了一种极其简单的位置编码方式",{"5":{"88":2}}],["它与期望方程的区别在于",{"5":{"156":2}}],["它与显式正则化技术可以协同作用",{"5":{"144":2}}],["它与随机变量具有相同的量纲",{"5":{"96":1}}],["它与随机变量",{"5":{"96":1}}],["它与现有的深度学习框架兼容良好",{"5":{"88":2}}],["它证明了任意周期函数都可以分解为不同频率正弦和余弦函数的线性组合",{"5":{"90":2}}],["它统一处理正弦和余弦分量",{"5":{"90":2}}],["它事先就知道序列中任意两个位置之间是否",{"5":{"92":1,"153":1}}],["它假设所有采样数据都是基于当前策略",{"5":{"154":2}}],["它假设所有位置的旋转角度是均匀的",{"5":{"88":2}}],["它假设依赖关系可以跨越任意距离",{"5":{"92":2,"153":2}}],["它捕获了query空间和key空间之间的关系",{"5":{"94":2}}],["它涉及到softmax和矩阵乘法的复合梯度",{"5":{"94":2}}],["它需要保留足够的细节以供后续层使用",{"5":{"94":2}}],["它需要",{"5":{"95":2}}],["它直接实现了策略梯度定理",{"5":{"155":2}}],["它直接作用于概率分布",{"5":{"70":2}}],["它直接关系到优化算法能否找到全局最优解",{"5":{"97":2}}],["它直接影响收敛速度和最终解的质量",{"5":{"97":2}}],["它决定了每个输入被分配给各个专家的概率",{"5":{"158":2}}],["它决定了每次更新的幅度",{"5":{"48":2}}],["它决定了参数在不同方向上的扩散速率",{"5":{"149":2}}],["它决定了优化的收敛速度",{"5":{"97":2}}],["这提供了最大的计算效率",{"5":{"159":2}}],["这显著减少了对内存带宽的需求",{"5":{"159":2}}],["这验证了条件计算范式的有效性",{"5":{"159":2}}],["这引导门控网络调整参数以使",{"5":{"158":2}}],["这会鼓励门控网络增加对该专家的路由",{"5":{"158":2}}],["这看起来可以接受",{"5":{"157":2}}],["这看似不会导致梯度爆炸",{"5":{"41":2}}],["这已经超出了单个高端gpu的显存容量",{"5":{"157":2}}],["这涉及复杂的梯度计算和反向传播过程",{"5":{"157":2}}],["这涉及到如何组合不同的损失函数",{"5":{"70":2}}],["这涉及到部分和",{"5":{"50":2}}],["这保证了算法向正确的方向优化",{"5":{"155":2}}],["这保证了损失函数值在收敛点处具有良好的连续性",{"5":{"136":2}}],["这大大提高了样本效率",{"5":{"154":2}}],["这完成了证明",{"5":{"149":2}}],["这补偿了relu激活导致的信号减半效应",{"5":{"148":2}}],["这减少了对裁剪的依赖",{"5":{"147":2}}],["这减少了任务间的梯度冲突",{"5":{"133":2}}],["这相当于对参数施加了一个先验约束",{"5":{"147":2}}],["这相当于一种自动的维度压缩",{"5":{"87":2,"152":2}}],["这丢失了原向量在",{"5":{"146":1}}],["这丢失了原向量在方向上的信息",{"5":{"146":1}}],["这节省了一次求和操作",{"5":{"146":2}}],["这导致了训练与推理之间的一致性问题",{"5":{"146":2}}],["这防止了模型对任何类别过度自信",{"5":{"144":2}}],["这实现了自适应的秩分配",{"5":{"142":2}}],["这实际上是一种参数冗余",{"5":{"93":2}}],["这通过引入温度参数",{"5":{"140":1}}],["这通过引入温度参数来实现",{"5":{"140":1}}],["这通常能够提高模型对词级语义的理解能力",{"5":{"101":2}}],["这通常超过了gpu",{"5":{"86":2,"151":2}}],["这确保在不同参数方向上的步长与信息量成反比",{"5":{"139":2}}],["这确保了模型训练和部署的一致性",{"5":{"140":2}}],["这确保了各任务对参数更新的贡献在尺度上是一致的",{"5":{"101":2}}],["这确保了底层的信息不会被高层完全覆盖",{"5":{"92":2,"153":2}}],["这确保了无论模型的隐藏维度如何变化",{"5":{"95":2}}],["这表现为性能曲线在涌现点附近的急剧上升",{"5":{"138":2}}],["这表明在给定",{"5":{"158":2}}],["这表明在给定方差的所有连续分布中",{"5":{"96":2}}],["这表明sgd的有效目标函数倾向于惩罚hessian特征值的总和",{"5":{"149":2}}],["这表明dropout引入的噪声在特征维度上是独立的",{"5":{"145":2}}],["这表明dropout的期望缩放效应可以与线性变换交换顺序",{"5":{"145":2}}],["这表明通过限制",{"5":{"144":2}}],["这表明有效的表示压缩是大模型获得良好泛化能力的关键机制之一",{"5":{"137":2}}],["这表明动量引入了有效的相位滞后",{"5":{"136":2}}],["这表明收敛过程中状态的变化量逐渐减小",{"5":{"136":2}}],["这表明参数与训练数据的相关性越强",{"5":{"133":2}}],["这表明",{"5":{"70":2,"137":2,"147":2}}],["这表明交叉熵的梯度与预测概率与真实标签的差值成正比",{"5":{"70":2}}],["这表明tanh比sigmoid更早进入饱和区域",{"5":{"41":2}}],["这表明拐点处的曲率为零",{"5":{"42":2}}],["这表明优化过程倾向于学习平滑的位置表示",{"5":{"89":2}}],["这表明每个频率成分具有相同的能量",{"5":{"90":2}}],["这表明位置编码的点积只依赖于相对位置",{"5":{"92":2,"153":2}}],["这项工作首次系统地揭示了",{"5":{"138":2}}],["这解释了为什么残差网络在处理",{"5":{"150":2}}],["这解释了为什么某些",{"5":{"138":2}}],["这解释了为什么某些能力在模型规模达到一定阈值时突然出现",{"5":{"137":2}}],["这解释了为什么大模型需要更多数据来维持良好的泛化能力",{"5":{"137":2}}],["这解决了adagrad学习率单调递减的问题",{"5":{"48":2}}],["这解决了深层网络的梯度消失问题",{"5":{"92":2,"153":2}}],["这增加了标签分布的熵",{"5":{"137":2}}],["这增加了优化的复杂性",{"5":{"97":2}}],["这鼓励策略减少选择坏动作的概率",{"5":{"154":2}}],["这鼓励策略增加选择好动作的概率",{"5":{"154":2}}],["这鼓励全局表示和局部表示之间共享信息",{"5":{"137":2}}],["这鼓励表示捕获输入的全局信息同时排除负样本的信息",{"5":{"133":2}}],["这称为",{"5":{"137":2}}],["这称为梯度流动",{"5":{"134":2}}],["这决定了任务",{"5":{"133":1}}],["这决定了任务对共享参数的影响程度",{"5":{"133":1}}],["这建立了",{"5":{"133":2}}],["这具有以下正则化效果",{"5":{"133":2}}],["这具有某种",{"5":{"41":2}}],["这等价于最小化",{"5":{"137":2}}],["这等价于计算查询与各位置标签的互信息近似",{"5":{"132":2}}],["这等于",{"5":{"93":2}}],["这只是表面上的相似",{"5":{"70":2}}],["这两项技术已经成为现代深度学习架构的标准组件",{"5":{"150":2}}],["这两个梯度公式指导了moe模型的端到端优化",{"5":{"158":2}}],["这两个参数的作用至关重要",{"5":{"146":2}}],["这两个定义密切相关但并不等价",{"5":{"41":2}}],["这两种实现方式的数学表达和对模型输出的影响有本质区别",{"5":{"145":2}}],["这两种损失函数分别适用于回归任务和分类任务",{"5":{"70":2}}],["这限制了它在大规模数据集上的效率",{"5":{"69":2}}],["这将为理解infonce的改进提供必要的背景",{"5":{"69":2}}],["这将在第四章详细讨论",{"5":{"47":2}}],["这将在下一节详细讨论",{"5":{"61":2}}],["这些指标提供了对负载均衡状态的全面定量评估",{"5":{"158":2}}],["这些策略从不同角度打破了专家坍缩的正反馈机制",{"5":{"158":2}}],["这些内容构成了理解和实现高效moe系统的理论基础",{"5":{"158":2}}],["这些内容为深入理解注意力机制奠定了坚实的理论基础",{"5":{"86":2,"151":2}}],["这些构成了现代强化学习算法的理论基石",{"5":{"154":2}}],["这些洞见将继续指导深度学习优化理论的发展和实践应用的改进",{"5":{"149":2}}],["这些实践建议都根植于本节所建立的数学理论框架之中",{"5":{"148":2}}],["这些局限性促使研究者开发了层归一化等替代方法",{"5":{"146":2}}],["这些技术的核心价值在于",{"5":{"146":2}}],["这些隐式正则化机制在现代深度学习模型中发挥着重要作用",{"5":{"144":2}}],["这些极小值通常具有更好的泛化性质",{"5":{"144":2}}],["这些经典理论性质需要谨慎对待",{"5":{"140":2}}],["这些经典理论仍然发挥着核心作用",{"5":{"139":2}}],["这些研究正在将涌现从一个神秘现象转变为一个可研究",{"5":{"138":2}}],["这些低频模式逐渐变得",{"5":{"138":2}}],["这些联系为设计新的优化算法提供了理论指导",{"5":{"137":2}}],["这些表达式揭示了互信息的不同侧面",{"5":{"137":2}}],["这些不动点的稳定性可能发生交换",{"5":{"136":2}}],["这些现象是深度学习训练动力学的核心特征",{"5":{"136":2}}],["这些",{"5":{"135":2}}],["这些信息论工具将继续指导我们理解和改进注意力机制",{"5":{"132":2}}],["这些任务在数学上都可以统一为某种形式的预测任务",{"5":{"101":2}}],["这些分析不仅深化了我们对训练动力学的理解",{"5":{"136":2}}],["这些分析建立了一个坚实的理论基础",{"5":{"69":2}}],["这些分析为实践中平衡优化稳定性和正则化强度提供了理论指导",{"5":{"144":2}}],["这些分析为理解大语言模型训练的动力学过程提供了坚实的数学基础",{"5":{"41":2}}],["这些分析为后续章节",{"5":{"70":2,"71":2}}],["这些分析有助于理解可学习编码如何从数据中学习最优的位置表示",{"5":{"89":2}}],["这些嵌入经过层归一化处理后",{"5":{"41":2}}],["这些机制可以从数学角度进行详细分析",{"5":{"41":2}}],["这些约束意味着归一化后的表示位于一个特定的流形上",{"5":{"41":2}}],["这些原则不仅解释了现有激活函数的设计逻辑",{"5":{"41":2}}],["这些神经元按照层次结构组织",{"5":{"46":2}}],["这些超平面的组合形成了一个复杂的决策边界",{"5":{"46":2}}],["这些激活函数各有特点",{"5":{"47":2}}],["这些点可能是极小值",{"5":{"48":2}}],["这些调度策略的目标是在训练初期快速收敛",{"5":{"48":2}}],["这些变换的复合最终将输入数据映射到所需的输出空间",{"5":{"45":2}}],["这些变体反映了深度学习优化研究的持续进展",{"5":{"48":2}}],["这些变体在保持注意力机制核心思想的同时",{"5":{"86":2,"151":2}}],["这些最小值通常具有更好的泛化能力",{"5":{"48":2}}],["这些统计量在机器学习的理论和实践中都有广泛应用",{"5":{"96":2}}],["这些高阶矩在统计分析中有一定应用",{"5":{"96":2}}],["这些定理为机器学习中许多方法的合理性提供了理论依据",{"5":{"96":2}}],["这些范数在稀疏优化和线性规划中有重要应用",{"5":{"50":2}}],["这些运算虽然初看复杂",{"5":{"143":2}}],["这些运算在大语言模型中有着直接的应用场景",{"5":{"50":2}}],["这些运算的吞吐量通常是全精度运算的数倍",{"5":{"50":2}}],["这些梯度可以直接通过损失函数的导数计算得到",{"5":{"148":2}}],["这些梯度可以通过链式法则推导",{"5":{"41":2}}],["这些梯度可以通过注意力输出对各矩阵的雅可比矩阵来计算",{"5":{"94":2}}],["这些梯度的计算非常直接",{"5":{"51":2}}],["这些性质直接决定了雅可比矩阵的谱半径",{"5":{"148":2}}],["这些性质为mle在大样本场景下的应用提供了理论保障",{"5":{"96":2}}],["这些性质对于在实际应用中选择和使用mse作为损失函数提供了理论指导",{"5":{"51":2}}],["这些性质对于理解其数学行为至关重要",{"5":{"87":2,"152":2}}],["这些样本服从某个参数化的概率分布",{"5":{"61":2}}],["这些理论知识为进一步研究注意力机制的理论性质",{"5":{"87":2,"152":2}}],["这些算法将计算复杂度从降低到或",{"5":{"87":1,"152":1}}],["这些算法将计算复杂度从",{"5":{"87":1,"152":1}}],["这些优势在rope的语境中表现为",{"5":{"88":2}}],["这些旋转组合成一个块对角矩阵",{"5":{"88":2}}],["这些矩阵乘法的计算效率直接影响模型的整体性能",{"5":{"50":2}}],["这些矩阵性质如何影响训练动态和泛化性能",{"5":{"89":2}}],["这些矩阵是",{"5":{"94":2}}],["这些基向量是从训练数据中学习的",{"5":{"89":2}}],["这些差异可以从多个角度分析",{"5":{"89":2}}],["这些序列的平均功率为",{"5":{"90":2}}],["这些方法通过直接控制表示中的信息量来正则化模型",{"5":{"133":2}}],["这些方法直接针对注意力分布的信息特性进行优化",{"5":{"132":2}}],["这些方向称为主成分",{"5":{"50":2}}],["这些方程表明",{"5":{"90":2}}],["这些向量之间存在特定的角度关系",{"5":{"91":2}}],["这些值随位置变化显著",{"5":{"91":2}}],["这些可视化有助于建立对位置编码几何结构的直观理解",{"5":{"91":2}}],["这些数学工具不仅为降维和压缩提供了理论基础",{"5":{"143":2}}],["这些数学工具为深入理解位置编码奠定了基础",{"5":{"90":2}}],["这些数学原理不仅指导了深度学习算法的设计",{"5":{"139":2}}],["这些数学分析为理解大语言模型的训练过程提供了必要的理论基础",{"5":{"42":2}}],["这些数学基础为理解后续章节中",{"5":{"61":2}}],["这些数据可以一次性加载到高速缓存中",{"5":{"92":2,"153":2}}],["这些依赖通常是局部的",{"5":{"92":2,"153":2}}],["这些依赖可以是长距离的",{"5":{"92":2,"153":2}}],["这些编码向量的几何分布取决于训练过程中学习到的模式",{"5":{"89":2}}],["这些编码方式将位置映射到一个环面上",{"5":{"92":2,"153":2}}],["这些偏置决定了模型擅长学习哪类函数",{"5":{"92":2,"153":2}}],["这些头输出按维度拼接起来",{"5":{"93":2}}],["这些配置的选择通常基于经验性的消融实验和计算资源约束",{"5":{"93":2}}],["这些解在训练损失上可能相近",{"5":{"97":2}}],["这些损失函数通过注意力机制实现信息聚合",{"5":{"101":2}}],["这意味着门控函数在训练和推理时有所不同",{"5":{"159":2}}],["这意味着超过",{"5":{"159":2}}],["这意味着不仅要增加专家数量",{"5":{"159":2}}],["这意味着不同方向上的噪声强度差异巨大",{"5":{"149":2}}],["这意味着随着计算预算的增加",{"5":{"159":2}}],["这意味着随机优化算法必须依赖某种机制来逃离鞍点",{"5":{"149":2}}],["这意味着增加参数必须同时增加等比例的计算成本",{"5":{"159":2}}],["这意味着状态转移概率只依赖于当前的状态和动作",{"5":{"156":2}}],["这意味着之前采样的轨迹可能不再服从新的策略分布",{"5":{"155":2}}],["这意味着我们需要一套可以直接优化参数化策略的方法论",{"5":{"155":2}}],["这意味着我们不需要显式地计算旋转后的向量",{"5":{"88":2}}],["这意味着归一化操作在开始时对输出影响较小",{"5":{"150":2}}],["这意味着sgd在逃离鞍点时",{"5":{"149":2}}],["这意味着sigmoid",{"5":{"71":2}}],["这意味着噪声的强度与曲率成正比",{"5":{"149":1}}],["这意味着该神经元",{"5":{"148":2}}],["这意味着每经过一层sigmoid激活",{"5":{"148":2}}],["这意味着每个参数应该被更新约",{"5":{"138":2}}],["这意味着每个维度都不是二元的或稀疏的",{"5":{"50":2}}],["这意味着每个位置的输出是值向量的凸组合",{"5":{"92":2,"153":2}}],["这意味着网络前端的权重几乎接收不到有意义的梯度信号",{"5":{"148":2}}],["这意味着网络几乎不学习任何信息",{"5":{"145":2}}],["这意味着即使网络只有有限的深度",{"5":{"150":2}}],["这意味着即使堆叠很多层",{"5":{"148":2}}],["这意味着即使没有显式的正则化项",{"5":{"147":2}}],["这意味着即使模型完全正确分类",{"5":{"70":2}}],["这意味着裁剪将一个可能高度非光滑的问题",{"5":{"147":2}}],["这意味着裁剪操作不会增加梯度的范数",{"5":{"147":2}}],["这意味着层归一化不仅归一化了特征的尺度",{"5":{"146":2}}],["这意味着层归一化在推理时可以处理任意大小的输入",{"5":{"146":2}}],["这意味着衡量了特征向量的",{"5":{"146":1}}],["这意味着为了将均值的估计误差减半",{"5":{"146":2}}],["这意味着为了将损失降低一个固定的量",{"5":{"138":2}}],["这意味着批归一化的计算可以并行化进行",{"5":{"146":2}}],["这意味着空间dropout不会改变特征图的空间维度",{"5":{"145":2}}],["这意味着dropout在引入噪声的同时",{"5":{"145":2}}],["这意味着当梯度爆炸时",{"5":{"147":2}}],["这意味着当梯度较大时",{"5":{"97":2}}],["这意味着当批量大小减小时",{"5":{"146":2}}],["这意味着当",{"5":{"145":1}}],["这意味着当时",{"5":{"145":1}}],["这意味着只有当奇异值超过阈值",{"5":{"144":2}}],["这意味着只要损失函数值有下界",{"5":{"136":2}}],["这意味着谱正则化通过在hessian矩阵的对角线上添加正定项来改善条件数",{"5":{"144":2}}],["这意味着保留概率",{"5":{"144":2}}],["这意味着参数量和数据量对损失降低的贡献大致相等",{"5":{"138":2}}],["这意味着数据量的增加同样重要",{"5":{"138":2}}],["这意味着要使估计精度提高一倍",{"5":{"137":2}}],["这意味着信息在数据处理过程中只会减少",{"5":{"137":2}}],["这意味着沿对应特征方向的迭代会发散",{"5":{"136":2}}],["这意味着收敛速度比任何线性速率都快",{"5":{"136":2}}],["这意味着交叉熵优化的二阶信息直接对应于",{"5":{"70":2}}],["这意味着交叉熵损失没有上界",{"5":{"70":2}}],["这意味着交叉熵在饱和区域的梯度比均方误差大约",{"5":{"70":2}}],["这意味着函数图上任意两点的连线位于函数图的上方",{"5":{"48":2}}],["这意味着函数具有全局最小值",{"5":{"70":2}}],["这意味着函数具有唯一的全局最小值",{"5":{"97":2}}],["这意味着曲率在参数空间中不是恒定的",{"5":{"70":2,"97":2}}],["这意味着均方误差的曲率在整个参数空间中是不变的",{"5":{"70":2}}],["这意味着反向传播的实现可以高度复用",{"5":{"69":2}}],["这意味着自注意力机制通过计算token之间的相似度",{"5":{"69":2}}],["这意味着",{"5":{"42":2,"87":3,"88":2,"89":2,"90":1,"93":2,"95":2,"138":2,"142":2,"145":2,"146":1,"149":1,"152":3,"154":2,"158":2,"159":6}}],["这意味着在合理初始化的网络中",{"5":{"148":2}}],["这意味着在极小点处",{"5":{"147":2}}],["这意味着在训练不稳定时",{"5":{"144":2}}],["这意味着在未饱和区域",{"5":{"41":2}}],["这意味着在附近",{"5":{"42":1}}],["这意味着在",{"5":{"42":1}}],["这意味着gelu不会像relu那样产生",{"5":{"71":2}}],["这意味着堆叠多层网络不会带来任何额外的表达能力",{"5":{"47":2}}],["这意味着如果我们沿着梯度方向移动",{"5":{"48":2}}],["这意味着正交变换是一种保距变换",{"5":{"50":2}}],["这意味着计算一个权重矩阵与向量的乘积需要次标量乘法和加法",{"5":{"50":1}}],["这意味着计算一个",{"5":{"50":1}}],["这意味着是一个随机矩阵",{"5":{"87":1,"152":1}}],["这意味着至少有一个特征值为1",{"5":{"87":2,"152":2}}],["这意味着注意力机制的信息压缩能力是常数级的",{"5":{"87":2,"152":2}}],["这意味着注意力矩阵可以用少数几个主成分近似表示",{"5":{"87":2,"152":2}}],["这意味着注意力权重必须是确定性的",{"5":{"92":2,"153":2}}],["这意味着注意力分数​只依赖于查询和键的相对位置",{"5":{"92":1,"153":1}}],["这意味着注意力分数",{"5":{"92":1,"153":1}}],["这意味着输出表示位于最多维的子空间中",{"5":{"87":1,"152":1}}],["这意味着输出表示位于最多",{"5":{"87":1,"152":1}}],["这意味着两个token之间的注意力分数只取决于它们的相对位置",{"5":{"88":2}}],["这意味着rope注入位置信息的方式是",{"5":{"88":2}}],["这意味着最低频的维度每个位置只旋转极小的角度",{"5":{"88":2}}],["这意味着采用rope的模型在理论上保证了相对位置感知的正确性",{"5":{"88":2}}],["这意味着向量绕原点逆时针旋转了",{"5":{"88":1}}],["这意味着向量",{"5":{"88":1}}],["这意味着满秩的可学习编码可以精确表示正弦余弦编码",{"5":{"89":2}}],["这意味着频率在对数域上是等间隔分布的",{"5":{"90":2}}],["这意味着相邻位置在编码中会有显著差异",{"5":{"90":2}}],["这意味着相邻位置在编码中差异较小",{"5":{"90":2}}],["这意味着​可以通过将​在复平面上旋转角度得到",{"5":{"90":1}}],["这意味着模型无法区分",{"5":{"91":2}}],["这意味着模型在高层通过注意力机制",{"5":{"92":2,"153":2}}],["这意味着模型可以自然地处理任意长度的序列",{"5":{"88":2}}],["这意味着模型可以通过学习和的投影矩阵",{"5":{"95":1}}],["这意味着模型可以通过学习",{"5":{"95":1}}],["这意味着位置编码隐式地编码了相对位置信息",{"5":{"91":2}}],["这意味着编码可以区分相邻位置",{"5":{"90":2}}],["这意味着编码向量都位于高维球面上",{"5":{"91":2}}],["这意味着对于相同的输入序列",{"5":{"91":2}}],["这意味着任意两个位置之间都存在直接的信息传递路径",{"5":{"92":2,"153":2}}],["这意味着从位置到位置的路径长度恒为1",{"5":{"92":1,"153":1}}],["这意味着从位置",{"5":{"92":1,"153":1}}],["这意味着各个头确实学习到了互补的信息",{"5":{"93":2}}],["这意味着多分类交叉熵损失简化为负对数似然",{"5":{"61":2}}],["这意味着多头机制在",{"5":{"93":2}}],["这意味着多头注意力在保持参数总量不变的情况下",{"5":{"94":2}}],["这意味着无论模型深度如何或序列长度如何",{"5":{"94":2}}],["这意味着变换后的表示必然位于一个​维的子空间中",{"5":{"94":1}}],["这意味着变换后的表示必然位于一个",{"5":{"94":1}}],["这意味着微小的输入变化几乎不会引起输出的变化",{"5":{"95":2}}],["这意味着梯度可以",{"5":{"150":2}}],["这意味着梯度总是非负的",{"5":{"42":2}}],["这意味着梯度几乎为零",{"5":{"95":2}}],["这意味着单层注意力需要存储约个浮点数",{"5":{"95":1}}],["这意味着单层注意力需要存储约",{"5":{"95":1}}],["这种损失函数对极端不均衡的惩罚更大",{"5":{"159":2}}],["这种训练",{"5":{"159":2}}],["这种正反馈机制可能导致少数专家主导路由决策",{"5":{"159":2}}],["这种正反馈机制导致专家",{"5":{"158":2}}],["这种非线性门控可以学习输入空间的非线性划分",{"5":{"159":2}}],["这种非线性的引入使得神经网络能够学习任意复杂的输入",{"5":{"71":2}}],["这种雅可比结构表明",{"5":{"159":2}}],["这种高稀疏度带来的效率优势是显著的",{"5":{"159":2}}],["这种稀疏激活模式意味着",{"5":{"159":2}}],["这种稀疏性意味着",{"5":{"159":2}}],["这种稀疏化趋势直接导致注意力矩阵的秩降低",{"5":{"141":2}}],["这种状态可以用专家分配概率的集中度来刻画",{"5":{"158":2}}],["这种负载感知的路由策略可以自然地平衡不同专家的计算负载",{"5":{"157":2}}],["这种通信模式是all",{"5":{"157":2}}],["这种通过学习得到的特征选择机制是注意力机制强大表达能力的重要来源",{"5":{"94":2}}],["这种思想正是ppo",{"5":{"155":2}}],["这种情况被称为",{"5":{"155":2}}],["这种更新方式的问题在于",{"5":{"154":2}}],["这种不平衡可以用专家分配概率的方差来量化",{"5":{"159":2}}],["这种不平衡不仅浪费了模型的大部分参数",{"5":{"159":2}}],["这种不变性意味着layer",{"5":{"150":2}}],["这种不对称性在机器学习中有其实际意义",{"5":{"61":2}}],["这种区别导致了几个重要的实际影响",{"5":{"150":2}}],["这种区分对于理解训练集损失与测试集性能的差异至关重要",{"5":{"51":2}}],["这种特征重用不仅提高了网络的表达能力",{"5":{"150":2}}],["这种特性使得dropout对不同激活值的噪声注入程度与其信号强度相适应",{"5":{"145":2}}],["这种特性使得训练过程对困难样本更加关注",{"5":{"70":2}}],["这种特性使得训练过程自动适应样本的",{"5":{"51":2}}],["这种特性使得模型在训练过程中会特别关注那些预测偏差较大的样本",{"5":{"51":2}}],["这种特性使得模型能够在生成过程中保持一定的随机性和多样性",{"5":{"96":2}}],["这种重新定义看似只是形式上的变化",{"5":{"150":2}}],["这种重参数化使得梯度可以通过",{"5":{"146":2}}],["这种各向异性的扩散导致参数空间的探索不是均匀的",{"5":{"149":2}}],["这种各向异性的压缩效应比各向同性的压缩效应更难以诊断和处理",{"5":{"148":2}}],["这种隐式正则化效应的强度与批量大小成反比",{"5":{"149":2}}],["这种隐式编码方式存在明显的局限性",{"5":{"91":2}}],["这种椭球的几何性质意味着",{"5":{"149":2}}],["这种秩亏性对于理解sgd的隐式正则化效应具有重要意义",{"5":{"149":2}}],["这种估计不可避免地引入了随机噪声",{"5":{"149":2}}],["这种指数敏感性意味着",{"5":{"148":2}}],["这种指数级的状态空间爆炸揭示了直接建模联合概率分布",{"5":{"140":1}}],["这种指数级的状态空间爆炸揭示了直接建模联合概率分布的不可行性",{"5":{"140":1}}],["这种临界状态是极其不稳定的",{"5":{"148":2}}],["这种平衡导致了参数范数的收缩",{"5":{"147":2}}],["这种平滑性是泛化的重要保障",{"5":{"90":2}}],["这种等效关系是非线性的",{"5":{"147":2}}],["这种等效变换在数学上可以理解为某种近端操作",{"5":{"147":2}}],["这种等价性被称为",{"5":{"147":2}}],["这种等价性揭示了dropout正则化效应的本质",{"5":{"145":2}}],["这种等价性提供了理解dropout正则化机制的重要直觉",{"5":{"145":2}}],["这种等价性仅是近似的",{"5":{"145":2}}],["这种等价性可以从两个角度理解",{"5":{"145":2}}],["这种等价性意味着dropout在优化过程中引入了隐式的正则化效应",{"5":{"144":2}}],["这种等价形式更直观地展示了信息瓶颈的优化方向",{"5":{"133":2}}],["这种裁剪方式独立地约束每个梯度分量的绝对值不超过",{"5":{"147":2}}],["这种增长通常遵循某种指数规律",{"5":{"147":2}}],["这种跳跃不仅使优化过程剧烈震荡",{"5":{"147":2}}],["这种缩放不变性意味着平移参数",{"5":{"146":1}}],["这种缩放不变性意味着平移参数不是必需的",{"5":{"146":1}}],["这种维度差异决定了它们在计算特性",{"5":{"146":2}}],["这种维度压缩并非缺点",{"5":{"94":2}}],["这种计算效率的提升在大规模模型",{"5":{"146":2}}],["这种计算方式在数学上与原始定义完全等价",{"5":{"61":2}}],["这种计算方式避免了显式计算任何",{"5":{"61":2}}],["这种操作使得不同样本在特征空间中的相对位置关系得到保持",{"5":{"146":2}}],["这种修正通常被省略",{"5":{"146":2}}],["这种monte",{"5":{"145":2}}],["这种一致性使得模型可以在训练时受益于dropout的正则化效果",{"5":{"145":2}}],["这种一致性有助于理解模型的内部机制",{"5":{"69":2}}],["这种期望缩放效应会在层与层之间累积",{"5":{"145":2}}],["这种期望表达式的形式在理论上具有重要的价值",{"5":{"95":2}}],["这种交叉连接使得一个尺度的特征能够影响另一个尺度的特征学习",{"5":{"150":2}}],["这种交互作用启示我们",{"5":{"144":2}}],["这种交替模式区别于单调收敛",{"5":{"136":2}}],["这种限制有助于提高模型对对抗扰动的鲁棒性",{"5":{"144":2}}],["这种梯度重新分配产生了类似于正则化的效果",{"5":{"144":2}}],["这种自适应机制使得highway网络能够处理更广泛的任务和数据分布",{"5":{"150":2}}],["这种自适应的学习率调整使优化过程更加稳定",{"5":{"97":2}}],["这种自动调节机制有助于稳定不同层的梯度流",{"5":{"144":2}}],["这种随机梯度等效于在原始梯度上添加了与权重成正比的正则化项",{"5":{"144":2}}],["这种机制允许模型在不同任务上自动选择合适的秩",{"5":{"142":2}}],["这种简单的门控往往导致门控权重高度集中于少数专家",{"5":{"159":2}}],["这种简单性掩盖了深层次的问题",{"5":{"88":2}}],["这种简化大大降低了计算复杂度",{"5":{"140":2}}],["这种线性复杂度使得长序列生成成为可能",{"5":{"140":2}}],["这种逐位置的路由策略允许序列中不同位置的token激活不同的专家",{"5":{"158":2}}],["这种逐步生成的方式不仅是计算上可行的",{"5":{"140":2}}],["这种逐元素应用的方式保持了向量结构",{"5":{"71":2}}],["这种条件门控的数学形式为",{"5":{"159":2}}],["这种条件化的计算模式意味着",{"5":{"159":2}}],["这种条件概率框架构成了自回归模型的基础",{"5":{"140":2}}],["这种条件依赖假设是自回归语言建模的理论基础",{"5":{"101":2}}],["这种近似虽然不是精确的梯度",{"5":{"157":2}}],["这种近似是现代统计学习的核心挑战",{"5":{"140":2}}],["这种近似在分析神经网络的学习动态",{"5":{"96":2}}],["这种概率视角不仅为我们提供了严格的数学框架",{"5":{"140":2}}],["这种概率解释不仅加深了我们对激活函数数学本质的理解",{"5":{"71":2}}],["这种概率解释与注意力机制中的softmax概率分布形成了数学上的呼应",{"5":{"71":2}}],["这种概率解释在逻辑回归",{"5":{"47":2}}],["这种概率解释具有优雅的理论性质",{"5":{"95":2}}],["这种解释将涌现能力从神秘现象转化为可量化的统计事件",{"5":{"138":2}}],["这种能力积累存在阈值效应",{"5":{"138":2}}],["这种行为模式被称为",{"5":{"138":2}}],["这种策略的核心思想是",{"5":{"138":2}}],["这种策略的优点是保留了最重要的连接",{"5":{"86":2,"151":2}}],["这种关系可以用一个简洁的数学形式来描述",{"5":{"138":2}}],["这种关系是位置编码能够表达位置信息的核心",{"5":{"91":2}}],["这种基于神经网络的估计方法具有高度的灵活性",{"5":{"137":2}}],["这种基于秩的外推假设了位置编码的平滑性",{"5":{"89":2}}],["这种动态行为降低了模型对特定参数配置的依赖",{"5":{"136":2}}],["这种动态系统的视角不仅深化了我们对优化算法的理解",{"5":{"134":2}}],["这种周期倍增过程会持续进行",{"5":{"136":2}}],["这种现象并非偶然的技术细节",{"5":{"148":2}}],["这种现象被称为",{"5":{"143":2,"154":2}}],["这种现象在动力系统中称为分岔",{"5":{"136":2}}],["这种现象称为梯度饱和",{"5":{"41":2,"42":2}}],["这种震荡现象既有积极的一面",{"5":{"136":2}}],["这种相图结构保证了优化过程不会",{"5":{"147":2}}],["这种相变是深度学习区别于其他方法的关键特征",{"5":{"133":2}}],["这种相似性表明",{"5":{"101":2}}],["这种相似性反映了两个函数在数学结构上的内在联系",{"5":{"42":2}}],["这种信息选择能力的数学基础正是信息论中的互信息和熵等核心概念",{"5":{"132":2}}],["这种偏好必然来自于优化过程本身的隐式效应",{"5":{"149":2}}],["这种偏好调整正是对齐的目标",{"5":{"101":2}}],["这种偏差可能表现为",{"5":{"138":2}}],["这种偏差",{"5":{"51":2}}],["这种专门化是多任务学习成功的关键因素之一",{"5":{"101":2}}],["这种专业化是transformer强大表达能力的重要来源",{"5":{"92":2,"153":2}}],["这种专业化现象部分源于随机初始化和训练过程中的随机梯度下降",{"5":{"93":2}}],["这种归一化确保了噪声的标准差与输入的门控得分成正比",{"5":{"159":2}}],["这种归一化确保了",{"5":{"159":2}}],["这种归一化确保每个任务对更新方向的贡献在范数上是一致的",{"5":{"101":2}}],["这种归一化通过在激活专家的权重上重新应用softmax实现",{"5":{"158":2}}],["这种归一化具有以下几个重要的数学效应",{"5":{"41":2}}],["这种混合注意力模式与前缀语言建模损失的设计是一致的",{"5":{"101":2}}],["这种混合方法在某些任务上可能有所提升",{"5":{"88":2}}],["这种跨章节的联系体现了",{"5":{"70":2}}],["这种深层的数学联系揭示了",{"5":{"70":2}}],["这种差异同样可以从概率论角度解释",{"5":{"138":2}}],["这种差异反映了强化学习与监督学习在优化目标上的根本区别",{"5":{"101":2}}],["这种差异可以从链式法则的角度理解",{"5":{"70":2}}],["这种差异在数学上体现为信息传递的路径长度",{"5":{"92":2,"153":2}}],["这种差异源于损失函数的数学结构和对数据分布的敏感性",{"5":{"97":2}}],["这种对比不仅体现在计算效率上",{"5":{"159":2}}],["这种对比不仅有助于深入理解损失函数的设计原理",{"5":{"70":2}}],["这种对数变换将概率空间的非线性关系转化为线性尺度上的度量",{"5":{"61":2}}],["这种对偶性体现在多个方面",{"5":{"90":2}}],["这种对偶性在位置编码中有深刻的体现",{"5":{"90":2}}],["这种对偶性使得位置编码能够同时在两个域中表示信息",{"5":{"90":2}}],["这种统一性不仅具有理论美感",{"5":{"69":2}}],["这种统一的数学结构将在本章第四节",{"5":{"61":2}}],["这种设计既保持了多尺度特性",{"5":{"150":2}}],["这种设计特别适合于需要处理多尺度信息的任务",{"5":{"150":2}}],["这种设计有效缓解了深层网络中的梯度消失问题",{"5":{"146":2}}],["这种设计有两个好处",{"5":{"92":2,"153":2}}],["这种设计被称为",{"5":{"146":2}}],["这种设计灵感来源于人类认知系统中的注意力现象",{"5":{"132":2}}],["这种设计反映了实际应用场景",{"5":{"101":2}}],["这种设计选择带来了几个关键优势",{"5":{"69":2}}],["这种设计保持了卷积的空间结构",{"5":{"41":2}}],["这种设计体现了数学的优雅",{"5":{"88":2}}],["这种设计的数学优势在于",{"5":{"158":2}}],["这种设计的数学形式为",{"5":{"93":2}}],["这种设计的思路是",{"5":{"88":2}}],["这种设计的优势在于简单直观",{"5":{"88":2}}],["这种设计的动机是",{"5":{"88":2}}],["这种设计的直觉是",{"5":{"89":2}}],["这种设计没有引入额外的可学习参数",{"5":{"88":2}}],["这种设计可以看作是",{"5":{"89":2}}],["这种设计与频率域和位置域的对偶性完全一致",{"5":{"90":2}}],["这种设计确保了频率分辨率与位置分辨率的匹配",{"5":{"90":2}}],["这种设计使得网络能够自适应地决定每个层应该执行多少非线性变换",{"5":{"150":2}}],["这种设计使得层归一化在处理变长序列和单个样本时更加稳定",{"5":{"146":2}}],["这种设计使得每个特征通道的归一化是独立进行的",{"5":{"146":2}}],["这种设计使得编码能够在多个数量级上同时捕获位置信息",{"5":{"90":2}}],["这种设计使得低维度能够编码位置的粗粒度信息",{"5":{"91":2}}],["这种设计使得位置编码成为模型的",{"5":{"91":2}}],["这种设计使得模型能够学习到什么样的信息应该被",{"5":{"94":2}}],["这种设计引入了一种归纳偏置",{"5":{"93":2}}],["这种设计在处理多语言",{"5":{"159":2}}],["这种设计在推理时特别有利",{"5":{"93":2}}],["这种设计在保持参数效率的同时",{"5":{"93":2}}],["这种设计在保持大部分推理效率提升的同时",{"5":{"93":2}}],["这种设计将计算量从降低到",{"5":{"93":1}}],["这种设计将计算量从",{"5":{"93":1}}],["这种设计允许模型学习到复杂的头间交互模式",{"5":{"93":2}}],["这种设计允许模型在不同的语义空间中表示同一个输入",{"5":{"94":2}}],["这种设计蕴含着深刻的语义洞见",{"5":{"94":2}}],["这种设计哲学",{"5":{"95":2}}],["这种",{"5":{"41":4,"69":4,"70":2,"71":8,"89":2,"91":4,"92":12,"93":4,"101":2,"138":4,"141":2,"144":2,"145":2,"148":2,"149":2,"150":2,"153":12,"159":6}}],["这种极端分布使得反向传播时产生大的梯度",{"5":{"41":2}}],["这种稳定性对于深层网络的成功训练至关重要",{"5":{"41":2}}],["这种规范化意味着激活值的尺度不会随网络深度增加而累积变化",{"5":{"41":2}}],["这种渐近行为表明sigmoid函数在正无穷处趋向于饱和值1",{"5":{"42":2}}],["这种积分",{"5":{"42":2}}],["这种单向梯度可能导致优化过程的",{"5":{"42":2}}],["这种局限性在处理现实世界的复杂数据时尤为突出",{"5":{"71":2}}],["这种有损压缩在机器学习中是有益的",{"5":{"71":2}}],["这种定义使得gelu具有概率解释",{"5":{"71":2}}],["这种分岔产生准周期轨道",{"5":{"136":2}}],["这种分岔称为超临界",{"5":{"136":2}}],["这种分岔称为",{"5":{"136":2}}],["这种分割不会影响计算的数学正确性",{"5":{"45":2}}],["这种分析不仅具有理论意义",{"5":{"71":2}}],["这种分析对于诊断模型过拟合和理解模型容量都很有价值",{"5":{"50":2}}],["这种分解性使得梯度可以通过反向传播算法高效计算",{"5":{"140":2}}],["这种分解性使得梯度计算变得可行",{"5":{"140":2}}],["这种分解对于分析注意力机制的信息流动方向至关重要",{"5":{"87":2,"152":2}}],["这种分解将原始的计算转换为计算",{"5":{"87":1,"152":1}}],["这种分解将原始的",{"5":{"87":1,"152":1}}],["这种分解将参数数量从减少到",{"5":{"87":1,"152":1}}],["这种分解将参数数量从",{"5":{"87":1,"152":1}}],["这种分布意味着噪声主要沿着少数几个方向",{"5":{"149":2}}],["这种分布的持续变化会增加模型训练的难度",{"5":{"146":2}}],["这种分布表明编码主要沿少数几个方向分散",{"5":{"89":2}}],["这种分布表明编码在各方向上均匀分散",{"5":{"89":2}}],["这种分布表明编码存在明显的维度分层",{"5":{"89":2}}],["这种分布在多个数量级上提供了一致的分辨率",{"5":{"90":2}}],["这种分布反映了位置编码的内在结构",{"5":{"89":2}}],["这种分布反映了编码向量在高维球面上的均匀性",{"5":{"91":2}}],["这种分离允许模型学习到高度非线性的匹配函数",{"5":{"94":2}}],["这种连接模式可以用矩阵乘法简洁地表示",{"5":{"46":2}}],["这种层次化的注意力学习是大语言模型能力的重要来源",{"5":{"71":2}}],["这种层叠结构通过矩阵运算的复合来表示",{"5":{"46":2}}],["这种层级化的依赖建模是transformer成功捕获复杂语言结构的关键因素",{"5":{"92":2,"153":2}}],["这种层级化的模式与人类语言的层级结构",{"5":{"92":2,"153":2}}],["这种层级化结构使得模型能够建模更加复杂的长程依赖",{"5":{"92":2,"153":2}}],["这种层级化可以通过信息论的工具进行量化",{"5":{"92":2,"153":2}}],["这种抽象化过程正是深度学习成功的关键数学机制",{"5":{"71":2}}],["这种抽象使得我们可以利用线性代数和逻辑代数的工具来分析神经网络的行为",{"5":{"47":2}}],["这种视角揭示了激活函数在信息处理中的深层作用",{"5":{"71":2}}],["这种视角对于理解神经网络的输出解释和损失函数设计具有重要意义",{"5":{"47":2}}],["这种矩阵表示的优势在于它可以利用现代硬件",{"5":{"47":2}}],["这种选择性与生物神经元的行为类似",{"5":{"71":2}}],["这种选择影响着生成样本的多样性和质量",{"5":{"96":2}}],["这种运算在模型的权重初始化",{"5":{"50":2}}],["这种密集表示使得模型能够在连续空间中学习平滑的语义关系",{"5":{"50":2}}],["这种压缩不是简单的信息丢弃",{"5":{"133":2}}],["这种压缩不一定是坏事",{"5":{"87":2,"152":2}}],["这种压缩本质上是信息的有损编码",{"5":{"71":1}}],["这种压缩本质上是信息的",{"5":{"71":1}}],["这种压缩是显著的",{"5":{"50":2}}],["这种转化在推导反向传播公式和优化算法时非常有用",{"5":{"50":2}}],["这种技术使得离散随机变量的重参数化成为可能",{"5":{"71":2}}],["这种技术在训练大语言模型时尤为重要",{"5":{"50":2}}],["这种几何特性进一步解释了为什么sgd能够有效地探索参数空间",{"5":{"149":2}}],["这种几何变换对优化过程有重要影响",{"5":{"144":2}}],["这种几何稀疏性在softmax归一化后被放大",{"5":{"141":2}}],["这种几何差异反映了两种损失函数对",{"5":{"70":2}}],["这种几何表示使得我们可以利用空间中的几何关系来理解和操作语义信息",{"5":{"50":2}}],["这种几何视角揭示了mse的一个核心性质",{"5":{"51":2}}],["这种几何关系在任何维度上都成立",{"5":{"51":2}}],["这种几何结构深刻地影响了sgd的优化轨迹",{"5":{"149":2}}],["这种几何结构在统计学习理论中具有基础性的地位",{"5":{"61":2}}],["这种几何结构使得注意力机制能够自然地学习到与位置距离相关的注意力模式",{"5":{"91":2}}],["这种几何解释与旋转位置编码",{"5":{"90":2}}],["这种几何解释与主成分分析",{"5":{"94":2}}],["这种敏感性可以从数学上量化",{"5":{"51":2}}],["这种敏感性差异在实际应用中有重要影响",{"5":{"51":2}}],["这种饱和效应会逐层累积放大",{"5":{"42":2}}],["这种饱和行为会导致两个问题",{"5":{"61":2}}],["这种饱和行为会导致严重的梯度问题",{"5":{"95":2}}],["这种融合不仅减少了冗余计算",{"5":{"61":2}}],["这种联系是理解sgd隐式正则化效应的关键",{"5":{"149":2}}],["这种联系最初由wan等人于2013年在论文",{"5":{"145":2}}],["这种联系揭示了为什么softmax与交叉熵的组合在优化上具有特殊的性质",{"5":{"71":2}}],["这种联系不仅在理论上具有重要意义",{"5":{"61":2}}],["这种联系表明",{"5":{"87":2,"152":2}}],["这种做法存在几个根本性的问题",{"5":{"61":2}}],["这种模式引入了局部性的归纳偏置",{"5":{"86":2,"151":2}}],["这种模式可以在保持计算量不变的情况下扩大感受野",{"5":{"86":2,"151":2}}],["这种模式在longformer",{"5":{"86":2,"151":2}}],["这种模式对应于非正定的gram矩阵",{"5":{"86":2,"151":2}}],["这种模式不利于gpu的并行执行",{"5":{"92":2,"153":2}}],["这种模式揭示了transformer处理长程依赖的策略",{"5":{"92":2,"153":2}}],["这种格式可以利用密集矩阵乘法的高效实现",{"5":{"86":2,"151":2}}],["这种映射关系使得sigmoid函数在概率建模中具有直接的应用价值",{"5":{"71":2}}],["这种映射的优点是数值稳定且计算简单",{"5":{"86":2,"151":2}}],["这种映射对应于多项式核函数",{"5":{"86":2,"151":2}}],["这种映射对应于高斯核或rbf核",{"5":{"86":2,"151":2}}],["这种递推关系揭示了sigmoid非线性变换的深层数学结构",{"5":{"42":2}}],["这种递归形式的复杂度为每步",{"5":{"86":1,"151":1}}],["这种递归形式的复杂度为",{"5":{"86":1,"151":1}}],["这种形式的计算复杂度为",{"5":{"86":2,"151":2}}],["这种形式的门控允许对不同位置",{"5":{"86":2,"151":2}}],["这种快速饱和特性仍然是tanh的主要局限性",{"5":{"42":2}}],["这种快速衰减的奇异值谱是低秩结构的直接证据",{"5":{"87":2,"152":2}}],["这种并行性恰恰是其致命缺陷的根源",{"5":{"88":2}}],["这种并行性是深度学习在大规模数据和模型上高效训练的基础",{"5":{"45":2}}],["这种并行性是transformer相比rnn在计算效率上具有巨大优势的根本原因",{"5":{"95":2}}],["这种加法操作虽然简单直接",{"5":{"88":2}}],["这种表示形式本质上是一种软混合",{"5":{"159":2}}],["这种表示强调了专家之间的并行性",{"5":{"158":2}}],["这种表示方法不仅是深度学习框架实现高效计算的理论基础",{"5":{"46":2}}],["这种表示并非数学游戏",{"5":{"88":2}}],["这种表示具有平移不变性和对称性",{"5":{"90":2}}],["这种多频率的设计实现了频谱的",{"5":{"88":2}}],["这种多分辨率分析与小波变换",{"5":{"90":2}}],["这种多尺度设计使得位置编码能够适应不同粒度的位置相关建模需求",{"5":{"90":2}}],["这种多尺度的频率设计",{"5":{"88":2}}],["这种多尺度的频率表示与人类感知和处理信息的方式有着有趣的相似性",{"5":{"90":2}}],["这种多尺度的频率结构具有深刻的数学意义",{"5":{"90":2}}],["这种多尺度的编码类似于小波变换",{"5":{"91":2}}],["这种多尺度的波动模式是位置编码的核心特征",{"5":{"91":2}}],["这种多样化的分布模式使得模型能够同时捕获全局信息和局部细节",{"5":{"92":2,"153":2}}],["这种多阶段训练策略反映了损失函数优化性质的复杂性和实际应用的需求",{"5":{"97":2}}],["这种变换保持了几何结构的许多性质",{"5":{"71":2}}],["这种变换是可逆的",{"5":{"88":2}}],["这种变换可以分解为旋转",{"5":{"94":2}}],["这种实现方式充分利用了现代gpu的向量化计算能力",{"5":{"88":2}}],["这种调制方式具有抗噪声能力强",{"5":{"88":2}}],["这种调整的效果是",{"5":{"88":2}}],["这种从经验到理论的发展",{"5":{"88":2}}],["这种灵活性使得softmax成为处理各种复杂模式的强大工具",{"5":{"69":2}}],["这种灵活性是可学习编码的主要优势",{"5":{"89":2}}],["这种采样偏差可能导致模型在某些位置上的表现不佳",{"5":{"89":2}}],["这种构造方式具有深刻的数学理论基础和优雅的性质",{"5":{"90":2}}],["这种编码将位置信息嵌入到固定范围的正弦和余弦函数中",{"5":{"71":2}}],["这种编码方式在数学上具有完备性",{"5":{"90":2}}],["这种编码利用不同频率的正弦和余弦函数来唯一标识序列中的每个位置",{"5":{"91":2}}],["这种编码利用不同频率的正弦和余弦函数来唯一标识每个位置",{"5":{"92":2,"153":2}}],["这种覆盖范围对于常见的序列长度",{"5":{"90":2}}],["这种先验信息与损失函数的交互决定了模型的学习动态",{"5":{"70":2}}],["这种先验防止模型学习过于高频的位置变化模式",{"5":{"90":2}}],["这种结构上的一致性暗示了一个更深层次的统一性",{"5":{"69":2}}],["这种结构与infonce中计算正样本相似度的结构高度相似",{"5":{"69":2}}],["这种结构与许多自然数据",{"5":{"71":2}}],["这种结构与交叉熵的",{"5":{"97":2}}],["这种结构在训练初期可能不稳定",{"5":{"41":2}}],["这种结构将层归一化置于残差分支内部",{"5":{"41":1}}],["这种结构使得梯度计算可以高效地逐元素进行",{"5":{"71":2}}],["这种结构影响着优化算法的行为",{"5":{"48":2}}],["这种结构恰好对应于rope的块对角矩阵形式",{"5":{"88":2}}],["这种结构化参数具有更好的泛化能力",{"5":{"90":2}}],["这种结合方式的数学意义在于",{"5":{"91":2}}],["这种均匀分布确保了不同位置之间的区分性",{"5":{"91":2}}],["这种方法通过在目标函数中添加kl散度惩罚项",{"5":{"154":2}}],["这种方法基于经验观察",{"5":{"147":2}}],["这种方法的数学基础是",{"5":{"145":2}}],["这种方法的数学基础正是低秩近似理论",{"5":{"50":2}}],["这种方法的时间复杂度为",{"5":{"88":2}}],["这种方法的核心思想是",{"5":{"89":2}}],["这种方法需要对attention计算流程进行较大改动",{"5":{"88":2}}],["这种方法在模型压缩和少样本学习中有一定的应用价值",{"5":{"48":2}}],["这种方法在推理时能够处理更长的序列",{"5":{"92":2,"153":2}}],["这种方式下",{"5":{"91":2}}],["这种方式进一步确保了被掩码位置的信息完全不会泄露",{"5":{"91":2}}],["这种数学统一性意味着",{"5":{"70":2}}],["这种数学结构的一致性意味着",{"5":{"61":2}}],["这种数学上的",{"5":{"70":2}}],["这种数学上的相似性意味着",{"5":{"70":2}}],["这种数学上的简洁性不仅在理论上令人赞叹",{"5":{"42":2}}],["这种数学上的同构性并非偶然",{"5":{"61":2}}],["这种数值分布反映了多尺度编码的特性",{"5":{"91":2}}],["这种下降在低维度",{"5":{"91":2}}],["这种直接依赖关系是注意力机制建模长程依赖的数学基础",{"5":{"92":2,"153":2}}],["这种全局",{"5":{"147":2}}],["这种全局信息访问能力是自注意力机制相比传统序列模型的关键优势",{"5":{"132":2}}],["这种全局性使得注意力机制在理论上能够建模任意复杂的长程依赖模式",{"5":{"92":2,"153":2}}],["这种全面的理解为学习后续章节",{"5":{"51":2}}],["这种全连接结构与卷积结构形成鲜明对比",{"5":{"92":2,"153":2}}],["这种双重非线性的组合使得transformer能够学习极其复杂的函数映射",{"5":{"71":2}}],["这种双重依赖使得注意力能够建模多种类型的长程依赖",{"5":{"92":2,"153":2}}],["这种组合损失结合了两种损失函数的优点",{"5":{"51":2}}],["这种组合策略在核方法文献中被称为",{"5":{"93":2}}],["这种在多个子空间中的并行计算赋予了多头注意力更强的特征提取能力",{"5":{"93":2}}],["这种配置在多头注意力的语境下使用",{"5":{"94":2}}],["这种配置下的投影矩阵都是的方阵",{"5":{"94":1}}],["这种配置下的投影矩阵都是",{"5":{"94":1}}],["这种初始化意味着初始位置编码不提供任何位置信息",{"5":{"89":2}}],["这种初始化确保了信号和梯度在正向传播和反向传播过程中都不会过度放大或衰减",{"5":{"94":2}}],["这种学习到的相似度度量不同于简单的余弦相似度或欧氏距离",{"5":{"94":2}}],["这种温度控制的视角揭示了一个重要的训练技巧",{"5":{"95":2}}],["这种掩码与因果掩码可以叠加使用",{"5":{"95":2}}],["这种恒定曲率的性质使得均方误差的优化具有可预测的动态特性",{"5":{"97":2}}],["这种参数量与计算量的解耦是moe最核心的优势",{"5":{"159":2}}],["这种参数化确保",{"5":{"145":2}}],["这种参数简化提高了样本效率",{"5":{"87":2,"152":2}}],["这种参数依赖的曲率使得交叉熵的优化动态比均方误差更为复杂",{"5":{"97":2}}],["这种性质部分源于过参数化带来的隐式正则化效应",{"5":{"97":2}}],["这种约束可以通过多种方式实现",{"5":{"154":2}}],["这种约束可能与",{"5":{"147":2}}],["这种约束有助于提高模型的稳定性和泛化能力",{"5":{"50":2}}],["这种约束在反向传播中会产生特定的梯度模式",{"5":{"97":2}}],["这个期望损失对门控网络参数",{"5":{"159":2}}],["这个期望可以解释为",{"5":{"69":2}}],["这个选择是确定性的",{"5":{"159":2}}],["这个选择背后有深刻的数学考量",{"5":{"71":2}}],["这个因子小于1",{"5":{"159":2}}],["这个经验公式表明",{"5":{"159":2}}],["这个损失只在",{"5":{"158":2}}],["这个损失的目标是使得",{"5":{"158":2}}],["这个损失对极端不均衡的惩罚更大",{"5":{"158":2}}],["这个损失存在问题",{"5":{"158":2}}],["这个损失在",{"5":{"158":6,"159":2}}],["这个损失函数鼓励奖励模型给人类偏好的回复更高的分数",{"5":{"154":2}}],["这个损失函数具有清晰的似然解释",{"5":{"61":2}}],["这个条件概率反映了专家之间的协同或竞争关系",{"5":{"158":2}}],["这个方程表明",{"5":{"156":2}}],["这个方程的解可以通过线性常微分方程的理论分析",{"5":{"97":2}}],["这个交互过程可以描述为",{"5":{"156":2}}],["这个框架包含以下几个核心要素",{"5":{"156":2}}],["这个奖励模型的输出可以看作是",{"5":{"155":2}}],["这个奖励函数直接由策略和参考策略的比率定义",{"5":{"101":2}}],["这个最优基线的计算通常也很复杂",{"5":{"155":2}}],["这个估计量是无偏的",{"5":{"155":2}}],["这个估计越可靠",{"5":{"96":2}}],["这个惩罚项保证了优化后的语言模型不会产生与人类标注数据风格差异过大的输出",{"5":{"154":2}}],["这个阶段的目标是得到一个",{"5":{"154":2}}],["这个比率衡量的是",{"5":{"154":2}}],["这个比例保证了足够的上下文信息同时提供了足够的预测挑战",{"5":{"101":2}}],["这个比例趋于平稳值",{"5":{"87":2,"152":2}}],["这个扩展的变换空间包含所有可能的恒等映射",{"5":{"150":2}}],["这个高斯近似不仅在理论上具有重要意义",{"5":{"149":2}}],["这个临界状态是极其脆弱的",{"5":{"148":2}}],["这个标量分析虽然高度简化",{"5":{"148":2}}],["这个简单的表达式揭示了梯度随深度变化的指数规律",{"5":{"148":2}}],["这个简洁的梯度形式是交叉熵损失被广泛采用的重要原因之一",{"5":{"96":2}}],["这个传播过程涉及大量雅可比矩阵的连乘",{"5":{"148":2}}],["这个解释在分析裁剪对优化动力学的影响时非常有用",{"5":{"147":2}}],["这个几何约束确保了参数更新的最大步长不超过",{"5":{"147":2}}],["这个操作的本质是将超球面外的梯度向量投影到超球面上",{"5":{"147":2}}],["这个均值表示第",{"5":{"146":1}}],["这个均值表示第个特征在当前批量中的中心位置",{"5":{"146":1}}],["这个引理表明",{"5":{"146":2}}],["这个kl散度项起到正则化作用",{"5":{"145":2}}],["这个二次函数的性质对于dropout的超参数选择具有指导意义",{"5":{"145":2}}],["这个独立性假设是dropout数学分析的基础",{"5":{"145":2}}],["这个正则化项倾向于将权重推向低秩方向",{"5":{"142":2}}],["这个熵值量化了注意力分布的集中程度",{"5":{"132":2}}],["这个问题有一个解析解",{"5":{"101":2}}],["这个问题看似简单",{"5":{"45":2}}],["这个更新可以理解为在欧几里得度量下进行的",{"5":{"70":2}}],["这个流形的几何性质",{"5":{"70":2}}],["这个流程在神经网络中的实现非常直接",{"5":{"61":2}}],["这个缩放因子的作用与温度参数类似",{"5":{"69":2}}],["这个理论结果解释了为什么对比学习能够学习到有用的表示",{"5":{"69":2}}],["这个下界表明",{"5":{"148":2}}],["这个下界会越来越紧",{"5":{"69":2}}],["这个下界确保了即使的梯度很小",{"5":{"41":1}}],["这个下界确保了即使",{"5":{"41":1}}],["这个初始化确保了激活值的方差在前向传播中保持稳定",{"5":{"41":2}}],["这个反函数在逻辑回归中具有重要意义",{"5":{"42":2}}],["这个关系表明",{"5":{"42":2}}],["这个定理对于理解深层网络中dropout的累积效应具有重要意义",{"5":{"145":2}}],["这个定理的深刻之处在于",{"5":{"155":2}}],["这个定理的关键洞察是",{"5":{"69":2}}],["这个定理的数学意义极其深远",{"5":{"71":2}}],["这个定理是反向传播中权重梯度计算的核心公式",{"5":{"44":2}}],["这个定理揭示了反向传播的本质机制",{"5":{"44":2}}],["这个定理揭示了前向传播的数学本质",{"5":{"45":2}}],["这个定理揭示了relu网络的一个重要性质",{"5":{"45":2}}],["这个定理揭示了一个深刻的事实",{"5":{"47":2}}],["这个定理揭示了单层神经网络的根本局限性",{"5":{"47":2}}],["这个定理虽然使用了简化的阈值激活函数",{"5":{"46":2}}],["这个定理表明",{"5":{"47":2,"145":2,"147":2,"148":2}}],["这个定义确保了输出权重满足概率分布的基本性质",{"5":{"159":2}}],["这个定义将注意力机制抽象为一个信息选择算子",{"5":{"132":2}}],["这个定义与标准交叉熵的区别在于",{"5":{"101":2}}],["这个定义包含了线性映射的两个核心性质",{"5":{"71":2}}],["这个定义的数学意义非常明确",{"5":{"51":2}}],["这个超平面将输入空间分为两个半空间",{"5":{"71":2}}],["这个恒等式可以从代数上验证",{"5":{"71":2}}],["这个不等式的含义是",{"5":{"69":2}}],["这个不等式给出了梯度范数的精确边界",{"5":{"41":2}}],["这个不等式表明",{"5":{"71":2}}],["这个变量在反向传播过程中起着桥梁作用",{"5":{"44":2}}],["这个变换的逆变换为​",{"5":{"71":1}}],["这个变换的逆变换为",{"5":{"71":1}}],["这个变换包含两个步骤",{"5":{"47":2}}],["这个变换可以分解为三个步骤",{"5":{"87":2,"152":2}}],["这个变换将从原始的嵌入空间",{"5":{"94":1}}],["这个变换将",{"5":{"94":1}}],["这个决策边界始终是线性的",{"5":{"46":2}}],["这个特征映射可以表示为复合函数",{"5":{"47":2}}],["这个信息对于确定参数更新的方向和幅度至关重要",{"5":{"48":2}}],["这个公式为设计moe模型的超参数提供了理论指导",{"5":{"159":2}}],["这个公式的数学含义是",{"5":{"157":2}}],["这个公式表明",{"5":{"48":2,"71":2,"138":2,"157":2}}],["这个公式清楚地表明",{"5":{"48":2}}],["这个公式在计算方差时更加实用",{"5":{"96":2}}],["这个公式中的底数10000和指数",{"5":{"88":2}}],["这个近似在局部区域非常准确",{"5":{"48":2}}],["这个近似将原始矩阵分解为k个秩一矩阵的和",{"5":{"50":2}}],["这个向量的每一个维度都编码了某种语义或语法特征",{"5":{"50":2}}],["这个向量形式的表示不仅在数学上更加简洁",{"5":{"51":2}}],["这个向量被旋转了一个角度",{"5":{"88":2}}],["这个向量随后经过softmax变换",{"5":{"96":2}}],["这个推导揭示了sgd隐式正则化效应的数学本质",{"5":{"149":2}}],["这个推导表明",{"5":{"51":2,"87":2,"152":2}}],["这个推论揭示了一个重要的简化",{"5":{"44":2}}],["这个子流形就是",{"5":{"51":2}}],["这个优化过程涉及对损失函数求导",{"5":{"48":2}}],["这个优化问题可以形式化为",{"5":{"51":2}}],["这个分布可以分解为多个子分布的组合",{"5":{"138":2}}],["这个分布在重参数化梯度计算中扮演关键角色",{"5":{"71":2}}],["这个分解不一定是最佳的",{"5":{"143":2}}],["这个分解利用了序列数据的有序性质",{"5":{"101":2}}],["这个分解表明",{"5":{"51":2,"89":2}}],["这个分解将模型在未知数据上的期望mse分解为三个部分",{"5":{"51":2}}],["这个分解的推导过程如下",{"5":{"51":2}}],["这个分解揭示了注意力矩阵与输入数据内在结构的关系",{"5":{"87":2,"152":2}}],["这个误差来源于数据的随机性",{"5":{"51":2}}],["这个异常样本可能主导mse",{"5":{"51":2}}],["这个矩阵乘积的范数可能因为累积效应而变得极小",{"5":{"148":2}}],["这个矩阵可以分解为对角矩阵与外积矩阵的差",{"5":{"61":2}}],["这个矩阵具有一些特殊的性质",{"5":{"87":2,"152":2}}],["这个矩阵类似于一个距离矩阵",{"5":{"87":2,"152":2}}],["这个矩阵的谱结构决定了梯度下降在概率空间中的几何行为",{"5":{"71":2}}],["这个矩阵的每一行对应一个位置",{"5":{"88":2}}],["这个平均值就越接近真实分布的熵",{"5":{"61":2}}],["这个表达式虽然复杂",{"5":{"158":2}}],["这个表达式可以理解为",{"5":{"158":2}}],["这个表达式可以解释为",{"5":{"101":2}}],["这个表达式也可以等价地写成向量形式",{"5":{"158":2}}],["这个表达式清晰地表明",{"5":{"155":2}}],["这个表达式清楚地表明",{"5":{"90":2}}],["这个表达式清楚地揭示了位置编码的生成机制",{"5":{"90":2}}],["这个表达式揭示了归一化的三个核心步骤",{"5":{"146":2}}],["这个表达式揭示了注意力分数的矩阵分解结构",{"5":{"94":2}}],["这个表达式在分类任务的训练中至关重要",{"5":{"71":2}}],["这个表达式是深度学习分类任务中最常用的损失函数形式",{"5":{"61":2}}],["这个表达式表明门控权重是针对每个token位置独立计算的",{"5":{"158":2}}],["这个表达式表明",{"5":{"94":2,"150":2}}],["这个极限表明",{"5":{"87":2,"152":2}}],["这个性质保证了正负优势函数相互抵消",{"5":{"155":2}}],["这个性质保证了重要性采样估计在期望上是正确的",{"5":{"154":2}}],["这个性质保证了裁剪不会放大不稳定的信号",{"5":{"147":2}}],["这个性质表明随机梯度是真实梯度的",{"5":{"149":1}}],["这个性质表明随机梯度是真实梯度的无偏估计",{"5":{"149":1}}],["这个性质称为正交性条件",{"5":{"51":2}}],["这个性质具有深刻的含义",{"5":{"87":2,"152":2}}],["这个性质与位置编码中的相对位置表示密切相关",{"5":{"90":2}}],["这个性质在分析注意力机制的频率特性时很有用",{"5":{"90":2}}],["这个性质可以从kl散度的凸性直接推导",{"5":{"97":2}}],["这个等式成立的原因是",{"5":{"155":2}}],["这个等式的关键意义在于",{"5":{"154":2}}],["这个等式的右边只依赖于",{"5":{"88":2}}],["这个等式揭示了残差连接的核心优势",{"5":{"150":2}}],["这个等式是rope理论的核心成果",{"5":{"88":2}}],["这个数学推导揭示了moe架构的核心价值",{"5":{"159":2}}],["这个数学事实揭示了一个深刻的原理",{"5":{"71":2}}],["这个数值足够大",{"5":{"88":2}}],["这个数值验证了公式",{"5":{"91":2}}],["这个证明揭示了sigmoid导数自化性质的数学根源",{"5":{"42":2}}],["这个证明的关键步骤是利用了旋转矩阵的可交换性",{"5":{"88":2}}],["这个展开式揭示了问题的复杂性",{"5":{"88":2}}],["这个映射满足群同态性质",{"5":{"88":2}}],["这个映射的复杂度取决于编码矩阵的秩",{"5":{"89":2}}],["这个位置应该有什么样的编码",{"5":{"89":2}}],["这个雅可比矩阵的结构影响梯度流动",{"5":{"89":2}}],["这个频谱表示表明",{"5":{"90":2}}],["这个结果意味着dropout不会改变激活层输出的期望值",{"5":{"145":2}}],["这个结果表明分层裁剪可以提供比全局裁剪更精细的控制",{"5":{"147":2}}],["这个结果表明",{"5":{"71":2,"97":2,"145":4,"149":2}}],["这个结果与二分类情况完全一致",{"5":{"61":2}}],["这个结果与之前的推导一致",{"5":{"61":2}}],["这个结果可以通过",{"5":{"61":2}}],["这个结果不依赖于任何可学习的参数",{"5":{"88":2}}],["这个结果干净得多",{"5":{"88":2}}],["这个结果清楚地表明",{"5":{"90":2}}],["这个结果具有深刻的统计学意义",{"5":{"51":2}}],["这个结果具有深刻的数学意义",{"5":{"91":2}}],["这个结论的严格证明需要用到多重频率系统的唯一性定理",{"5":{"90":2}}],["这个算法也存在一些明显的局限性",{"5":{"155":2}}],["这个算法清晰地展示了反向传播的执行流程",{"5":{"44":2}}],["这个算子可以表示为",{"5":{"90":2}}],["这个间隔随增大而减小",{"5":{"90":1}}],["这个间隔随",{"5":{"90":1}}],["这个相位差正好对应点积公式中的项",{"5":{"90":1}}],["这个相位差正好对应点积公式中的",{"5":{"90":1}}],["这个值越接近1",{"5":{"157":2}}],["这个值的选择基于以下考量",{"5":{"91":2}}],["这个值是通过实验调优确定的",{"5":{"91":2}}],["这个过程与sigmoid将logits转换为概率的过程在数学上是相似的",{"5":{"71":2}}],["这个过程可以理解为位置信息的",{"5":{"91":2}}],["这个梯度表达式表明",{"5":{"158":2}}],["这个梯度在",{"5":{"157":2}}],["这个梯度的方向告诉我们",{"5":{"155":2}}],["这个梯度具有清晰的几何意义",{"5":{"69":2}}],["这个梯度可以展开为",{"5":{"92":2,"153":2}}],["这个度量表示它实际关注的",{"5":{"92":2,"153":2}}],["这个归一化过程有两个重要作用",{"5":{"92":2,"153":2}}],["这个递推关系定义了迭代映射",{"5":{"147":2}}],["这个递推关系的解为",{"5":{"41":2}}],["这个递推关系展示了信息如何在层间流动",{"5":{"92":2,"153":2}}],["这个函数应当满足以下核心要求",{"5":{"88":2}}],["这个函数可以表示为",{"5":{"92":2,"153":2}}],["这个例子直观地说明了线性模型的表达能力边界",{"5":{"71":2}}],["这个例子表明",{"5":{"92":2,"153":2}}],["这个运算被高度优化",{"5":{"50":2}}],["这个运算可以视为张量收缩",{"5":{"93":2}}],["这个复数编码与实数编码的关系为",{"5":{"90":2}}],["这个复杂度与单头注意力相同",{"5":{"93":2}}],["这个复杂度分析与头数无关",{"5":{"93":1}}],["这个复杂度分析与头数",{"5":{"93":1}}],["这个嵌入矩阵是整个注意力计算的起点",{"5":{"94":2}}],["这个视角将注意力的计算置于更一般的二次型框架中",{"5":{"94":2}}],["这个看似简洁的公式蕴含了注意力计算的完整逻辑",{"5":{"95":2}}],["这个求和需要次乘法和次加法",{"5":{"45":1}}],["这个求和需要",{"5":{"45":1}}],["这个求和运算遍历了查询向量和键向量的所有维度",{"5":{"95":2}}],["这个区间对于softmax函数来说是",{"5":{"95":2}}],["这个权重满足非负性和归一性",{"5":{"95":2}}],["这个",{"5":{"96":2}}],["这个常微分方程描述了参数在损失函数梯度的负方向上的连续运动",{"5":{"97":2}}],["这是softmax归一化带来的必然结果",{"5":{"159":2}}],["这是理想的负载均衡状态",{"5":{"158":2}}],["这是理解dropout期望保持性质的关键",{"5":{"145":2}}],["这是理解机器学习模型泛化误差的核心理论工具",{"5":{"51":2}}],["这是transformer架构中前馈网络的标准配置",{"5":{"158":2}}],["这是transformer原始论文提出的设计",{"5":{"86":2,"151":2}}],["这是整个策略梯度方法的理论基础",{"5":{"155":2}}],["这是rlhf流程中最具创新性的环节",{"5":{"154":2}}],["这是截断后的目标",{"5":{"154":2}}],["这是标准的策略梯度目标",{"5":{"154":2}}],["这是为了保持数值稳定性",{"5":{"146":2}}],["这是基于经验观察和理论分析的结果",{"5":{"145":2}}],["这是微分动力系统的经典定理",{"5":{"135":2}}],["这是微积分基本定理的直接推论",{"5":{"44":2}}],["这是微积分中的基本定理",{"5":{"45":2}}],["这是矩阵谱半径的基本性质",{"5":{"135":2}}],["这是深度网络优于浅层网络的关键原因",{"5":{"133":2}}],["这是深度网络优于线性模型的关键",{"5":{"133":2}}],["这是深度学习高效训练的关键技术基础",{"5":{"47":2}}],["这是",{"5":{"132":2,"134":2,"158":2}}],["这是机器学习中的核心问题之一",{"5":{"132":2}}],["这是最严重的负载不均衡",{"5":{"158":2}}],["这是最常见的形式",{"5":{"147":2}}],["这是最大熵分布",{"5":{"132":2}}],["这是最直观的实现方式",{"5":{"88":2}}],["这是自注意力机制的理论优势",{"5":{"132":2}}],["这是自注意力机制的优势",{"5":{"88":2}}],["这是导致深层神经网络训练困难的根本原因之一",{"5":{"41":2}}],["这是曲线的",{"5":{"42":2}}],["这是反向传播的最后一步",{"5":{"44":2}}],["这是线性代数中的基本定理",{"5":{"45":2}}],["这是神经网络理论中最深刻的结果之一",{"5":{"71":2}}],["这是神经网络强大表达能力的关键来源",{"5":{"47":2}}],["这是不可能拟合复杂非线性函数的原因",{"5":{"47":2}}],["这是所谓的",{"5":{"96":2}}],["这是mse作为回归任务标准损失的统计学理论基础",{"5":{"51":2}}],["这是回归分析中统计量的几何基础",{"5":{"51":1}}],["这是回归分析中",{"5":{"51":1}}],["这是因为涌现取决于多个因素的复杂交互",{"5":{"138":2}}],["这是因为",{"5":{"61":2,"93":2,"155":2}}],["这是因为内存带宽瓶颈在长序列下更加严重",{"5":{"86":2,"151":2}}],["这是因为softmax函数的分子是指数函数",{"5":{"87":2,"152":2}}],["这是因为当位置索引很大时",{"5":{"88":2}}],["这是因为可学习编码可以适应训练数据的特定分布",{"5":{"89":2}}],["这是因为正弦编码天然支持外推",{"5":{"89":2}}],["这是因为正弦和余弦函数的周期性导致编码向量不会总是聚集在球面的某个区域",{"5":{"91":2}}],["这是因为注意力分数只依赖于查询和键向量的内容",{"5":{"92":1,"153":1}}],["这是因为注意力分数",{"5":{"92":1,"153":1}}],["这是在用模型分布",{"5":{"61":2}}],["这是实现高效训练的关键数学基础",{"5":{"61":2}}],["这是位置编码实用性的重要指标",{"5":{"88":2}}],["这是其他位置编码方法难以企及的优势",{"5":{"88":2}}],["这是常见配置",{"5":{"89":2}}],["这是一个非常可观的显存需求",{"5":{"157":2}}],["这是一个非常重要的结论",{"5":{"87":2,"152":2}}],["这是一个非常重要的特性",{"5":{"93":2}}],["这是一个有限值",{"5":{"156":2}}],["这是一个有限的常数值",{"5":{"70":2}}],["这是一个嵌入在高维参数空间",{"5":{"142":1}}],["这是一个嵌入在高维参数空间中的",{"5":{"142":1}}],["这是一个精确的数学恒等式",{"5":{"140":2}}],["这是一个天文数字",{"5":{"140":2}}],["这是一个维的紧致流形",{"5":{"70":1}}],["这是一个半正定矩阵",{"5":{"70":1}}],["这是一个半负定矩阵",{"5":{"70":1}}],["这是一个线性的梯度结构",{"5":{"70":1}}],["这是一个",{"5":{"69":2,"70":4}}],["这是一个超平面",{"5":{"47":2}}],["这是一个重要的性质",{"5":{"96":2}}],["这是一个平凡特征向量",{"5":{"87":2,"152":2}}],["这是一个秩1矩阵",{"5":{"87":4,"152":4}}],["这是一个极其迅速的过程",{"5":{"148":2}}],["这是一个极其重要的性质",{"5":{"70":2}}],["这是一个极强的数学保证",{"5":{"88":2}}],["这是一个相当可观的参数量",{"5":{"89":2}}],["这是一个无约束优化问题",{"5":{"89":2}}],["这是一个值得反复品味的结论",{"5":{"88":2}}],["这是一个值得深入分析的问题",{"5":{"90":2}}],["这是一个简单的乘法交互",{"5":{"71":2}}],["这是一个简单的矩阵乘法梯度",{"5":{"94":2}}],["这是一个关键的突破",{"5":{"148":2}}],["这是一个关键发现",{"5":{"95":2}}],["这是一个关于的多项式方程",{"5":{"50":1}}],["这是一个关于",{"5":{"50":1}}],["这是一个常数矩阵",{"5":{"97":2}}],["这是一种在保持门控可学习性的同时实现高效计算的关键技术",{"5":{"159":2}}],["这是一种",{"5":{"92":2,"153":2}}],["这是建模长程依赖的重要能力",{"5":{"92":2,"153":2}}],["这是正交投影矩阵的两个关键性质",{"5":{"51":2}}],["这是正弦位置编码难以做到的",{"5":{"92":2,"153":2}}],["这是多数实际应用中的常见假设",{"5":{"48":2}}],["这是多头注意力设计的另一个精妙之处",{"5":{"93":2}}],["这就",{"5":{"42":2}}],["这就是梯度协方差矩阵所要刻画的内容",{"5":{"149":2}}],["这就是梯度下降算法的理论基础",{"5":{"48":2}}],["这就是为什么使用sigmoid激活的深层网络几乎不可避免地面临严重的梯度消失问题",{"5":{"148":2}}],["这就是为什么在某些深度网络中",{"5":{"148":2}}],["这就是为什么transformer在处理长序列时面临计算挑战",{"5":{"50":2}}],["这就是张量分解的范畴",{"5":{"143":2}}],["这就是均方误差在分类任务中表现不佳的根本原因",{"5":{"70":2}}],["这就是nce的理论基础",{"5":{"69":2}}],["这就是著名的独立于无关选择的特性",{"5":{"69":1}}],["这就是著名的",{"5":{"69":1,"71":2}}],["这就是所谓的",{"5":{"95":2}}],["这一操作将门控决策与专家计算结果融合",{"5":{"158":2}}],["这一统计技术",{"5":{"154":2}}],["这一思想的提出源于对策略梯度方法局限性的深刻洞察",{"5":{"154":2}}],["这一核心洞见",{"5":{"149":2}}],["这一关系可以直接从协方差矩阵的定义推导得出",{"5":{"149":2}}],["这一关系将反向传播解释为",{"5":{"135":2}}],["这一结论具有深刻的优化含义",{"5":{"149":2}}],["这一结论假设激活函数近似线性",{"5":{"148":2}}],["这一结论虽然难以给出严格的数学证明",{"5":{"45":2}}],["这一通用表达式",{"5":{"146":2}}],["这一方向的信息可能对模型性能有重要贡献",{"5":{"146":2}}],["这一观察为模型压缩提供了数学基础",{"5":{"143":2}}],["这一观点与深度学习中的",{"5":{"45":2}}],["这一现象在计算机视觉模型",{"5":{"142":2}}],["这一复杂度成为限制模型可处理序列长度的瓶颈",{"5":{"141":2}}],["这一概率视角不仅为我们理解大语言模型的数学本质提供了清晰的理论框架",{"5":{"140":2}}],["这一经验法则为实际训练提供了简单而有效的指导",{"5":{"138":2}}],["这一问题可以表述为",{"5":{"138":2}}],["这一发现不仅解释了标准注意力机制的理论局限性",{"5":{"141":2}}],["这一发现不仅为模型设计提供了可量化的指导框架",{"5":{"138":2}}],["这一发现对资源分配策略有深远影响",{"5":{"138":2}}],["这一发现解释了两个重要现象",{"5":{"133":2}}],["这一非对称性反映了信息方向的差异",{"5":{"137":2}}],["这一上界在均匀分布时达到",{"5":{"137":2}}],["这一边界可通过分析增强系统的特征值随参数变化来确定",{"5":{"136":2}}],["这一常数在广泛的非线性系统中具有普适性",{"5":{"136":2}}],["这一",{"5":{"135":2}}],["这一形式将参数更新视为状态在相空间中的",{"5":{"134":2}}],["这一理论不仅深化了我们对模型训练动力学的理解",{"5":{"133":2}}],["这一轨迹揭示了网络学习的不同阶段",{"5":{"133":2}}],["这一权衡决定了模型的表示质量和泛化能力",{"5":{"133":2}}],["这一转换使得注意力权重具有概率论意义",{"5":{"132":2}}],["这一原则直接来自于任务数学结构与损失函数的一致性",{"5":{"70":2}}],["这一性质对于理解批量大小与优化轨迹之间的关系至关重要",{"5":{"149":2}}],["这一性质对于深度网络的训练稳定性具有重要意义",{"5":{"146":2}}],["这一性质对于语言模型捕捉局部模式",{"5":{"88":2}}],["这一性质仅在统计量",{"5":{"146":2}}],["这一性质表明熵可以解释为",{"5":{"137":2}}],["这一性质大大简化了反向传播的实现",{"5":{"44":2}}],["这一性质在以下场景中有重要应用",{"5":{"137":2}}],["这一性质在反向传播算法的推导中至关重要",{"5":{"96":2}}],["这一性质在变分推断和高斯过程回归中非常重要",{"5":{"96":2}}],["这一性质被称为",{"5":{"42":2}}],["这一性质被用于分析梯度分布",{"5":{"96":2}}],["这一性质的证明直接来源于对数函数的性质和概率的非负性",{"5":{"61":2}}],["这一性质可以用数学语言精确描述",{"5":{"92":2,"153":2}}],["这一过程可以形式化为信息过滤和重要性加权的组合",{"5":{"132":2}}],["这一过程可以与前向传播并行进行",{"5":{"45":2}}],["这一过程实现了损失信息从输出到输入的逆向流动",{"5":{"44":2}}],["这一过程称为反向传播",{"5":{"48":2}}],["这一局限性的根本原因在于",{"5":{"46":2}}],["这一局限性直到多层神经网络",{"5":{"47":2}}],["这一小批样本的梯度是整体数据梯度的一个良好近似",{"5":{"96":2}}],["这一几何性质不仅解释了mse的最优性",{"5":{"51":2}}],["这一极值性质可以通过拉格朗日乘数法或jensen不等式严格证明",{"5":{"61":2}}],["这一重要性质可以通过jensen不等式严格证明",{"5":{"61":2}}],["这一分解具有深刻的理论意义",{"5":{"61":2}}],["这一等价性建立了机器学习损失函数与经典统计推断之间的桥梁",{"5":{"61":2}}],["这一等价变换揭示了一个重要事实",{"5":{"61":2}}],["这一梯度表达式将在后续的梯度计算部分得到详细展开",{"5":{"61":2}}],["这一领域经历了从",{"5":{"88":2}}],["这一公式在形式上具有完美的对称性",{"5":{"88":2}}],["这一趋势的背后有多重原因",{"5":{"88":2}}],["这一范式具有清晰的语义解释",{"5":{"95":2}}],["这一步计算了所有查询",{"5":{"95":2}}],["这一步根据注意力权重对值矩阵进行加权求和",{"5":{"95":2}}],["这一设计确实能够在一定程度上覆盖不同尺度的位置信息",{"5":{"88":2}}],["这一设计确保了梯度能够有效地反向传播",{"5":{"95":2}}],["这样",{"5":{"50":4,"61":2,"93":2}}],["这样语义完全不同的句子",{"5":{"91":2}}],["这样可以避免softmax计算中指数运算可能导致的数值溢出问题",{"5":{"96":2}}],["这正是逃离鞍点所需的",{"5":{"149":2}}],["这正是梯度协方差矩阵不同所导致的",{"5":{"149":2}}],["这正是he初始化的方差选择",{"5":{"148":2}}],["这正是负的对数似然函数除以样本数量的结果",{"5":{"140":2}}],["这正是具有加速度的动力学方程",{"5":{"134":2}}],["这正是神经网络迭代优化的自然数学描述",{"5":{"134":2}}],["这正是变分信息瓶颈",{"5":{"133":2}}],["这正是著名的",{"5":{"101":2}}],["这正是回归任务的目标",{"5":{"70":2}}],["这正是对比学习的核心目标",{"5":{"69":2}}],["这正是交叉熵损失的标准形式",{"5":{"69":2}}],["这正是",{"5":{"47":1,"61":1,"70":2,"71":2,"88":1,"137":2}}],["这正是矩阵乘法的定义",{"5":{"44":2,"45":2}}],["这正是复合函数的定义",{"5":{"45":2}}],["这正是异或函数的输出",{"5":{"46":2}}],["这正是深度学习",{"5":{"46":2}}],["这正是4",{"5":{"46":2}}],["这正是层网络的等效仿射变换形式",{"5":{"47":1}}],["这正是单样本仿射变换的定义",{"5":{"47":2}}],["这正是多分类交叉熵损失函数",{"5":{"61":2}}],["这正是二元交叉熵损失",{"5":{"61":1}}],["这正是最小化交叉熵与最大化似然估计等价的直接体现",{"5":{"61":2}}],["这正是旋转矩阵的可加性",{"5":{"88":2}}],["这正是平移等变性的群论表述",{"5":{"88":1}}],["这正是建模长程依赖所需要的",{"5":{"92":2,"153":2}}],["这正好是概率值的取值范围",{"5":{"71":2}}],["这与标准的前馈网络形成了鲜明对比",{"5":{"150":2}}],["这与标准的分类任务中logit",{"5":{"101":2}}],["这与经验观察一致",{"5":{"149":2}}],["这与许多简化分析中假设的",{"5":{"149":2}}],["这与批归一化形成对比",{"5":{"146":2}}],["这与显式l2正则化的梯度形式完全一致",{"5":{"144":2}}],["这与温度缩放相关",{"5":{"140":2}}],["这与",{"5":{"133":2}}],["这与4",{"5":{"101":2}}],["这与随机变量",{"5":{"70":1}}],["这与随机变量在条件分布下的期望具有相同的形式",{"5":{"70":1}}],["这与概率分布的归一化约束",{"5":{"70":1}}],["这与概率分布的归一化约束完全一致",{"5":{"70":1}}],["这与概率的边界行为一致",{"5":{"47":2}}],["这与鲁棒统计中的思想形成对比",{"5":{"70":2}}],["这与均方误差关于预测值的梯度形式",{"5":{"70":2}}],["这与均方误差中使用的平方运算形成了鲜明对比",{"5":{"61":2}}],["这与infonce中温度参数的作用在数学上是等价的",{"5":{"69":2}}],["这与多项式分布的参数空间完全一致",{"5":{"71":2}}],["这与gelu输入的范围需求相匹配",{"5":{"71":2}}],["这与反向模式自动微分的定义完全一致",{"5":{"44":2}}],["这与样条函数",{"5":{"45":2}}],["这与第六章讨论的交叉熵损失有着深刻的联系",{"5":{"47":2}}],["这与最大化对数似然是等价的",{"5":{"61":2}}],["这与人类记忆的遗忘特性有某种相似性",{"5":{"86":2,"151":2}}],["这与通信系统中广泛使用的调制技术具有相同的数学本质",{"5":{"88":2}}],["这与位置依赖的平滑性假设一致",{"5":{"89":2}}],["这与正弦余弦编码的极限相同",{"5":{"89":2}}],["这与正弦编码的多尺度频率结构有某种对应关系",{"5":{"89":2}}],["这与编码向量",{"5":{"90":2}}],["这与rnn的",{"5":{"92":2,"153":2}}],["这与深度学习中常见的",{"5":{"93":2}}],["这避免了sigmoid和tanh的梯度饱和问题",{"5":{"71":2}}],["这里讨论的是",{"5":{"149":1}}],["这里讨论的是批量内噪声",{"5":{"149":1}}],["这里使用的是有偏估计",{"5":{"146":2}}],["这里是对每个样本",{"5":{"146":1}}],["这里是对每个样本的所有特征求和",{"5":{"146":1}}],["这里是一个的矩阵",{"5":{"141":1}}],["这里是全局最小点",{"5":{"136":1}}],["这里是虚数单位",{"5":{"88":1}}],["这里没有",{"5":{"70":2}}],["这里的门",{"5":{"150":2}}],["这里的高斯近似并不假设噪声在所有方向上具有相同的方差",{"5":{"149":2}}],["这里的",{"5":{"69":2}}],["这里softmax的作用同样是",{"5":{"69":2}}],["这里softmax的作用是",{"5":{"69":4}}],["这里",{"5":{"51":2,"61":2,"70":4,"71":2,"88":1,"93":2,"136":1,"141":1,"159":2}}],["这里我们关注激活函数在神经元模型中的整体角色",{"5":{"47":2}}],["这里为避免混淆使用不同字母",{"5":{"87":2,"152":2}}],["这里对应位置域",{"5":{"90":2}}],["这里对分数矩阵的每一行独立进行缩放和softmax归一化",{"5":{"95":2}}],["这需要深入理解任务的数学特性和激活函数的数学性质",{"5":{"41":2}}],["这需要对上述单样本梯度计算进行扩展",{"5":{"44":2}}],["这组神经元构成神经网络的一个层",{"5":{"46":2}}],["这为设计更好的多任务学习算法提供了理论基础",{"5":{"133":2}}],["这为理解自适应优化器的行为提供了新的视角",{"5":{"149":2}}],["这为理解数据偏差和设计平衡策略",{"5":{"137":2}}],["这为理解信息瓶颈提供了可分析的例子",{"5":{"133":2}}],["这为理解attention的信息整合能力提供了理论基础",{"5":{"69":2}}],["这为模型提供了坚实的理论基础",{"5":{"69":2}}],["这为使用梯度下降等优化算法提供了数学基础",{"5":{"47":2}}],["这为损失函数的设计提供了理论基础",{"5":{"47":2}}],["这为架构设计提供了额外的灵活性",{"5":{"94":2}}],["这可以是同等配置稠密层的",{"5":{"158":2}}],["这可以看作是连续时间朗之万动力学的欧拉",{"5":{"149":2}}],["这可以看作是对注意力输出张量",{"5":{"143":1}}],["这可以看作是对注意力输出张量沿第一个模的",{"5":{"143":1}}],["这可以看作是对编码幅度的约束",{"5":{"89":2}}],["这可以用信息瓶颈目标形式化",{"5":{"137":2}}],["这可以加速优化过程",{"5":{"70":2}}],["这可以从",{"5":{"70":2}}],["这可以被理解为一种",{"5":{"69":1}}],["这可以被理解为一种参数化投影",{"5":{"69":1}}],["这可以被视为一种",{"5":{"69":2}}],["这可以通过分析",{"5":{"142":1}}],["这可以通过分析的奇异值分解与的特征值之间的关系来理解",{"5":{"142":1}}],["这可以通过矩阵运算高效地实现",{"5":{"47":2}}],["这可以通过修改正弦编码的定义",{"5":{"91":2}}],["这可能导致极其不稳定的更新",{"5":{"154":2}}],["这可能限制了模型的表达能力",{"5":{"150":2}}],["这可能是因为神经网络具有大量的对称性",{"5":{"48":2}}],["这可能成为计算瓶颈",{"5":{"93":2}}],["这从参数命名就可以看出",{"5":{"96":2}}],["这在大规模训练场景下是难以承受的计算负担",{"5":{"159":2}}],["这在大规模应用中可能非常昂贵",{"5":{"155":2}}],["这在现代高速网络",{"5":{"157":2}}],["这在",{"5":{"144":2}}],["这在许多情况下是计算上不可行的",{"5":{"69":2}}],["这在近端策略优化",{"5":{"96":2}}],["这在将展平后的向量恢复为嵌入序列时非常有用",{"5":{"50":2}}],["这在后续与最大似然估计的联系中将发挥关键作用",{"5":{"61":2}}],["这在实际中是达不到的",{"5":{"69":2}}],["这在实践中被证明是困难的",{"5":{"88":2}}],["这在加性注入的框架下是困难的",{"5":{"88":2}}],["这在理论上可以防止初始化时的梯度消失或爆炸问题",{"5":{"50":2}}],["这在理论上消除了边界",{"5":{"92":2,"153":2}}],["这在某些情况下可能过于",{"5":{"145":2}}],["这在某些数值精度下可能导致问题",{"5":{"61":2}}],["这在某些初始化方案中是常见的",{"5":{"95":2}}],["这在不同参数方向上可能不是最优的",{"5":{"97":2}}],["这对应于雅可比矩阵",{"5":{"148":1}}],["这对应于雅可比矩阵的某些对角元素为零",{"5":{"148":1}}],["这对于需要长距离依赖建模的任务",{"5":{"150":2}}],["这对于下游任务的成功至关重要",{"5":{"133":2}}],["这对于学习位置敏感的token交互模式至关重要",{"5":{"71":4}}],["这对于训练深层网络和循环神经网络尤为重要",{"5":{"48":2}}],["这对于主动学习和探索",{"5":{"96":2}}],["这对训练生成对抗网络",{"5":{"50":2}}],["这本质上就是计算查询矩阵与转置后的键矩阵的乘积",{"5":{"50":2}}],["这本质上是一个概率分布",{"5":{"61":2}}],["这有效地限制了策略更新的步长",{"5":{"101":2}}],["这有效缓解了深层网络中的梯度消失问题",{"5":{"61":2}}],["这有助于避免激活值的偏移效应",{"5":{"148":2}}],["这有助于理解不同方法之间的内在联系与本质差异",{"5":{"146":2}}],["这有助于理解其参数效率",{"5":{"50":2}}],["这有助于提高模型的泛化能力",{"5":{"47":2,"150":2}}],["这反映了分类任务中",{"5":{"70":2}}],["这反映了概率分布的约束条件",{"5":{"61":2}}],["这反映了交叉熵对真实分布和模型分布的区别对待",{"5":{"61":2}}],["这反映了相对位置的对称性",{"5":{"90":2}}],["这极大地简化了梯度表达式",{"5":{"61":2}}],["这并非偶然",{"5":{"88":2}}],["这带来了两个重要的数学优势",{"5":{"42":2}}],["这带来了两个重要优势",{"5":{"88":2}}],["这降低了工程实现的复杂度",{"5":{"88":2}}],["这约能编码3400比特的信息",{"5":{"90":2}}],["这类似于dropout的正则化效果",{"5":{"41":2}}],["这类依赖关系的跨度可能跨越数十个词元",{"5":{"92":2,"153":2}}],["这类头往往表现出较强的局部注意力模式和明确的方向性",{"5":{"92":2,"153":2}}],["这类头对位置编码敏感",{"5":{"92":2,"153":2}}],["这不仅节省了减法和平方操作",{"5":{"146":2}}],["这不仅数值更稳定",{"5":{"140":2}}],["这不仅提供了更深刻的理解",{"5":{"88":2}}],["这不仅导致计算效率低下",{"5":{"92":2,"95":2,"153":2}}],["这催生了各种稀疏注意力和线性注意力变体的研究",{"5":{"92":2,"153":2}}],["这句话的序列",{"5":{"93":2}}],["这句话中存在着多种语义关系",{"5":{"93":2}}],["这同样与单头注意力相同",{"5":{"93":2}}],["这三个因素并非独立",{"5":{"138":2}}],["这三个项之间存在内在的权衡关系",{"5":{"51":2}}],["这三个向量构成了注意力机制的",{"5":{"94":2}}],["这三个向量并非凭空产生",{"5":{"94":2}}],["这三个线性变换可以统一表示为矩阵乘法的形式",{"5":{"94":2}}],["这三个步骤的数学本质和物理意义将在后续小节中逐一详细阐述",{"5":{"95":2}}],["这三个阶段的计算可以自然地并行化",{"5":{"95":2}}],["这也是大语言模型能够实现高效训练和推理的关键技术基础",{"5":{"50":2}}],["这也是后续出现各种稀疏注意力",{"5":{"95":2}}],["这体现了奥卡姆剃刀原理",{"5":{"139":2}}],["这体现了sigmoid函数的代数闭合性",{"5":{"42":2}}],["这体现了transformer架构的优雅性",{"5":{"95":2}}],["这使得算法可以更高效地利用样本数据",{"5":{"155":2}}],["这使得激活函数工作在导数较大的区域",{"5":{"148":2}}],["这使得高斯dropout在实现上更加简单",{"5":{"145":2}}],["这使得高斯分布成为最易于处理的分布之一",{"5":{"96":2}}],["这使得我们能够基于有限的实验结果推算更大规模模型的性能",{"5":{"138":2}}],["这使得模型的输出具有清晰的概率意义",{"5":{"69":2}}],["这使得模型能够更有效地学习位置相关的模式",{"5":{"70":2}}],["这使得模型能够学习与位置无关的相对依赖模式",{"5":{"90":2}}],["这使得模型能够学习到与距离相关的注意力模式",{"5":{"92":2,"153":2}}],["这使得模型能够学习到与相对位置相关的注意力模式",{"5":{"92":2,"153":2}}],["这使得模型可以根据输入动态地调整计算分配",{"5":{"93":2}}],["这使得负输入也会进入饱和区域",{"5":{"41":2}}],["这使得",{"5":{"41":2,"61":2}}],["这使得tanh在训练初期通常比sigmoid收敛更快",{"5":{"42":2}}],["这使得在保持二阶信息的同时实现了可扩展性",{"5":{"48":2}}],["这使得贝叶斯推断在计算上更加方便",{"5":{"96":2}}],["这使得求解正交矩阵的逆矩阵变得极其简单",{"5":{"50":2}}],["这使得它天然适合于变长输入的处理",{"5":{"150":2}}],["这使得它特别适合于变长序列处理和循环神经网络",{"5":{"150":2}}],["这使得它在以下场景中特别有用",{"5":{"41":2}}],["这使得它能够自然地处理变长序列和流式输入",{"5":{"86":2,"151":2}}],["这使得线性注意力特别适合在线推理和流式处理场景",{"5":{"86":2,"151":2}}],["这使得学习到的特征直接与下游任务相关",{"5":{"94":2}}],["这使得梯度可以",{"5":{"92":2,"153":2}}],["这使得梯度更新不受概率边界约束的限制",{"5":{"96":2}}],["这使得优化过程具有可预测的动态特性",{"5":{"70":2}}],["这使得优化过程可预测且稳定",{"5":{"97":2}}],["这使其不受概率公理的限制",{"5":{"96":2}}],["这被认为有助于模型在早期阶段找到一个更好的参数区域",{"5":{"48":2}}],["这被称为",{"5":{"97":2,"149":2}}],["这影响了最优学习率的选择和超参数调优的难度",{"5":{"97":2}}],["这影响了",{"5":{"97":2}}],["当一个专家的",{"5":{"158":2}}],["当一个头的注意力分布能由另一个头预测时",{"5":{"132":2}}],["当moe模型的规模超出单个设备的计算和存储能力时",{"5":{"157":2}}],["当优势函数",{"5":{"154":4}}],["当策略梯度较大时",{"5":{"154":2}}],["当不考虑",{"5":{"150":2}}],["当不同类别的样本高度重叠时",{"5":{"70":2,"97":2}}],["当不同任务的损失函数具有不同的尺度时",{"5":{"70":2,"101":2}}],["当这些矩阵的范数乘积远离1时",{"5":{"148":2}}],["当这些梯度在拼接处合并时",{"5":{"41":2}}],["当权重的绝对值小于1时",{"5":{"148":2}}],["当权重的绝对值大于1时",{"5":{"148":2}}],["当损失曲面在某个方向上具有较大的正曲率",{"5":{"149":2}}],["当损失停止下降时放松阈值",{"5":{"147":2}}],["当损失函数的梯度趋近于零时",{"5":{"70":2}}],["当裁剪发生时",{"5":{"147":2}}],["当裁剪阈值足够大",{"5":{"147":2}}],["当保留概率",{"5":{"145":4}}],["当网络层数",{"5":{"148":1}}],["当网络层数较大时",{"5":{"148":1}}],["当网络层数增加时",{"5":{"145":2}}],["当网络的层数增加时",{"5":{"148":2}}],["当网络较深时",{"5":{"42":2}}],["当标准差",{"5":{"144":2}}],["当等高线为细长椭圆时",{"5":{"144":2}}],["当等高线接近圆形时",{"5":{"144":2}}],["当目标是降维或保留全局结构时",{"5":{"143":2}}],["当目标是发现稀疏的潜在因子时",{"5":{"143":2}}],["当核张量是对角超张量",{"5":{"143":2}}],["当寻找最相似的时",{"5":{"141":1}}],["当序列长度",{"5":{"141":1}}],["当序列长度远大于隐藏维度时",{"5":{"141":1}}],["当序列长度超出训练范围时",{"5":{"88":2}}],["当规模",{"5":{"138":2}}],["当训练数据量或模型规模增加时",{"5":{"138":2}}],["当训练数据量",{"5":{"138":4}}],["当固定时",{"5":{"137":1}}],["当事件完全确定时",{"5":{"137":2}}],["当确定性时",{"5":{"137":1}}],["当对数底数为",{"5":{"137":2}}],["当对所有成立时",{"5":{"61":1}}],["当参数",{"5":{"136":1}}],["当参数逐渐增大并经过临界值时",{"5":{"136":1}}],["当系统参数变化经过临界值时",{"5":{"136":2}}],["当特征值为负实数时",{"5":{"136":2}}],["当特征值为复数且模小于1时",{"5":{"136":2}}],["当噪声强度超过临界值时",{"5":{"135":2}}],["当乘积因子小于",{"5":{"135":2}}],["当学习率很小时",{"5":{"149":1}}],["当学习率满足",{"5":{"136":1}}],["当学习率满足时",{"5":{"136":1}}],["当学习率按多项式衰减时",{"5":{"136":2}}],["当学习率按周期性计划变化时",{"5":{"136":2}}],["当学习率过大或参数配置不当时",{"5":{"136":2}}],["当学习率继续增大时",{"5":{"136":2}}],["当学习率超过临界值时",{"5":{"136":2}}],["当学习率超过临界值时会产生震荡",{"5":{"136":2}}],["当学习率",{"5":{"134":2,"149":1}}],["当编码器输出概率分布",{"5":{"133":1}}],["当编码器输出概率分布时",{"5":{"133":1}}],["当编码向量线性无关时",{"5":{"89":2}}],["当服从联合高斯分布时",{"5":{"133":1}}],["当所有",{"5":{"157":2}}],["当所有关键子分布的拟合精度都超过某个阈值时",{"5":{"138":2}}],["当所有分数相等",{"5":{"132":2}}],["当所有行都是one",{"5":{"87":2,"152":2}}],["当选择的冗余度低时",{"5":{"132":2}}],["当分布集中于单点时熵为0",{"5":{"132":2}}],["当分布均匀时熵达到最大值",{"5":{"132":2}}],["当偏好差异",{"5":{"101":1}}],["当偏好差异较小时",{"5":{"101":2}}],["当偏好差异较大时",{"5":{"101":1}}],["当从1减小到0时",{"5":{"70":1}}],["当或时",{"5":{"70":1}}],["当远离",{"5":{"70":1}}],["当与某个负样本相似度很高时",{"5":{"69":1}}],["当负样本数量时",{"5":{"133":1}}],["当负样本数量有限时",{"5":{"101":2}}],["当负样本数量",{"5":{"69":1,"133":1}}],["当负样本数量足够大时",{"5":{"69":1}}],["当",{"5":{"41":15,"42":11,"45":2,"46":4,"47":6,"50":1,"51":1,"61":18,"69":5,"70":6,"71":2,"86":1,"87":7,"88":2,"89":9,"90":5,"91":2,"92":5,"94":3,"95":4,"96":8,"132":4,"133":3,"134":18,"135":4,"136":22,"137":3,"138":2,"139":2,"140":6,"141":3,"142":2,"143":1,"144":12,"145":22,"147":2,"148":6,"150":6,"151":1,"152":7,"153":5,"154":12,"155":4,"156":4,"157":4,"158":10,"159":12}}],["当时发生分岔",{"5":{"136":1}}],["当时达到",{"5":{"132":1}}],["当时",{"5":{"41":8,"42":5,"45":1,"46":4,"47":6,"50":1,"61":3,"69":3,"70":2,"71":2,"87":4,"89":1,"90":2,"92":3,"94":1,"95":1,"96":5,"136":19,"140":5,"141":1,"143":1,"145":1,"148":3,"152":4,"153":3}}],["当时可归入任一模式",{"5":{"45":1}}],["当激活函数输出接近其渐近边界时",{"5":{"41":2}}],["当较小时",{"5":{"41":1,"95":1,"133":1,"145":1}}],["当较大时趋近于零",{"5":{"148":1}}],["当较大时",{"5":{"41":1,"86":1,"90":1,"95":1,"132":1,"133":1,"137":1,"151":1}}],["当接近0或1时",{"5":{"41":1,"70":1}}],["当接近1时",{"5":{"41":1}}],["当输入特征发生微小变化时",{"5":{"159":2}}],["当输入方差",{"5":{"146":1}}],["当输入方差非常小",{"5":{"146":1}}],["当输入较大时",{"5":{"145":1}}],["当输入输出之间的互信息超过某个阈值时",{"5":{"137":2}}],["当输入的绝对值很大时",{"5":{"42":1}}],["当输入",{"5":{"42":1,"145":1}}],["当输入信号的累积效应超过某个阈值时",{"5":{"47":2}}],["当输入值的量级较大时",{"5":{"95":2}}],["当且时",{"5":{"42":1,"140":1}}],["当且仅当时取等号",{"5":{"139":1}}],["当且仅当和独立时取等号",{"5":{"137":2}}],["当且仅当和独立时互信息为零",{"5":{"71":1}}],["当且仅当和独立时互信息为0",{"5":{"96":1}}],["当且仅当服从均匀分布时取等号",{"5":{"137":1}}],["当且仅当为确定性变量",{"5":{"137":1}}],["当且仅当对于任意",{"5":{"70":1,"97":1}}],["当且仅当对于任意和任意",{"5":{"70":1,"97":1}}],["当且仅当对于任意向量和任意标量",{"5":{"71":1}}],["当且仅当对于任意向量",{"5":{"71":1}}],["当且仅当",{"5":{"61":6,"71":1,"96":1,"134":2,"137":6,"139":1}}],["当且仅当分布为退化分布",{"5":{"61":2}}],["当多个这样的曲面进行线性组合时",{"5":{"71":2}}],["当我们设计一个神经网络架构并训练其参数时",{"5":{"45":2}}],["当我们计算损失函数关于某个特定参数的偏导数时",{"5":{"48":2}}],["当我们计算验证集上的性能指标时",{"5":{"96":2}}],["当我们计算批次中所有样本的前向传播时",{"5":{"50":2}}],["当我们计算两个加了位置编码的向量的内积时",{"5":{"88":2}}],["当我们比较多个模型或在多个测试集上评估时",{"5":{"96":2}}],["当我们训练一个神经网络时",{"5":{"51":2}}],["当我们用kl散度作为优化目标时",{"5":{"61":2}}],["当我们使用交叉熵作为损失函数时",{"5":{"61":2}}],["当我们同时平移两个位置",{"5":{"88":2}}],["当我们在位置应用rope时",{"5":{"88":1}}],["当我们在位置",{"5":{"88":1}}],["当我们想要从该信息源中提取与某个特定查询相关的信息时",{"5":{"95":2}}],["当dropout率为",{"5":{"96":2}}],["当使用学习率衰减时",{"5":{"147":2}}],["当使用变分近似",{"5":{"133":1}}],["当使用变分近似和时",{"5":{"133":1}}],["当使用以2为底的对数时",{"5":{"96":2}}],["当使用自然对数时",{"5":{"96":2,"137":2}}],["当独立同分布的随机变量样本量趋向无穷时",{"5":{"96":2}}],["当样本量时",{"5":{"96":1,"137":1}}],["当样本量",{"5":{"51":1,"96":1,"137":1}}],["当样本量趋向无穷时",{"5":{"51":1}}],["当两个形状不同的张量进行逐元素运算时",{"5":{"50":2}}],["当单个样本太大以至于无法在gpu内存中容纳时",{"5":{"50":2}}],["当批量大小为",{"5":{"149":1}}],["当批量大小为时",{"5":{"149":1}}],["当批量大小",{"5":{"149":1}}],["当批量大小翻倍时",{"5":{"149":2}}],["当批量大小足够大时",{"5":{"96":4,"149":1}}],["当批量较小时",{"5":{"146":2}}],["当批量太小时",{"5":{"96":2}}],["当批大小受限于gpu内存时",{"5":{"50":2}}],["当模型参数量",{"5":{"138":2}}],["当模型参数量远大于最优配置时",{"5":{"138":2}}],["当模型规模",{"5":{"138":2}}],["当模型规模较小时",{"5":{"138":4}}],["当模型规模超过临界值",{"5":{"137":1}}],["当模型规模超过临界值时",{"5":{"137":1}}],["当模型容量有限时",{"5":{"101":2}}],["当模型容量足够大且训练数据足够多时",{"5":{"51":2}}],["当模型对一个样本完全",{"5":{"70":2}}],["当模型对某个预测非常有信心",{"5":{"70":2}}],["当模型复杂度适中时",{"5":{"51":2}}],["当模型复杂度很低时",{"5":{"51":2}}],["当模型复杂度很高时",{"5":{"51":2}}],["当模型学习到过于",{"5":{"87":2,"152":2}}],["当模型生成目标语言的某个词时",{"5":{"95":2}}],["当测试mse较高时",{"5":{"51":2}}],["当很大时",{"5":{"51":1,"69":1,"141":1}}],["当数据集中在一个设备上时",{"5":{"157":2}}],["当数据集规模",{"5":{"149":2}}],["当数据天然具有更高维的结构时",{"5":{"143":2}}],["当数据量充足时",{"5":{"138":2}}],["当数据中可能存在异常值或噪声较大时",{"5":{"51":2}}],["当数据质量较高且需要更精确的拟合时",{"5":{"51":2}}],["当数据矩阵的列高度相关时",{"5":{"97":2}}],["当预测分布",{"5":{"71":2}}],["当预测接近真实值时",{"5":{"51":2}}],["当预测偏离真实值时",{"5":{"51":2}}],["当真实的数据生成过程超出这个空间时",{"5":{"71":2}}],["当真实类别的预测概率",{"5":{"61":2}}],["当新输入​到来时",{"5":{"86":1,"151":1}}],["当新输入",{"5":{"86":1,"151":1}}],["当注意力分布趋于均匀时",{"5":{"41":2}}],["当注意力权重越均匀分布时",{"5":{"87":2,"152":2}}],["当注意力权重越集中时",{"5":{"87":2,"152":2}}],["当注意力权重趋于均匀分布时",{"5":{"87":2,"152":2}}],["当注意力权重趋于极端分布时",{"5":{"87":2,"152":2}}],["当注意力权重接近均匀分布时",{"5":{"87":2,"152":2}}],["当注意力权重接近one",{"5":{"87":2,"152":2}}],["当注意力权重分布相对",{"5":{"95":2}}],["当注意力模式过于",{"5":{"87":2,"152":2}}],["当注意力均匀分布在所有位置时",{"5":{"92":2,"153":2}}],["当注意力完全集中于一个位置时",{"5":{"92":2,"153":2}}],["当条件数很大时",{"5":{"87":4,"89":2,"152":4}}],["当是真实分布而是模型预测分布时",{"5":{"96":1}}],["当是自注意力且时",{"5":{"87":1,"152":1}}],["当上一层输出的均值不为零时",{"5":{"42":2}}],["当上下文长度从8k扩展到32k时",{"5":{"88":2}}],["当处理长序列",{"5":{"86":2,"151":2}}],["当处理离散序列",{"5":{"90":2}}],["当增大时",{"5":{"42":1,"92":1,"153":1}}],["当增加1时",{"5":{"90":1}}],["当足够大时",{"5":{"90":1}}],["当大时",{"5":{"91":1}}],["当位置从变为时",{"5":{"90":1}}],["当位置从",{"5":{"90":1}}],["当位置差较小时",{"5":{"91":1}}],["当位置差较大时",{"5":{"91":1}}],["当位置差增加时",{"5":{"91":1}}],["当位置差",{"5":{"91":3}}],["当​时",{"5":{"89":4,"94":1}}],["当​很大时这是一笔不小的开销",{"5":{"91":1}}],["当​​时",{"5":{"95":1}}],["当信息需要经过很多步传递时",{"5":{"92":2,"153":2}}],["当某个专家的得分显著提高时",{"5":{"158":2}}],["当某个神经元的输入持续为负时",{"5":{"148":2}}],["当某个时达到",{"5":{"132":1}}],["当某个类别的预测概率接近",{"5":{"70":2}}],["当某个",{"5":{"61":2,"95":1,"132":1,"138":2}}],["当某个​接近1",{"5":{"95":1}}],["当某些位置的重要性显著高于其他位置时",{"5":{"92":2,"153":2}}],["当前价值等于即时奖励的期望加上下一状态的折扣价值的期望",{"5":{"156":2}}],["当前层的输入分布会发生变化",{"5":{"146":2}}],["当前更新方向不是当前梯度方向",{"5":{"136":2}}],["当前一步的计算未完成时",{"5":{"92":2,"153":2}}],["当前位置",{"5":{"92":2,"153":2}}],["当前的transformer架构",{"5":{"101":2}}],["当达到百万甚至十亿级别时",{"5":{"92":1,"153":1}}],["当然",{"5":{"93":2}}],["当头数增加时",{"5":{"93":2}}],["当的标准差为1时",{"5":{"41":1}}],["当的方差较大时",{"5":{"94":1}}],["当维度匹配时",{"5":{"50":2}}],["当维度​增大时",{"5":{"95":1}}],["当维度较大时",{"5":{"95":1}}],["当维度",{"5":{"95":2}}],["当softmax温度",{"5":{"141":1}}],["当softmax温度足够小时",{"5":{"141":1}}],["当softmax输出接近one",{"5":{"41":2}}],["当softmax的输入量级较大时",{"5":{"95":2}}],["当点积被适当地缩放后",{"5":{"95":2}}],["当logits与交叉熵损失函数结合使用时",{"5":{"96":2}}],["当梯度超过阈值时",{"5":{"147":2}}],["当梯度可能爆炸时",{"5":{"147":2}}],["当梯度范数",{"5":{"147":1}}],["当梯度范数变得极大时",{"5":{"147":1}}],["当梯度范数急剧增大时",{"5":{"147":2}}],["当梯度爆炸发生时",{"5":{"144":2}}],["当梯度被裁剪时",{"5":{"144":2,"147":2}}],["当梯度信号被压缩到接近零时",{"5":{"41":2}}],["当梯度较小时",{"5":{"97":2}}],["只保存每个专家的输入激活",{"5":{"157":2}}],["只保留权重最大的",{"5":{"159":2}}],["只保留rms归一化",{"5":{"41":2}}],["只保留多个独立的query投影",{"5":{"86":2,"151":2}}],["只缩放范数",{"5":{"147":2}}],["只缩放其大小",{"5":{"41":2,"97":2}}],["只会减小或保持它",{"5":{"147":2}}],["只会随机",{"5":{"145":2}}],["只能看到位置",{"5":{"140":1}}],["只能关注位置",{"5":{"92":2,"95":1,"101":1,"153":2}}],["只能关注位置到",{"5":{"92":2,"153":2}}],["只能为正或零",{"5":{"42":1}}],["只能学习直线或超平面作为决策边界",{"5":{"71":2}}],["只能捕获输入的线性可分特征",{"5":{"45":2}}],["只能解决线性可分",{"5":{"47":2}}],["只能处理线性可分问题",{"5":{"47":2}}],["只能",{"5":{"91":1}}],["只要门值",{"5":{"150":2}}],["只要不超出幂律的适用边界",{"5":{"138":2}}],["只要不使用激活函数",{"5":{"71":2}}],["只要模型结构固定",{"5":{"71":2}}],["只要激活函数满足一定的数学条件",{"5":{"71":2}}],["只要激活函数是",{"5":{"71":2}}],["只要至少有一个输入为1",{"5":{"47":2}}],["只要样本量足够大",{"5":{"96":2}}],["只要序列长度不超过最低频率成分的周期",{"5":{"90":1}}],["只要序列长度",{"5":{"90":1}}],["只要存在一种注意力权重配置",{"5":{"92":2,"153":2}}],["只有特定部分保持独立",{"5":{"158":2}}],["只有",{"5":{"158":2}}],["只有少数几个键向量具有较高的相似性",{"5":{"141":2}}],["只有后层能够学习",{"5":{"41":2}}],["只有超过阈值的信号才能通过",{"5":{"71":2}}],["只有两个输入均为0时输出为0",{"5":{"47":2}}],["只有当专家",{"5":{"158":2}}],["只有当模型容量足以覆盖所有",{"5":{"138":2}}],["只有当和不相关时等号才成立",{"5":{"96":1}}],["只有当",{"5":{"96":1}}],["只有方阵才可能存在逆矩阵",{"5":{"50":2}}],["只有满秩",{"5":{"50":2}}],["只有可对角化矩阵",{"5":{"50":2}}],["只有位置相关的旋转介入内积计算",{"5":{"88":2}}],["只有被选中的头参与输出计算",{"5":{"93":2}}],["只有经过适当的变换后才能转化为有效的概率分布",{"5":{"96":2}}],["只是没有",{"5":{"145":1}}],["只是没有因子",{"5":{"145":1}}],["只是",{"5":{"144":2}}],["只是权重因子的计算方式不同",{"5":{"101":2}}],["只是输入和输出的形式有所不同",{"5":{"101":2}}],["只是重排",{"5":{"101":2}}],["只是relu网络的",{"5":{"45":2}}],["只是需要明确哪个变量被当作中间变量",{"5":{"48":2}}],["只是在不同的应用场景下使用",{"5":{"61":2}}],["只是平移",{"5":{"87":2,"152":2}}],["只是根据重要性进行加权",{"5":{"92":2,"153":2}}],["只是简单地修改输入矩阵",{"5":{"95":2}}],["只是概率质量的分布会根据logits的相对大小进行调整",{"5":{"96":2}}],["只需要学习一个低秩的增量",{"5":{"142":2}}],["只需要正确排名即可",{"5":{"70":2}}],["只需要知道当前的值即可",{"5":{"42":1}}],["只需要知道当前的",{"5":{"42":1}}],["只需要按顺序执行各层的矩阵运算即可",{"5":{"45":2}}],["只需要训练个参数而不是个参数",{"5":{"50":1}}],["只需要训练",{"5":{"50":1}}],["只需要",{"5":{"61":2}}],["只需要存储稀疏的注意力权重和值向量",{"5":{"86":2,"151":2}}],["只需要将更长的索引映射到更大的角度",{"5":{"88":2}}],["只需要计算预测误差",{"5":{"51":2}}],["只需要计算相对位置",{"5":{"88":2}}],["只需要计算三角函数并进行逐元素乘法",{"5":{"88":2}}],["只需要简单的三角函数计算和逐元素乘法",{"5":{"88":2}}],["只需进行转置操作",{"5":{"50":2}}],["只适用于对称正定矩阵",{"5":{"50":2}}],["只使用了",{"5":{"87":1,"152":1}}],["只改变方向",{"5":{"50":2}}],["只改变它的方向",{"5":{"88":2}}],["只重新定向它在高维空间中的指向",{"5":{"88":2}}],["只留下相对位置信息",{"5":{"88":2}}],["只计算新增位置的编码",{"5":{"89":2}}],["只与位置本身有关",{"5":{"91":2}}],["只关注位置",{"5":{"92":2,"153":2}}],["只依赖于相对位置",{"5":{"88":1,"90":3}}],["只依赖于查询和键向量的内容",{"5":{"92":1,"153":1}}],["只依赖于",{"5":{"94":2}}],["只在关键步骤使用全精度",{"5":{"50":2}}],["只在输入和输出端使用",{"5":{"97":2}}],["​重复出现",{"5":{"141":1}}],["​引理",{"5":{"139":1}}],["​p",{"5":{"69":1}}],["​​",{"5":{"41":1,"48":1,"50":2,"61":4,"69":2,"87":1,"89":1,"94":1,"143":2,"152":1}}],["​​在训练时从未见过",{"5":{"89":1}}],["​​的引入部分原因就是为了控制注意力矩阵的条件数",{"5":{"87":1,"152":1}}],["​​的引入部分解决了这个问题",{"5":{"87":1,"152":1}}],["​​的秩较低",{"5":{"89":1}}],["​​的范数决定了梯度在传递过程中的缩放程度",{"5":{"92":1,"153":1}}],["​​的方差被控制在一个稳定的范围内",{"5":{"94":2}}],["​​的设计正是为了抵消维度增长对点积量级的影响",{"5":{"95":1}}],["​​可以视为一种核矩阵",{"5":{"87":1,"152":1}}],["​​可以表示为前",{"5":{"89":1}}],["​​可以被理解为一个温度参数",{"5":{"95":1}}],["​​这一缩放因子是scaled",{"5":{"95":1}}],["​​成正比",{"5":{"95":1}}],["​​为缩放后的注意力分数矩阵",{"5":{"95":1}}],["​​时",{"5":{"95":1}}],["​",{"5":{"41":20,"42":13,"44":15,"45":11,"46":6,"47":6,"48":8,"50":3,"51":3,"61":2,"69":9,"70":3,"71":3,"86":10,"87":20,"88":8,"89":18,"90":21,"91":6,"92":7,"93":13,"94":8,"95":1,"96":12,"97":1,"101":2,"139":1,"141":3,"142":2,"143":11,"144":2,"145":2,"146":10,"148":12,"151":10,"152":20,"153":7}}],["​f",{"5":{"42":1}}],["​j",{"5":{"42":1}}],["​和",{"5":{"44":1,"87":1,"90":4,"91":3,"92":2,"93":2,"94":2,"152":1,"153":2}}],["​和任意",{"5":{"51":1}}],["​和value块",{"5":{"86":1,"151":1}}],["​和​是左右奇异向量",{"5":{"87":1,"152":1}}],["​和​是权重矩阵",{"5":{"92":1,"153":1}}],["​和键向量",{"5":{"92":1,"153":1}}],["​和注意力权重的约束",{"5":{"93":1}}],["​和分别定义了键空间和值空间的投影",{"5":{"94":1}}],["​为输出",{"5":{"44":2}}],["​为第",{"5":{"44":1}}],["​为偏置向量",{"5":{"45":2,"46":2}}],["​为",{"5":{"45":1,"86":1,"95":2,"151":1}}],["​为后向传播的中间值",{"5":{"45":1}}],["​为网络输出",{"5":{"46":2}}],["​为零",{"5":{"86":2,"151":2}}],["​为的第行",{"5":{"95":1}}],["​是矩阵",{"5":{"143":1}}],["​是矩阵在frobenius范数或谱范数意义下的最佳秩近似",{"5":{"143":1}}],["​是谱范数",{"5":{"143":2}}],["​是保留前",{"5":{"142":1}}],["​是对角矩阵",{"5":{"44":1}}],["​是概率分布",{"5":{"44":1}}],["​是各层映射的复合",{"5":{"45":1}}],["​是最大值",{"5":{"86":2,"151":2}}],["​是位置对位置的原始注意力分数",{"5":{"86":1,"151":1}}],["​是位置",{"5":{"86":1,"89":1,"90":1,"91":1,"92":1,"94":1,"151":1,"153":1}}],["​是位置在基​上的坐标",{"5":{"89":1}}],["​是位置的位置key",{"5":{"91":1}}],["​是位置的编码向量",{"5":{"92":1,"153":1}}],["​是位置处词元的嵌入向量",{"5":{"94":1}}],["​是上三角矩阵",{"5":{"87":2,"152":2}}],["​是",{"5":{"87":2,"89":2,"92":6,"152":2,"153":6}}],["​是左右奇异向量",{"5":{"87":1,"152":1}}],["​是预设的最大序列长度",{"5":{"89":2}}],["​是的第行",{"5":{"87":2,"152":2}}],["​是的前列",{"5":{"89":1}}],["​是的奇异值分解中的对应项",{"5":{"89":1}}],["​是表达能力的硬上限",{"5":{"89":1}}],["​是第个频率的周期参数",{"5":{"90":1}}],["​是第个频率成分的频率",{"5":{"90":1}}],["​是第",{"5":{"90":3,"92":1,"153":1}}],["​是第层的残差贡献",{"5":{"92":1,"153":1}}],["​是与频率​对应的频率索引",{"5":{"90":1}}],["​是与频率",{"5":{"90":1}}],["​是互不相同的无理数倍数",{"5":{"90":1}}],["​是可学习编码残差",{"5":{"89":2}}],["​是可学习的投影矩阵",{"5":{"91":2}}],["​是不同的",{"5":{"91":1}}],["​是旋转角度",{"5":{"92":2,"153":2}}],["​是时刻的隐藏状态",{"5":{"92":1,"153":1}}],["​是时刻的输入",{"5":{"92":1,"153":1}}],["​是时刻",{"5":{"92":2,"153":2}}],["​是权重矩阵",{"5":{"92":1,"153":1}}],["​是所有位置",{"5":{"92":1,"153":1}}],["​是多头注意力能够表示的函数类",{"5":{"93":2}}],["​是独立学习的",{"5":{"93":1}}],["​是单头注意力能够表示的函数类",{"5":{"93":1}}],["​是随机矩阵",{"5":{"93":1}}],["​次加法",{"5":{"45":1}}],["​次flops",{"5":{"45":1}}],["​后合并结果",{"5":{"45":1}}],["​zj",{"5":{"46":1}}],["​var",{"5":{"46":1}}],["​定义为",{"5":{"46":1,"91":1}}],["​附近",{"5":{"48":1}}],["​范数",{"5":{"51":1}}],["​来自偏置加法",{"5":{"45":2}}],["​来最小化期望损失",{"5":{"51":1}}],["​视为空间中的两个点",{"5":{"51":1}}],["​分别是行归一化和列归一化操作",{"5":{"86":1,"151":1}}],["​到来时",{"5":{"86":1,"151":1}}],["​表示第",{"5":{"46":1}}],["​表示从位置到位置的信息流动比例",{"5":{"87":1,"152":1}}],["​表示从位置",{"5":{"87":1,"152":1}}],["​表示位置",{"5":{"91":2,"92":2,"153":2}}],["​表示位置对位置的注意力权重",{"5":{"92":2,"153":2}}],["​表示查询矩阵",{"5":{"95":2}}],["​满足",{"5":{"87":1,"92":1,"152":1,"153":1}}],["​且",{"5":{"87":1,"152":1}}],["​个输出元素的总flops为​",{"5":{"45":1}}],["​个输出元素的总flops为",{"5":{"45":1}}],["​个基向量张成的子空间中",{"5":{"89":1}}],["​上的坐标",{"5":{"89":1}}],["​空间中的表示",{"5":{"45":1}}],["​空间",{"5":{"89":1}}],["​时",{"5":{"89":4,"94":1}}],["​的选择需要权衡内存开销和泛化能力",{"5":{"89":2}}],["​的秩不超过",{"5":{"89":1}}],["​的奇异值分解为",{"5":{"89":1}}],["​的行",{"5":{"89":1}}],["​的某些维度",{"5":{"89":1}}],["​的每个维度配对",{"5":{"90":1}}],["​的几何直觉是",{"5":{"91":1}}],["​的序列",{"5":{"91":1}}],["​的信息",{"5":{"92":1,"153":1}}],["​的信息量最多为​位",{"5":{"93":1}}],["​的信息量最多为",{"5":{"93":1}}],["​的影响需要经过",{"5":{"92":1,"153":1}}],["​的梯度",{"5":{"92":2,"153":2}}],["​的梯度为例",{"5":{"94":1}}],["​的线性变换",{"5":{"92":1,"153":1}}],["​的维度为",{"5":{"93":2}}],["​的qkv投影",{"5":{"93":1}}],["​的集合",{"5":{"93":1}}],["​的第一列",{"5":{"94":1}}],["​的具体值",{"5":{"94":1}}],["​的期望为",{"5":{"95":1}}],["​替代",{"5":{"89":1}}],["​行的参数",{"5":{"89":1}}],["​允许处理更长的序列",{"5":{"89":1}}],["​这些精确表达式表明",{"5":{"142":2}}],["​这种结构将层归一化置于残差分支内部",{"5":{"41":1}}],["​这一重要性质",{"5":{"89":1}}],["​较小",{"5":{"90":2,"92":3,"153":3}}],["​较大",{"5":{"90":2,"91":2}}],["​较大时",{"5":{"95":1}}],["​通常与模型的隐藏维度相当",{"5":{"87":2,"152":2}}],["​通常不会精确为零",{"5":{"92":1,"153":1}}],["​通过以下积分计算",{"5":{"90":1}}],["​通过残差连接直接参与到最终层的表示",{"5":{"92":1,"153":1}}],["​共同决定了可用的频率范围",{"5":{"90":1}}],["​使得",{"5":{"90":1}}],["​按频率成分展开",{"5":{"90":1}}],["​在复平面上旋转角度",{"5":{"90":1}}],["​经过",{"5":{"90":1}}],["​比特的信息",{"5":{"90":1}}],["​成反比",{"5":{"90":1}}],["​随位置",{"5":{"90":1}}],["​随维度",{"5":{"91":1}}],["​对应的频率索引",{"5":{"90":1}}],["​对应的周期仍然大于序列长度",{"5":{"91":1}}],["​不等于",{"5":{"91":1}}],["​维空间",{"5":{"89":1}}],["​维子空间的映射",{"5":{"93":1}}],["​维投影",{"5":{"93":1}}],["​维的空间",{"5":{"91":1}}],["​维的子空间中",{"5":{"94":1}}],["​维的查询空间",{"5":{"94":1}}],["​维的查询向量",{"5":{"95":1}}],["​进入注意力计算",{"5":{"91":1}}],["​很大时这是一笔不小的开销",{"5":{"91":1}}],["​能够感知到",{"5":{"92":1,"153":1}}],["​直接依赖于所有​",{"5":{"92":1,"153":1}}],["​直接依赖于所有",{"5":{"92":1,"153":1}}],["​既编码了位置信息",{"5":{"92":1,"153":1}}],["​同时依赖于位置",{"5":{"92":1,"153":1}}],["​可以分配给不同的设备",{"5":{"45":1}}],["​可以通过将",{"5":{"90":1}}],["​可以视为频率域中的",{"5":{"90":1}}],["​可以预先计算",{"5":{"91":1}}],["​可能很小",{"5":{"92":1,"153":1}}],["​可比",{"5":{"93":1}}],["​只依赖于查询和键的相对位置",{"5":{"92":1,"153":1}}],["​参数化",{"5":{"92":1,"153":1}}],["​正确地反映了位置",{"5":{"92":1,"153":1}}],["​接近1",{"5":{"93":2,"95":1}}],["​接近0",{"5":{"93":2}}],["​就位于张量的第",{"5":{"93":1}}],["​沿头维度拼接",{"5":{"93":1}}],["​隐含地学习了一种加权方案",{"5":{"93":1}}],["​与正弦序列",{"5":{"90":1}}],["​与堆叠投影矩阵",{"5":{"93":1}}],["​将输入矩阵",{"5":{"93":1}}],["​位",{"5":{"93":1}}],["​变换后得到query向量",{"5":{"94":1}}],["​增大时",{"5":{"95":1}}],["​都加上一个较大的常数",{"5":{"95":1}}],["​作为相对位置",{"5":{"90":1}}],["​作为相关信息来源的概率",{"5":{"95":1}}],["nbsp",{"5":{"156":21}}],["n=1",{"5":{"143":1}}],["n+j",{"5":{"143":2}}],["nh",{"5":{"137":2}}],["nn",{"5":{"132":20}}],["nce",{"5":{"133":2,"137":4}}],["nce的理论保证需要",{"5":{"69":2}}],["nce的损失函数为这个二分类问题的二元交叉熵",{"5":{"69":2}}],["nce的核心技巧是将密度估计问题转化为一个二分类问题",{"5":{"69":2}}],["nce将其标记为正样本",{"5":{"69":2}}],["nce损失",{"5":{"69":2}}],["noisy",{"5":{"159":2}}],["noise",{"5":{"61":2,"69":4,"137":2,"149":4}}],["nofollow",{"5":{"21":10,"131":51}}],["noopener",{"5":{"21":10,"131":51}}],["node",{"5":{"45":2,"136":2}}],["not",{"5":{"47":2}}],["not组合表示",{"5":{"47":2}}],["norms",{"5":{"136":10}}],["normalization成为自然语言处理和语音处理的首选归一化方法",{"5":{"150":2}}],["normalization使得训练数百甚至数千层的深度网络成为可能",{"5":{"150":2}}],["normalization对每个位置独立归一化",{"5":{"150":2}}],["normalization对输入的线性变换是不敏感的",{"5":{"150":2}}],["normalization对序列中不同位置的表示是独立归一化的",{"5":{"150":2}}],["normalization最著名的案例",{"5":{"150":2}}],["normalization通过重新缩放梯度来避免梯度的极端值",{"5":{"150":2}}],["normalization通过将每层的激活值归一化到固定的分布范围",{"5":{"150":2}}],["normalization通过以下机制改善梯度流",{"5":{"148":2}}],["normalization通常需要维护运行统计量",{"5":{"150":2}}],["normalization不需要在训练和推理时保持一致的行为",{"5":{"150":2}}],["normalization不依赖于批次大小",{"5":{"150":2}}],["normalization不同",{"5":{"150":2}}],["normalization在特征维度上进行归一化",{"5":{"150":2}}],["normalization在批次维度上进行归一化",{"5":{"150":2}}],["normalization的本质区别",{"5":{"150":1}}],["normalization的本质区别是理解layer",{"5":{"150":1}}],["normalization的一个重要性质",{"5":{"150":2}}],["normalization的核心作用机制",{"5":{"150":2}}],["normalization的关键",{"5":{"150":2}}],["normalization的数学定义为",{"5":{"150":2}}],["normalization是在单个样本的特征维度上进行归一化",{"5":{"150":2}}],["normalization或残差连接来改善深层网络的梯度流",{"5":{"148":2}}],["normalization可以视为在每一层插入了一个",{"5":{"148":2}}],["normalization如何从数学上改善梯度稳定性",{"5":{"148":2}}],["normalization之后",{"5":{"145":2}}],["normalization之前或之后应用",{"5":{"145":2}}],["normalization和dropout都改变了网络中的信息流",{"5":{"145":2}}],["normalization",{"2":{"146":2,"150":1},"5":{"41":8,"50":4,"94":2,"96":2,"145":2,"146":12,"147":4,"148":4,"150":17}}],["normal",{"5":{"96":2,"136":2,"145":3}}],["norm",{"5":{"50":6,"87":2,"136":8,"144":2,"147":2,"152":2}}],["none",{"5":{"61":2,"132":3}}],["nabla",{"5":{"142":8,"144":14}}],["nas",{"5":{"41":2}}],["nas被用于自动发现新的激活函数",{"5":{"41":2}}],["nat",{"5":{"96":2}}],["nats",{"5":{"61":2,"137":2}}],["nesterov",{"5":{"136":6}}],["nesterov动量是动量方法的一个变体",{"5":{"48":2}}],["neimark",{"5":{"136":4}}],["neg",{"5":{"133":4,"137":8}}],["negative",{"5":{"61":8}}],["next",{"5":{"101":2}}],["neq",{"5":{"42":8,"61":10}}],["neural",{"5":{"46":2,"47":2,"88":2,"92":2,"95":2,"137":2,"145":4,"153":2}}],["neuron",{"5":{"47":4}}],["nervous",{"5":{"47":2}}],["network可以训练数百层甚至上千层的网络",{"5":{"150":2}}],["network是残差连接思想的重要扩展",{"5":{"150":2}}],["networks",{"5":{"145":4,"158":2}}],["network",{"5":{"46":2,"47":2,"50":2,"71":2,"92":2,"95":2,"132":4,"146":2,"153":2,"159":2}}],["net",{"5":{"47":2,"132":6}}],["neox同样使用rope",{"5":{"88":1}}],["neox的原始论文讨论了rope与flash",{"5":{"88":2}}],["neox",{"5":{"88":1}}],["need",{"5":{"91":2,"146":2}}],["n",{"2":{"140":1,"154":2,"155":3,"156":5},"5":{"46":36,"133":2,"136":6,"137":8,"140":12,"141":14,"143":19}}],["num",{"5":{"132":16}}],["numpy",{"5":{"50":2}}],["numbers",{"5":{"96":2}}],["number",{"5":{"87":2,"137":2,"148":2,"152":2}}],["null",{"5":{"50":2}}],["nuclear",{"5":{"50":2,"87":2,"144":2,"152":2}}],["ntk",{"5":{"88":4}}],["定量的",{"5":{"145":2}}],["定性的",{"5":{"145":2}}],["定义重要性采样比率",{"5":{"154":1}}],["定义经验风险",{"5":{"149":2}}],["定义有效放大因子",{"5":{"148":1}}],["定义有效依赖跨度",{"5":{"92":2,"153":2}}],["定义层间雅可比矩阵",{"5":{"148":2}}],["定义误差向量",{"5":{"148":2}}],["定义的神经网络函数",{"5":{"140":1,"159":2}}],["定义的二次型下",{"5":{"94":1}}],["定义扩展状态变量为",{"5":{"136":2}}],["定义类似",{"5":{"135":2}}],["定义从输入",{"5":{"135":2}}],["定义从输入到输出的完整",{"5":{"135":2}}],["定义从位置到位置的最小路径长度为",{"5":{"92":1,"153":1}}],["定义从位置",{"5":{"92":1,"153":1}}],["定义稀疏注意力掩码",{"5":{"132":1}}],["定义稀疏注意力掩码来限制注意力计算的范围",{"5":{"132":1}}],["定义三个信息处理向量",{"5":{"132":2}}],["定义一个有效位置掩码",{"5":{"101":1}}],["定义一个有效位置掩码​",{"5":{"101":1}}],["定义一个概率分布",{"5":{"69":1}}],["定义一个概率分布使得",{"5":{"69":1}}],["定义正样本的归一化分数为",{"5":{"69":2}}],["定义为未被激活的专家比例",{"5":{"159":2}}],["定义为",{"5":{"42":6,"45":3,"51":4,"61":2,"70":2,"71":5,"86":1,"87":5,"92":2,"95":2,"96":5,"136":1,"141":1,"151":1,"152":5,"153":2,"158":2}}],["定义为​",{"5":{"71":1,"96":1}}],["定义为使后验概率等于0",{"5":{"47":2}}],["定义为所有元素平方和的平方根",{"5":{"50":2}}],["定义为在给定参数",{"5":{"61":2}}],["定义为奇异值的和",{"5":{"87":2,"152":2}}],["定义为矩阵的特征值的最大模",{"5":{"87":1,"152":1}}],["定义为矩阵",{"5":{"87":1,"152":1}}],["定义3",{"5":{"41":28,"42":8,"71":20}}],["定义饱和深度函数来量化输入值距离饱和区域的远近",{"5":{"41":2}}],["定义激活函数的饱和度量来量化其饱和程度",{"5":{"41":2}}],["定义激活函数的梯度稳定性指标",{"5":{"41":2}}],["定义2",{"5":{"44":13,"45":18,"46":8,"47":20}}],["定义了两个区域",{"5":{"147":2}}],["定义了信息选择的依据",{"5":{"132":2}}],["定义了",{"5":{"132":2}}],["定义了相似度函数",{"5":{"69":1}}],["定义了流形上的几何结构",{"5":{"70":2,"71":2}}],["定义了所有可能的预测向量集合",{"5":{"51":1}}],["定义了query空间和key空间之间的映射",{"5":{"87":2,"152":2}}],["定义了一个参数化的分布",{"5":{"69":2}}],["定义了一个预测函数",{"5":{"51":1}}],["定义了一个有向加权图",{"5":{"87":2,"92":1,"152":2,"153":1}}],["定义了一种依赖关系图",{"5":{"92":1,"153":1}}],["定义",{"5":{"61":10,"69":12,"70":6,"97":14,"101":30,"132":38,"133":24,"134":28,"135":28,"136":30,"137":18,"139":11,"140":24,"141":4,"142":13,"143":18,"144":2,"147":2,"148":1,"149":2,"154":1,"156":2}}],["定义注意力权重",{"5":{"69":2}}],["定义注意力权重的熵",{"5":{"92":2,"153":2}}],["定义注意力输出为",{"5":{"86":2,"151":2}}],["定义复数形式的编码分量",{"5":{"90":2}}],["定义位置的复数编码向量为",{"5":{"90":1}}],["定义位置",{"5":{"90":1}}],["定义角频率为",{"5":{"91":1}}],["定义角频率",{"5":{"91":1}}],["定义logits向量",{"5":{"96":2}}],["定理10",{"5":{"145":4}}],["定理的离散形式",{"5":{"134":2}}],["定理的证明思路基于stone",{"5":{"71":2}}],["定理",{"5":{"69":2,"88":2,"101":4,"132":34,"133":16,"134":14,"135":20,"136":30,"137":6,"139":14,"140":6,"141":10,"142":8,"143":9,"144":18,"145":12,"146":6,"147":6,"148":4,"149":2}}],["定理3",{"5":{"41":12,"42":8,"71":4}}],["定理2",{"5":{"44":24,"45":26,"46":8,"47":14}}],["为读者提供全面的实践指导",{"5":{"157":2}}],["为actor的更新提供了稳定的参考",{"5":{"155":2}}],["为actor的决策提供反馈",{"5":{"155":2}}],["为粗尺度分配较少的通道数以处理全局信息",{"5":{"150":2}}],["为解决这些问题提供了革命性的方案",{"5":{"150":2}}],["为解决这一挑战提供了优雅的数学框架",{"5":{"142":2}}],["为条件数",{"5":{"149":1}}],["为参数",{"5":{"149":1}}],["为参数向量的欧几里得范数的平方",{"5":{"144":2}}],["为各样本损失的平均",{"5":{"149":2}}],["为恒等映射",{"5":{"148":1}}],["为奇异值",{"5":{"148":2}}],["为某个与网络结构相关的常数",{"5":{"148":2}}],["为某个阈值",{"5":{"92":2,"153":2}}],["为网络输出",{"5":{"148":2}}],["为网络输入",{"5":{"46":1}}],["为裁剪后的梯度",{"5":{"147":2}}],["为原始梯度",{"5":{"147":2}}],["为梯度范数场",{"5":{"147":2}}],["为便于查阅和比较",{"5":{"146":1}}],["为通道数",{"5":{"146":1}}],["为批量大小",{"5":{"146":3}}],["为可学习的平移参数向量",{"5":{"146":2}}],["为可学习的缩放参数向量",{"5":{"146":2}}],["为计算得到的标准差向量",{"5":{"146":2}}],["为计算得到的均值向量",{"5":{"146":2}}],["为归一化后的输出张量",{"5":{"146":2}}],["为归一化前的输入张量",{"5":{"146":2}}],["为特征维度",{"5":{"146":6}}],["为dropout层的输入向量",{"5":{"145":1}}],["为独立的伯努利掩码向量",{"5":{"145":2}}],["为保留概率",{"5":{"144":2}}],["为保证",{"5":{"133":2}}],["为保证信息无损传递",{"5":{"132":2}}],["为样本数量",{"5":{"144":2}}],["为样本总数",{"5":{"61":1}}],["为数据矩阵",{"5":{"143":1}}],["为代表的方法",{"5":{"142":2}}],["为隐藏维度",{"5":{"141":2,"158":2}}],["为词汇表",{"5":{"140":1}}],["为模型参数向量",{"5":{"144":1}}],["为模型的训练和评估提供坚实的数学基础",{"5":{"139":2}}],["为模型分布",{"5":{"61":2}}],["为避免数值下溢",{"5":{"139":2}}],["为涌现能力提供了更深刻的数学洞见",{"5":{"138":1}}],["为这一领域奠定了基础",{"5":{"138":1}}],["为单点分布",{"5":{"137":1}}],["为常数",{"5":{"137":1}}],["为凸函数",{"5":{"137":1}}],["为确定性变量",{"5":{"137":1}}],["为邻近分布",{"5":{"137":2}}],["为两点均匀分布",{"5":{"137":2}}],["为两个映射",{"5":{"45":1}}],["为类别数",{"5":{"137":2}}],["为类别标签",{"5":{"47":1}}],["为理论分析提供了计算工具",{"5":{"135":2}}],["为理解表示学习",{"5":{"137":2}}],["为理解大模型的能力提供了新的视角",{"5":{"137":2}}],["为理解大学习率的泛化优势提供了新视角",{"5":{"136":2}}],["为理解大语言模型中非线性变换的数学本质奠定坚实基础",{"5":{"71":2}}],["为理解循环学习率等策略提供了理论基础",{"5":{"136":2}}],["为理解反向传播提供必要的背景知识",{"5":{"45":2}}],["为理解投影矩阵的几何性质提供了强大的工具",{"5":{"94":2}}],["为理解和优化大规模神经网络训练提供了理论指导",{"5":{"135":2}}],["为理解和优化大语言模型提供了核心的数学工具",{"5":{"48":2}}],["为理解和优化深度学习训练过程提供了深刻的数学洞见",{"5":{"134":2}}],["为理解和改进注意力机制提供了信息论工具",{"5":{"132":2}}],["为理解和改进大语言模型的训练策略提供了理论基础",{"5":{"97":2}}],["为其",{"5":{"135":2}}],["为其他位置",{"5":{"69":2}}],["为对称矩阵",{"5":{"135":2}}],["为超参数选择提供理论指导",{"5":{"134":2}}],["为平衡点",{"5":{"134":2}}],["为平移向量",{"5":{"47":2}}],["为什么sgd能够在深度网络的训练中表现出色",{"5":{"149":2}}],["为什么过度参数化的神经网络倾向于收敛到",{"5":{"149":2}}],["为什么深度网络能够学习紧凑表示",{"5":{"133":2}}],["为什么需要反向传播",{"2":{"44":1},"5":{"44":1}}],["为编码",{"5":{"132":1}}],["为编码个位置的相对关系",{"5":{"132":1}}],["为设计更有效的位置编码方法提供了理论依据",{"5":{"132":2}}],["为设计和改进大语言模型的训练策略提供理论指导",{"5":{"41":2}}],["为每个位置分配唯一的",{"5":{"101":2}}],["为分析两种应用提供了共同的语言",{"5":{"70":2}}],["为所有样本",{"5":{"70":2}}],["为第",{"5":{"41":1,"44":1,"45":1,"46":1,"146":1,"148":3}}],["为第层激活函数的导数",{"5":{"44":1}}],["为第层的雅可比矩阵",{"5":{"148":1}}],["为第层的维度",{"5":{"148":1}}],["为第层的净输入",{"5":{"148":1}}],["为第层的输入",{"5":{"41":1}}],["为第层的激活函数",{"5":{"46":1}}],["为负区域提供有限的梯度传递",{"5":{"41":2}}],["为未来研究和实践提供了理论指导",{"5":{"41":2}}],["为饱和阈值",{"5":{"41":1}}],["为损失函数",{"5":{"41":1}}],["为正则化系数",{"5":{"144":1}}],["为正",{"5":{"42":1}}],["为",{"5":{"47":2,"61":4,"69":2,"89":4,"90":5,"91":2,"92":2,"96":2,"135":10,"145":4,"147":2,"149":2,"153":2,"154":2,"158":2,"159":2}}],["为了使噪声幅度适应输入的尺度",{"5":{"159":2}}],["为了增强门控的表达能力",{"5":{"159":2}}],["为了增强线性注意力的表达能力",{"5":{"86":2,"151":2}}],["为了缓解这个问题",{"5":{"159":2}}],["为了缓解这种累积效应",{"5":{"145":2}}],["为了处理这个问题",{"5":{"157":2}}],["为了处理变长序列",{"5":{"101":2}}],["为了评估一个策略的好坏",{"5":{"156":2}}],["为了平衡偏差",{"5":{"155":2}}],["为了证明基线不改变梯度的无偏性",{"5":{"155":2}}],["为了解决reinforce算法的高方差问题",{"5":{"155":2}}],["为了解决这个问题",{"5":{"158":2,"159":2}}],["为了解决这一问题",{"5":{"146":2}}],["为了解决这些问题并适应不同的应用需求",{"5":{"86":2,"151":2}}],["为了防止策略",{"5":{"154":2}}],["为了简化计算",{"5":{"155":2}}],["为了简化",{"5":{"150":2}}],["为了直观理解这一关系",{"5":{"149":2}}],["为了建立这种联系",{"5":{"149":2}}],["为了建立梯度消失与爆炸的精确数学条件",{"5":{"148":2}}],["为了准确理解梯度协方差矩阵的意义",{"5":{"149":2}}],["为了保持",{"5":{"148":4}}],["为了保持期望",{"5":{"145":2}}],["为了保持方差稳定",{"5":{"41":2}}],["为了补偿这种噪声带来的不确定性",{"5":{"145":2}}],["为了生成新的文本序列",{"5":{"140":2}}],["为了提高计算效率",{"5":{"101":2}}],["为了提高数值稳定性",{"5":{"96":2}}],["为了深入理解infonce的优化特性",{"5":{"69":2}}],["为了深刻理解这一数学本质",{"5":{"71":2}}],["为了克服nce的局限性",{"5":{"69":2}}],["为了突破这一限制",{"5":{"46":2}}],["为了将仿射变换统一表示为线性变换的形式",{"5":{"47":2}}],["为了高效地实现这些计算",{"5":{"48":2}}],["为了校正偏差",{"5":{"48":2}}],["为了在现代硬件上高效实现稀疏注意力",{"5":{"86":2,"151":2}}],["为了理解ppo的数学机制",{"5":{"154":2}}],["为了理解这种各向异性",{"5":{"149":2}}],["为了理解这一点",{"5":{"88":2}}],["为了理解现代自回归模型的演进历程",{"5":{"140":1}}],["为了理解注意力机制的突破",{"5":{"92":2,"153":2}}],["为了理解缩放的必要性",{"5":{"95":2}}],["为了量化注意力机制建模长程依赖的能力",{"5":{"92":2,"153":2}}],["为了更有效地利用基线技术",{"5":{"155":2}}],["为了更清晰地理解三种归一化方法的本质区别",{"5":{"146":2}}],["为了更直观地理解这些特征",{"5":{"149":2}}],["为了更直观地理解mse的几何解释",{"5":{"51":2}}],["为了更直观地理解rope的运作机制",{"5":{"88":2}}],["为了更直观地理解正弦余弦位置编码",{"5":{"91":2}}],["为了更精确地分析注意力机制在长程依赖建模方面的优势",{"5":{"92":2,"153":2}}],["为了分析多层注意力如何建模长程依赖",{"5":{"92":2,"153":2}}],["为了便于分析和控制",{"5":{"95":2}}],["为基准",{"5":{"71":1}}],["为克罗内克函数",{"5":{"44":2}}],["为矩阵变量",{"5":{"44":1}}],["为输入向量",{"5":{"44":2,"47":1,"143":2,"148":2}}],["为输入",{"5":{"44":1}}],["为输入数",{"5":{"44":1,"45":1}}],["为输入信号",{"5":{"47":1}}],["为输出数",{"5":{"44":1,"45":1}}],["为输出向量",{"5":{"47":2}}],["为序列中的每个位置添加位置信息",{"5":{"71":2}}],["为序列中的每个位置分配数学表示",{"5":{"71":2}}],["为序列长度",{"5":{"45":2,"141":1,"146":1,"158":2}}],["为我们进一步学习反向传播算法",{"5":{"45":2}}],["为逐元素应用的激活函数",{"5":{"46":2}}],["为该层的输出向量",{"5":{"46":2}}],["为连接权重",{"5":{"47":2}}],["为激活函数",{"5":{"45":2,"46":2,"47":1}}],["为激活阈值",{"5":{"47":2}}],["为阶跃激活函数",{"5":{"47":2}}],["为神经网络中某一层的权重矩阵",{"5":{"142":1}}],["为神经网络的现代发展奠定了基础",{"5":{"47":2}}],["为神经元输出",{"5":{"47":2}}],["为权重矩阵平均谱半径",{"5":{"135":2}}],["为权重矩阵",{"5":{"45":1,"46":2}}],["为权重向量",{"5":{"47":2}}],["为偏置向量",{"5":{"46":2}}],["为偏置项",{"5":{"47":2}}],["为误差函数",{"5":{"47":2}}],["为变换矩阵",{"5":{"47":1}}],["为标量权重",{"5":{"148":1}}],["为标量损失函数",{"5":{"44":1}}],["为标准正态分布的累积分布函数",{"5":{"47":1}}],["为行向量",{"5":{"47":1}}],["为大语言模型提供了处理不确定性的理论基础",{"5":{"96":2}}],["为后续的高阶分解奠定数学基础",{"5":{"143":2}}],["为后续机器学习中的损失函数设计提供了深刻的理论启示",{"5":{"61":2}}],["为后续讨论kl散度提供了几何背景",{"5":{"61":2}}],["为后续学习更复杂的损失函数奠定坚实的数学基础",{"5":{"51":2}}],["为后续学习更复杂的位置编码方案",{"5":{"90":2}}],["为后续学习多头注意力",{"5":{"95":2}}],["为绝对值很大的负数时",{"5":{"61":2}}],["为在某种语义上有意义的位置",{"5":{"69":2}}],["为在实际应用中选择和设计合适的注意力机制提供理论指导",{"5":{"86":2,"151":2}}],["为经过rope编码的query和key向量",{"5":{"88":2}}],["为深入理解这一架构奠定坚实的理论基础",{"5":{"159":2}}],["为深入理解位置编码的数学本质奠定坚实基础",{"5":{"91":2}}],["为深层网络的稳定训练提供了数学保障",{"5":{"41":2}}],["为整数",{"5":{"91":2}}],["为简化展示",{"5":{"91":2}}],["为值矩阵",{"5":{"92":2,"153":2}}],["为注意力机制的优化提供了新的思路",{"5":{"132":2}}],["为注意力机制的超参数设计提供了理论指导",{"5":{"132":2}}],["为注意力权重矩阵",{"5":{"92":2,"153":2}}],["为注意力输出",{"5":{"94":1}}],["为真实梯度",{"5":{"149":2}}],["为真实方差",{"5":{"146":1}}],["为真实分布",{"5":{"61":1}}],["为真实标签",{"5":{"96":2,"148":1}}],["为离散随机变量",{"5":{"96":1}}],["为预测概率",{"5":{"96":1}}],["其余分量被置零或完全忽略",{"5":{"159":2}}],["其余为0",{"5":{"61":2,"159":2}}],["其余为",{"5":{"61":2,"137":2}}],["其路由概率",{"5":{"158":2}}],["其结果可以被所有专家复用",{"5":{"158":2}}],["其状态转移概率和奖励函数都是未知的",{"5":{"155":2}}],["其随机微分方程为",{"5":{"149":2}}],["其平稳分布与某个有效势能函数相关",{"5":{"149":2}}],["其表达式包含权重乘积",{"5":{"148":2}}],["其表达能力随深度指数增长",{"5":{"133":2}}],["其表达能力不再受限于线性变换的复合",{"5":{"71":2}}],["其表达能力远超单个超平面",{"5":{"46":2}}],["其表达能力等价于单层线性变换",{"5":{"47":2}}],["其表达能力与参数量的关系如何",{"5":{"89":2}}],["其更新量趋近于零",{"5":{"148":2}}],["其梯度通常较小",{"5":{"147":2}}],["其梯度通常较大",{"5":{"147":2}}],["其梯度为行向量",{"5":{"44":2}}],["其一",{"5":{"147":2}}],["其一是计算出的",{"5":{"61":2}}],["其归一化输出",{"5":{"146":1}}],["其归一化输出可以仅基于计算得到",{"5":{"146":1}}],["其训练与推理阶段使用不同的统计量计算方式",{"5":{"146":2}}],["其期望的映射为",{"5":{"150":2}}],["其期望为",{"5":{"145":2}}],["其期望定义为",{"5":{"96":2}}],["其优势在于推理阶段的计算更加简洁",{"5":{"145":2}}],["其优化景观包含多个局部极小值",{"5":{"97":2}}],["其软阈值操作是许多稀疏建模技术的数学基础",{"5":{"144":2}}],["其形式类似于权重衰减",{"5":{"144":2}}],["其奇异值分解为",{"5":{"144":2}}],["其效果等价于在权重上施加某种形式的权重衰减",{"5":{"144":2}}],["其效果是两个点的",{"5":{"90":2}}],["其基本思想是在损失函数中添加与权重范数平方成正比的惩罚项",{"5":{"144":2}}],["其基本思想是用有限差分近似计算梯度",{"5":{"44":2}}],["其本质是通过在损失函数中添加惩罚项或修改优化过程来约束模型的复杂度",{"5":{"144":2}}],["其本质就是针对协方差矩阵的svd分解",{"5":{"143":2}}],["其重要性将更加凸显",{"5":{"143":2}}],["其元素形式简洁优雅",{"5":{"143":2}}],["其元素为",{"5":{"143":3}}],["其元素为​",{"5":{"143":1}}],["其元素表示生成词汇表中每个词的概率",{"5":{"96":2}}],["其svd分解为",{"5":{"143":2}}],["其softmax梯度与缩放后的版本有所不同",{"5":{"94":2}}],["其权重矩阵表现出显著的低秩特性",{"5":{"142":2}}],["其权重矩阵为",{"5":{"47":2}}],["其特点是高度不均匀",{"5":{"141":2}}],["其特征值为",{"5":{"70":1,"97":1,"134":2}}],["其特征值为和​",{"5":{"70":1,"97":1}}],["其特征值分布在单位圆附近",{"5":{"41":2}}],["其特征值和特征向量可以通过傅里叶变换分析",{"5":{"87":2,"152":2}}],["其特征值非负",{"5":{"87":2,"152":2}}],["其特征向量可能与离散余弦变换",{"5":{"87":2,"152":2}}],["其联合概率分布可以分解为",{"5":{"140":2}}],["其联合分布为",{"5":{"69":2}}],["其与真实分布",{"5":{"138":2}}],["其二",{"5":{"147":2}}],["其二阶近似导出的",{"5":{"137":2}}],["其二是后续的",{"5":{"61":2}}],["其学习过程可以从",{"5":{"137":2}}],["其熵满足",{"5":{"137":2}}],["其取值空间为",{"5":{"137":2}}],["其边界由特征值条件",{"5":{"136":1}}],["其边界由特征值条件决定",{"5":{"136":1}}],["其吸引域",{"5":{"136":1}}],["其吸引域定义为",{"5":{"136":1}}],["其线性化系统的解矩阵",{"5":{"136":1}}],["其线性化系统的解矩阵满足",{"5":{"136":1}}],["其行为介于周期轨道和混沌轨道之间",{"5":{"136":2}}],["其行列式可以表示为特征值的乘积",{"5":{"87":2,"152":2}}],["其收敛性分析是理解更复杂优化器的基础",{"5":{"136":2}}],["其收敛速度由的特征值决定",{"5":{"97":1}}],["其收敛速度由",{"5":{"97":1}}],["其解析解为",{"5":{"144":2}}],["其解",{"5":{"135":2}}],["其解为​",{"5":{"41":1}}],["其解为",{"5":{"41":1}}],["其条件数定义为",{"5":{"135":2}}],["其条件数为",{"5":{"134":2}}],["其在正则化中的应用",{"5":{"133":2}}],["其关于",{"5":{"70":1}}],["其关于和的梯度结构与上述交叉熵关于的梯度结构高度相似",{"5":{"70":1}}],["其hessian矩阵",{"5":{"142":2}}],["其hessian矩阵为",{"5":{"70":2,"97":2}}],["其hessian恰好是fisher信息矩阵",{"5":{"70":2}}],["其任意两点间的弦位于函数图像上方",{"5":{"70":2,"97":2}}],["其图像是一个抛物面",{"5":{"70":2}}],["其图像呈标准的s形曲线",{"5":{"42":2}}],["其次是缩放归一化",{"5":{"146":2}}],["其次是卷积对偶",{"5":{"90":2}}],["其次",{"5":{"41":6,"42":2,"48":4,"50":10,"61":4,"69":2,"70":2,"71":2,"86":2,"87":2,"89":2,"90":2,"91":2,"93":2,"96":4,"146":6,"148":4,"149":2,"150":2,"151":2,"152":2}}],["其次分析谱范数",{"5":{"87":2,"152":2}}],["其中稀疏门控权重",{"5":{"158":2}}],["其中ln表示layer",{"5":{"150":2}}],["其中噪声项",{"5":{"149":2}}],["其中最著名的是它与hessian矩阵的联系",{"5":{"149":2}}],["其中期望是对数据分布取的",{"5":{"149":2}}],["其中期望保持性质",{"5":{"145":1}}],["其中上标",{"5":{"149":2}}],["其中梯度不会增长过快",{"5":{"147":2}}],["其中各符号的定义如下",{"5":{"146":2}}],["其中各矩阵的定义和性质如下所述",{"5":{"143":2}}],["其中分别是模1",{"5":{"143":1}}],["其中包含前个左奇异向量",{"5":{"143":1}}],["其中包含所有奇异值",{"5":{"41":1}}],["其中查询矩阵",{"5":{"140":2}}],["其中查询向量与键向量在特征维度上进行点积运算",{"5":{"50":2}}],["其中关键发现是",{"5":{"138":2}}],["其中正则化强度与周期幅度相关",{"5":{"136":2}}],["其中一个不动点穿过另一个",{"5":{"136":2}}],["其中称为",{"5":{"136":1}}],["其中不动点满足",{"5":{"136":2}}],["其中有",{"5":{"135":2}}],["其中三变量互信息",{"5":{"133":1}}],["其中三变量互信息衡量共享表示中与各任务相关的信息冗余程度",{"5":{"133":1}}],["其中能量",{"5":{"132":2}}],["其中权重反映任务重要性",{"5":{"133":1}}],["其中权重",{"5":{"101":1,"133":1}}],["其中权重是时间的函数",{"5":{"101":1}}],["其中权重由相似度决定",{"5":{"70":2}}],["其中权重由",{"5":{"61":2}}],["其中被人类偏好于",{"5":{"101":1}}],["其中到是前缀",{"5":{"101":1}}],["其中也涉及",{"5":{"70":2}}],["其中注意力权重",{"5":{"69":1}}],["其中注意力权重度量位置对位置的",{"5":{"69":1}}],["其中是温度",{"5":{"149":1}}],["其中是温度参数",{"5":{"137":1}}],["其中是正交特征向量矩阵",{"5":{"149":1}}],["其中是正则化系数",{"5":{"133":1}}],["其中是从",{"5":{"149":1}}],["其中是与范数选择相关的常数",{"5":{"148":1}}],["其中是与最相似的键向量",{"5":{"141":1}}],["其中是放大系数",{"5":{"147":1}}],["其中是范数",{"5":{"146":1}}],["其中是保留概率",{"5":{"145":1}}],["其中是权重张量",{"5":{"143":1}}],["其中是权衡压缩和预测的超参数",{"5":{"137":1}}],["其中是核张量的模n展开",{"5":{"143":1}}],["其中是核函数",{"5":{"87":1,"152":1}}],["其中是frobenius范数",{"5":{"143":1}}],["其中是旋转",{"5":{"143":1}}],["其中是上三角掩码矩阵",{"5":{"140":1}}],["其中是真实数据分布",{"5":{"140":1}}],["其中是真实的目标函数",{"5":{"70":1}}],["其中是样本数量",{"5":{"137":1}}],["其中是均匀分布",{"5":{"137":1}}],["其中是均值",{"5":{"41":1}}],["其中是均值参数",{"5":{"96":1}}],["其中是均值向量",{"5":{"96":1}}],["其中是反馈增益矩阵",{"5":{"136":1}}],["其中是增强系统的",{"5":{"136":1}}],["其中是分岔参数",{"5":{"136":1}}],["其中是动量系数",{"5":{"136":1}}],["其中是学习率参数",{"5":{"136":1}}],["其中是周期",{"5":{"136":1}}],["其中是对抗攻击扰动",{"5":{"133":1}}],["其中是教师表示",{"5":{"133":1}}],["其中是表示空间与数据流形拓扑的差异度量",{"5":{"133":1}}],["其中是潜在变量",{"5":{"133":1}}],["其中是任务的私有表示",{"5":{"133":1}}],["其中是任务的权重",{"5":{"101":1}}],["其中是任务权重",{"5":{"133":1}}],["其中是假设与训练数据之间的互信息",{"5":{"133":1}}],["其中是负样本",{"5":{"137":1}}],["其中是负样本数量",{"5":{"133":1}}],["其中是负样本表示",{"5":{"133":1}}],["其中是负样本的数量",{"5":{"69":1,"71":1}}],["其中是",{"5":{"133":1,"136":3,"137":1}}],["其中是特征映射函数",{"5":{"132":1}}],["其中是特征维度",{"5":{"86":1,"151":1}}],["其中是三变量互信息",{"5":{"132":1}}],["其中是各维度的可能取值数",{"5":{"132":1}}],["其中是归一化常数",{"5":{"101":1}}],["其中是sigmoid函数",{"5":{"101":2}}],["其中是共享参数",{"5":{"101":1}}],["其中是被掩码的span集合",{"5":{"101":1}}],["其中是模型参数向量",{"5":{"149":1}}],["其中是模型参数",{"5":{"101":1}}],["其中是模型的隐藏维度",{"5":{"95":1}}],["其中是第次采样的单样本梯度",{"5":{"149":1}}],["其中是第次分岔发生的参数值",{"5":{"136":1}}],["其中是第层的表示",{"5":{"137":1}}],["其中是第层的信息损失",{"5":{"133":1}}],["其中是第步的",{"5":{"133":1}}],["其中是第步的参数",{"5":{"48":1}}],["其中是第个训练序列",{"5":{"140":1}}],["其中是第个任务的权重",{"5":{"70":1}}],["其中是第个位置的输入embedding",{"5":{"88":1}}],["其中是黎曼度量张量",{"5":{"70":1}}],["其中是常数",{"5":{"70":1,"97":1}}],["其中是预测误差",{"5":{"70":1}}],["其中是位置的隐藏状态",{"5":{"101":1}}],["其中是位置与其他位置之间的互信息",{"5":{"69":1}}],["其中是位置到向量的映射",{"5":{"91":1}}],["其中是查询",{"5":{"132":1}}],["其中是查询与位置的相似度分数",{"5":{"132":1}}],["其中是查询与键的相似度",{"5":{"69":1}}],["其中是查询向量",{"5":{"69":1}}],["其中是由激活函数在各位置的导数组成的对角矩阵",{"5":{"148":1}}],["其中是由公式",{"5":{"146":1}}],["其中是由transformer参数化的复杂函数",{"5":{"140":1}}],["其中是由参数定义的神经网络函数",{"5":{"140":1}}],["其中是由softmax定义的概率分布",{"5":{"69":1}}],["其中是由网络参数定义的函数映射",{"5":{"45":1}}],["其中是噪声样本与正样本的比例",{"5":{"69":1}}],["其中是裁剪阈值",{"5":{"41":1}}],["其中是的最小特征值",{"5":{"70":1}}],["其中是的最大奇异值",{"5":{"41":1}}],["其中是的均方根",{"5":{"41":1}}],["其中是的第列",{"5":{"47":1}}],["其中是输入分布的支持域",{"5":{"41":1}}],["其中是输入层到隐藏层的权重矩阵",{"5":{"71":1}}],["其中是输入权重向量",{"5":{"71":1}}],["其中是输入神经元的数量",{"5":{"96":1}}],["其中是输入特征",{"5":{"51":1}}],["其中是双曲正割函数",{"5":{"42":1}}],["其中是两个权重矩阵的乘积",{"5":{"71":1}}],["其中是变换矩阵",{"5":{"71":1}}],["其中是非线性激活函数",{"5":{"71":1}}],["其中是gumbel噪声",{"5":{"71":1}}],["其中是随机噪声源",{"5":{"71":1}}],["其中是激活函数",{"5":{"71":1,"145":1}}],["其中是网络对第类的",{"5":{"69":1}}],["其中是网络的原始输出",{"5":{"71":1}}],["其中是网络预测",{"5":{"44":1}}],["其中是行向量还是列向量需要注意维度匹配",{"5":{"44":1}}],["其中是批均值",{"5":{"41":1}}],["其中是批量预测矩阵",{"5":{"44":1}}],["其中是批量隐藏激活矩阵",{"5":{"44":1}}],["其中是批量大小",{"5":{"50":1}}],["其中是批大小",{"5":{"50":1}}],["其中是仿射变换算子",{"5":{"47":1}}],["其中是全1向量",{"5":{"47":2,"89":1}}],["其中是外层函数关于中间变量的导数",{"5":{"48":1}}],["其中是梯度与方向之间的夹角",{"5":{"48":1}}],["其中是残差函数的雅可比矩阵",{"5":{"48":1}}],["其中是速度向量",{"5":{"48":1}}],["其中是衰减系数",{"5":{"48":1}}],["其中是成功概率",{"5":{"96":1}}],["其中是率参数",{"5":{"96":1}}],["其中是未知参数",{"5":{"96":1}}],["其中是似然函数",{"5":{"96":1}}],["其中是标准正态分布的cdf",{"5":{"96":1}}],["其中是单位矩阵",{"5":{"50":1}}],["其中是序列长度",{"5":{"50":1,"141":1}}],["其中是设计矩阵",{"5":{"51":1,"70":1}}],["其中是设计矩阵的最小非零特征值",{"5":{"97":1}}],["其中是投影矩阵",{"5":{"51":1}}],["其中是one",{"5":{"61":1}}],["其中是逐元素乘法",{"5":{"132":1}}],["其中是逐元素乘方",{"5":{"86":1,"151":1}}],["其中是逐元素累积的梯度平方",{"5":{"48":1}}],["其中是频率参数",{"5":{"86":1,"151":1}}],["其中是数据矩阵",{"5":{"87":1,"152":1}}],["其中是有效秩",{"5":{"87":1,"152":1}}],["其中是角度为",{"5":{"88":1}}],["其中是一个非线性函数",{"5":{"88":1}}],["其中是一个缩放因子",{"5":{"88":1}}],["其中是一个可学习的参数",{"5":{"88":1}}],["其中是一个的下三角矩阵",{"5":{"95":1}}],["其中是某个可学习的距离函数",{"5":{"92":1,"153":1}}],["其中是卷积核",{"5":{"93":1}}],["其中是卷积矩阵",{"5":{"93":1}}],["其中是多头注意力中的头数",{"5":{"94":1}}],["其中是缩放后的相似度分数",{"5":{"132":1}}],["其中是缩放后注意力分数矩阵的softmax梯度矩阵",{"5":{"94":1}}],["其中是缩放因子",{"5":{"44":1}}],["其中是注意力衰减长度",{"5":{"132":1}}],["其中是注意力输出",{"5":{"132":1}}],["其中是注意力阈值",{"5":{"132":1}}],["其中是注意力头的数量",{"5":{"50":1}}],["其中是注意力权重矩阵",{"5":{"97":1}}],["其中是损失函数的利普希茨常数",{"5":{"97":1}}],["其中是初始学习率",{"5":{"97":1}}],["其中是总训练步数",{"5":{"97":1}}],["其中",{"5":{"41":11,"42":1,"44":12,"45":8,"46":6,"47":13,"48":9,"50":15,"51":16,"61":23,"69":11,"70":11,"71":10,"86":17,"87":28,"88":8,"89":25,"90":17,"91":13,"92":21,"93":16,"94":10,"95":5,"96":15,"97":7,"101":18,"132":11,"133":17,"134":30,"135":26,"136":15,"137":7,"138":12,"139":8,"140":10,"141":10,"142":3,"143":13,"144":26,"145":14,"146":9,"147":17,"148":20,"149":11,"150":12,"151":17,"152":28,"153":21,"154":10,"155":10,"157":20,"158":30,"159":41}}],["其中为标量权重",{"5":{"148":1}}],["其中为标准正态分布的累积分布函数",{"5":{"47":1}}],["其中为真实标签",{"5":{"148":1}}],["其中为真实方差",{"5":{"146":1}}],["其中为真实分布",{"5":{"61":1}}],["其中为通道数",{"5":{"146":1}}],["其中为批量大小",{"5":{"146":3}}],["其中为正则化系数",{"5":{"144":1}}],["其中为模型参数向量",{"5":{"144":1}}],["其中为序列长度",{"5":{"141":1,"146":1}}],["其中为网络输入",{"5":{"46":1}}],["其中为权重矩阵",{"5":{"45":1,"46":2}}],["其中为输入",{"5":{"44":1}}],["其中为输入信号",{"5":{"47":1}}],["其中为矩阵变量",{"5":{"44":1}}],["其中为激活函数",{"5":{"47":1}}],["其中为变换矩阵",{"5":{"47":1}}],["其中为的摩尔",{"5":{"47":1}}],["其中为样本总数",{"5":{"61":1}}],["其中每一层的变换和激活将前一层的表示转换为新的表示",{"5":{"71":1}}],["其中每一层的变换",{"5":{"71":1}}],["其中每个专家",{"5":{"159":2}}],["其中每个表示序列中的第个词元",{"5":{"140":1}}],["其中每个点在映射",{"5":{"136":1}}],["其中每个点在映射作用下循环出现",{"5":{"136":1}}],["其中每个是子词单元",{"5":{"101":1}}],["其中每个是来自词汇表的token",{"5":{"101":1}}],["其中每个层映射",{"5":{"46":1}}],["其中每个层映射​定义为",{"5":{"46":1}}],["其中每个",{"5":{"48":2,"101":2,"140":1}}],["其中每个元素",{"5":{"50":2}}],["其中每个词被映射为空间中的一个点",{"5":{"50":2}}],["其中每个节点",{"5":{"92":2,"153":2}}],["其中每个头独立地进行注意力计算",{"5":{"93":1}}],["其中每个头",{"5":{"93":1}}],["其中每个分量代表模型对第个词元的原始置信度评分",{"5":{"96":1}}],["其中每个分量",{"5":{"96":1}}],["其中假设",{"5":{"44":2}}],["其中​",{"5":{"44":1,"48":1,"96":1,"141":1,"143":1}}],["其中​是保留前个奇异值的截断svd近似",{"5":{"142":1}}],["其中​是概率分布",{"5":{"44":1}}],["其中​是第个频率成分在位置处的相位",{"5":{"90":1}}],["其中​表示位置的内容嵌入向量",{"5":{"91":1}}],["其中​表示位置的位置编码向量",{"5":{"91":1}}],["其中矩阵乘法的顺序不可交换",{"5":{"44":2,"45":2}}],["其中和",{"5":{"143":1}}],["其中和分别是该批次数据的均值和方差向量",{"5":{"148":1}}],["其中和分别是的最小和最大奇异值",{"5":{"41":1}}],["其中和分别是输入和输出的维度",{"5":{"41":1}}],["其中和均为可微函数",{"5":{"45":1}}],["其中和是正交矩阵",{"5":{"45":1,"143":1}}],["其中和是激活函数",{"5":{"48":1}}],["其中和是未编码位置的原始向量",{"5":{"88":1}}],["其中和的维度较低",{"5":{"87":1,"152":1}}],["其中来自矩阵乘法",{"5":{"45":1}}],["其中的列向量是的特征向量",{"5":{"45":1}}],["其中的计算结果与完全相同",{"5":{"46":1}}],["其中行索引对应输出神经元的索引",{"5":{"46":1}}],["其中行索引",{"5":{"46":1}}],["其中节点表示变量或操作",{"5":{"48":2}}],["其中且",{"5":{"96":1,"132":1}}],["其中第一项是模型对数据的描述长度",{"5":{"137":2}}],["其中第一项是重构损失",{"5":{"137":2}}],["其中第一项是注意力熵",{"5":{"132":2}}],["其中第三项是多头之间的协同信息",{"5":{"132":2}}],["其中第个元素为",{"5":{"96":1}}],["其中第",{"5":{"96":1}}],["其中表示任意相容的矩阵范数",{"5":{"148":1}}],["其中表示层归一化",{"5":{"146":1}}],["其中表示逐元素hadamard乘法",{"5":{"145":1}}],["其中表示逐元素乘法",{"5":{"44":1,"97":1}}],["其中表示在之前的所有词元",{"5":{"140":1}}],["其中表示函数的次复合",{"5":{"136":1}}],["其中表示时刻的系统状态",{"5":{"136":1}}],["其中表示除位置外所有token的序列",{"5":{"101":1}}],["其中表示位置是有效token",{"5":{"101":1}}],["其中表示完全正相关",{"5":{"96":1}}],["其中表示第个样本的权重",{"5":{"51":1}}],["其中表示模型对第",{"5":{"61":1}}],["其中表示分块对角矩阵",{"5":{"93":1}}],["其中条件均值为",{"5":{"96":2}}],["其中对数底数决定了信息的计量单位",{"5":{"61":1}}],["其中对数底数",{"5":{"61":1}}],["其中由真实标签决定",{"5":{"61":1}}],["其中核函数为",{"5":{"87":2,"152":2}}],["其中旋转角度",{"5":{"88":2}}],["其中旋转角度随位置非线性变化",{"5":{"88":2}}],["其中通常远小于",{"5":{"93":1}}],["其中query与所有key的相似度被用来计算加权平均",{"5":{"69":2}}],["其中query",{"5":{"94":2}}],["其中在填充位置为",{"5":{"95":1}}],["其",{"5":{"41":2,"135":6}}],["其复杂性源于归一化操作的非线性",{"5":{"41":2}}],["其导数为零",{"5":{"157":2}}],["其导数为",{"5":{"148":6}}],["其导数通常也接近零",{"5":{"41":2}}],["其导数在大多数区域都接近1",{"5":{"41":2}}],["其导数的值域为",{"5":{"148":2}}],["其导数的",{"5":{"42":2}}],["其激活值对输入的微小变化不敏感",{"5":{"42":2}}],["其根本原因在于激活函数引入的非线性变换",{"5":{"71":2}}],["其能表示的函数就被限制在这个有限维的函数空间中",{"5":{"71":2}}],["其参数不会被访问",{"5":{"159":2}}],["其参数量只有gopher的1",{"5":{"138":2}}],["其参数集合为",{"5":{"45":2}}],["其参数数量为",{"5":{"71":2}}],["其注意力层和前馈层堆叠形成深层结构",{"5":{"71":2}}],["其第",{"5":{"44":1,"89":1,"95":1}}],["其第行为样本的误差信号​",{"5":{"44":1}}],["其第行第列元素为",{"5":{"89":1}}],["其第个元素表示第个查询向量与第个键向量的点积相似度",{"5":{"95":1}}],["其数学原理和实际效果对于理解moe的训练动态至关重要",{"5":{"159":2}}],["其数学本质是通过减少每次前向传播中实际参与的参数数量来降低计算成本",{"5":{"159":2}}],["其数学意义体现在以下几个方面",{"5":{"156":2}}],["其数学表示可以清晰地揭示线性变换的本质局限性",{"5":{"71":2}}],["其数学定义为",{"5":{"47":2}}],["其数学定义看似简单",{"5":{"51":2}}],["其数学性质更容易分析",{"5":{"41":2}}],["其数学性质直接决定了注意力计算的行为和效率",{"5":{"87":2,"152":2}}],["其数学形式简洁优雅",{"5":{"159":2}}],["其数学形式包括范数裁剪",{"5":{"147":2}}],["其数学形式为",{"5":{"47":2,"88":2,"147":2,"159":2}}],["其数学形式是在注意力分数矩阵上应用一个掩码矩阵",{"5":{"95":2}}],["其雅可比矩阵的范数接近1",{"5":{"148":2}}],["其雅可比矩阵为",{"5":{"44":2}}],["其雅可比矩阵",{"5":{"45":2}}],["其计算图包含以下节点",{"5":{"45":2}}],["其后继节点之前被处理",{"5":{"45":2}}],["其所有依赖值已经就绪",{"5":{"45":2}}],["其连接强度决定了信号传递的效率",{"5":{"47":2}}],["其损失函数为交叉熵损失",{"5":{"47":2}}],["其长度",{"5":{"48":2}}],["其上方的图构成一个凸集",{"5":{"48":2}}],["其pmf记为",{"5":{"96":2}}],["其pdf为",{"5":{"96":2}}],["其他专家被激活的条件概率为",{"5":{"158":2}}],["其他专家几乎不被使用",{"5":{"158":2}}],["其他所有专家的路由概率都会相应降低",{"5":{"158":2}}],["其他所有token作为隐式负样本",{"5":{"101":2}}],["其他位置接近0",{"5":{"141":2}}],["其他键的概率接近0",{"5":{"69":2}}],["其他情况输出为0",{"5":{"47":2}}],["其他为0",{"5":{"96":2}}],["其他形式的损失函数可以转化为mse或与之等价",{"5":{"51":2}}],["其他高频成分仍然能够区分不同位置",{"5":{"90":2}}],["其协方差矩阵为",{"5":{"149":2}}],["其协方差矩阵是一个的对称半正定矩阵",{"5":{"96":1}}],["其协方差矩阵",{"5":{"96":1}}],["其维度为",{"5":{"61":2,"71":2}}],["其维度分别代表批量大小",{"5":{"50":2}}],["其列向量",{"5":{"50":2}}],["其mse为",{"5":{"51":2}}],["其自信息",{"5":{"61":2}}],["其真值表为",{"5":{"47":2}}],["其真实类别是唯一确定的",{"5":{"61":2}}],["其度量张量由fisher信息矩阵给出",{"5":{"61":2}}],["其定义可以延伸到概率空间",{"5":{"70":2}}],["其定义域为",{"5":{"42":4}}],["其定义为",{"5":{"61":2}}],["其非线性程度比二次函数更高",{"5":{"70":2}}],["其非零元素集中在主对角线附近的条对角线上",{"5":{"86":1,"151":1}}],["其非零元素集中在主对角线附近的",{"5":{"86":1,"151":1}}],["其核心创新在于引入了参数学习机制",{"5":{"47":2}}],["其核心思想是通过最小化路由概率的方差来强制各专家获得均衡负载",{"5":{"158":2}}],["其核心思想是通过分块计算",{"5":{"86":2,"151":2}}],["其核心思想是将专家权重分布到多个设备上",{"5":{"157":2}}],["其核心思想是保持前向传播和反向传播过程中激活值与梯度的方差一致",{"5":{"148":1}}],["其核心思想是控制梯度的整体范数不超过预设阈值",{"5":{"147":2}}],["其核心思想是",{"5":{"142":2,"148":1,"154":2,"157":2}}],["其核心思想是在应用梯度更新之前",{"5":{"147":2}}],["其核心思想是在训练过程中随机",{"5":{"145":2}}],["其核心思想是在保留关于目标变量",{"5":{"133":1}}],["其核心思想是在保留关于目标变量的最大信息的同时",{"5":{"133":1}}],["其核心思想是在计算当前块的同时预加载下一块的数据",{"5":{"86":2,"151":2}}],["其核心思想是让所有注意力头共享同一个key和value投影",{"5":{"86":2,"151":2}}],["其谱半径为1",{"5":{"41":2}}],["其谱范数至少为1",{"5":{"87":2,"152":2}}],["其谱性质因输入而异",{"5":{"87":2,"152":2}}],["其谱性质可以用toeplitz矩阵的理论分析",{"5":{"87":2,"152":2}}],["其左特征向量和右特征向量不同",{"5":{"87":2,"152":2}}],["其历史渊源可以追溯到正弦位置编码的原始论文",{"5":{"88":2}}],["其原因在于",{"5":{"88":2}}],["其原因可能在于",{"5":{"88":2}}],["其实际性能显著优于标准实现",{"5":{"86":2,"151":2}}],["其实现与llama类似",{"5":{"88":2}}],["其理论依据是",{"5":{"88":2}}],["其性质可以被严格证明",{"5":{"88":2}}],["其性质取决于注意力机制的实现",{"5":{"89":2}}],["其秩为的最佳近似",{"5":{"89":1}}],["其秩为",{"5":{"89":1,"143":2}}],["其dft定义为",{"5":{"90":2}}],["其模为1",{"5":{"90":2}}],["其辐角为",{"5":{"90":2}}],["其傅里叶变换为",{"5":{"90":2}}],["其频率为​",{"5":{"90":1}}],["其频率为",{"5":{"90":1}}],["其角速度为​",{"5":{"90":1}}],["其角速度为",{"5":{"90":1}}],["其值大于等于任一变量单独的不确定性",{"5":{"137":2}}],["其值与样本的",{"5":{"70":2,"97":2}}],["其值保持不变",{"5":{"91":2}}],["其隐藏状态的更新公式为",{"5":{"92":2,"153":2}}],["其信息熵定义为",{"5":{"139":2}}],["其信息量应当越大",{"5":{"61":2}}],["其信息也可能被其他位置",{"5":{"92":2,"153":2}}],["其对角元素为非负实数",{"5":{"45":2}}],["其对角元素为",{"5":{"44":2}}],["其对角线元素为奇异值",{"5":{"94":2}}],["其设计直接决定了模型的稀疏激活模式和训练效率",{"5":{"159":2}}],["其设计直接影响模型的整体表达能力和计算效率",{"5":{"158":2}}],["其设计目标是实现最优的信息压缩",{"5":{"133":2}}],["其设计目标是保持每层激活值和梯度的方差在各层之间稳定",{"5":{"94":2}}],["其设计天然契合概率分布间的差异度量需求",{"5":{"61":2}}],["其设计源于对点积运算统计学特性的深入分析",{"5":{"95":2}}],["其方差远小于蒙特卡洛回报的方差",{"5":{"155":2}}],["其方差为​",{"5":{"94":1}}],["其方差为",{"5":{"94":1,"95":4}}],["其方向决定了模型在评估",{"5":{"94":2}}],["其输出为零",{"5":{"148":2}}],["其输出可以写为",{"5":{"93":2}}],["其输出接近确定性分布",{"5":{"95":2}}],["其每一行是一个​维的查询向量",{"5":{"95":1}}],["其每一行是一个维的键向量",{"5":{"95":1}}],["其每一行是一个维的值向量",{"5":{"95":1}}],["其每一行是一个",{"5":{"95":3}}],["其每一行都是一个有效的概率分布",{"5":{"95":2}}],["其标准差会以的速度增长",{"5":{"95":1}}],["其标准差会以",{"5":{"95":1}}],["其绝对数值本身并不具有直接的概率意义",{"5":{"96":2}}],["sft",{"5":{"154":4}}],["snapshot",{"5":{"145":2}}],["sne和umap",{"5":{"50":2}}],["sampling",{"5":{"154":4}}],["sample",{"5":{"51":2,"146":4}}],["sacker",{"5":{"136":4}}],["saddle",{"5":{"134":5,"136":2}}],["simple",{"5":{"145":2}}],["simplex",{"5":{"61":2,"70":2}}],["sims",{"5":{"137":4}}],["sim",{"5":{"137":6}}],["size=",{"2":{"154":1,"155":1,"156":2}}],["size",{"5":{"41":2,"50":2,"96":2,"132":12,"146":2}}],["size可能为1",{"5":{"41":2}}],["sighted",{"5":{"156":2}}],["sign",{"5":{"144":4}}],["signal",{"5":{"44":2}}],["sigma^2",{"5":{"146":8}}],["sigmaj^2",{"5":{"142":1}}],["sigma",{"5":{"42":14,"46":4,"86":6,"141":12,"142":9,"143":2,"151":6}}],["sigmoid和tanh的导数小于1",{"5":{"148":2}}],["sigmoid和softmax激活函数的输出可以解释为分类概率",{"5":{"47":2}}],["sigmoid",{"5":{"41":1,"48":2,"61":2,"70":12,"132":2,"133":8,"148":2}}],["sigmoid函数定义为",{"5":{"148":2}}],["sigmoid函数",{"2":{"148":1},"5":{"41":1,"42":5,"47":2,"71":1,"148":1}}],["sigmoid函数的值域为",{"5":{"148":2}}],["sigmoid函数的饱和区域分析",{"2":{"41":1},"5":{"41":1}}],["sigmoid函数的梯度饱和区域定义为",{"5":{"41":2}}],["sigmoid函数的数学定义",{"2":{"42":1},"5":{"42":1}}],["sigmoid函数的反函数为对数几率函数",{"5":{"42":2}}],["sigmoid函数的导数与性质",{"2":{"42":1},"5":{"42":1}}],["sigmoid函数的导数具有一个极为优美的性质",{"5":{"42":2}}],["sigmoid函数的导数为",{"5":{"42":1}}],["sigmoid函数的导数取值范围是",{"5":{"42":2}}],["sigmoid函数的分母是分子的积分",{"5":{"42":2}}],["sigmoid函数的高阶导数可以递归地通过低阶导数表示",{"5":{"42":2}}],["sigmoid函数的二阶导数为",{"5":{"42":2}}],["sigmoid函数的逆函数正是对数几率的变换",{"5":{"71":2}}],["sigmoid函数的输出范围为",{"5":{"47":2}}],["sigmoid函数是分析梯度饱和的经典案例",{"5":{"41":1}}],["sigmoid函数是神经网络中最经典的激活函数之一",{"5":{"42":2}}],["sigmoid函数是单调递增函数",{"5":{"42":2}}],["sigmoid函数与伯努利分布",{"2":{"71":1},"5":{"71":1}}],["sigmoid函数将整个实数轴压缩到区间",{"5":{"42":1}}],["sigmoid函数将整个实数轴压缩到",{"5":{"42":1}}],["sigmoid函数将实数域映射到区间",{"5":{"71":1}}],["sigmoid函数将线性组合转换为概率",{"5":{"71":1}}],["sigmoid函数将线性组合",{"5":{"71":1}}],["sigmoid函数恰好可以将神经元的净输入映射为概率值",{"5":{"47":2}}],["sigmoid函数可以将任意实数解释为伯努利分布的成功概率",{"5":{"71":1}}],["sigmoid函数可以将任意实数",{"5":{"71":1}}],["sigmoid函数可以拟合任意单调递增的概率函数",{"5":{"47":2}}],["sigmoid曲线在其两端进入平坦区域",{"5":{"41":2}}],["sigmoid的导数",{"5":{"70":1}}],["sigmoid的导数是",{"5":{"70":2}}],["sigmoid的导数变得很小",{"5":{"70":1}}],["sigmoid的饱和区域",{"5":{"41":2}}],["sigmoid的二阶导数",{"5":{"42":2}}],["sigmoid以0",{"5":{"42":2}}],["sigmoid导数的自化形式",{"5":{"42":2}}],["sigmoid导数的这种",{"5":{"42":2}}],["sigmoid导数",{"5":{"42":2}}],["sigmoid输出区间",{"5":{"42":1}}],["sigmoid输出",{"5":{"42":1}}],["sigmoid输出常用于二分类任务的概率预测",{"5":{"71":2}}],["sigmoid变换可以理解为将先验信息",{"5":{"71":2}}],["sigmoid激活",{"5":{"44":2}}],["sigmoid神经元与伯努利分布的对应",{"5":{"47":2}}],["sigmoid神经元的输出可以解释为给定输入时",{"5":{"47":1}}],["sigmoid神经元",{"5":{"47":1}}],["sink",{"5":{"134":5}}],["sinkhorn稀疏化是一种基于最优传输的稀疏化方法",{"5":{"86":2,"151":2}}],["sinkhorn迭代定义为",{"5":{"86":2,"151":2}}],["single",{"5":{"46":2}}],["singular",{"5":{"50":2,"94":2,"143":2}}],["sinusoidal",{"5":{"88":2,"91":2,"92":2,"153":2}}],["s形",{"5":{"41":2,"71":2}}],["sqrt",{"5":{"42":4,"46":4,"69":2,"97":2,"132":2,"141":8,"142":2}}],["square",{"5":{"41":2,"146":2}}],["squared",{"5":{"51":2}}],["swish函数通过可学习的参数提供了自适应的激活特性",{"5":{"41":1}}],["swish函数",{"5":{"41":1}}],["source",{"5":{"134":1}}],["sorted",{"5":{"132":4}}],["sort",{"5":{"132":4,"133":2}}],["soma",{"5":{"47":2}}],["softmax收敛于均匀分布",{"5":{"159":2}}],["softmax收敛于one",{"5":{"159":2}}],["softmax来近似top",{"5":{"159":2}}],["softmax门控的数学定义为",{"5":{"159":2}}],["softmax门控函数是完全可微的",{"5":{"159":2}}],["softmax门控函数是最基础也是最广泛使用的门控设计",{"5":{"159":2}}],["softmax门控函数",{"2":{"159":1},"5":{"159":1}}],["softmax策略",{"5":{"101":2}}],["softmax对所有位置加权求和",{"5":{"101":2}}],["softmax结构的统一性",{"2":{"70":1},"5":{"70":1}}],["softmax输入",{"5":{"69":1}}],["softmax输出",{"5":{"69":1,"94":1}}],["softmax输出总是有效的概率分布",{"5":{"69":2}}],["softmax输出为​",{"5":{"96":1}}],["softmax输出为",{"5":{"96":1}}],["softmax输出的概率表示样本属于第类的概率估计",{"5":{"71":1}}],["softmax输出的概率",{"5":{"71":1}}],["softmax输出的每个元素都是正的",{"5":{"92":2,"153":2}}],["softmax输出保持在合理的分布范围内",{"5":{"94":1}}],["softmax在transformer中的广泛使用",{"5":{"69":2}}],["softmax在三大场景中的统一性",{"2":{"69":1},"5":{"69":1}}],["softmax同样用于将相似度分数转换为概率分布",{"5":{"69":2}}],["softmax将这些相似度转换为概率分布",{"5":{"69":2}}],["softmax将query",{"5":{"69":2}}],["softmax将网络的logits输出转换为类别概率分布",{"5":{"69":2}}],["softmax将每一行归一化为概率分布",{"5":{"71":2}}],["softmax将不可避免地进入饱和区域",{"5":{"95":2}}],["softmax是多项logit模型",{"5":{"69":2}}],["softmax是gumbel分布和softmax分布的结合",{"5":{"71":2}}],["softmax是非线性变换",{"5":{"87":4,"152":4}}],["softmax不是简单的线性投影",{"5":{"69":2}}],["softmax操作",{"5":{"69":2}}],["softmax操作的几何与概率本质",{"2":{"69":1},"5":{"69":1}}],["softmax操作可以在每个线程束内高效地归约求和",{"5":{"93":2}}],["softmax操作可以在行级别并行执行",{"5":{"95":2}}],["softmax",{"2":{"61":1,"69":1,"132":1},"5":{"42":4,"48":2,"61":67,"69":9,"70":28,"96":2,"97":8,"132":49,"141":12,"157":2,"159":4}}],["softmax函数确保输出向量满足概率分布的约束",{"5":{"140":2}}],["softmax函数与多项式分布",{"2":{"71":1},"5":{"71":1}}],["softmax函数与概率单纯形",{"5":{"71":2}}],["softmax函数定义为",{"5":{"69":2,"71":2}}],["softmax函数是sigmoid在多类别情况下的推广",{"5":{"71":2}}],["softmax函数是大语言模型",{"5":{"47":2}}],["softmax函数将任意实数向量",{"5":{"69":1}}],["softmax函数将任意实数向量映射到概率单纯形上",{"5":{"69":1}}],["softmax函数将个神经元的输出归一化为概率分布",{"5":{"47":1}}],["softmax函数将",{"5":{"47":1,"96":1}}],["softmax函数将模型的原始输出",{"5":{"96":2}}],["softmax函数将相似度转换为概率分布",{"5":{"95":2}}],["softmax函数将空间中的向量映射到概率单纯形上",{"5":{"96":1}}],["softmax函数",{"5":{"61":2,"69":2,"96":1,"140":1}}],["softmax函数的逐分量形式为",{"5":{"159":2}}],["softmax函数的核心特性是它将任意实数向量转换为有效的概率分布",{"5":{"158":2,"159":2}}],["softmax函数的饱和行为分析",{"2":{"95":1},"5":{"95":1}}],["softmax函数的定义为",{"5":{"95":2}}],["softmax函数的输入都能保持在一个合理的范围内",{"5":{"95":2}}],["softmax函数的梯度可以表示为雅可比矩阵的形式",{"5":{"95":2}}],["softmax函数的分母对所有分子的指数项进行归一化",{"5":{"96":2}}],["softmax函数具有以下重要性质",{"5":{"47":2,"140":1}}],["softmax函数具有一个重要特性",{"5":{"95":2}}],["softmax函数具有",{"5":{"96":2}}],["softmax函数在注意力计算中扮演着至关重要的角色",{"5":{"95":2}}],["softmax的归一化也意味着每个专家的路由概率不仅取决于自己的得分",{"5":{"158":2}}],["softmax的",{"5":{"69":2}}],["softmax的极端梯度",{"5":{"41":2}}],["softmax的输出满足概率分布的所有公理",{"5":{"71":1}}],["softmax的输出",{"5":{"71":1}}],["softmax的输出分布不会过于",{"5":{"95":2}}],["softmax的输出将趋向于one",{"5":{"95":2}}],["softmax的输出将满足对于所有",{"5":{"95":1}}],["softmax的输出将满足",{"5":{"95":1}}],["softmax的数学性质由温度参数控制",{"5":{"71":1}}],["softmax的数学性质由温度参数",{"5":{"71":1}}],["softmax的数学性质",{"5":{"47":2,"140":2}}],["softmax的非线性确保了注意力权重具有归一化的概率解释",{"5":{"71":2}}],["softmax的非线性变换可能",{"5":{"87":2,"152":2}}],["softmax的非线性变换使得注意力权重具有一定的",{"5":{"92":2,"153":2}}],["softmax的梯度可以通过重参数化技巧精确计算",{"5":{"157":2}}],["softmax的梯度为",{"5":{"89":2}}],["softmax的梯度形式已在7",{"5":{"94":2}}],["softmax的梯度流分析",{"2":{"95":1},"5":{"95":1}}],["softmax的雅可比矩阵在对角线上的元素接近1",{"5":{"95":2}}],["softmax及其梯度特性与交叉熵损失的配合至关重要",{"5":{"41":2}}],["softmax分布为",{"5":{"159":2}}],["softmax分布",{"5":{"71":2}}],["softmax样本定义为",{"5":{"71":2}}],["softmax趋向于",{"5":{"69":2}}],["softmax趋向于one",{"5":{"71":2}}],["softmax趋向于均匀分布",{"5":{"69":4,"71":2}}],["softmax提供了一种可微的采样近似",{"5":{"71":2}}],["softmax提供token间的交互非线性",{"5":{"71":2}}],["softmax等激活函数与概率分布的内在联系",{"5":{"71":2}}],["softmax激活",{"5":{"44":2}}],["softmax神经元与多项分布",{"5":{"47":2}}],["softmax数值不稳定的根源",{"2":{"61":1},"5":{"61":1}}],["softmax与交叉熵的组合源于信息论的基本原理",{"5":{"69":2}}],["softmax与交叉熵的梯度计算",{"2":{"61":1},"5":{"61":1}}],["softmax与离散分布采样",{"2":{"71":1},"5":{"71":1}}],["softmax注意力可以表示负定的交互模式",{"5":{"86":2,"151":2}}],["softmax变换正是这种对数几率的指数化和归一化",{"5":{"71":2}}],["softmax变换对秩的影响",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["softmax变换是逐行进行的",{"5":{"87":2,"152":2}}],["softmax变换可以视为核矩阵的",{"5":{"87":2,"152":2}}],["softmax和交叉熵的组合在数学上等价于最大似然估计",{"5":{"47":2}}],["softmax和交叉熵通常会被融合计算",{"5":{"61":1}}],["softmax和交叉熵通常会被",{"5":{"61":1}}],["softmax和输出投影则通常使用全精度以保持数值稳定性",{"5":{"93":2}}],["softmax可能过于",{"5":{"69":2}}],["softmax可以写为",{"5":{"95":2}}],["softmax归一化确保了路由概率满足概率分布的基本性质",{"5":{"158":2}}],["softmax归一化",{"5":{"95":2}}],["soft",{"5":{"92":2,"144":2,"153":2,"159":2}}],["seq",{"5":{"132":12}}],["sequential",{"5":{"132":2}}],["sequence",{"5":{"50":2,"95":4}}],["setminus",{"5":{"69":2}}],["separable",{"5":{"47":2}}],["self",{"5":{"21":10,"42":2,"44":1,"45":2,"47":2,"48":3,"50":2,"61":3,"88":2,"96":3,"131":51,"132":44,"141":2,"143":1,"146":2,"148":2}}],["selection",{"5":{"92":2,"153":2}}],["script><",{"2":{"154":1,"155":1,"156":2}}],["script",{"2":{"154":1,"155":1,"156":2}}],["score函数提供了有效的梯度信息",{"5":{"139":2}}],["score函数与似然梯度",{"5":{"139":2}}],["scores",{"5":{"132":8}}],["score",{"5":{"132":6,"133":8}}],["scale",{"2":{"150":1},"5":{"146":4,"150":3}}],["scaled",{"0":{"95":1},"4":{"95":1},"5":{"61":4,"71":2,"88":2,"95":9,"131":5,"132":2}}],["scaling",{"5":{"44":2,"138":4,"146":2,"157":2,"159":2}}],["schwarz",{"5":{"135":2}}],["schwarz不等式",{"5":{"87":2,"152":2}}],["scheduling",{"5":{"48":2}}],["schmidt正交化过程是构建正交基的标准算法",{"5":{"50":2}}],["schmidt正交化可以移除表示中的冗余成分",{"5":{"50":2}}],["schmidt正交化",{"5":{"50":2}}],["ste相当于用",{"5":{"157":2}}],["ste的核心思想是",{"5":{"157":2}}],["ste",{"5":{"157":2}}],["step",{"5":{"48":2,"97":2}}],["style=",{"2":{"154":1,"155":1,"156":2}}],["stopping",{"5":{"144":2}}],["store",{"5":{"132":4}}],["stone",{"5":{"71":2}}],["stochastic",{"5":{"44":2,"87":2,"152":2}}],["structure",{"5":{"87":2,"152":2}}],["state",{"5":{"156":8}}],["stationary",{"5":{"87":2,"152":2}}],["stable",{"5":{"134":6}}],["stack",{"5":{"132":2}}],["standard",{"5":{"96":2,"145":2}}],["sgd自然地偏好平坦极小值",{"5":{"149":2}}],["sgd在每一步仅使用一小批数据",{"5":{"149":2}}],["sgd中的噪声模型",{"2":{"149":1},"5":{"149":1}}],["sgd的极限行为可以用扩散过程来近似",{"5":{"149":2}}],["sgd的核心思想是随机选取一小批样本",{"5":{"149":2}}],["sgd的隐式正则化效应倾向于将优化过程推向这些低秩平坦区域",{"5":{"142":2}}],["sgd的随机性本身就提供了一种隐式的噪声源",{"5":{"48":2}}],["sgd倾向于收敛到的平坦极小值通常对应于低秩的权重配置",{"5":{"142":2}}],["sgd倾向于收敛到平坦的局部最小值",{"5":{"48":2}}],["sgd",{"5":{"97":2,"133":8,"134":20,"135":2,"136":2,"147":12,"149":2}}],["skewness",{"5":{"96":2}}],["s",{"2":{"154":1,"155":2,"156":2},"5":{"96":2,"132":2,"133":4,"137":6,"155":1}}],["specialization",{"5":{"70":2}}],["spectral",{"5":{"50":8,"87":4,"144":2,"148":2,"152":4}}],["spatial",{"5":{"145":2}}],["sparsity",{"5":{"132":2,"159":2}}],["sparse",{"5":{"86":6,"87":2,"93":8,"132":4,"151":6,"152":2}}],["span>",{"2":{"154":1,"155":1,"156":3}}],["span掩码策略连续掩码一个token片段",{"5":{"101":2}}],["span掩码",{"5":{"101":2}}],["span",{"5":{"92":2,"101":2,"153":2}}],["space=",{"2":{"155":1,"156":1}}],["space",{"5":{"50":4,"90":2,"134":2,"156":4}}],["spline",{"5":{"45":2}}],["synapse",{"5":{"47":2}}],["system",{"5":{"50":2}}],["svd不仅是理解高阶张量分解的基础",{"5":{"143":2}}],["svd正是这种几何变换的参数化表示",{"5":{"143":2}}],["svd将线性变换分解为",{"5":{"143":2}}],["svd具有优美的几何解释",{"5":{"143":2}}],["svd的核心应用之一是低秩近似",{"5":{"143":2}}],["svd的几何解释",{"2":{"143":1},"5":{"143":1}}],["svd的数学定义",{"2":{"143":1},"5":{"143":1}}],["svd",{"0":{"143":1},"4":{"143":1},"5":{"50":2,"94":2,"131":5,"143":8,"148":2}}],["surely",{"5":{"136":2}}],["surrogate",{"5":{"70":2}}],["subgradient",{"5":{"147":2}}],["subwords",{"5":{"140":2}}],["sublinear",{"5":{"136":2}}],["subtraction问题",{"2":{"61":1},"5":{"61":1}}],["supervised",{"5":{"154":2}}],["superlinear",{"5":{"136":2}}],["super",{"5":{"132":2}}],["sum",{"5":{"42":38,"46":22,"50":2,"61":22,"97":8,"132":4,"139":4,"141":16,"142":8,"143":10,"144":2}}],["sumj",{"5":{"97":2}}],["sharding",{"5":{"157":2}}],["sharp",{"5":{"136":2}}],["shared",{"5":{"132":6}}],["shape",{"5":{"132":4}}],["shannon",{"5":{"61":4,"137":12}}],["shift",{"5":{"41":2,"146":4,"154":2}}],["sliding",{"5":{"86":2,"151":2}}],["src=",{"5":{"44":1,"45":2,"47":2,"48":3,"50":2,"61":1,"96":3,"143":1,"148":2}}],["sram的容量",{"5":{"86":2,"151":2}}],["smoothing",{"5":{"96":2,"137":2,"144":2,"146":2}}],["sm",{"5":{"93":2}}],["和数据量",{"5":{"159":2}}],["和数学家沃尔特",{"5":{"47":2}}],["和计算量",{"5":{"159":2}}],["和前馈网络的矩阵乘法",{"5":{"159":2}}],["和负载均衡辅助损失",{"5":{"158":2}}],["和路由概率",{"5":{"158":2}}],["和专家参数",{"5":{"158":2}}],["和专门的推理引擎",{"5":{"93":2}}],["和所有策略",{"5":{"156":2}}],["和方差",{"5":{"155":2}}],["和方差向量",{"5":{"146":3}}],["和critic网络参数",{"5":{"155":2}}],["和奖励函数",{"5":{"155":2}}],["和回复",{"5":{"154":2}}],["和策略优化",{"5":{"154":2}}],["和pre",{"5":{"150":2}}],["和某个",{"5":{"148":1}}],["和偏移",{"5":{"148":2}}],["和复合而成的函数",{"5":{"146":1}}],["和复杂度",{"5":{"133":1}}],["和transformer等变长序列模型中存在困难",{"5":{"146":2}}],["和transformer中",{"5":{"50":2}}],["和替代批量统计量",{"5":{"146":1}}],["和无放缩版本",{"5":{"145":1}}],["和自然语言处理模型",{"5":{"142":2}}],["和误差参数",{"5":{"141":1}}],["和键矩阵",{"5":{"141":1}}],["和键向量",{"5":{"95":1,"141":2}}],["和特殊符号",{"5":{"140":2}}],["和模型预测分布",{"5":{"137":1}}],["和模型分布",{"5":{"137":1,"139":2}}],["和模型维度",{"5":{"88":1}}],["和联合熵的非负性得",{"5":{"137":1}}],["和均匀分布",{"5":{"137":1}}],["和之间共享的信息",{"5":{"137":1}}],["和不同数据量下的性能",{"5":{"138":2}}],["和不稳定的鞍点",{"5":{"136":1}}],["和不可减少误差",{"5":{"51":2}}],["和正则化系数",{"5":{"147":2}}],["和正整数",{"5":{"136":2}}],["和正定矩阵",{"5":{"135":2}}],["和正交性",{"5":{"88":2}}],["和编码变量",{"5":{"133":1}}],["和key",{"5":{"69":1}}],["和key向量",{"5":{"88":2}}],["和个负样本键",{"5":{"69":1}}],["和反向模式",{"5":{"45":2}}],["和平移变换",{"5":{"45":2}}],["和任意",{"5":{"48":1,"70":1,"71":1,"97":1}}],["和任意标量",{"5":{"71":1}}],["和符号微分",{"5":{"45":2,"48":2}}],["和",{"5":{"41":7,"42":1,"44":2,"45":4,"46":1,"47":2,"48":6,"50":12,"51":3,"61":8,"69":11,"70":6,"71":9,"86":3,"87":8,"88":12,"89":3,"90":5,"91":16,"92":14,"93":8,"94":2,"95":4,"96":10,"97":5,"101":4,"132":2,"133":6,"134":2,"135":10,"137":12,"138":8,"139":5,"142":1,"143":6,"145":3,"146":9,"148":2,"150":8,"151":3,"152":8,"153":14,"154":4,"155":4,"156":2,"157":4,"158":16,"159":12}}],["和传递",{"5":{"41":2}}],["和​",{"5":{"41":1,"69":1,"86":1,"146":1,"151":1}}],["和​分别是行归一化和列归一化操作",{"5":{"86":1,"151":1}}],["和输出梯度",{"5":{"41":1}}],["和表示的形式",{"5":{"42":1}}],["和奇数维度",{"5":{"71":2}}],["和激活",{"5":{"71":1}}],["和其他深度学习高级主题奠定了坚实的理论基础",{"5":{"45":2}}],["和其上下文",{"5":{"71":1}}],["和罗纳德",{"5":{"44":2}}],["和突触",{"5":{"47":2}}],["和逻辑非",{"5":{"47":2}}],["和倾向于同向变化",{"5":{"96":1}}],["和倾向于反向变化",{"5":{"96":1}}],["和线性无关",{"5":{"96":1}}],["和中位数都等于均值",{"5":{"96":2}}],["和向量",{"5":{"96":2}}],["和向量4",{"5":{"88":1}}],["和交叉熵",{"5":{"70":2,"96":2}}],["和交叉熵在分类任务中的应用",{"5":{"61":2}}],["和渐近有效的",{"5":{"96":2}}],["和备择假设",{"5":{"96":2}}],["和相关系数",{"5":{"96":2}}],["和归一化",{"5":{"150":2}}],["和归一化操作",{"5":{"48":2}}],["和归一性",{"5":{"96":1}}],["和随机变量",{"5":{"96":1}}],["和分配律",{"5":{"50":2}}],["和分层张量分解",{"5":{"50":2}}],["和分组查询注意力",{"5":{"93":2}}],["和分别是该层的输入和输出维度",{"5":{"148":1}}],["和分别是教师和学生模型的软化概率分布",{"5":{"137":1}}],["和分别是键和值的维度",{"5":{"132":1}}],["和分别为第层的权重矩阵和偏置向量",{"5":{"46":1}}],["和分别表示键空间和值空间的维度",{"5":{"95":1}}],["和矩阵",{"5":{"50":2}}],["和张量收缩",{"5":{"50":2}}],["和qr算法来计算特征值和特征向量",{"5":{"50":2}}],["和防止对抗攻击都很重要",{"5":{"50":2}}],["和连接节点的边",{"5":{"50":2}}],["和标量",{"5":{"50":1,"140":1}}],["和上三角矩阵",{"5":{"50":2}}],["和对角矩阵",{"5":{"50":2}}],["和对称性得证",{"5":{"137":1}}],["和对称性",{"5":{"51":2}}],["和是可学习的缩放和偏移参数",{"5":{"41":1}}],["和是可学习的逐元素缩放和偏移参数",{"5":{"41":1}}],["和是垂直的",{"5":{"51":1}}],["和是原始的query和key向量",{"5":{"88":1}}],["和是向量的第和个分量",{"5":{"92":1,"153":1}}],["和是正交矩阵",{"5":{"94":1}}],["和预测分布",{"5":{"71":1}}],["和预测向量",{"5":{"51":1}}],["和重计算",{"5":{"86":2,"151":2}}],["和低秩近似",{"5":{"142":1}}],["和低秩近似​",{"5":{"142":1}}],["和低秩矩阵恢复问题",{"5":{"50":2}}],["和低秩结构",{"5":{"87":2,"152":2}}],["和0",{"5":{"87":2,"152":2}}],["和核方法中的核心概念",{"5":{"87":2,"152":2}}],["和保持完整",{"5":{"88":1}}],["和维度索引",{"5":{"88":1}}],["和嵌入维度",{"5":{"90":1}}],["和虚部序列",{"5":{"90":1}}],["和余弦序列",{"5":{"90":1}}],["和考虑位置的编码向量",{"5":{"91":1}}],["和位置",{"5":{"91":2,"92":4,"93":1,"153":4}}],["和内容",{"5":{"92":1,"153":1}}],["和转置操作",{"5":{"93":2}}],["和头维度",{"5":{"93":1}}],["和value",{"5":{"94":2}}],["和的偏导数等于偏导数的和",{"5":{"48":2}}],["和的第个分量的乘积​的期望为",{"5":{"95":1}}],["和注意力维度",{"5":{"50":1}}],["和注意力权重矩阵",{"5":{"95":1}}],["和第八章",{"5":{"101":2}}],["ics",{"5":{"146":2}}],["i|",{"5":{"144":1}}],["i|f",{"5":{"144":1}}],["ip",{"5":{"143":2}}],["ir",{"5":{"143":2}}],["irrelevant",{"5":{"69":2}}],["irreducible",{"5":{"51":4}}],["i1",{"5":{"143":1}}],["i=r+1",{"5":{"142":2}}],["i=1",{"5":{"42":4,"46":8,"61":10,"141":8,"142":2,"144":2}}],["i^",{"5":{"141":4}}],["i^2",{"5":{"42":2,"141":8,"142":6}}],["iter",{"5":{"132":6}}],["iteration",{"5":{"50":4,"133":2,"137":2}}],["i",{"2":{"154":3,"155":4,"156":8},"5":{"132":34,"133":36,"137":4,"143":2}}],["iia",{"5":{"69":2}}],["ijk",{"5":{"143":4}}],["ij",{"5":{"42":4,"46":2}}],["inline",{"2":{"154":1,"155":1,"156":3}}],["inference",{"5":{"145":2}}],["infomax",{"5":{"133":4,"137":4}}],["information",{"5":{"61":2,"69":4,"70":2,"71":2,"87":2,"96":2,"132":8,"133":2,"137":2,"149":2,"152":2}}],["infonce可能无法有效区分真正相似的样本和表面相似的样本",{"5":{"101":2}}],["infonce可以被解释为一个多分类问题的交叉熵损失",{"5":{"69":2}}],["infonce通过正负样本对比来最大化互信息的下界",{"5":{"69":2}}],["infonce最小化这个分布与",{"5":{"69":2}}],["infonce最小化的正是这个预测分布与",{"5":{"69":2}}],["infonce是互信息的下界",{"5":{"69":2}}],["infonce作为交叉熵的一种特殊形式",{"5":{"69":2}}],["infonce与交叉熵的等价性是本节核心论点的基础",{"5":{"69":2}}],["infonce与注意力的softmax统一<",{"5":{"131":1}}],["infonce与注意力的softmax统一",{"0":{"69":1},"4":{"69":1},"5":{"61":4,"69":1,"70":2,"131":4}}],["infonce将nce从二分类扩展到多分类",{"5":{"69":2}}],["infonce损失为",{"5":{"69":4}}],["infonce损失与互信息满足以下关系",{"5":{"69":2}}],["infonce损失定义为",{"5":{"69":2}}],["infonce损失",{"5":{"69":2}}],["infonce损失函数的数学基础",{"2":{"69":1},"5":{"69":1}}],["infonce损失提供了互信息的下界估计",{"5":{"71":2}}],["infonce的目标是最大化互信息的下界",{"5":{"101":2}}],["infonce的目标是从所有其他位置中选择出与",{"5":{"69":2}}],["infonce的一个重要理论性质是它与互信息",{"5":{"69":2}}],["infonce的梯度为",{"5":{"101":2}}],["infonce的梯度同样具有这种简洁的形式",{"5":{"69":2}}],["infonce的梯度会",{"5":{"69":2}}],["infonce的梯度结构与优化动力学",{"2":{"69":1},"5":{"69":1}}],["infonce的互信息下界性质",{"2":{"69":1},"5":{"69":1}}],["infonce的数学定义与推导",{"2":{"69":1},"5":{"69":1}}],["infonce",{"5":{"61":4,"69":4,"70":8,"97":5,"133":2,"137":8}}],["info",{"5":{"61":4,"132":2}}],["in=1",{"5":{"143":1}}],["init",{"5":{"132":4}}],["initialize",{"5":{"132":2}}],["input",{"5":{"47":2}}],["intrinsic",{"5":{"149":2}}],["intra",{"5":{"149":2}}],["int8",{"5":{"50":2}}],["internal",{"5":{"21":10,"41":2,"131":51,"146":4}}],["interpolation",{"5":{"88":2}}],["interaction",{"5":{"93":2}}],["invariance",{"5":{"146":2}}],["invariant",{"5":{"91":2,"92":2,"153":2}}],["inverted",{"5":{"145":4}}],["inverse",{"5":{"50":2}}],["independence",{"5":{"69":2}}],["index",{"4":{"131":1}}],["inductive",{"5":{"92":2,"153":2}}],["ik",{"5":{"42":2}}],["ideas",{"5":{"47":2}}],["ieee",{"5":{"61":2}}],["ib",{"5":{"87":2,"133":2,"152":2}}],["importance",{"2":{"158":1},"5":{"154":4,"158":15}}],["implicit",{"5":{"88":2}}],["immanent",{"5":{"47":2}}],["is",{"5":{"91":2,"146":2}}],["agent",{"5":{"156":4}}],["aggregate",{"5":{"95":2}}],["axis",{"5":{"146":2}}],["axon",{"5":{"47":2}}],["average",{"5":{"146":2}}],["actor",{"2":{"155":1},"5":{"155":10}}],["active",{"5":{"148":4}}],["activity",{"5":{"47":2}}],["activation",{"5":{"47":2,"93":2,"148":2}}],["action",{"5":{"88":2,"156":6}}],["accelerating",{"5":{"146":2}}],["abilities",{"5":{"138":2}}],["absolute",{"5":{"51":2,"88":4,"91":2}}],["asymptotically",{"5":{"134":2}}],["attn",{"5":{"132":8}}],["attention和前馈网络",{"5":{"146":2}}],["attention",{"0":{"95":1},"4":{"95":1},"5":{"46":2,"61":10,"69":2,"70":2,"86":34,"87":6,"88":8,"91":2,"93":10,"95":5,"131":5,"132":12,"141":4,"146":2,"151":34,"152":6}}],["attention定义为",{"5":{"71":2}}],["attention中",{"5":{"61":2,"69":4}}],["attention的定义",{"5":{"86":2,"151":2}}],["attention的兼容性",{"5":{"88":2}}],["attention的高效实现",{"5":{"88":2}}],["attention的框架下",{"5":{"93":2}}],["attention的数学公式",{"5":{"94":2}}],["attention的数学定义如公式",{"5":{"95":2}}],["attention的数学定义与query",{"5":{"93":2}}],["attention的数学定义与推导",{"5":{"95":2}}],["attention的每一个组成部分",{"5":{"95":2}}],["attention的关键创新之一",{"5":{"95":2}}],["attention的标准形式",{"5":{"95":2}}],["attention的完整计算流程可以用三个矩阵运算阶段来描述",{"5":{"95":2}}],["attention等算法通过精心设计的数据分块策略",{"5":{"92":2,"153":2}}],["attention计算过程中的核心中间结果",{"5":{"87":2,"152":2}}],["attention计算",{"5":{"93":2}}],["attention算法通过分块计算和io感知优化",{"5":{"93":2}}],["attention以其简洁的数学形式",{"5":{"95":2}}],["append",{"5":{"132":2,"136":4}}],["approximation",{"5":{"145":2}}],["approximate",{"5":{"48":2,"135":2}}],["approx",{"5":{"42":2,"46":8,"141":6,"143":12,"144":2}}],["arg",{"5":{"142":2}}],["args",{"5":{"132":2}}],["argmin",{"5":{"132":2}}],["argument",{"5":{"88":2}}],["autoregressive",{"5":{"140":2}}],["automatic",{"5":{"44":2,"45":2,"48":2}}],["auc",{"5":{"70":2}}],["advantage",{"5":{"101":2,"155":4}}],["adjoint",{"5":{"45":2}}],["adalora可以自动将某些",{"5":{"142":1}}],["adalora可以自动将某些推向零",{"5":{"142":1}}],["adalora引入对角缩放矩阵",{"5":{"142":1}}],["adalora引入对角缩放矩阵来动态调整不同奇异值的贡献",{"5":{"142":1}}],["adalora",{"5":{"142":1}}],["adalora的稀疏化效应",{"5":{"142":2}}],["adalora的数学形式",{"5":{"142":2}}],["adagrad为每个参数维护一个累积梯度平方和",{"5":{"48":2}}],["adagrad特别适合处理稀疏特征和非均匀分布的梯度",{"5":{"48":2}}],["adagrad",{"5":{"97":6}}],["adam",{"5":{"48":4,"70":4,"97":4,"134":12,"135":8,"144":2,"149":2}}],["adam维护两个指数移动平均",{"5":{"48":2}}],["adam的超参数和通常使用默认值",{"5":{"48":1}}],["adam的超参数",{"5":{"48":1}}],["adaptive",{"5":{"48":2,"86":2,"151":2}}],["adaptation",{"5":{"50":2,"142":4}}],["additivity",{"5":{"71":2}}],["addition",{"5":{"91":2}}],["a>",{"5":{"21":10,"131":51}}],["als迭代",{"5":{"143":1}}],["als算法通过交替优化提供了一种实用的求解方法",{"5":{"143":2}}],["alternatives",{"5":{"69":2}}],["alt=",{"5":{"44":1,"45":2,"47":2,"48":3,"50":2,"61":1,"96":3,"143":1,"148":2}}],["alpha",{"5":{"42":12,"132":18,"141":4}}],["align",{"2":{"154":1,"155":1,"156":2},"5":{"42":16,"61":12,"97":4}}],["aligned",{"5":{"86":8,"151":8}}],["alibi等都属于这一类别",{"5":{"88":2}}],["alibi",{"5":{"88":2}}],["alibi的优势在于实现极其简单",{"5":{"88":2}}],["alibi也有明显的局限",{"5":{"88":2}}],["all的",{"5":{"157":2}}],["all通信和梯度聚合的all",{"5":{"157":2}}],["all通信可以在",{"5":{"157":2}}],["all通信",{"5":{"157":4}}],["all",{"5":{"91":2,"136":2,"146":2,"157":5,"159":4}}],["a",{"5":{"47":2,"96":6,"135":6,"137":2,"141":8,"142":8,"143":26,"145":2}}],["affine",{"5":{"47":2,"146":2}}],["anisotropy",{"5":{"149":2}}],["and",{"5":{"47":2}}],["annealing",{"5":{"48":2,"97":2}}],["another",{"5":{"88":2}}],["anaphora",{"5":{"88":2}}],["amplitude",{"5":{"136":2}}],["amp",{"5":{"42":30,"61":18,"86":5,"97":4,"143":15,"144":7,"151":5}}],["amsgrad是adam的一个修改版本",{"5":{"48":2}}],["aware",{"5":{"88":6}}],["aware的思想来改善长上下文性能",{"5":{"88":2}}],["的设置允许每个输入由两个专家共同处理",{"5":{"159":2}}],["的设计虽然简单直观",{"5":{"159":2}}],["的设计选择背后有深刻的数学原因",{"5":{"71":2}}],["的设计哲学",{"5":{"88":2}}],["的设计原理和数学直觉",{"5":{"91":2}}],["的技巧",{"5":{"159":2}}],["的神经网络",{"5":{"159":2}}],["的神经元",{"5":{"148":2}}],["的协调配合",{"5":{"159":2}}],["的协方差结构",{"5":{"146":2}}],["的协方差矩阵为对角矩阵",{"5":{"145":2}}],["的路由概率",{"5":{"158":2}}],["的路径长度相比",{"5":{"132":1}}],["的路径长度为",{"5":{"92":2,"153":2}}],["的路径长度恒为1",{"5":{"92":2,"153":2}}],["的实际分配频率",{"5":{"158":2}}],["的实现细节",{"5":{"133":2}}],["的负载均衡损失可以定义为交叉熵形式",{"5":{"158":2}}],["的负对数似然",{"5":{"70":2}}],["的门控参数保持不变",{"5":{"158":2}}],["的门控权重向量",{"5":{"158":2}}],["的门控权重矩阵",{"5":{"158":2}}],["的门控机制惊人地相似",{"5":{"150":2}}],["的当前负载为",{"5":{"157":2}}],["的局部梯度",{"5":{"157":2}}],["的局部最小值",{"5":{"51":2}}],["的专家参数在每次前向传播中不会被访问",{"5":{"159":2}}],["的专家在训练过程中无法得到充分更新",{"5":{"159":2}}],["的专家",{"5":{"157":2}}],["的上界为",{"5":{"156":2}}],["的上界或下界",{"5":{"148":1}}],["的动作",{"5":{"155":4}}],["的收益差异",{"5":{"155":2}}],["的累积回报",{"5":{"155":2}}],["的情况下",{"5":{"158":2}}],["的情况",{"5":{"155":2}}],["的深入理解",{"2":{"155":1},"5":{"155":1}}],["的深刻转变",{"5":{"88":2}}],["的调整",{"5":{"154":1}}],["的调整是这种方法的关键技巧",{"5":{"154":1}}],["的变化范围来解决这个问题的",{"5":{"154":2}}],["的变换",{"5":{"47":1,"94":2}}],["的意义",{"5":{"150":1}}],["的意义在于它们允许模型恢复归一化操作可能丢失的表示能力",{"5":{"150":1}}],["的学习范式",{"5":{"159":2}}],["的学习",{"5":{"150":2}}],["的角色",{"5":{"149":2}}],["的角度出发",{"5":{"138":2}}],["的角度理解",{"5":{"61":2}}],["的角度",{"5":{"87":2,"152":2}}],["的角度审视多头注意力",{"5":{"93":2}}],["的匹配关系并非巧合",{"5":{"149":2}}],["的匹配程度",{"5":{"132":1}}],["的某些奇异值可能为零",{"5":{"148":1}}],["的某些对角元素为零",{"5":{"148":1}}],["的某些行是线性相关的",{"5":{"141":1}}],["的噪声",{"5":{"147":2}}],["的噪声使得优化过程更倾向于探索",{"5":{"147":2}}],["的噪声可能减慢收敛但有助于逃离鞍点",{"5":{"97":2}}],["的隐式正则化",{"5":{"147":4}}],["的隐藏状态",{"5":{"92":1,"101":1,"153":1}}],["的导数",{"5":{"147":2}}],["的导数为",{"5":{"42":2,"45":1,"145":2}}],["的归一化只依赖于该特征在批量中的统计量",{"5":{"146":1}}],["的额外开销",{"5":{"146":2}}],["的超球面内",{"5":{"147":2}}],["的超球面上",{"5":{"146":2,"147":2}}],["的超平面与第一卦限相交形成的三角形区域",{"5":{"70":2}}],["的差异",{"5":{"146":2}}],["的残差连接之后",{"5":{"146":2}}],["的影响",{"5":{"146":3}}],["的作用",{"5":{"146":2}}],["的出现",{"5":{"146":2}}],["的出现才得到解决",{"5":{"47":2}}],["的提出",{"5":{"146":2}}],["的逐元素乘法定义了dropout操作",{"5":{"145":1}}],["的逐层求值",{"5":{"45":2}}],["的每个元素",{"5":{"145":1}}],["的每个元素独立地以概率",{"5":{"145":2}}],["的每一行是一个概率分布",{"5":{"87":1,"92":2,"152":1,"153":2}}],["的每一行只依赖于对应的查询行和整个键矩阵",{"5":{"92":1,"153":1}}],["的默认实现方式",{"5":{"145":2}}],["的极小点",{"5":{"147":2}}],["的极小值区域",{"5":{"147":2}}],["的极小值",{"5":{"144":2}}],["的极端特征值",{"5":{"135":2}}],["的lipschitz常数为",{"5":{"144":2}}],["的高阶svd定义为",{"5":{"143":1}}],["的高斯分布",{"5":{"96":1}}],["的高斯噪声",{"5":{"96":1}}],["的椭圆",{"5":{"143":1,"144":2}}],["的构造",{"5":{"143":1}}],["的灰度图像可以表示为一个",{"5":{"143":1}}],["的水平",{"5":{"143":2}}],["的cp分解形式为",{"5":{"143":2}}],["的和",{"5":{"143":2}}],["的和为1",{"5":{"97":1}}],["的rademacher复杂度满足",{"5":{"142":1}}],["的奇异值",{"5":{"143":1}}],["的奇异值为",{"5":{"142":1}}],["的奇异值对齐",{"5":{"142":1}}],["的奇异值与",{"5":{"142":1}}],["的奇异值分解与",{"5":{"142":1}}],["的奇异值分解为",{"5":{"41":1,"89":1}}],["的奇异值分解中的对应项",{"5":{"89":1}}],["的曲率方向",{"5":{"142":4}}],["的时间",{"5":{"158":2}}],["的时间和空间复杂度",{"5":{"141":2}}],["的时间复杂度如下",{"5":{"92":2,"153":2}}],["的基数为",{"5":{"140":1}}],["的基向量相关",{"5":{"87":2,"152":2}}],["的文本序列定义为",{"5":{"140":1}}],["的假设",{"5":{"140":2}}],["的交互中学习",{"5":{"156":2}}],["的交叉熵定义为",{"5":{"139":1}}],["的交替模式",{"5":{"136":2}}],["的交替结构是深度学习成功的关键数学基础",{"5":{"71":2}}],["的似然函数为",{"5":{"139":1}}],["的随机梯度估计方差",{"5":{"139":2}}],["的随机性",{"5":{"136":2}}],["的随机性可帮助系统逃离",{"5":{"134":2}}],["的kl散度较大",{"5":{"138":2}}],["的key向量",{"5":{"87":1,"152":1}}],["的模n展开",{"5":{"143":1}}],["的模型",{"5":{"138":2}}],["的模式",{"5":{"87":2,"152":2}}],["的模式与现代gpu的架构特性高度匹配",{"5":{"92":2,"153":2}}],["的模式来推断该头负责的依赖类型",{"5":{"92":1,"153":1}}],["的涌现可能遵循不同的规律",{"5":{"138":2}}],["的现象并非简单的经验规律",{"5":{"138":2}}],["的样本所需的平均编码长度",{"5":{"137":1}}],["的样本时所需要的额外信息量",{"5":{"96":1}}],["的有用信息与压缩关于输入的冗余之间取得平衡",{"5":{"137":1}}],["的有效预条件矩阵",{"5":{"135":2}}],["的理解不充分",{"5":{"137":1}}],["的理论基础",{"5":{"71":2}}],["的联合熵定义为它们联合分布的不确定性",{"5":{"137":1}}],["的联系",{"5":{"51":2,"69":2}}],["的离散随机变量",{"5":{"137":1}}],["的统计量",{"5":{"147":2}}],["的统计性质",{"5":{"137":2}}],["的统计学原理",{"5":{"95":1}}],["的知识迁移到小模型",{"5":{"137":2}}],["的轨道在足够长时间后分离至少",{"5":{"136":1}}],["的不可行性",{"5":{"140":1}}],["的不确定性",{"5":{"137":1}}],["的不动点稳定性",{"5":{"136":1}}],["的不同理解",{"5":{"70":2}}],["的速率收敛到真实熵",{"5":{"137":1}}],["的速率收敛到最优解",{"5":{"136":1}}],["的速率趋于零",{"5":{"136":1}}],["的速度收敛",{"5":{"48":1}}],["的速度收敛到最优解附近",{"5":{"51":1}}],["的速度线性增长",{"5":{"95":1}}],["的速度增长",{"5":{"95":1}}],["的几何解释",{"5":{"143":2}}],["的几何结构",{"5":{"135":2}}],["的几何直觉可以通过圆周运动来理解",{"5":{"90":1}}],["的宽而浅的区域",{"5":{"135":2}}],["的小特征值占主导",{"5":{"135":2}}],["的病态程度定义为条件数",{"5":{"135":2}}],["的邻域",{"5":{"136":1}}],["的邻域内连续可微",{"5":{"135":2}}],["的邻域集合",{"5":{"86":1,"151":1}}],["的关系为",{"5":{"144":2}}],["的关系",{"5":{"135":2}}],["的关系可用条件互信息描述",{"5":{"133":1}}],["的可微映射",{"5":{"135":2}}],["的预条件化解释",{"5":{"134":2}}],["的预测分布",{"5":{"133":1}}],["的预测",{"5":{"70":1}}],["的预测概率",{"5":{"70":1}}],["的更新规则直接可得",{"5":{"135":2}}],["的更新规则为",{"5":{"134":2}}],["的更新可表示为随机动力系统",{"5":{"134":2}}],["的连续性和极限的交换性",{"5":{"136":1}}],["的连续性",{"5":{"136":1}}],["的连续时间近似为",{"5":{"134":2}}],["的连续时间极限",{"5":{"134":2}}],["的连续形式",{"5":{"134":2}}],["的连续调节能力",{"5":{"71":2}}],["的训练过程建模为动态系统",{"5":{"134":2}}],["的训练算法包含编码器",{"5":{"133":2}}],["的状态转变为",{"5":{"138":2}}],["的状态",{"5":{"134":2,"138":2}}],["的状态空间大小",{"5":{"96":1}}],["的网络参数为",{"5":{"157":2}}],["的网络对裁剪的需求较低",{"5":{"147":2}}],["的网络",{"5":{"133":1}}],["的网络可以等效于宽度为指数级别的浅层网络",{"5":{"71":1}}],["的私有表示",{"5":{"133":1}}],["的泛化误差上界最小",{"5":{"133":1}}],["的互信息估计",{"5":{"137":2}}],["的互信息",{"5":{"133":4}}],["的优劣程度",{"5":{"155":2}}],["的优势函数",{"5":{"155":2}}],["的优势之一",{"5":{"134":2}}],["的优势",{"5":{"133":2}}],["的优化涉及策略优化和奖励模型学习两个相互关联的过程",{"5":{"97":2}}],["的下界",{"5":{"137":1}}],["的下界估计",{"5":{"133":2}}],["的下三角矩阵",{"5":{"95":1}}],["的正定性",{"5":{"135":2}}],["的正则化效果",{"5":{"133":2}}],["的正样本的软概率",{"5":{"69":1}}],["的目标函数时特别有效",{"5":{"150":2}}],["的目标函数",{"5":{"133":2}}],["的目标是计算一个加权平均",{"5":{"70":2}}],["的目标是学习一个编码函数",{"5":{"70":2}}],["的目标是找到参数",{"5":{"61":2}}],["的拼接中学习门控值",{"5":{"132":2}}],["的判别标准",{"5":{"132":2}}],["的稳定性条件与梯度下降类似",{"5":{"134":2}}],["的稳定性条件",{"5":{"134":2}}],["的稳定性分析方法不仅深化了我们对深度学习训练动力学的理解",{"5":{"135":2}}],["的稳定性分析<",{"5":{"131":1}}],["的稳定性分析",{"0":{"135":1},"4":{"135":1},"5":{"131":4,"135":1}}],["的稳定性由矩阵的特征值决定",{"5":{"50":1}}],["的稳定性由矩阵",{"5":{"50":1}}],["的权重贡献",{"5":{"158":2}}],["的权重矩阵集合",{"5":{"142":1}}],["的权重矩阵被近似为",{"5":{"50":1}}],["的权重",{"5":{"101":1,"145":2,"157":2}}],["的损失函数为",{"5":{"101":1}}],["的双向上下文表示",{"5":{"101":1}}],["的token集合的函数",{"5":{"101":1}}],["的token嵌入经过多层自注意力计算后的隐藏状态",{"5":{"101":1}}],["的token",{"5":{"101":2}}],["的估计",{"5":{"101":2,"135":2,"155":2}}],["的功能",{"5":{"101":2}}],["的具体身份",{"5":{"101":1}}],["的具体场景",{"5":{"101":2}}],["的具体数值完全由训练数据决定",{"5":{"89":1}}],["的映射",{"5":{"70":1,"92":2,"153":2}}],["的映射函数",{"5":{"70":1}}],["的项",{"5":{"70":1,"97":1}}],["的组合",{"5":{"70":1}}],["的区分",{"5":{"70":2}}],["的区域被压缩到零点",{"5":{"45":2}}],["的区域保持不变",{"5":{"45":2}}],["的区域对应类别1",{"5":{"47":2}}],["的区域对应类别0",{"5":{"47":2}}],["的机制",{"5":{"69":1}}],["的锐度",{"5":{"69":1}}],["的编码",{"5":{"69":1,"89":1,"90":3}}],["的编码方案来编码来自分布",{"5":{"61":3}}],["的编码向量",{"5":{"89":1,"90":2,"91":3,"92":2,"153":2}}],["的编码向量视为一个离散序列",{"5":{"90":1}}],["的编码矩阵",{"5":{"89":1}}],["的编码分辨率与频率",{"5":{"90":2}}],["的编码映射到位置",{"5":{"90":1}}],["的编码可以表示为初始编码",{"5":{"90":1}}],["的编码与位置",{"5":{"91":1}}],["的编码是初始编码经过各频率成分独立旋转",{"5":{"90":1}}],["的编码是对称的",{"5":{"91":1}}],["的infonce",{"5":{"69":2}}],["的核心理念源于对计算资源优化配置的理论洞察",{"5":{"159":2}}],["的核心组件",{"5":{"69":2}}],["的核心正是通过这种矩阵表示实现高效的神经网络计算",{"5":{"47":2}}],["的核心思想是通过限制每个位置只能关注少数其他位置",{"5":{"86":2,"151":2}}],["的核心思想是通过替换softmax函数",{"5":{"86":2,"151":2}}],["的核心运算是query",{"5":{"88":2}}],["的选择需要在负载均衡和主任务性能之间取得平衡",{"5":{"158":2,"159":2}}],["的选择需要仔细调整",{"5":{"155":2}}],["的选择",{"5":{"154":1}}],["的选择直接影响训练的稳定性和学习效率",{"5":{"154":1}}],["的选择直接影响优化轨迹和最终性能",{"5":{"101":1}}],["的选择可以遵循以下原则",{"5":{"145":2}}],["的选择对性能有显著影响",{"5":{"69":1}}],["的选择是一个关键的超参数",{"5":{"101":1}}],["的选择是一个关键的设计决策",{"5":{"91":1}}],["的选择是一个超参数调优问题",{"5":{"41":1}}],["的选择是一个重要的权衡问题",{"5":{"93":2}}],["的选择并非随意",{"5":{"88":2}}],["的元素独立同分布",{"5":{"135":2}}],["的元素",{"5":{"46":1,"142":1}}],["的元素为",{"5":{"61":2}}],["的元素为对角元素的对角矩阵",{"5":{"61":2}}],["的伴随值为",{"5":{"45":2}}],["的线性组合来表示原始张量",{"5":{"143":2}}],["的线性变换",{"5":{"133":1}}],["的线性变换中最大的拉伸因子",{"5":{"50":1}}],["的线性投影",{"5":{"45":1}}],["的线性函数",{"5":{"45":2}}],["的线性子空间上的所有函数",{"5":{"89":1}}],["的伪逆",{"5":{"45":1}}],["的流动",{"5":{"45":2}}],["的表示编码全局",{"5":{"45":2}}],["的表示捕获数据的局部",{"5":{"45":2}}],["的表示可能影响训练动态和最终性能",{"5":{"89":2}}],["的表示与原始位置",{"5":{"92":1,"153":1}}],["的分配比例为",{"5":{"157":2}}],["的分段线性逼近有相似之处",{"5":{"45":2}}],["的分析揭示了它们如何通过稳定激活分布来改善梯度传播",{"5":{"41":2}}],["的分布",{"5":{"145":2}}],["的分布范围可能达到或更宽",{"5":{"41":1}}],["的分布范围可能达到",{"5":{"41":1}}],["的分布趋向于标准正态分布",{"5":{"96":1}}],["的分布特性",{"5":{"95":1}}],["的分子分母分别计算时是稳定的",{"5":{"61":2}}],["的分层混合",{"5":{"89":2}}],["的分离",{"5":{"94":2}}],["的分量都是独立同分布的随机变量",{"5":{"95":1}}],["的计算可以通过对",{"5":{"159":2}}],["的计算模式在资源效率上存在显著浪费",{"5":{"159":2}}],["的计算负载为",{"5":{"157":2}}],["的计算会消除特征向量中的某些信息",{"5":{"146":2}}],["的计算会遇到严重的数值问题",{"5":{"61":2}}],["的计算复杂度取决于模型架构",{"5":{"140":1}}],["的计算也是并行的",{"5":{"45":2}}],["的计算就隐含了条件期望的概念",{"5":{"96":2}}],["的计算量",{"5":{"50":1}}],["的计算涉及softmax操作",{"5":{"70":1}}],["的计算涉及",{"5":{"61":2}}],["的计算是数值稳定的",{"5":{"61":2}}],["的计算",{"5":{"61":2,"87":1,"152":1}}],["的计算结果与",{"5":{"46":1}}],["的计算结果是一个",{"5":{"87":1,"152":1}}],["的计算结果",{"5":{"95":1}}],["的计算为",{"5":{"88":2}}],["的计算与单头注意力相同",{"5":{"93":1}}],["的计算过程可以分解为三个连续的矩阵运算步骤",{"5":{"95":2}}],["的最大特征值是实数且严格大于其他特征值的模",{"5":{"135":2}}],["的最大特征值的平方根",{"5":{"50":1}}],["的最大和最小特征值",{"5":{"135":2}}],["的最大信息的同时",{"5":{"133":1}}],["的最大化互信息的目标",{"5":{"70":2}}],["的最大奇异值为",{"5":{"148":1}}],["的最大奇异值",{"5":{"41":1,"87":1,"92":1,"152":1,"153":1}}],["的最佳近似",{"5":{"50":1,"89":1}}],["的最优问题",{"5":{"51":1}}],["的最优预测是",{"5":{"51":1}}],["的最优预测",{"5":{"51":1}}],["的最小值附近的高斯噪声观测产生",{"5":{"149":1}}],["的最小值为",{"5":{"61":2}}],["的最小奇异值为",{"5":{"148":1}}],["的最小奇异值为的最大奇异值为",{"5":{"148":1}}],["的最小和最大特征值",{"5":{"135":2}}],["的最小和最大奇异值",{"5":{"41":1}}],["的最小特征值",{"5":{"70":1}}],["的最小路径长度为",{"5":{"92":1,"153":1}}],["的最小非零特征值",{"5":{"97":1}}],["的最终表示保留了原始嵌入的直接成分",{"5":{"92":1,"153":1}}],["的对称性",{"5":{"135":2}}],["的对称半正定矩阵",{"5":{"96":1}}],["的对角元素满足",{"5":{"148":2}}],["的对角元素",{"5":{"41":1,"143":1}}],["的对角线分类",{"5":{"46":4}}],["的对角线元素",{"5":{"50":2}}],["的对数成反比",{"5":{"138":2}}],["的对数几率",{"5":{"71":1}}],["的对数几率为",{"5":{"71":1}}],["的对数",{"5":{"91":2}}],["的信息不会增加关于",{"5":{"137":1}}],["的信息论动机",{"5":{"133":2}}],["的信息论特性",{"5":{"133":2}}],["的信息流动",{"5":{"133":2}}],["的信息流动比例",{"5":{"87":1,"152":1}}],["的信息动力学分析可得",{"5":{"133":2}}],["的信息特性体现了三角函数编码的优势",{"5":{"132":1}}],["的信息分解",{"5":{"132":2}}],["的信息",{"5":{"50":2,"71":1,"91":4,"93":1,"95":1,"133":2,"137":2,"154":2}}],["的信息量",{"5":{"69":1,"71":1,"137":1}}],["的信息量受到其维度",{"5":{"93":1}}],["的信息时",{"5":{"61":5}}],["的信息聚合方式",{"5":{"92":2,"153":2}}],["的信息整合方式",{"5":{"92":2,"153":2}}],["的信息传递效率定义为互信息与自信息之比",{"5":{"132":1}}],["的信息传递效率为互信息之比",{"5":{"132":1}}],["的信息传递",{"5":{"92":1,"153":1}}],["的信息传递路径是",{"5":{"92":1,"153":1}}],["的信息有多少能够传递到位置",{"5":{"92":1,"153":1}}],["的信息对位置",{"5":{"92":1,"153":1}}],["的信息就可以",{"5":{"92":1,"153":1}}],["的信息通过位置",{"5":{"92":1,"153":1}}],["的梯度分别为",{"5":{"158":2}}],["的梯度计算更为复杂",{"5":{"157":2}}],["的梯度表达式",{"5":{"155":2}}],["的梯度流",{"5":{"148":2}}],["的梯度流对训练尤其有害",{"5":{"148":2}}],["的梯度更新",{"5":{"142":1}}],["的梯度估计",{"5":{"134":2}}],["的梯度与共享参数",{"5":{"133":1}}],["的梯度与ppo目标的梯度在期望意义下是相近的",{"5":{"101":1}}],["的梯度向量",{"5":{"101":1}}],["的梯度高度相似",{"5":{"101":2}}],["的梯度是",{"5":{"70":1}}],["的梯度结构高度相似",{"5":{"70":1}}],["的梯度结构与上述交叉熵关于",{"5":{"70":1}}],["的梯度结构",{"5":{"70":1}}],["的梯度",{"5":{"69":7,"70":1,"89":1,"146":1}}],["的梯度为",{"5":{"41":1,"44":1,"45":1,"51":1,"61":2,"96":1,"101":1,"157":2,"158":2,"159":2}}],["的梯度完全消失",{"5":{"41":1}}],["的梯度很小",{"5":{"41":1}}],["的梯度特性使得gelu特别适合训练非常深的网络",{"5":{"71":2}}],["的梯度具有简洁的形式",{"5":{"70":2,"96":2}}],["的梯度在预测概率接近0或1时会变得非常小",{"5":{"61":2}}],["的梯度可以通过链式法则计算",{"5":{"159":2}}],["的梯度可以表示为",{"5":{"158":2}}],["的梯度可以直接传播",{"5":{"41":2}}],["的梯度可以写为",{"5":{"94":2}}],["的梯度形式具有优美的残差结构",{"5":{"96":2}}],["的函数",{"5":{"51":1,"61":2,"71":1,"90":3,"96":1,"101":1,"145":2}}],["的第列仅依赖于的第列和的所有列",{"5":{"45":1}}],["的第",{"5":{"44":2,"45":4,"47":3,"87":2,"89":1,"92":1,"94":1,"95":4,"141":1,"145":1,"152":2,"153":1,"158":2}}],["的第行仅依赖于的第行和",{"5":{"45":1}}],["的第行为为行向量",{"5":{"47":1}}],["的第行对应序列中第个位置的嵌入表示",{"5":{"94":1}}],["的第一个维度",{"5":{"94":1}}],["的饱和特性与sigmoid类似",{"5":{"41":1}}],["的雅可比矩阵",{"5":{"135":2}}],["的雅可比矩阵为",{"5":{"45":1,"159":2}}],["的雅可比矩阵的范数满足",{"5":{"41":1}}],["的尺度与相关",{"5":{"41":1}}],["的尺度与",{"5":{"41":1}}],["的谱范数定义为最大奇异值",{"5":{"144":2}}],["的谱性质",{"2":{"135":1},"5":{"135":1}}],["的谱性质决定了梯度如何被",{"5":{"41":1}}],["的谱半径也满足",{"5":{"148":1}}],["的谱半径接近1",{"5":{"148":2}}],["的谱半径定义为",{"5":{"135":2}}],["的谱半径",{"5":{"41":1}}],["的均方根",{"5":{"41":1}}],["的绝对值很大时",{"5":{"42":1}}],["的",{"5":{"51":1,"61":4,"69":3,"70":4,"71":3,"87":3,"88":3,"92":5,"97":8,"132":3,"133":4,"135":4,"136":1,"138":2,"146":1,"147":2,"152":3,"153":5}}],["的激活函数可能需要更少的神经元",{"5":{"71":2}}],["的显式注入",{"5":{"71":2}}],["的hessian矩阵与fisher信息矩阵密切相关",{"5":{"71":2}}],["的形式包含类似",{"5":{"70":1}}],["的形式包含类似的项",{"5":{"70":1}}],["的形式",{"5":{"69":2,"71":1,"101":4,"143":2,"150":2}}],["的全连接层",{"5":{"45":1}}],["的全连接网络",{"5":{"71":1}}],["的后验分布与似然函数和先验的乘积成正比",{"5":{"71":1}}],["的过程",{"5":{"44":2}}],["的误差信号",{"5":{"44":1}}],["的fisher信息矩阵正好是softmax雅可比矩阵与自身转置的乘积",{"5":{"71":1}}],["的flops约为",{"5":{"44":2}}],["的数学表示通常采用两层前馈神经网络的形式",{"5":{"158":2}}],["的数学意义",{"2":{"156":1},"5":{"156":1}}],["的数学定义",{"2":{"156":1},"5":{"156":1}}],["的数学目标正是通过适当设置权重的方差",{"5":{"148":2}}],["的数学依据是",{"5":{"146":1}}],["的数学建模",{"2":{"145":1},"5":{"145":1}}],["的数学框架涉及如何组合不同任务的损失函数",{"5":{"101":2}}],["的数学改进",{"5":{"101":2}}],["的数学协同关系",{"5":{"71":2}}],["的数学基础与几何解释<",{"5":{"131":1}}],["的数学基础与几何解释",{"0":{"51":1},"4":{"51":1},"5":{"51":3,"131":4}}],["的数学基础建立在对生物神经元",{"5":{"47":2}}],["的数学本质",{"5":{"61":2}}],["的数学公式",{"0":{"95":1},"4":{"95":1},"5":{"95":1,"131":4}}],["的数学公式<",{"5":{"131":1}}],["的数学原理",{"5":{"88":2,"89":2,"93":2}}],["的所有元素都会被用于计算每个输入",{"5":{"159":2}}],["的所有样本",{"5":{"146":1}}],["的所有特征",{"5":{"146":1}}],["的所有特征值满足",{"5":{"134":2,"135":2}}],["的所有列",{"5":{"45":1}}],["的所有模式",{"5":{"96":1}}],["的所有行和为",{"5":{"61":2}}],["的抽象与简化之上",{"5":{"47":2}}],["的操作",{"5":{"47":2}}],["的问题",{"5":{"47":2,"88":2}}],["的仿射变换定义为",{"5":{"47":1}}],["的摩尔",{"5":{"47":1}}],["的总flops约为前向传播的4倍",{"5":{"44":2}}],["的总影响是通过所有中间变量",{"5":{"48":1}}],["的偏导数为",{"5":{"48":1,"96":1}}],["的偏导数与它对",{"5":{"48":1}}],["的偏导数的乘积之和",{"5":{"48":1}}],["的矩阵",{"5":{"48":1,"50":3,"87":2,"93":3,"94":1,"95":1,"141":1,"143":1,"148":2,"152":2}}],["的矩阵与一个",{"5":{"50":1}}],["的矩阵组合成一个",{"5":{"50":1}}],["的矩阵乘法",{"5":{"95":2}}],["的等概率密度面是一个极度扁平的椭球体",{"5":{"149":2}}],["的等高线是同心圆",{"5":{"144":2}}],["的等高线是椭圆",{"5":{"144":2}}],["的等式依次代入",{"5":{"46":1}}],["的等值面上",{"5":{"48":1}}],["的单调性来保证收敛性",{"5":{"48":1}}],["的一个无偏估计",{"5":{"155":2}}],["的一个随机估计量",{"5":{"149":2}}],["的一半",{"5":{"148":1}}],["的一种形式",{"5":{"70":2}}],["的一种特殊情况",{"5":{"44":2}}],["的一次试验版本",{"5":{"96":2}}],["的定义出发",{"5":{"135":2}}],["的定义",{"5":{"96":2}}],["的定义直接求导验证",{"5":{"61":2}}],["的one",{"5":{"96":2}}],["的渐近行为",{"5":{"96":2}}],["的效果",{"5":{"96":2}}],["的语言模型",{"5":{"96":1}}],["的pdf为",{"5":{"96":1}}],["的条件熵的期望",{"5":{"137":1}}],["的条件熵是对所有可能",{"5":{"137":1}}],["的条件熵定义为在该已知信息下",{"5":{"137":1}}],["的条件数",{"5":{"134":2,"136":2}}],["的条件数定义为",{"5":{"41":1}}],["的条件分布仍然是高斯分布",{"5":{"96":1}}],["的条件下独立",{"5":{"137":1}}],["的条件下",{"5":{"51":2,"137":1}}],["的条件期望",{"5":{"51":1}}],["的支持集必须包含",{"5":{"96":1}}],["的支持集",{"5":{"96":1}}],["的运算",{"5":{"50":2,"88":2}}],["的方法",{"5":{"143":2}}],["的方向调整",{"5":{"158":2}}],["的方向",{"5":{"71":4,"149":2}}],["的方向导数为",{"5":{"48":1}}],["的方阵才是可逆的",{"5":{"50":2}}],["的方阵",{"5":{"50":1,"94":1}}],["的方式",{"5":{"92":2,"153":2}}],["的方差分析",{"2":{"145":1},"5":{"145":1}}],["的方差很大",{"5":{"69":2}}],["的方差较大时",{"5":{"94":1}}],["的方差为",{"5":{"95":1}}],["的概念",{"5":{"69":2,"149":2}}],["的概念推广到任意维度",{"5":{"50":2}}],["的概率会终止",{"5":{"156":2}}],["的概率增加量",{"5":{"155":2}}],["的概率增加量大于",{"5":{"155":2}}],["的概率与旧策略选择该动作的概率之比",{"5":{"154":2}}],["的概率随训练进行而增加",{"5":{"147":1}}],["的概率分布",{"5":{"140":1,"154":2}}],["的概率分布为",{"5":{"61":1}}],["的概率为",{"5":{"61":6,"69":2,"96":1,"157":2}}],["的概率为1",{"5":{"96":1}}],["的概率",{"5":{"96":3,"156":4}}],["的乘积",{"5":{"41":1,"42":2,"50":3}}],["的乘法可以看作是收缩掉两个张量的一个公共维度",{"5":{"50":2}}],["的张量",{"5":{"50":2}}],["的二阶偏导数连续",{"5":{"48":1}}],["的二阶泰勒展开为",{"5":{"48":1}}],["的二维张量重塑为",{"5":{"50":1}}],["的二维张量",{"5":{"50":1}}],["的二次型定义和链式法则可得",{"5":{"135":2}}],["的二次增长对应于",{"5":{"70":1}}],["的二次增长对应于范数的平方",{"5":{"70":1}}],["的二次复杂度",{"5":{"95":1}}],["的三维张量",{"5":{"50":3,"93":1}}],["的三维张量运算",{"5":{"50":1}}],["的三阶张量表示为",{"5":{"50":1}}],["的三阶段范式",{"5":{"95":2}}],["的查询不会注意到位置",{"5":{"140":1}}],["的查询",{"5":{"50":1,"92":1,"153":1}}],["的平均路由概率",{"5":{"158":4}}],["的平均代价",{"5":{"137":1}}],["的平均",{"5":{"70":2}}],["的平面相交得到的一元函数曲线的切线斜率",{"5":{"48":1}}],["的平方根成正比",{"5":{"146":2}}],["的平方范数加上不可解释部分",{"5":{"51":2}}],["的平方范数",{"5":{"51":2}}],["的平方",{"5":{"51":2,"70":1}}],["的平稳分布",{"5":{"87":1,"152":1}}],["的敏感性是其最著名的性质之一",{"5":{"51":2}}],["的标准差为1时",{"5":{"41":1}}],["的标准",{"5":{"50":2}}],["的标准定义",{"5":{"61":2}}],["的标准自注意力配置",{"5":{"95":1}}],["的标量函数",{"5":{"51":1}}],["的点",{"5":{"51":1,"134":2,"136":1}}],["的点集",{"5":{"61":1}}],["的集合与",{"5":{"159":2}}],["的集合",{"5":{"51":1}}],["的列是正交的",{"5":{"143":1}}],["的列向量是",{"5":{"45":1}}],["的列向量称为左奇异向量",{"5":{"50":2}}],["的列向量称为右奇异向量",{"5":{"50":2}}],["的列向量",{"5":{"50":1}}],["的列向量决定了",{"5":{"94":2}}],["的列空间",{"5":{"51":1}}],["的期望值为",{"5":{"154":2}}],["的期望回报",{"5":{"154":2}}],["的期望",{"5":{"51":1}}],["的值通常通过小规模实验拟合得到",{"5":{"159":2}}],["的值裁剪得到",{"5":{"147":2}}],["的值趋近于0",{"5":{"61":2}}],["的值很大",{"5":{"61":2}}],["的值很大时",{"5":{"61":2}}],["的值会很大",{"5":{"61":2}}],["的值会是一个绝对值很大的负数",{"5":{"61":2}}],["的值被限制在",{"5":{"61":2}}],["的值",{"5":{"92":1,"153":1,"159":2}}],["的值向量",{"5":{"92":3,"153":3}}],["的本质",{"5":{"61":2}}],["的行数",{"5":{"135":2}}],["的行为类似于信息选择器",{"5":{"71":2}}],["的行方向归一化",{"5":{"61":2}}],["的加权平均",{"5":{"61":2,"92":1,"153":1}}],["的工具",{"5":{"61":2}}],["的事件",{"5":{"61":1}}],["的熵量化了模型预测的不确定性",{"5":{"137":1}}],["的熵",{"5":{"61":2}}],["的位置集合",{"5":{"157":2}}],["的位置差异",{"5":{"146":2}}],["的位置",{"5":{"61":2,"69":2,"89":3,"92":2,"153":2}}],["的位置信息",{"5":{"91":2}}],["的位置query",{"5":{"91":1}}],["的位置key",{"5":{"91":1}}],["的位置对",{"5":{"92":1,"153":1}}],["的位置编码向量",{"5":{"91":1}}],["的位置编码是一个有前景的研究方向",{"5":{"101":2}}],["的解决方案",{"5":{"154":2}}],["的解",{"5":{"147":2}}],["的解析解为",{"5":{"144":2}}],["的解等价于求解",{"5":{"50":1}}],["的解释",{"5":{"61":2}}],["的复合",{"5":{"45":2}}],["的复数",{"0":{"88":1},"4":{"88":1},"5":{"88":1,"131":5}}],["的复数编码向量",{"5":{"90":1}}],["的复数编码",{"5":{"90":1}}],["的复杂度成为效率和可扩展性的主要障碍",{"5":{"86":2,"151":2}}],["的复杂度成为性能瓶颈",{"5":{"95":2}}],["的稀疏注意力模式",{"5":{"86":2,"151":2}}],["的序列需要",{"5":{"140":1}}],["的序列有",{"5":{"140":1}}],["的序列",{"5":{"86":2,"92":2,"140":1,"143":2,"151":2,"153":2}}],["的序列上",{"5":{"90":1}}],["的序列长度",{"5":{"95":1}}],["的输出为",{"5":{"69":1,"132":1,"150":2}}],["的输出可以解释为给定输入",{"5":{"47":1}}],["的输出",{"5":{"92":1,"153":1,"159":2}}],["的输出计算",{"5":{"92":1,"153":1}}],["的输出时只能依赖于位置",{"5":{"95":1}}],["的输入",{"5":{"86":1,"92":1,"150":2,"151":1,"153":1}}],["的输入序列",{"5":{"89":1}}],["的原理是选择使对数似然最大的参数值",{"5":{"61":1}}],["的原因有以下几点",{"5":{"91":2}}],["的原始门控得分",{"5":{"158":2,"159":2}}],["的原始注意力分数",{"5":{"86":1,"151":1}}],["的原始嵌入",{"5":{"92":1,"153":1}}],["的循环依赖",{"5":{"86":1,"151":1}}],["的注意力模式",{"5":{"86":2,"151":2}}],["的注意力模式时",{"5":{"87":2,"152":2}}],["的注意力矩阵近似为",{"5":{"141":1}}],["的注意力矩阵通常比浅层",{"5":{"87":2,"152":2}}],["的注意力矩阵有更快的衰减",{"5":{"87":2,"152":2}}],["的注意力矩阵",{"5":{"87":1,"152":1}}],["的注意力矩阵计算",{"5":{"87":1,"152":1}}],["的注意力权重",{"5":{"92":3,"95":1,"153":3}}],["的注意力权重矩阵为",{"5":{"92":1,"153":1}}],["的注意力主要集中在哪些位置",{"5":{"92":1,"153":1}}],["的注意力计算",{"5":{"93":1}}],["的注意力输出",{"5":{"93":1}}],["的注意力分数设为",{"5":{"91":1}}],["的注意力分数矩阵",{"5":{"95":1}}],["的任意特征值",{"5":{"87":1,"152":1}}],["的任意行的",{"5":{"87":1,"152":1}}],["的子空间中",{"5":{"87":1,"152":1}}],["的秩满足",{"5":{"141":2}}],["的秩与",{"5":{"87":1,"152":1}}],["的秩为",{"5":{"87":1,"89":1,"143":1,"152":1}}],["的秩",{"5":{"89":1}}],["的秩是影响其性质的关键参数",{"5":{"89":1}}],["的秩最多为",{"5":{"87":2,"152":2}}],["的秩最大为",{"5":{"94":1}}],["的低秩矩阵",{"5":{"141":1}}],["的低秩矩阵有",{"5":{"87":1,"152":1}}],["的低秩近似",{"5":{"135":2}}],["的低秩结构",{"2":{"135":1},"5":{"135":1}}],["的低秩结构意味着",{"5":{"87":1,"152":1}}],["的截断svd",{"5":{"87":1,"152":1}}],["的维度尽可能低",{"5":{"137":1}}],["的维度为",{"5":{"44":1}}],["的维度较低",{"5":{"87":1,"152":1}}],["的维度均为",{"5":{"93":2}}],["的诞生",{"5":{"88":2,"146":2}}],["的框架中",{"5":{"88":2}}],["的取值范围是",{"5":{"88":2}}],["的取值范围是整个实数轴",{"5":{"96":2}}],["的群结构由以下性质刻画",{"5":{"88":2}}],["的群论基础",{"5":{"70":2}}],["的群论表述",{"5":{"88":1}}],["的逆元是反向旋转",{"5":{"88":2}}],["的四个元素",{"5":{"88":2}}],["的转变不能用简单的标度律来解释",{"5":{"138":2}}],["的转变",{"5":{"88":2}}],["的旋转矩阵",{"5":{"88":3}}],["的旋转块独立运作",{"5":{"88":1}}],["的前",{"5":{"89":2,"133":2}}],["的逼近",{"5":{"89":1}}],["的相关性可用互信息度量",{"5":{"133":1}}],["的相变",{"5":{"133":2}}],["的相似度分数",{"5":{"132":1}}],["的相似度分数转换为归一化的注意力权重",{"5":{"70":2}}],["的相似度",{"5":{"69":2}}],["的相似度决定",{"5":{"61":2}}],["的相邻位置差较小",{"5":{"90":2}}],["的相邻位置差较大",{"5":{"90":2}}],["的周期轨道",{"5":{"136":1}}],["的周期为1",{"5":{"90":2}}],["的周期为10000",{"5":{"90":2}}],["的周期函数",{"5":{"90":1}}],["的dft为",{"5":{"90":2}}],["的思想一致",{"5":{"90":2}}],["的思想",{"5":{"91":2}}],["的趋势",{"5":{"91":2}}],["的结论",{"5":{"91":2}}],["的结果",{"5":{"91":1,"144":2}}],["的结构取决于",{"5":{"70":2}}],["的结构",{"5":{"94":1,"135":2}}],["的热力图",{"5":{"91":2}}],["的先验分布为",{"5":{"96":2}}],["的先验",{"5":{"91":2}}],["的先验假设",{"5":{"92":4,"153":4}}],["的内积",{"5":{"70":2,"87":1,"152":1}}],["的内容如何复杂或简单",{"5":{"159":2}}],["的内容嵌入向量",{"5":{"91":1}}],["的内容query",{"5":{"91":1}}],["的内容key",{"5":{"91":1}}],["的同一频率振动",{"5":{"91":1}}],["的配置",{"5":{"89":2,"159":2}}],["的配对",{"5":{"91":1}}],["的增加而减小",{"5":{"138":2}}],["的增加",{"5":{"91":1}}],["的参数量",{"5":{"91":1}}],["的大矩阵",{"5":{"50":1}}],["的大小",{"5":{"150":2}}],["的大小受到卷积核尺寸的限制",{"5":{"92":2,"153":2}}],["的大小可以衡量依赖关系的强度",{"5":{"92":2,"153":2}}],["的发展进一步增强了注意力机制建模位置相关依赖的能力",{"5":{"92":2,"153":2}}],["的融合",{"5":{"92":2,"153":2}}],["的依赖关系",{"5":{"92":2,"153":2}}],["的引入正是为了解决这一根本性问题",{"5":{"91":2}}],["的引入是必要的",{"5":{"92":2,"153":2}}],["的距离有多远",{"5":{"92":1,"153":1}}],["的卷积为例",{"5":{"92":1,"153":1}}],["的贡献也较小",{"5":{"92":1,"153":1}}],["的存在",{"5":{"92":1,"150":2,"153":1}}],["的存储和计算都不可行",{"5":{"92":2,"153":2}}],["的存储空间",{"5":{"95":1}}],["的键",{"5":{"92":1,"153":1}}],["的重要性与",{"5":{"132":1}}],["的重要性与成正比",{"5":{"132":1}}],["的重要性",{"5":{"92":1,"153":1}}],["的主题",{"5":{"93":2}}],["的处所",{"5":{"93":2}}],["的策略打破了参数量与计算量之间的刚性绑定",{"5":{"159":2}}],["的策略与人类处理语言的认知过程有相似之处",{"5":{"92":2,"153":2}}],["的策略使得模型能够从多个角度同时建模输入数据",{"5":{"93":2}}],["的能力",{"5":{"138":2}}],["的能力正是通用逼近能力的几何解释",{"5":{"71":2}}],["的能力增强是多头注意力设计的精妙之处",{"5":{"93":2}}],["的多项式方程",{"5":{"50":1}}],["的多头注意力变体通过只激活部分头来降低计算成本",{"5":{"93":2}}],["的头往往具有较小的有效依赖跨度",{"5":{"92":2,"153":2}}],["的头往往具有较大的有效依赖跨度",{"5":{"92":2,"153":2}}],["的头子集",{"5":{"93":2}}],["的视角来看",{"5":{"87":2,"152":2}}],["的视角",{"5":{"93":2}}],["的query向量",{"5":{"87":1,"93":1,"152":1}}],["的query",{"5":{"93":1}}],["的各分量",{"5":{"145":1}}],["的各分量独立",{"5":{"145":2}}],["的各分量独立假设",{"5":{"145":2}}],["的各分量独立且方差为",{"5":{"135":2}}],["的各分量方差为",{"5":{"135":2}}],["的各向异性程度",{"5":{"135":2}}],["的各元素独立同分布且方差为",{"5":{"41":1}}],["的各元素方差为",{"5":{"41":1}}],["的各维度之间存在相关性",{"5":{"93":1}}],["的场景",{"5":{"93":1}}],["的约数",{"5":{"93":1}}],["的softmax概率",{"5":{"158":2}}],["的softmax变换",{"5":{"87":2,"152":2}}],["的softmax梯度矩阵",{"5":{"94":1}}],["的量级",{"5":{"94":1}}],["的特征方向",{"5":{"135":2}}],["的特征向量与",{"5":{"149":1}}],["的特征向量一致",{"5":{"149":2}}],["的特征向量",{"5":{"45":1,"50":1}}],["的特征值分布往往具有长尾特性",{"5":{"149":2}}],["的特征值分解",{"5":{"87":1,"149":1,"152":1}}],["的特征值小于1",{"5":{"144":2}}],["的特征值之间的关系来理解",{"5":{"142":1}}],["的特征值特征值较小的方向对应",{"5":{"142":1}}],["的特征值",{"5":{"136":1,"142":1,"143":2}}],["的特征值不会太小",{"5":{"135":2}}],["的特征值并要求模小于",{"5":{"135":2}}],["的特征值集合",{"5":{"135":2}}],["的特征值为",{"5":{"134":2,"135":2,"148":1}}],["的特征值接近1",{"5":{"41":1}}],["的特征值决定",{"5":{"50":1,"97":1,"134":2}}],["的特征值的平方根",{"5":{"50":1}}],["的特征值的最大模",{"5":{"87":1,"152":1}}],["的特征",{"2":{"135":1},"5":{"92":2,"135":1,"153":2}}],["的特征表示",{"5":{"94":2}}],["的特例",{"5":{"95":2,"135":2}}],["的特性",{"5":{"96":2,"135":2}}],["的投影矩阵",{"5":{"95":1,"97":2}}],["的非主特征值很小",{"5":{"41":1}}],["的非负性保证了输出概率的非负性",{"5":{"96":1}}],["的自注意力层中",{"5":{"91":2}}],["的自注意力机制中",{"5":{"97":2}}],["的性能对温度参数和负样本选择非常敏感",{"5":{"97":2}}],["的鞍点",{"5":{"97":1}}],["的比值",{"5":{"97":1,"137":1}}],["神经网络输出一个概率向量",{"5":{"140":2}}],["神经网络参数化",{"5":{"140":2}}],["神经网络作为函数逼近器",{"5":{"140":2}}],["神经网络作为复合函数",{"5":{"45":2}}],["神经网络训练中极限环的出现场景",{"5":{"136":2}}],["神经网络训练中的不动点类型",{"5":{"134":2}}],["神经网络训练中的稳定性",{"5":{"134":2}}],["神经网络层",{"5":{"46":2}}],["神经网络可以表示为各层映射的复合",{"5":{"45":2}}],["神经网络的概率参数化",{"2":{"140":1},"5":{"140":1}}],["神经网络的前向传播",{"5":{"135":2}}],["神经网络的信息容量定义为在满足信息约束的条件下",{"5":{"133":2}}],["神经网络的矩阵形式<",{"5":{"21":1,"131":1}}],["神经网络的矩阵形式",{"0":{"46":1},"4":{"46":1},"5":{"21":4,"46":1,"131":4}}],["神经网络的表达能力与其参数矩阵的有效秩密切相关",{"5":{"41":2}}],["神经网络的目标通常是最大化输入不同部分之间的互信息",{"5":{"71":2}}],["神经网络的训练目标是最小化某个损失函数",{"5":{"44":2}}],["神经网络的激活值等",{"5":{"96":2}}],["神经网络的层数等来衡量",{"5":{"51":2}}],["神经网络之所以能够拟合任意复杂的函数模式",{"5":{"71":2}}],["神经网络之所以被称为",{"5":{"71":2}}],["神经网络通用逼近能力的核心定理表明",{"5":{"71":2}}],["神经网络通过编码器和分别编码和",{"5":{"71":1}}],["神经网络通过编码器",{"5":{"71":1}}],["神经网络通常具有非凸的损失景观",{"5":{"51":2}}],["神经网络",{"5":{"47":2,"135":2}}],["神经网络本质上是一个嵌套的复合函数",{"5":{"48":2}}],["神经网络定义了一个参数化的概率分布",{"5":{"61":2}}],["神经元要么被完全保留",{"5":{"145":2}}],["神经元级并行",{"5":{"45":1}}],["神经元级",{"5":{"45":2}}],["神经元的数学模型<",{"5":{"21":1,"131":1}}],["神经元的数学模型",{"0":{"47":1},"4":{"47":1},"5":{"21":4,"47":1,"131":4}}],["神经元的几何解释",{"2":{"47":1},"5":{"47":1}}],["神经元的几何分离能力",{"5":{"47":2}}],["神经元的特征空间映射",{"5":{"47":2}}],["神经元的矩阵表示使得批量处理成为可能",{"5":{"47":2}}],["神经元被激活并通过轴突向其他神经元传递信号",{"5":{"47":2}}],["神经元可以理解为将输入向量从原始输入空间映射到新的特征空间的变换",{"5":{"47":1}}],["神经元模型的矩阵表示",{"2":{"47":1},"5":{"47":1}}],["神经元模型不仅可以从几何角度理解",{"5":{"47":2}}],["神经元是神经网络的基本计算单元",{"5":{"47":2}}],["神经元",{"5":{"47":1}}],["神经架构搜索",{"5":{"41":2}}],["神经架构搜索和语言模型微调中有着应用",{"5":{"96":2}}],["神经生理学家沃伦",{"5":{"47":2}}],["神经活动中内在思想的逻辑演算",{"5":{"47":2}}],["7倍",{"5":{"148":2}}],["7表明",{"5":{"145":2}}],["7",{"0":{"151":1,"157":1,"158":1,"159":1},"2":{"44":1,"45":1,"47":1,"50":1,"51":1,"61":1,"86":1,"87":2,"88":1,"89":1,"90":1,"92":1,"93":1,"94":1,"95":1,"96":1,"101":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"139":1,"143":1,"144":1,"145":1,"148":1,"151":14,"152":2,"153":1,"157":3,"158":6,"159":8},"4":{"105":1,"106":1,"107":1,"108":1,"151":1,"157":1,"158":1,"159":1},"5":{"41":2,"44":5,"45":5,"47":5,"50":1,"51":1,"61":1,"71":2,"86":1,"87":2,"88":1,"89":1,"90":1,"92":1,"93":1,"94":1,"95":1,"96":1,"101":3,"131":17,"132":9,"133":7,"134":7,"135":7,"136":11,"137":7,"139":3,"140":4,"143":3,"144":1,"145":4,"146":2,"148":1,"151":19,"152":2,"153":1,"157":4,"158":7,"159":9}}],["754",{"5":{"61":2}}],["6表明",{"5":{"145":2}}],["60",{"5":{"138":2}}],["64",{"5":{"134":2}}],["6",{"0":{"86":1,"88":1,"89":1,"90":1,"91":1,"97":1,"152":1},"2":{"44":1,"45":1,"47":1,"50":1,"51":1,"61":1,"70":1,"71":1,"86":14,"87":1,"88":11,"89":8,"90":8,"91":7,"92":1,"93":1,"94":1,"95":1,"96":1,"97":5,"101":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"139":1,"143":1,"144":1,"145":1,"146":1,"148":1,"149":1,"151":1,"152":25,"153":1,"158":1,"159":1},"4":{"86":1,"88":1,"89":1,"90":1,"91":1,"97":1,"104":1,"152":1},"5":{"41":4,"44":7,"45":5,"47":5,"50":1,"51":1,"61":1,"70":1,"71":3,"86":19,"87":1,"88":12,"89":9,"90":9,"91":16,"92":1,"93":1,"94":1,"95":1,"96":1,"97":30,"101":3,"131":26,"132":9,"133":9,"134":9,"135":9,"136":9,"137":9,"139":5,"140":4,"143":3,"144":1,"145":5,"146":3,"148":1,"149":1,"151":1,"152":26,"153":1,"158":1,"159":1}}],["6本节小结",{"2":{"41":1},"5":{"41":1}}],["5到0的变化量",{"5":{"70":2}}],["5的变化量等于0",{"5":{"70":2}}],["5的输入点集",{"5":{"47":2}}],["5",{"0":{"86":1,"87":2,"92":1,"93":1,"94":1,"95":1,"101":1,"150":1,"151":1,"152":1,"153":2},"2":{"41":1,"44":1,"45":1,"47":1,"50":1,"51":1,"61":1,"70":1,"71":1,"86":14,"87":49,"88":1,"89":1,"90":1,"91":1,"92":26,"93":20,"94":13,"95":15,"96":1,"97":1,"101":8,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"139":1,"140":1,"142":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":4,"151":14,"152":25,"153":52,"154":1,"155":1,"156":1,"158":1,"159":1},"4":{"86":1,"87":2,"92":1,"93":1,"94":1,"95":1,"101":1,"102":1,"103":2,"104":1,"105":1,"150":1,"151":1,"152":1,"153":2},"5":{"41":9,"44":7,"45":5,"47":5,"50":1,"51":1,"61":3,"69":2,"70":1,"71":3,"86":19,"87":51,"88":1,"89":1,"90":1,"91":1,"92":27,"93":27,"94":14,"95":22,"96":1,"97":3,"101":45,"131":38,"132":13,"133":13,"134":9,"135":9,"136":11,"137":11,"139":7,"140":5,"142":1,"143":2,"144":1,"145":6,"146":3,"147":1,"148":1,"149":1,"150":5,"151":19,"152":26,"153":54,"154":1,"155":1,"156":1,"158":1,"159":1}}],["5附近时变化最快",{"5":{"42":2}}],["5为中心",{"5":{"42":2}}],["5x",{"5":{"42":2}}],["54",{"5":{"91":2}}],["58",{"5":{"91":2}}],["4倍",{"5":{"148":2}}],["4倍的加速",{"5":{"86":2,"151":2}}],["49152",{"5":{"46":4}}],["4n",{"5":{"46":4}}],["4的递归形式可得",{"5":{"46":2}}],["4节",{"5":{"45":2,"101":4}}],["4节详细讨论",{"5":{"45":2}}],["4节中",{"5":{"45":2}}],["4",{"0":{"44":1,"51":1,"61":1,"69":2,"70":1,"88":1,"92":1,"97":1,"101":1,"150":1},"2":{"41":1,"44":10,"45":1,"47":1,"50":1,"51":16,"61":8,"69":9,"70":7,"71":1,"86":1,"87":1,"88":11,"89":1,"90":1,"91":1,"92":26,"93":1,"94":1,"95":1,"96":1,"97":4,"101":8,"132":1,"133":1,"134":1,"135":1,"137":1,"139":1,"140":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":5,"151":1,"152":1,"154":1,"155":1,"156":1,"158":1,"159":1},"4":{"44":1,"51":1,"61":1,"69":2,"70":1,"88":1,"92":1,"97":1,"101":1,"102":1,"150":1},"5":{"21":5,"41":7,"42":4,"44":69,"45":5,"46":4,"47":7,"50":1,"51":17,"61":39,"69":53,"70":18,"71":3,"86":1,"87":1,"88":15,"89":1,"90":1,"91":1,"92":27,"93":1,"94":1,"95":1,"96":1,"97":27,"101":47,"131":44,"132":13,"133":15,"134":9,"135":9,"136":8,"137":11,"138":2,"139":7,"140":9,"142":3,"143":1,"144":1,"145":5,"146":7,"147":1,"148":3,"149":1,"150":6,"151":1,"152":1,"154":1,"155":1,"156":1,"157":2,"158":1,"159":1}}],["40",{"5":{"42":2}}],["41",{"5":{"42":2}}],["42",{"5":{"42":2}}],["第十四章",{"2":{"131":1},"5":{"131":1}}],["第十三章",{"2":{"131":1},"5":{"131":1}}],["第十二章",{"2":{"131":1},"5":{"131":1}}],["第十一章",{"2":{"131":1},"5":{"131":1}}],["第十章",{"2":{"131":1},"5":{"131":1}}],["第九章",{"2":{"131":1},"5":{"131":1}}],["第八章",{"2":{"131":1},"5":{"131":1}}],["第七章",{"2":{"131":1},"5":{"131":1}}],["第14章",{"4":{"132":1,"133":1,"137":1},"5":{"131":6}}],["第13章",{"4":{"134":1,"135":1,"136":1},"5":{"131":6}}],["第12章",{"4":{"138":1,"139":1,"140":1},"5":{"131":6}}],["第11章",{"4":{"141":1,"142":1,"143":1},"5":{"131":6}}],["第10章",{"4":{"144":1,"145":1,"146":1},"5":{"131":6}}],["第1章",{"4":{"48":1,"50":1,"96":1},"5":{"21":6,"131":6}}],["第9章",{"4":{"112":1,"113":1,"114":1,"147":1,"148":1,"149":1},"5":{"131":6}}],["第8章",{"4":{"109":1,"110":1,"111":1,"154":1,"155":1,"156":1},"5":{"131":6}}],["第7章",{"4":{"106":1,"107":1,"108":1,"157":1,"158":1,"159":1},"5":{"131":6}}],["第个神经元的输出被置零",{"5":{"145":1}}],["第个神经元的输出被保留",{"5":{"145":1}}],["第个token的分布仅依赖于其前面的token",{"5":{"101":1}}],["第个样本的第个输出",{"5":{"45":1}}],["第",{"5":{"41":1,"44":7,"45":1,"46":1,"89":1,"90":1,"92":1,"101":1,"135":4,"145":2,"148":1,"150":2,"153":1,"159":2}}],["第2",{"5":{"45":2}}],["第2章",{"4":{"44":1,"45":1,"46":1,"47":1},"5":{"21":8,"131":8}}],["第四",{"5":{"44":2,"45":2,"47":2,"48":2,"50":2,"69":2,"70":2,"88":2,"91":2}}],["第四章",{"2":{"131":1},"5":{"131":1}}],["第四章损失函数",{"5":{"71":2}}],["第四章详细讨论的交叉熵损失正是衡量两个概率分布",{"5":{"71":1}}],["第四章详细讨论的交叉熵损失",{"5":{"71":1}}],["第四项是纯粹的位置",{"5":{"91":1}}],["第四项",{"5":{"91":3}}],["第三阶段",{"5":{"154":2}}],["第三阶段是值聚合",{"5":{"95":2}}],["第三",{"5":{"41":4,"44":2,"45":2,"46":2,"47":2,"48":4,"50":8,"51":2,"61":4,"69":4,"70":4,"71":2,"87":4,"88":2,"89":4,"90":4,"91":6,"146":2,"148":2,"150":2,"152":4}}],["第三章",{"2":{"21":1,"131":1},"5":{"21":1,"131":1}}],["第三是对称性的缺失",{"5":{"87":2,"152":2}}],["第三是平移不变性",{"5":{"87":2,"152":2}}],["第三是分辨率对偶",{"5":{"90":2}}],["第三项为对角矩阵",{"5":{"44":2}}],["第三项是位置与内容之间的交叉项",{"5":{"91":1}}],["第三项",{"5":{"91":1}}],["第三步是矩阵乘法与值聚合",{"5":{"95":2}}],["第二阶段",{"5":{"154":2}}],["第二阶段是缩放与归一化",{"5":{"95":2}}],["第二部分是",{"5":{"154":2}}],["第二部分是kl散度",{"5":{"61":2}}],["第二部分",{"5":{"154":4}}],["第二层输入的期望为",{"5":{"145":2}}],["第二层输出为1",{"5":{"46":4}}],["第二层输出为0",{"5":{"46":4}}],["第二层",{"5":{"46":2}}],["第二层有一个神经元",{"5":{"46":2}}],["第二层的输入为",{"5":{"93":2}}],["第二章",{"2":{"21":1,"131":1},"5":{"21":1,"131":1}}],["第二",{"5":{"44":2,"45":2,"46":2,"47":2,"51":2,"61":2,"69":2,"70":2,"87":4,"88":4,"89":2,"90":2,"91":6,"92":2,"96":2,"152":4,"153":2}}],["第二项是score",{"5":{"159":2}}],["第二项是变分后验与先验之间的kl散度",{"5":{"145":2}}],["第二项是模型本身的描述长度",{"5":{"137":2}}],["第二项是后验近似与先验的",{"5":{"137":2}}],["第二项是编码分布与边际分布的",{"5":{"133":2}}],["第二项是信息保留的收益",{"5":{"133":2}}],["第二项是内容与位置之间的交叉项",{"5":{"91":1}}],["第二项惩罚相似位置的重复选择",{"5":{"132":2}}],["第二项中",{"5":{"44":2}}],["第二项",{"5":{"91":1}}],["第二项最小化输出与输入的冗余",{"5":{"87":2,"152":2}}],["第二个神经元学习",{"5":{"46":2}}],["第二个特征向量",{"5":{"87":2,"152":2}}],["第二个向量取共轭",{"5":{"90":2}}],["第二是重缩放效应",{"5":{"144":2}}],["第二是行随机性",{"5":{"87":2,"152":2}}],["第二是数据依赖性",{"5":{"87":2,"152":2}}],["第二种情况给出",{"5":{"91":2}}],["第二种是one",{"5":{"87":2,"152":2}}],["第二种是掩码同时作用于位置编码",{"5":{"91":2}}],["第二步是缩放与softmax归一化",{"5":{"95":2}}],["第一阶段",{"5":{"154":2}}],["第一阶段是查询",{"5":{"95":2}}],["第一部分大于第二部分",{"5":{"154":2}}],["第一部分是",{"5":{"154":2}}],["第一部分是真实分布",{"5":{"61":2}}],["第一部分",{"5":{"154":4}}],["第一层输出为",{"5":{"46":8}}],["第一层神经元2",{"5":{"46":2}}],["第一层神经元1",{"5":{"46":2}}],["第一层有两个神经元",{"5":{"46":2}}],["第一层学习到某种",{"5":{"92":2,"153":2}}],["第一层的输出是第二层的输入",{"5":{"93":2}}],["第一章",{"2":{"21":1,"131":1},"5":{"21":1,"131":1}}],["第一",{"5":{"44":2,"45":2,"46":2,"47":2,"51":2,"61":2,"69":2,"70":2,"87":4,"88":4,"89":2,"90":2,"91":4,"92":2,"96":2,"152":4,"153":2}}],["第一个专家承担几乎所有负载",{"5":{"158":2}}],["第一个神经元学习",{"5":{"46":2}}],["第一个特征向量",{"5":{"87":2,"152":2}}],["第一个奇异值远大于其他",{"5":{"87":2,"152":2}}],["第一项是数据对数似然的期望",{"5":{"145":2}}],["第一项是期望负对数似然",{"5":{"133":2}}],["第一项是压缩成本",{"5":{"133":2}}],["第一项是原始内容之间的注意力分数",{"5":{"91":1}}],["第一项为",{"5":{"44":2}}],["第一项最大化输出与目标的相关性",{"5":{"87":2,"152":2}}],["第一项",{"5":{"91":3}}],["第一是收缩效应",{"5":{"144":2}}],["第一是非负性",{"5":{"87":2,"152":2}}],["第一是稀疏性",{"5":{"87":2,"152":2}}],["第一种情况直接给出",{"5":{"91":2}}],["第一种是均匀注意力",{"5":{"87":2,"152":2}}],["第一种是掩码作用于注意力分数",{"5":{"91":2}}],["第一步是矩阵乘法",{"5":{"95":2}}],["第3章",{"4":{"41":1,"42":1,"71":1},"5":{"21":6,"131":6}}],["第层的净输入为",{"5":{"148":1}}],["第层的输入为",{"5":{"46":1}}],["第层的计算为",{"5":{"41":1}}],["第层的计算定义为",{"5":{"44":1}}],["第层的误差信号矩阵为",{"5":{"44":1}}],["第层的感受野大小约为",{"5":{"92":1,"153":1}}],["第层误差信号",{"5":{"44":1}}],["第层权重的梯度为",{"5":{"44":2}}],["第层偏置的梯度为",{"5":{"44":2}}],["第五",{"5":{"44":2,"91":2}}],["第五章",{"2":{"131":1},"5":{"131":1}}],["第4章",{"4":{"51":1,"61":1,"69":1,"70":1,"97":1,"101":1},"5":{"131":12}}],["第5章",{"4":{"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"102":1,"103":1,"104":1,"105":1,"150":1,"151":1,"152":1,"153":1},"5":{"131":14}}],["第6章",{"4":{"88":1,"89":1,"90":1,"91":1},"5":{"131":8}}],["第六章位置编码",{"5":{"71":2}}],["第六章",{"2":{"131":1},"5":{"131":1}}],["第行第列元素为",{"5":{"89":1}}],["第次谐波的角频率为​",{"5":{"90":1}}],["3规模的模型",{"5":{"45":2}}],["3",{"0":{"41":2,"42":1,"45":1,"48":1,"70":1,"71":1,"89":1,"93":1,"132":1,"136":1,"138":1,"141":1,"144":1,"147":1,"154":1,"157":1},"2":{"41":13,"42":4,"44":1,"45":8,"46":1,"47":1,"48":4,"50":1,"51":1,"61":1,"69":1,"70":7,"71":7,"86":2,"87":1,"88":1,"89":8,"90":1,"91":1,"92":1,"93":20,"94":1,"95":1,"96":1,"101":1,"132":10,"133":1,"134":1,"135":1,"136":8,"137":1,"138":4,"139":1,"140":1,"141":3,"144":7,"145":1,"146":1,"147":6,"148":1,"149":1,"150":1,"151":2,"152":1,"153":1,"154":6,"155":1,"156":1,"157":4,"158":1,"159":1},"4":{"41":2,"42":1,"45":1,"48":1,"70":1,"71":1,"89":1,"93":1,"108":1,"111":1,"114":1,"132":1,"136":1,"138":1,"141":1,"144":1,"147":1,"154":1,"157":1},"5":{"21":27,"41":73,"42":59,"44":11,"45":65,"46":5,"47":11,"48":5,"50":1,"51":1,"61":3,"69":3,"70":16,"71":12,"86":2,"87":1,"88":4,"89":9,"90":1,"91":1,"92":1,"93":27,"94":1,"95":1,"96":1,"97":2,"101":5,"131":82,"132":173,"133":13,"134":9,"135":9,"136":133,"137":13,"138":5,"139":7,"140":19,"141":4,"142":2,"143":8,"144":12,"145":4,"146":9,"147":9,"148":5,"149":1,"150":1,"151":2,"152":1,"153":1,"154":11,"155":1,"156":1,"157":5,"158":1,"159":1}}],["3节中",{"5":{"44":2,"69":2}}],["3节中证明",{"5":{"44":2}}],["30",{"5":{"42":2,"61":2,"69":2,"142":2}}],["31",{"5":{"42":2,"61":2,"69":2}}],["32",{"5":{"42":2,"69":2}}],["33",{"5":{"42":2}}],["34",{"5":{"42":2}}],["35",{"5":{"42":2}}],["363em",{"2":{"155":1,"156":2}}],["36",{"5":{"42":2,"141":2}}],["37",{"5":{"42":2,"141":2}}],["38",{"5":{"42":2,"141":2}}],["39",{"5":{"42":2}}],["3进一步引入了异步执行和tensor",{"5":{"86":2,"151":2}}],["3针对hopper架构gpu",{"5":{"86":2,"151":2}}],["3采用的标准rope实现",{"5":{"88":2}}],["3中",{"5":{"88":2}}],["3则增加了头数到96个",{"5":{"93":2}}],["3的直接应用",{"5":{"44":2}}],["3的96个头",{"5":{"93":2}}],["2之间",{"5":{"154":2}}],["2或0",{"5":{"154":2}}],["2的分析",{"5":{"136":2}}],["2的主要改进包括",{"5":{"86":2,"151":2}}],["2节的推导",{"5":{"101":2}}],["2节中二元交叉熵的形式完全一致",{"5":{"101":2}}],["2节中",{"5":{"45":2,"69":6,"144":2}}],["2n",{"5":{"46":4,"137":2,"143":2}}],["2",{"0":{"42":1,"44":1,"45":1,"46":2,"47":1,"61":1,"90":1,"94":1,"96":1,"133":1,"135":1,"139":1,"142":1,"145":1,"148":1,"155":1,"158":1},"2":{"41":1,"42":4,"44":10,"45":8,"46":7,"47":8,"48":1,"50":1,"51":1,"61":8,"69":1,"70":1,"71":1,"86":1,"87":1,"88":1,"89":1,"90":8,"91":1,"92":1,"93":1,"94":13,"95":1,"96":9,"97":1,"101":1,"132":1,"133":9,"134":1,"135":9,"136":1,"137":1,"138":1,"139":9,"140":1,"142":3,"143":1,"145":9,"146":1,"147":1,"148":10,"149":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":7,"156":2,"157":1,"158":7,"159":1},"4":{"42":1,"44":1,"45":1,"46":2,"47":1,"61":1,"90":1,"94":1,"96":1,"107":1,"110":1,"113":1,"133":1,"135":1,"139":1,"142":1,"145":1,"148":1,"155":1,"158":1},"5":{"21":31,"41":9,"42":97,"44":27,"45":17,"46":77,"47":19,"48":1,"50":1,"51":1,"61":39,"69":9,"70":5,"71":7,"86":1,"87":3,"88":6,"89":1,"90":9,"91":1,"92":1,"93":3,"94":14,"95":1,"96":10,"97":7,"101":9,"131":86,"132":33,"133":140,"134":9,"135":126,"136":15,"137":15,"138":1,"139":55,"140":21,"141":2,"142":8,"143":17,"144":4,"145":18,"146":9,"147":5,"148":17,"149":3,"150":1,"151":1,"152":3,"153":1,"154":5,"155":7,"156":1,"157":3,"158":8,"159":1}}],["25",{"2":{"92":1,"153":1},"5":{"41":4,"42":6,"61":2,"92":1,"132":2,"141":2,"142":2,"144":2,"145":1,"148":2,"153":1}}],["27",{"5":{"42":2,"144":2}}],["29",{"5":{"42":2,"61":2,"69":2,"142":2,"144":2}}],["2022年",{"5":{"138":2}}],["2021年su等人发表rope论文后短短几年内",{"5":{"88":2}}],["20",{"2":{"87":1,"92":1,"152":1,"153":1},"5":{"42":2,"46":2,"87":1,"92":1,"97":2,"132":2,"133":2,"135":2,"136":2,"138":4,"142":2,"144":2,"152":1,"153":1}}],["21",{"2":{"87":1,"92":1,"152":1,"153":1},"5":{"42":2,"46":2,"87":1,"92":1,"132":2,"135":2,"142":2,"143":2,"144":4,"145":2,"152":1,"153":1}}],["22",{"2":{"87":1,"92":1,"152":1,"153":1},"5":{"42":2,"46":2,"61":2,"87":1,"92":1,"97":2,"132":2,"135":2,"141":2,"142":2,"143":2,"144":2,"152":1,"153":1}}],["23",{"2":{"87":1,"92":1,"152":1,"153":1},"5":{"42":2,"46":2,"61":2,"87":1,"92":1,"132":2,"141":2,"144":2,"152":1,"153":1}}],["24",{"2":{"87":1,"92":1,"152":1,"153":1},"5":{"42":2,"46":2,"61":2,"87":1,"92":1,"132":2,"141":4,"144":2,"152":1,"153":1}}],["26",{"5":{"42":2,"61":2,"132":2,"141":4,"142":2,"144":2}}],["28",{"5":{"42":2,"61":2,"142":2,"144":2}}],["2与flash注意力",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["2和llama",{"5":{"88":2}}],["2采用了rope的变体",{"5":{"88":1}}],["2等现代大语言模型就采用了分组查询注意力配置",{"5":{"93":2}}],["2^t",{"5":{"97":2}}],["1×",{"5":{"157":8}}],["1或0",{"5":{"154":2}}],["1次",{"5":{"146":2}}],["1n",{"5":{"143":2}}],["1e9",{"5":{"132":2}}],["1e",{"5":{"132":2}}],["1损失",{"5":{"70":2}}],["1比预测0",{"5":{"70":2}}],["1的预测误差和0",{"5":{"70":2}}],["1的简化形式",{"5":{"44":2}}],["1个正样本个负样本",{"5":{"69":1}}],["1个正样本",{"5":{"69":3}}],["1到0",{"5":{"95":2,"101":2,"154":2}}],["1到6",{"5":{"96":2}}],["15em",{"2":{"154":1}}],["15",{"2":{"51":1,"86":1,"87":1,"92":1,"93":1,"151":1,"152":1,"153":1},"5":{"46":2,"51":1,"86":1,"87":1,"92":1,"93":1,"97":2,"101":2,"132":6,"133":2,"135":2,"136":4,"137":2,"142":2,"143":4,"144":2,"145":2,"151":1,"152":1,"153":1}}],["1节和4",{"5":{"69":2}}],["1节和2",{"5":{"45":2}}],["1节中定义的单个神经元的数学形式",{"5":{"46":2}}],["1节中看到的",{"5":{"46":2}}],["1节中给出",{"5":{"94":2}}],["1节中引入的缩放因子​​",{"5":{"94":1}}],["1节中引入的缩放因子",{"5":{"94":1}}],["1节的scaled",{"5":{"69":2}}],["1节的分析",{"5":{"89":2}}],["13",{"0":{"134":1,"135":1,"136":1},"2":{"50":1,"51":1,"87":1,"92":1,"93":1,"95":1,"134":9,"135":8,"136":7,"152":1,"153":1},"4":{"134":1,"135":1,"136":1},"5":{"41":2,"45":2,"46":2,"50":1,"51":1,"86":2,"87":1,"92":1,"93":1,"95":1,"101":2,"131":12,"132":6,"133":2,"134":94,"135":117,"136":130,"137":2,"140":2,"143":4,"144":2,"145":2,"151":2,"152":1,"153":1}}],["12288",{"5":{"46":4}}],["12",{"0":{"138":1,"139":1,"140":1},"2":{"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"138":3,"139":8,"140":5,"151":1,"152":1,"153":1},"4":{"138":1,"139":1,"140":1},"5":{"41":2,"44":2,"45":2,"46":2,"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"101":2,"131":12,"132":6,"133":4,"134":4,"135":4,"136":6,"137":2,"138":4,"139":47,"140":82,"143":4,"145":2,"151":1,"152":1,"153":1}}],["128个奇异值通常能够达到接近原始编码的性能",{"5":{"89":2}}],["11",{"0":{"141":1,"142":1,"143":1},"2":{"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"141":2,"142":3,"143":5,"151":1,"152":1,"153":1},"4":{"141":1,"142":1,"143":1},"5":{"41":2,"44":2,"45":2,"46":4,"50":1,"51":1,"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"97":2,"101":2,"131":12,"132":6,"133":4,"134":4,"135":4,"136":6,"137":2,"140":4,"141":27,"142":38,"143":60,"145":2,"151":1,"152":1,"153":1}}],["1000",{"5":{"138":2}}],["100",{"5":{"70":2,"138":4}}],["10^",{"5":{"46":2}}],["10",{"0":{"144":1,"145":1,"146":1},"2":{"50":1,"51":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"144":6,"145":8,"146":6,"151":1,"152":1,"153":1},"4":{"144":1,"145":1,"146":1},"5":{"41":2,"44":2,"45":2,"47":2,"50":1,"51":1,"71":2,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"97":2,"101":2,"131":12,"132":6,"133":4,"134":4,"135":4,"136":6,"137":2,"138":2,"140":4,"141":2,"143":2,"144":51,"145":53,"146":41,"151":1,"152":1,"153":1}}],["10步前的位置",{"5":{"92":2,"153":2}}],["1",{"0":{"47":1,"48":1,"50":2,"51":1,"71":1,"91":1,"95":1,"96":1,"134":1,"137":1,"140":1,"143":1,"146":1,"149":1,"156":1,"159":1},"2":{"41":1,"42":1,"44":1,"45":1,"46":1,"47":8,"48":4,"50":27,"51":16,"61":1,"69":1,"70":1,"71":7,"86":1,"87":1,"88":1,"89":1,"91":7,"92":1,"93":1,"94":1,"95":15,"96":9,"97":1,"101":1,"132":1,"133":1,"134":10,"135":1,"136":1,"137":8,"138":1,"139":1,"140":6,"141":1,"142":1,"143":6,"144":1,"145":1,"146":7,"147":1,"148":1,"149":7,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"156":6,"157":1,"158":1,"159":8},"4":{"47":1,"48":1,"50":2,"51":1,"71":1,"91":1,"95":1,"96":1,"106":1,"109":1,"112":1,"134":1,"137":1,"140":1,"143":1,"146":1,"149":1,"156":1,"159":1},"5":{"21":27,"41":11,"42":45,"44":19,"45":9,"46":27,"47":67,"48":9,"50":35,"51":17,"61":35,"69":9,"70":15,"71":40,"86":3,"87":1,"88":5,"89":1,"91":18,"92":1,"93":5,"94":1,"95":28,"96":10,"97":22,"101":7,"131":82,"132":57,"133":25,"134":101,"135":33,"136":23,"137":139,"138":3,"139":11,"140":32,"141":7,"142":1,"143":44,"144":15,"145":8,"146":26,"147":5,"148":3,"149":16,"150":1,"151":3,"152":1,"153":1,"154":5,"155":1,"156":7,"157":1,"158":1,"159":9}}],["14",{"0":{"132":1,"133":1,"137":1},"2":{"51":1,"87":1,"92":1,"93":1,"95":1,"132":9,"133":8,"137":7,"152":1,"153":1},"4":{"132":1,"133":1,"137":1},"5":{"41":2,"51":1,"87":1,"92":1,"93":1,"95":1,"97":2,"101":2,"131":12,"132":166,"133":119,"134":2,"135":4,"136":6,"137":112,"143":4,"144":2,"145":2,"152":1,"153":1}}],["1999",{"5":{"133":2}}],["1986年",{"5":{"44":2}}],["19",{"2":{"87":1,"92":1,"93":1,"152":1,"153":1},"5":{"42":2,"47":8,"48":4,"87":1,"92":1,"93":1,"132":4,"133":2,"135":2,"136":2,"142":4,"144":2,"152":1,"153":1}}],["1958年",{"5":{"47":2}}],["1948",{"5":{"137":2}}],["1948年",{"5":{"61":2}}],["1943年",{"5":{"47":2}}],["18",{"2":{"87":1,"92":1,"93":1,"152":1,"153":1},"5":{"42":2,"46":2,"86":2,"87":1,"92":1,"93":1,"132":4,"133":2,"135":2,"136":2,"137":2,"142":4,"144":2,"151":2,"152":1,"153":1}}],["16",{"2":{"87":1,"92":1,"93":1,"152":1,"153":1},"5":{"46":2,"87":1,"92":1,"93":1,"132":6,"133":2,"135":2,"136":2,"137":2,"142":4,"143":2,"144":2,"145":2,"152":1,"153":1}}],["17",{"2":{"92":1,"93":1,"153":1},"5":{"46":2,"92":1,"93":1,"132":6,"133":2,"135":2,"136":2,"137":2,"144":4,"153":1}}],["1始终是特征值",{"5":{"87":2,"152":2}}],["1矩阵的和来描述",{"5":{"89":2}}],["1傅里叶分析基础",{"2":{"90":1},"5":{"90":1}}],["1^t",{"5":{"97":2}}],["饱和期",{"5":{"134":2}}],["饱和现象的数学定义",{"2":{"41":1},"5":{"41":1}}],["饱和区域可以精确计算",{"5":{"41":2}}],["饱和区域为",{"5":{"41":6}}],["饱和区域",{"5":{"44":2}}],["饱和深度的量化分析",{"5":{"41":2}}],["饱和行为呈现对称分布",{"5":{"41":2}}],["饱和效应的累积放大机制",{"2":{"41":1},"5":{"41":1}}],["饱和效应会逐层累积放大",{"5":{"41":2}}],["饱和效应与权重初始化密切相关",{"5":{"41":2}}],["饱和的网络层叠效应",{"5":{"41":2}}],["饱和状态",{"5":{"95":2}}],["保持激活值和梯度方差",{"5":{"148":1}}],["保持激活方差和梯度方差的平衡",{"5":{"135":2}}],["保持前向传播和反向传播过程中激活值与梯度的方差一致",{"5":{"148":1}}],["保持梯度的方向不变",{"5":{"147":2}}],["保持梯度流稳定性",{"2":{"41":1},"5":{"41":1}}],["保持了输出的归一化性质",{"5":{"159":2}}],["保持了信号的平均强度",{"5":{"145":2}}],["保持了跨不同维度模型的可比性",{"5":{"95":2}}],["保持原始概率分布",{"5":{"140":2}}],["保持输入方差沿网络传播",{"5":{"135":2}}],["保持统一的压缩正则化项",{"5":{"133":2}}],["保持或缓慢增长",{"5":{"133":2}}],["保持模型生成的多样性",{"5":{"101":2}}],["保持完整",{"5":{"88":1}}],["保持维度不变",{"5":{"94":2}}],["保持核心计算不变",{"5":{"95":2}}],["保持在合理的分布范围内",{"5":{"94":1}}],["保持在较大的值",{"5":{"95":1}}],["保存所有层的和",{"5":{"44":1}}],["保存所有层的",{"5":{"44":1}}],["保证回报收敛",{"5":{"156":2}}],["保证",{"5":{"136":2}}],["保证收敛的稳定性",{"5":{"136":2}}],["保证能到达任意精度",{"5":{"136":2}}],["保证信息流动的全局性",{"5":{"132":2}}],["保证网络的表达能力",{"5":{"47":2}}],["保证梯度计算的可操作性",{"5":{"47":2}}],["保证二次可导和光滑性",{"5":{"51":2}}],["保证了求和结果至少为",{"5":{"61":2}}],["保证这是一个有效的概率分布",{"5":{"61":2}}],["保证其可以作为神经网络的输出层进行端到端优化",{"5":{"61":1}}],["保证质量",{"5":{"93":2}}],["保留概率应该使得归一化后的激活值具有适当的方差分布",{"5":{"145":2}}],["保留概率",{"5":{"145":3}}],["保留概率与噪声方差的关系",{"5":{"145":2}}],["保留关于",{"5":{"137":1}}],["保留关于的信息",{"5":{"137":1}}],["保留关于目标的信息",{"5":{"71":1}}],["保留关于目标",{"5":{"71":1}}],["保留信息效率高的层",{"5":{"133":2}}],["保留权衡",{"5":{"133":2}}],["保留相对结构信息",{"5":{"132":2}}],["保留与任务最相关的信息",{"5":{"132":2}}],["保留",{"5":{"132":2,"133":2,"148":2}}],["保留前64",{"5":{"89":2}}],["保留了这一方向的信息",{"5":{"146":2}}],["保留了输入的关键信息",{"5":{"137":2}}],["保留了内容编码和位置编码的相对重要性",{"5":{"91":2}}],["保留了更多的表达能力",{"5":{"93":2}}],["导数范围",{"5":{"148":1}}],["导数范围为",{"5":{"148":2}}],["导数也为零",{"5":{"148":2}}],["导数未定义",{"5":{"148":2}}],["导数",{"5":{"70":2}}],["导数的调制",{"5":{"70":6}}],["导数的值取决于和的乘积",{"5":{"41":1}}],["导数的值取决于",{"5":{"41":1}}],["导数趋近于零",{"5":{"41":4}}],["导数达到最大值而非最小值",{"5":{"41":2}}],["导数达到最大值0",{"5":{"41":2}}],["导数最小值接近0而非0",{"5":{"41":2}}],["导数为常数1",{"5":{"41":2}}],["导数推导与梯度特性",{"0":{"42":1},"4":{"42":1},"5":{"21":4,"42":1,"131":4}}],["导数推导与梯度特性<",{"5":{"21":1,"131":1}}],["导数推导与自化性质",{"2":{"42":1},"5":{"42":1}}],["导数推导与性质分析",{"2":{"42":1},"5":{"42":1}}],["导数可以用函数自身来表示",{"5":{"42":2}}],["导数表达式变得越来越复杂",{"5":{"42":2}}],["导致其他专家几乎不被激活",{"5":{"158":2}}],["导致损失爆炸",{"5":{"158":2}}],["导致损失函数震荡甚至发散",{"5":{"147":2}}],["导致策略梯度估计的方差很大",{"5":{"155":2}}],["导致学习信号失真",{"5":{"154":2}}],["导致累积雅可比矩阵",{"5":{"148":1}}],["导致累积雅可比矩阵的某些奇异值可能为零",{"5":{"148":1}}],["导致网络前端的权重几乎无法更新或更新幅度失控",{"5":{"148":2}}],["导致归一化效果不稳定",{"5":{"146":2}}],["导致深层网络的信号强度衰减",{"5":{"145":2}}],["导致深层网络的训练困难",{"5":{"42":2}}],["导致信号在传播过程中被严重衰减",{"5":{"145":2}}],["导致信息损失",{"5":{"94":2}}],["导致注意力权重集中在少数几个位置上",{"5":{"141":2}}],["导致过拟合",{"5":{"137":2}}],["导致的欠压缩或过压缩问题",{"5":{"133":1}}],["导致有效梯度非常小",{"5":{"70":2}}],["导致训练行为发生质的改变",{"5":{"136":2}}],["导致训练停滞",{"5":{"70":2}}],["导致训练开始时梯度就很小",{"5":{"41":2}}],["导致训练不稳定",{"5":{"87":2,"97":2,"152":2}}],["导致这些层无法学习到有效的特征表示",{"5":{"148":2}}],["导致这些层无法学习",{"5":{"41":2}}],["导致参数更新极其缓慢甚至停止",{"5":{"41":2}}],["导致参数更新幅度过大",{"5":{"41":2}}],["导致梯度的估计方差也更大",{"5":{"149":2}}],["导致梯度不稳定",{"5":{"148":2}}],["导致梯度接近0",{"5":{"70":2}}],["导致梯度爆炸",{"5":{"41":4,"135":2}}],["导致梯度爆炸问题",{"5":{"92":2,"153":2}}],["导致梯度消失",{"5":{"41":6,"44":2,"70":2}}],["导致梯度消失问题",{"5":{"61":2,"92":2,"153":2}}],["导致softmax的输出接近one",{"5":{"41":2}}],["导致表示过度平均",{"5":{"41":2}}],["导致测试集上的实际mse可能与训练mse有所不同",{"5":{"51":2}}],["导致系统性的预测偏差",{"5":{"51":2}}],["导致",{"5":{"61":4,"86":1,"151":1}}],["导致溢出",{"5":{"86":1,"151":1}}],["导致数值计算不稳定",{"5":{"87":2,"152":2}}],["导致内积同时受到两个绝对位置的影响",{"5":{"88":2}}],["导致平移等变性不再严格成立",{"5":{"88":2}}],["导致不同训练集产生截然不同的预测",{"5":{"51":2}}],["导致不同位置的编码变得难以区分",{"5":{"88":2}}],["导致长程依赖难以建模",{"5":{"91":2}}],["导致强凸性系数很小",{"5":{"97":2}}],["导致下溢",{"5":{"97":2}}],["揭示为什么在实际应用中注意力矩阵往往表现出显著的低秩结构",{"5":{"141":2}}],["揭示模型能力随规模增长的数学规律",{"5":{"138":2}}],["揭示数据压缩",{"5":{"137":2}}],["揭示它们在张量维度处理上的本质区别",{"5":{"146":2}}],["揭示它们的内在联系与本质区别",{"5":{"70":2}}],["揭示它们与激活函数特性",{"5":{"41":2}}],["揭示它们与统计学和信息论之间的深层联系",{"5":{"71":2}}],["揭示它为何能够成为连接分类损失",{"5":{"69":2}}],["揭示信息在网络中传递和保持的数学原理",{"5":{"41":2}}],["揭示这些技术如何从根本上改善深层网络的训练动态",{"5":{"41":2}}],["揭示其与条件熵的联系",{"5":{"133":2}}],["揭示其与激活函数导数特性的内在联系",{"5":{"41":2}}],["揭示其与网络架构和初始化策略的关系",{"5":{"41":2}}],["揭示其作为",{"5":{"71":2}}],["揭示其作为最小二乘估计量的数学本质",{"5":{"51":2}}],["揭示其低秩结构的成因和理论含义",{"5":{"87":2,"152":2}}],["揭示了两种架构设计哲学的根本差异",{"5":{"159":2}}],["揭示了两者在结构上的相似性和差异性",{"5":{"93":2}}],["揭示了layer",{"5":{"150":1}}],["揭示了mhc在捕捉多尺度信息方面的优势",{"5":{"150":1}}],["揭示了mse的最优解对应于目标向量在预测空间上的正交投影",{"5":{"51":2}}],["揭示了highway",{"5":{"150":1}}],["揭示了深度学习中梯度噪声的各向异性本质",{"5":{"149":2}}],["揭示了深度网络在信息平面上的训练轨迹",{"5":{"133":2}}],["揭示了谱范数",{"5":{"144":2}}],["揭示了其本质是学习精度的量化跃升",{"5":{"138":1}}],["揭示了优化过程的信息论本质",{"5":{"137":2}}],["揭示了条件信息与联合信息之间的关系",{"5":{"137":2}}],["揭示了梯度下降在不同条件下的收敛行为差异",{"5":{"136":2}}],["揭示了网络深度",{"5":{"135":2}}],["揭示了信息如何从输出层反向传播到输入层和参数",{"5":{"135":2}}],["揭示了动量",{"5":{"134":2}}],["揭示了任务间的梯度冲突机制",{"5":{"133":2}}],["揭示了为什么深层网络能够学习有效的特征表示",{"5":{"133":2}}],["揭示了温度参数在控制探索",{"5":{"132":2}}],["揭示了多头注意力的信息融合机制",{"5":{"132":2}}],["揭示了选择多样化信息源的数学原理",{"5":{"132":2}}],["揭示了",{"5":{"70":2,"132":4}}],["揭示了激活函数导数特性与饱和现象的内在联系",{"5":{"41":2}}],["揭示了大语言模型中非线性变换的数学本质",{"5":{"71":2}}],["揭示了编码矩阵的秩不超过嵌入维度​这一重要性质",{"5":{"89":1}}],["揭示了编码矩阵的秩不超过嵌入维度",{"5":{"89":1}}],["揭示了编码向量可以视为不同频率正弦波的叠加",{"5":{"90":2}}],["揭示了点积​作为相对位置的函数",{"5":{"90":1}}],["揭示了点积",{"5":{"90":1}}],["揭示了模型捕获的依赖类型",{"5":{"92":2,"153":2}}],["揭示了传统循环结构在处理长距离依赖时的根本性困难",{"5":{"92":2,"153":2}}],["揭示了局部注意力与全局注意力的区别",{"5":{"92":2,"153":2}}],["揭示了一个重要的数学事实",{"5":{"93":2}}],["揭示了不同损失函数在全局最优性",{"5":{"97":2}}],["斜率为",{"5":{"138":2}}],["斜率趋近于零",{"5":{"41":2}}],["斜率同样趋近于零",{"5":{"41":2}}],["斜率的最大值出现在处",{"5":{"42":1}}],["斜率的最大值出现在",{"5":{"42":1}}],["僵尸状态",{"5":{"41":2}}],["循环学习率",{"5":{"136":2}}],["循环层",{"5":{"46":2}}],["循环神经网络中的dropout",{"2":{"145":1},"5":{"145":1}}],["循环神经网络",{"5":{"41":2,"91":2}}],["循环神经网络的梯度流问题",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["残差网络可以看作是对残差函数空间",{"5":{"150":2}}],["残差网络",{"5":{"147":2}}],["残差网络的可训练性",{"5":{"135":2}}],["残差块的输出为",{"5":{"135":2}}],["残差块的",{"5":{"135":2}}],["残差连接通过显式地让网络学习",{"5":{"150":2}}],["残差连接通过引入",{"5":{"150":2}}],["残差连接等技术来维持训练过程中的梯度稳定",{"5":{"148":2}}],["残差连接确保梯度可以直接从深层流向浅层",{"5":{"135":2}}],["残差连接确保信息可以跨层无损传递",{"5":{"132":2}}],["残差连接确保了单位矩阵的存在",{"5":{"92":1,"153":1}}],["残差连接确保了单位矩阵",{"5":{"92":1,"153":1}}],["残差连接改善了梯度信息流",{"5":{"132":2}}],["残差连接提供了信息保护机制",{"5":{"132":1}}],["残差连接提供了一条",{"5":{"92":2,"153":2}}],["残差连接提供了一条梯度传播的",{"5":{"92":2,"153":2}}],["残差连接和层归一化等组件都从信息论角度具有重要意义",{"5":{"132":2}}],["残差连接与归一化<",{"5":{"131":1}}],["残差连接与归一化",{"0":{"150":1},"4":{"102":1,"150":1},"5":{"131":4,"150":1}}],["残差连接与梯度流动",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["残差连接",{"5":{"41":6,"44":1,"92":2,"132":1,"147":2,"150":2,"153":2}}],["残差连接定义为",{"5":{"41":2}}],["残差连接的引入将变换空间扩展为",{"5":{"150":2}}],["残差连接的核心思想源于一个看似简单却极具洞察力的观察",{"5":{"150":2}}],["残差连接的核心数学保证是",{"5":{"41":2}}],["残差连接的稳定化效应",{"5":{"135":2}}],["残差连接的",{"2":{"135":1},"5":{"135":3}}],["残差连接的信息保护机制是深层",{"5":{"132":2}}],["残差连接的信息保护作用和层归一化的信息影响",{"5":{"132":2}}],["残差连接的信息保护",{"5":{"132":2}}],["残差连接的数学原理",{"2":{"150":1},"5":{"150":1}}],["残差连接的数学作用",{"2":{"41":1},"5":{"41":1}}],["残差连接的数学分析展示了它如何通过恒等路径保证梯度流的稳定",{"5":{"41":2}}],["残差连接是向量加法的典型应用",{"5":{"50":2}}],["残差连接保留的原始信息",{"5":{"92":2,"153":2}}],["残差的梯度流分析",{"5":{"41":2}}],["残差的梯度下界",{"5":{"41":2}}],["残差路径的恒等保证",{"5":{"41":2}}],["残差路径的梯度可以直接通过恒等连接传递",{"5":{"41":2}}],["残差部分",{"5":{"51":2}}],["残差",{"5":{"89":2,"150":2}}],["全开",{"5":{"159":2}}],["全精度",{"5":{"157":4}}],["全批量梯度下降的计算目标是计算该损失函数对参数的精确梯度",{"5":{"149":2}}],["全上下文依赖",{"5":{"140":2}}],["全局裁剪和分层裁剪等多种变体",{"5":{"147":2}}],["全局裁剪",{"5":{"147":2}}],["全局裁剪与分层裁剪",{"2":{"147":1},"5":{"147":1}}],["全局结构",{"5":{"143":1}}],["全局最优值",{"5":{"136":2}}],["全局最小值同样是",{"5":{"134":2}}],["全局收敛",{"5":{"136":2}}],["全局信息访问是注意力机制区别于传统序列模型的关键特征",{"5":{"132":2}}],["全局信息访问",{"5":{"132":2}}],["全局信息访问意味着信息可以在任意位置对之间直接流动",{"5":{"132":2}}],["全局上下文",{"5":{"101":2}}],["全局梯度裁剪",{"5":{"41":2}}],["全局梯度裁剪将梯度的范数裁剪到一个固定的上界",{"5":{"41":2,"97":2}}],["全局",{"5":{"86":2,"92":2,"151":2,"153":2}}],["全局位置",{"5":{"86":2,"151":2}}],["全局稀疏",{"5":{"87":2,"152":2}}],["全局注意力",{"5":{"92":2,"153":2}}],["全局注意力指的是注意力权重分布在整个序列上",{"5":{"92":2,"153":2}}],["全局注意力满足",{"5":{"92":2,"153":2}}],["全局注意力适合捕获跨越长距离的语义关联",{"5":{"92":2,"153":2}}],["全局函数",{"5":{"92":2,"153":2}}],["全连接网络中的dropout",{"2":{"145":1},"5":{"145":1}}],["全连接",{"5":{"46":2}}],["全连接层之前",{"5":{"146":2}}],["全连接层可以统一表示为单一的矩阵乘法",{"5":{"46":2}}],["全连接层的矩阵表示是多个神经元并行计算的紧凑写法",{"5":{"46":2}}],["全连接层的矩阵运算等价于对每个输出神经元进行独立的仿射变换计算",{"5":{"46":2}}],["全连接层的计算展开",{"5":{"46":2}}],["全连接层的数学定义为",{"5":{"46":2}}],["全连接层是神经网络中最基本的层类型",{"5":{"46":2}}],["全连接层",{"5":{"46":4,"61":2}}],["全连接层就是典型的线性变换加上非线性激活函数的组合",{"5":{"50":2}}],["全连接注意力模式等",{"5":{"86":2,"151":2}}],["全连接结构与信息直达",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["全连接假设",{"5":{"92":2,"153":2}}],["全覆盖",{"5":{"88":2}}],["裁剪的约束效应",{"5":{"147":2}}],["裁剪可以被视为一种",{"5":{"147":2}}],["裁剪可能改变优化轨迹",{"5":{"147":2}}],["裁剪不影响最终收敛到的极小值位置",{"5":{"147":2}}],["裁剪不发挥作用",{"5":{"147":2}}],["裁剪限制了参数探索的范围",{"5":{"147":2}}],["裁剪过于激进",{"5":{"147":2}}],["裁剪修改优化过程",{"5":{"147":2}}],["裁剪作为隐式正则化",{"5":{"147":2}}],["裁剪边界",{"5":{"147":2}}],["裁剪强制优化器以恒定的步长向梯度方向移动",{"5":{"147":2}}],["裁剪应该被视为稳定化技术",{"5":{"147":2}}],["裁剪操作将映射",{"5":{"147":2}}],["裁剪操作实际上不会发生",{"5":{"147":2}}],["裁剪操作引入了非光滑性",{"5":{"147":2}}],["裁剪操作等价于将梯度向量投影到半径为",{"5":{"147":2}}],["裁剪操作确保当",{"5":{"101":1}}],["裁剪操作确保当偏离1太远时",{"5":{"101":1}}],["裁剪对泛化的影响取决于具体场景",{"5":{"147":2}}],["裁剪对泛化的影响",{"2":{"147":1},"5":{"147":1}}],["裁剪对收敛性的影响",{"2":{"147":1},"5":{"147":1}}],["裁剪动力学的相图分析",{"2":{"147":1},"5":{"147":1}}],["裁剪与其他技术的组合",{"2":{"147":1},"5":{"147":1}}],["裁剪与平滑",{"2":{"147":1},"5":{"147":1}}],["裁剪与正则化的交互",{"5":{"144":2}}],["裁剪模式",{"5":{"136":2}}],["裁剪",{"5":{"101":2,"147":2}}],["裁剪阈值可能需要相应调整",{"5":{"147":2}}],["裁剪阈值通常设置为梯度的第99百分位数或固定值",{"5":{"136":2}}],["裁剪阈值的选择",{"5":{"41":2}}],["裁剪阈值的选择是一个超参数调优问题",{"5":{"41":1}}],["裁剪阈值",{"5":{"41":1}}],["裁剪后的优化可以理解为在损失函数上添加了一个隐式的惩罚项",{"5":{"147":2}}],["裁剪后的梯度仍然指向下坡方向",{"5":{"147":2}}],["裁剪后的梯度",{"5":{"136":2}}],["裁剪后的梯度方向与原梯度方向相同",{"5":{"41":2}}],["裁剪后的梯度范数始终不超过",{"5":{"44":2}}],["裁剪后的有效学习率为",{"5":{"97":2}}],["捷径",{"5":{"41":2,"92":2,"153":2}}],["瓶颈层宽度",{"5":{"133":1}}],["瓶颈层宽度必须满足以下下界",{"5":{"133":1}}],["瓶颈宽度的信息论下界",{"5":{"133":2}}],["瓶颈",{"5":{"41":2}}],["除一个乘子恒为1",{"5":{"136":2}}],["除最大特征值外其余特征值趋近于零",{"5":{"41":2}}],["除了ppo",{"5":{"154":2}}],["除了初始化策略",{"5":{"148":2}}],["除了在边界情况",{"5":{"159":2}}],["除了在原点",{"5":{"148":2}}],["除了在极端负值区域",{"5":{"71":2}}],["除了范数裁剪",{"5":{"147":2}}],["除了谱范数",{"5":{"144":2}}],["除了前面介绍的奇异值分解",{"5":{"50":2}}],["除了softmax",{"2":{"61":1},"5":{"61":1}}],["除了计算复杂度",{"5":{"92":2,"153":2}}],["除非学习率或其他参数发生变化",{"5":{"148":2}}],["除非有平移分量",{"5":{"71":2}}],["除非",{"5":{"87":2,"147":2,"152":2}}],["减小比率会提高目标函数值",{"5":{"154":2}}],["减小了协变量偏移post",{"5":{"41":1}}],["减小了协变量偏移",{"5":{"41":1}}],["减去均值",{"5":{"146":2}}],["减少训练初期的波动",{"5":{"146":2}}],["减少模型参数与训练数据之间的互信息可以改善泛化性能",{"5":{"137":2}}],["减少overshooting",{"5":{"136":2}}],["减少学习率",{"5":{"136":2}}],["减少压缩压力",{"5":{"133":2}}],["减少干扰",{"5":{"133":2}}],["减少信息量",{"5":{"132":2}}],["减少对",{"5":{"101":2}}],["减少内部协变量偏移",{"5":{"41":2}}],["减少了层间的相互依赖",{"5":{"146":2}}],["减少了层与层之间的依赖关系",{"5":{"41":2}}],["减少了梯度偏移问题",{"5":{"42":2}}],["减少了协变量偏移问题",{"5":{"42":2}}],["减少了过拟合的风险",{"5":{"88":2}}],["减少到",{"5":{"50":1,"87":2,"89":3,"93":1,"143":1,"152":2}}],["节",{"5":{"70":2}}],["节点的值只依赖于其前驱节点",{"5":{"45":2}}],["节点",{"5":{"45":2}}],["节省了加法和除法操作",{"5":{"41":2}}],["节省了约30倍的内存带宽",{"5":{"86":2,"151":2}}],["节省计算",{"5":{"93":2}}],["效率方向",{"5":{"149":1}}],["效果类似于显式的模型平均",{"5":{"145":2}}],["效果",{"5":{"144":1}}],["效果相当",{"5":{"41":2}}],["效用",{"5":{"69":2}}],["效应使得非常深的网络成为可能",{"5":{"41":2}}],["效应",{"5":{"41":2}}],["效应量",{"5":{"96":2}}],["虽然模型定义了",{"5":{"159":2}}],["虽然序列长度通常固定",{"5":{"146":2}}],["虽然完整矩阵可能是满秩的",{"5":{"143":2}}],["虽然softmax是非线性变换",{"5":{"141":2}}],["虽然自回归模型使用链式法则",{"5":{"140":2}}],["虽然自然语言处理很少直接使用傅里叶变换",{"5":{"50":2}}],["虽然我们已经了解一些涌现的统计本质",{"5":{"138":2}}],["虽然数据量增加通常带来性能提升",{"5":{"138":2}}],["虽然两种损失函数源自不同的理论框架",{"5":{"70":2}}],["虽然均方误差和交叉熵源自完全不同的数学框架",{"5":{"70":2}}],["虽然这种设计在大多数情况下能够正常工作",{"5":{"146":2}}],["虽然这种改变不是直接作用于注意力权重",{"5":{"101":2}}],["虽然这种传递不是直接的",{"5":{"92":2,"153":2}}],["虽然这里的",{"5":{"61":2}}],["虽然这看起来是显然的",{"5":{"92":2,"153":2}}],["虽然训练高效",{"5":{"41":2}}],["虽然在大规模语言模型中sigmoid已被relu及其变体取代",{"5":{"42":2}}],["虽然计算高效",{"5":{"86":2,"151":2}}],["虽然的计算结果是一个的矩阵",{"5":{"87":1,"152":1}}],["虽然理论上注意力矩阵的秩不超过",{"5":{"87":2,"152":2}}],["虽然",{"5":{"87":1,"152":1,"158":2}}],["虽然词汇相同",{"5":{"88":2}}],["虽然相对位置是注意力计算的核心",{"5":{"88":2}}],["虽然正弦函数的差可以表示为另一个正弦函数",{"5":{"88":2}}],["虽然复数提供了优雅的数学语言",{"5":{"88":2}}],["虽然某些语言现象确实需要绝对位置",{"5":{"88":2}}],["虽然rope理论上支持任意长度的外推",{"5":{"88":2}}],["虽然位置编码不需要实际计算fft",{"5":{"90":2}}],["虽然隐含在编码中",{"5":{"91":2}}],["虽然通过堆叠多层卷积可以扩大感受野",{"5":{"92":2,"153":2}}],["虽然注意力的渐进复杂度高于rnn的",{"5":{"92":1,"153":1}}],["虽然注意力的渐进复杂度",{"5":{"92":1,"153":1}}],["虽然投影矩阵在结构上是分块对角的",{"5":{"93":2}}],["虽然总参数数量不变",{"5":{"93":2}}],["虽然transformer中没有使用relu",{"5":{"94":2}}],["虽然标准的transformer架构没有显式地使用温度调度",{"5":{"95":2}}],["弯曲",{"5":{"41":2}}],["程度",{"5":{"41":2,"61":2,"94":2,"133":2,"137":2}}],["针对relu激活函数进行了优化",{"5":{"41":2,"148":2}}],["针对文档级别的长上下文设计",{"5":{"88":1}}],["针对特定任务",{"5":{"41":2}}],["针对特定的损失函数",{"5":{"101":2}}],["很低",{"5":{"158":2}}],["很少丢弃",{"5":{"145":2}}],["很相似",{"5":{"69":1}}],["很小或为零",{"5":{"158":2}}],["很小时",{"5":{"149":1}}],["很小",{"5":{"41":1}}],["很大的区域",{"5":{"147":2}}],["很大",{"5":{"70":1,"158":2,"159":2}}],["很大而没有进行适当的缩放",{"5":{"41":1}}],["很大时",{"5":{"51":1,"69":1,"141":1}}],["很高",{"5":{"97":1,"158":2}}],["否",{"5":{"41":1}}],["否则它可能永久保持死亡状态",{"5":{"148":2}}],["否则序列收敛到临界点",{"5":{"136":2}}],["否则为0",{"5":{"96":2,"159":2}}],["否则形状不兼容",{"5":{"50":2}}],["否则",{"5":{"89":2}}],["否则可能失败",{"5":{"89":2}}],["否则会引入虚假信息",{"5":{"95":2}}],["天然具有概率解释",{"5":{"42":2}}],["证明概述",{"5":{"149":2}}],["证明直接由范数的三角不等式和正交性假设得出",{"5":{"147":2}}],["证明框架",{"5":{"136":2,"139":1}}],["证明依赖于不动点附近的局部坐标变换",{"5":{"135":2}}],["证明思路",{"5":{"69":2,"141":2,"143":2,"144":2,"148":2}}],["证明",{"5":{"42":8,"44":24,"45":26,"46":8,"47":14,"88":2,"101":2,"132":22,"133":34,"134":16,"135":56,"136":24,"137":22,"139":10,"140":4,"141":4,"144":4,"145":4,"146":2}}],["证明了数据量充足的重要性",{"5":{"138":2}}],["证明了",{"5":{"132":1}}],["证明了路径长度相比",{"5":{"132":1}}],["证明了通过错误信号的反向传播可以有效训练多层神经网络",{"5":{"44":2}}],["证明了激活函数对于突破神经网络线性瓶颈的关键作用",{"5":{"47":2}}],["证明了在高斯噪声假设下",{"5":{"51":2}}],["证明了它可以分解为偏差平方",{"5":{"51":2}}],["证明了1始终是特征值",{"5":{"87":2,"152":2}}],["证明了不同位置具有不同的编码向量",{"5":{"91":2}}],["证明了点积的方差随维度线性增长",{"5":{"95":1}}],["证明了点积的方差随维度",{"5":{"95":1}}],["证明注意力机制在理论上具有强大的长程依赖建模能力",{"5":{"92":2,"153":2}}],["证毕",{"5":{"42":8}}],["钟形",{"5":{"42":2}}],["补偿relu的零导数效应",{"5":{"148":1}}],["补",{"5":{"42":6}}],["增加探索",{"5":{"159":2}}],["增加路由决策的随机性",{"5":{"158":2}}],["增加参数量通常需要增加相应的计算量",{"5":{"138":2}}],["增加批量大小会按平方根关系减少梯度噪声的幅度",{"5":{"149":2}}],["增加批量大小",{"5":{"134":2}}],["增加压缩压力",{"5":{"133":2}}],["增加网络深度可以在不增加参数的情况下提高信息容量",{"5":{"133":2}}],["增加熵",{"5":{"132":2}}],["增加对",{"5":{"101":2}}],["增加到",{"5":{"70":2}}],["增加模型复杂度可以显著降低mse",{"5":{"51":2}}],["增加1时",{"5":{"90":1}}],["增加时",{"5":{"91":1}}],["增加头数不增加计算成本",{"5":{"93":2}}],["增加",{"5":{"93":2}}],["增加了输出的平滑性",{"5":{"159":2}}],["增加了灵活性",{"5":{"150":2}}],["增加了函数的",{"5":{"41":2}}],["增加了表示的丰富性",{"5":{"94":2}}],["增加了生成的多样性",{"5":{"96":2}}],["增强系统的",{"5":{"135":2}}],["增强状态空间",{"5":{"135":2}}],["增强对困难负样本的关注",{"5":{"69":2}}],["增强表达能力",{"5":{"86":2,"151":2}}],["增强了模型的表达能力",{"5":{"93":2}}],["增长",{"5":{"86":1,"95":1,"135":2,"151":1}}],["增长率为频率​",{"5":{"90":1}}],["增长率为频率",{"5":{"90":1}}],["增大比率",{"5":{"154":2}}],["增大批量大小通常会使优化过程更加稳定",{"5":{"149":2}}],["增大",{"5":{"136":2}}],["增大时",{"5":{"42":1,"92":1,"153":1}}],["增大而减小",{"5":{"90":1}}],["局部极小方向",{"5":{"149":2}}],["局部极小值的质量分析",{"5":{"97":2}}],["局部极小值的质量",{"5":{"97":2}}],["局部收敛",{"5":{"136":2}}],["局部渐近稳定",{"5":{"135":2}}],["局部线性化定理",{"2":{"135":1},"5":{"135":1}}],["局部最大值",{"5":{"134":2}}],["局部最小值通常是不动点中的",{"5":{"134":2}}],["局部最小值的",{"5":{"134":2}}],["局部最小值的等价性是大规模深度学习中的一个有趣现象",{"5":{"48":2}}],["局部最小值在特定条件下是渐近稳定的平衡点",{"5":{"134":2}}],["局部最小值",{"5":{"134":2}}],["局部最小值可能不是全局最小值",{"5":{"51":2}}],["局部基函数与函数重构",{"2":{"71":1},"5":{"71":1}}],["局部基函数",{"5":{"71":2}}],["局部探测器",{"5":{"71":2}}],["局部曲面叠加",{"5":{"71":2}}],["局部混合注意力",{"5":{"86":2,"151":2}}],["局部位置只与邻近位置交互",{"5":{"86":2,"151":2}}],["局部稀疏",{"5":{"87":2,"152":2}}],["局部",{"5":{"92":4,"147":2,"153":4}}],["局部注意力与全局注意力",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["局部注意力",{"5":{"92":2,"153":2}}],["局部注意力指的是注意力权重集中在相邻或接近的位置",{"5":{"92":2,"153":2}}],["局部注意力满足",{"5":{"92":2,"153":2}}],["局部注意力类似于卷积操作",{"5":{"92":2,"153":2}}],["局部注意力适合捕获短语结构",{"5":{"92":2,"153":2}}],["局部函数",{"5":{"92":2,"153":2}}],["局部性",{"5":{"92":2,"93":2,"153":2}}],["局部的",{"5":{"93":2}}],["曲线两端的平坦性正是梯度饱和的几何表现",{"5":{"41":2}}],["曲线在该点由上凸转为下凸",{"5":{"42":2}}],["曲线下凸",{"5":{"42":2}}],["曲线上凸",{"5":{"42":2}}],["曲线趋近于渐近线",{"5":{"41":4}}],["曲线趋于水平",{"5":{"42":2}}],["曲线也趋于水平",{"5":{"42":2}}],["曲线几乎为常数",{"5":{"91":2}}],["曲面或更复杂的几何结构",{"5":{"71":2}}],["曲面",{"5":{"71":2}}],["曲率",{"5":{"149":2}}],["曲率由hessian矩阵描述",{"5":{"144":2}}],["曲率变化与优化轨迹",{"2":{"144":1},"5":{"144":1}}],["曲率与奇异值的关联",{"5":{"142":2}}],["曲率与逃逸动力学",{"5":{"97":2}}],["曲率分析",{"5":{"135":2}}],["曲率特性",{"5":{"70":2}}],["曲率条件数",{"5":{"97":2}}],["曲率和优化难度上的差异",{"5":{"97":2}}],["图像等数据中充满了非线性模式",{"5":{"71":2}}],["折扣因子的选择会影响模型学习到的行为模式",{"5":{"156":2}}],["折扣因子",{"2":{"156":1},"5":{"156":5}}],["折叠",{"5":{"45":2,"90":2,"91":2,"143":2}}],["折线或更复杂的形状",{"5":{"71":2}}],["支持向量机",{"5":{"71":2}}],["功能",{"5":{"71":2}}],["抑制区",{"5":{"71":2}}],["光滑",{"5":{"71":2}}],["光滑的损失函数",{"5":{"51":2}}],["优势函数的性质",{"5":{"155":1}}],["优势函数的性质包含以下几个重要方面",{"5":{"155":1}}],["优势函数的概念同样适用",{"5":{"155":2}}],["优势函数的符号直接指示了策略改进的方向",{"5":{"155":2}}],["优势函数与策略改进",{"5":{"155":2}}],["优势函数在动作空间上的加权平均为零",{"5":{"155":2}}],["优势函数是强化学习中一个核心而深刻的概念",{"5":{"155":2}}],["优势函数衡量的是",{"5":{"155":4}}],["优势函数定义为",{"5":{"155":2}}],["优势函数",{"2":{"155":1},"5":{"155":4}}],["优点",{"5":{"147":1}}],["优雅",{"5":{"71":2}}],["优雅的注意力计算范式",{"5":{"95":2}}],["优化退化为标准梯度下降",{"5":{"147":2}}],["优化路径更加平滑",{"5":{"144":2}}],["优化性质",{"5":{"139":2}}],["优化算法",{"5":{"147":2}}],["优化算法和信息论之间的深层联系",{"5":{"139":2}}],["优化算法可能收敛到不同的局部最优解",{"5":{"97":2}}],["优化难点",{"5":{"134":2}}],["优化难度",{"5":{"97":1}}],["优化问题的不动点可分为",{"5":{"134":2}}],["优化更新为",{"5":{"147":2}}],["优化更复杂但梯度特性更好",{"5":{"70":2}}],["优化更加灵活高效",{"5":{"42":2}}],["优化相对简单",{"5":{"70":2}}],["优化器的更新方向为",{"5":{"70":2}}],["优化变慢",{"5":{"70":4,"97":2}}],["优化轨迹是线性的",{"5":{"70":2}}],["优化趋于收敛",{"5":{"69":2}}],["优化的目标是最小化这个距离",{"5":{"51":2}}],["优化目标是最大化奖励模型给出的期望分数",{"5":{"154":2}}],["优化目标是最大化净信息贡献",{"5":{"132":2}}],["优化目标就是最小化或",{"5":{"96":1}}],["优化目标就是最小化",{"5":{"96":1}}],["优化目标",{"5":{"61":2}}],["优化模型效率",{"5":{"87":2,"152":2}}],["优化过程被限制在参数空间的一个有界区域内",{"5":{"147":2}}],["优化过程本身会引入隐式正则化效应",{"5":{"147":2}}],["优化过程可能在几步迭代内完全失控",{"5":{"147":2}}],["优化过程可以调整所有位置的重要性",{"5":{"92":2,"153":2}}],["优化过程趋向于找到更",{"5":{"144":2}}],["优化过程即为寻找稳定不动点的过程",{"5":{"134":2}}],["优化过程变得更加稳定",{"5":{"96":2}}],["优化过程更加自由和稳定",{"5":{"96":2}}],["优化景观的几何分析",{"2":{"97":1},"5":{"97":1}}],["优化收敛变慢",{"5":{"97":2}}],["已知状态转移概率",{"5":{"155":2}}],["已经与",{"5":{"69":1}}],["已成为llama等现代大语言模型的默认选择",{"5":{"41":2}}],["已在2",{"5":{"44":2}}],["已有理论结果表明",{"5":{"71":2}}],["已有的编码从缓存读取",{"5":{"89":2}}],["已被证明在许多任务上优于单一核函数的使用",{"5":{"93":2}}],["成立",{"5":{"50":1,"61":4,"71":1,"87":2,"90":2,"96":2,"132":1,"134":6,"135":2,"136":1,"140":1,"148":1,"152":2,"156":2,"158":6,"159":2}}],["成立时最小",{"5":{"158":2}}],["成立时",{"5":{"61":1}}],["成功概率的对数几率",{"5":{"71":1}}],["成功概率",{"5":{"71":1}}],["成正比时等号成立",{"5":{"158":2}}],["成正比",{"5":{"44":1,"69":2,"70":3,"132":1,"136":1,"146":1,"149":2,"159":2}}],["成分可解释",{"5":{"143":1}}],["成分",{"5":{"87":2,"152":2}}],["成反比",{"5":{"90":1}}],["成为性能的关键瓶颈",{"5":{"159":2}}],["成为事实上的标准选择",{"5":{"146":2}}],["成为理解损失函数理论的理想切入点",{"5":{"51":2}}],["成为transformer架构中的经典设计",{"5":{"91":2}}],["成为",{"5":{"92":1,"153":1}}],["成为大语言模型中不可或缺的数学基础",{"5":{"95":2}}],["成为现代大语言模型的核心计算单元",{"5":{"95":2}}],["限制每个专家最多处理的样本数量",{"5":{"158":2}}],["限制每次参数更新时策略的变化幅度",{"5":{"154":2}}],["限制策略更新幅度的数学意义在于",{"5":{"154":2}}],["限制策略更新幅度",{"2":{"154":1},"5":{"154":1}}],["限制策略更新的幅度",{"5":{"101":2}}],["限制权重矩阵的最大奇异值",{"5":{"144":2}}],["限制信息维度",{"5":{"132":1}}],["限制在区间内",{"5":{"71":1}}],["限制在",{"5":{"71":1}}],["限制梯度的范数以防止梯度爆炸",{"5":{"48":2}}],["限制了比率的变化范围",{"5":{"154":2}}],["限制了激活值的尺度",{"5":{"144":2}}],["限制了输出值的范围",{"5":{"71":2}}],["限制了过拟合的风险",{"5":{"89":2}}],["限制了可分析的频率范围和频率分辨率",{"5":{"90":1}}],["限制了它在分类任务中的应用",{"5":{"97":2}}],["软top",{"5":{"159":2}}],["软约束",{"5":{"147":4}}],["软标签",{"5":{"137":2}}],["软最优策略",{"5":{"101":2}}],["软最大",{"5":{"69":4}}],["软最大化",{"5":{"96":2}}],["软对比",{"5":{"69":6}}],["软正样本分布",{"5":{"69":2}}],["软infonce",{"5":{"69":2}}],["软化",{"5":{"69":2,"87":2,"152":2}}],["软选择",{"5":{"71":2,"92":2,"153":2}}],["软选择保留了所有位置的信息",{"5":{"92":2,"153":2}}],["软",{"5":{"70":2,"71":2,"87":2,"152":2}}],["软性",{"5":{"61":2}}],["硬件实现",{"5":{"45":1}}],["硬",{"5":{"69":4,"71":2,"87":2,"92":2,"95":2,"145":2,"152":2,"153":2}}],["参与最终的决策",{"5":{"150":2}}],["参与前向传播计算",{"5":{"101":2}}],["参数使用",{"5":{"159":1}}],["参数初始化和训练过程中的数值变化可能导致各层激活值的分布发生剧烈变化",{"5":{"150":2}}],["参数在某些方向上的扩散速度远快于其他方向",{"5":{"149":2}}],["参数最容易发散的方向",{"5":{"149":2}}],["参数估计具有很高的不确定性",{"5":{"149":2}}],["参数范数的双重约束",{"5":{"147":2}}],["参数应该位于某个有界区域内",{"5":{"147":2}}],["参数规模的扩大",{"5":{"140":2}}],["参数下的似然函数为",{"5":{"140":1}}],["参数标度指数",{"5":{"138":2}}],["参数不足导致的损失和数据不足导致的损失",{"5":{"138":2}}],["参数空间中的平坦极小值通常对应更简单的函数假设",{"5":{"149":2}}],["参数空间中",{"5":{"136":2}}],["参数空间中出现对应的周期轨道",{"5":{"136":2}}],["参数组合使得增强系统的特征值位于单位圆上而非内部",{"5":{"136":1}}],["参数满足",{"5":{"136":2}}],["参数向量",{"5":{"136":2}}],["参数影响量化",{"5":{"134":2}}],["参数以平衡学习速度和压缩程度",{"5":{"133":1}}],["参数选择等工程问题",{"5":{"133":2}}],["参数间的互信息随训练步骤衰减",{"5":{"133":2}}],["参数复杂度的信息论解释",{"5":{"133":2}}],["参数复杂度从降低到",{"5":{"50":1}}],["参数复杂度从",{"5":{"50":1}}],["参数依赖的",{"5":{"70":1}}],["参数的似然函数为",{"5":{"139":1}}],["参数的更新也会变得非常缓慢",{"5":{"70":2}}],["参数的后验分布与似然函数和先验的乘积成正比",{"5":{"71":1}}],["参数的后验分布为",{"5":{"96":2}}],["参数可以通过反向传播自动学习",{"5":{"41":1}}],["参数",{"5":{"41":1,"71":1,"86":1,"95":1,"97":1,"139":1,"140":2,"144":6,"151":1,"159":2}}],["参数梯度的完整推导",{"2":{"44":1},"5":{"44":1}}],["参数梯度计算",{"5":{"44":2}}],["参数梯度可以通过误差信号与前向传播保存的激活值计算",{"5":{"44":2}}],["参数优化需要计算损失函数对各参数的梯度",{"5":{"44":2}}],["参数优化和性能评估的全过程",{"5":{"96":2}}],["参数高效微调以及模型蒸馏等场景中具有重要应用",{"5":{"50":2}}],["参数高效微调和特定网络结构的设计",{"5":{"50":2}}],["参数变换",{"5":{"87":2,"152":2}}],["参数压缩",{"5":{"89":2}}],["参数量",{"5":{"159":6}}],["参数量大",{"5":{"157":1}}],["参数量适中",{"5":{"157":1}}],["参数量节省",{"5":{"142":2}}],["参数量小于",{"5":{"138":2}}],["参数量和数据量都随计算量的增加而增加",{"5":{"138":2}}],["参数量约为",{"5":{"89":4}}],["参数量从减少到​",{"5":{"143":1}}],["参数量从减少到",{"5":{"89":2}}],["参数量从",{"5":{"89":2,"143":1}}],["参数量为​",{"5":{"89":1}}],["参数量为",{"5":{"89":1}}],["参数化形式",{"5":{"142":1}}],["参数化与似然函数",{"2":{"140":1},"5":{"140":1}}],["参数化投影",{"5":{"69":1}}],["参数化的灵活性是可学习编码的主要优势",{"5":{"89":2}}],["参数化为低秩分解",{"5":{"89":1}}],["参数节省显著",{"5":{"89":2}}],["参数由数学公式确定",{"5":{"91":2}}],["参数效率高",{"5":{"138":2}}],["参数效率与实践考量",{"2":{"89":1},"5":{"89":1}}],["参数效率为100",{"5":{"91":2}}],["参数数量",{"5":{"143":1}}],["参数数量巨大",{"5":{"140":2}}],["参数数量的对数",{"5":{"137":2}}],["参数数量与维度配置",{"2":{"93":1},"5":{"93":1}}],["参数数量为",{"5":{"93":2,"94":2}}],["参数更新可能导致新旧策略差异巨大",{"5":{"154":2}}],["参数更新的幅度",{"5":{"147":2}}],["参数更新幅度可能超出合理范围",{"5":{"147":2}}],["参数更新幅度过大导致模型权重溢出",{"5":{"41":2}}],["参数更新方向几乎相同",{"5":{"93":2}}],["参数更新",{"5":{"97":2}}],["参数通过神经网络架构和权重来参数化这些条件概率分布引理",{"5":{"140":1}}],["参数通过梯度下降优化",{"5":{"86":1,"151":1}}],["参数通常表示序列长度",{"5":{"95":1}}],["参数随时间演化的梯度流方程为",{"5":{"97":1}}],["参见第四章",{"5":{"71":2}}],["参见第六章",{"5":{"71":4}}],["参考",{"5":{"61":2}}],["普遍选择gelu作为",{"5":{"71":2}}],["双向",{"5":{"101":2}}],["双向线性注意力",{"5":{"86":2,"151":2}}],["双曲正切函数定义为",{"5":{"148":1}}],["双曲正切函数",{"5":{"42":4,"47":2}}],["双重非线性",{"5":{"71":2}}],["双精度浮点数的最大值约为",{"5":{"61":2}}],["偶数维度",{"5":{"71":2}}],["能获得的期望回报",{"5":{"156":4}}],["能区分任意两点",{"5":{"71":2}}],["能够学习复杂的条件概率函数",{"5":{"140":2}}],["能够执行任务",{"5":{"138":2}}],["能够更精确地拟合每个子分布",{"5":{"138":2}}],["能够更有效地训练模型的表示能力",{"5":{"138":2}}],["能够更准确地反映参数空间的几何结构",{"5":{"137":2}}],["能够更好地捕获位置",{"5":{"101":1}}],["能够通过线性变换提取相对位置",{"5":{"132":2}}],["能够编码不同尺度的位置信息",{"5":{"132":2}}],["能够编码位置的整体结构",{"5":{"91":2}}],["能够扩展到数百层的关键技术基础",{"5":{"132":2}}],["能够在输入空间中形成",{"5":{"71":1}}],["能够捕捉局部的",{"5":{"150":2}}],["能够捕捉全局的",{"5":{"150":2}}],["能够捕捉长距离的位置依赖",{"5":{"88":2}}],["能够捕获输入之间的复杂依赖关系",{"5":{"71":1}}],["能够区分相邻位置",{"5":{"88":2}}],["能够区分非常远的位置",{"5":{"88":2}}],["能够区分非常接近的位置",{"5":{"90":2}}],["能够覆盖较长的位置范围",{"5":{"90":2}}],["能够覆盖整个长序列",{"5":{"90":2}}],["能够唯一表示个位置的最小频率间隔",{"5":{"90":1}}],["能够唯一表示",{"5":{"90":1}}],["能够精细区分相近的位置",{"5":{"90":4}}],["能够精确区分相近的位置",{"5":{"91":2}}],["能够同时捕获全局和局部的位置特征",{"5":{"91":2}}],["能够建立跨整个序列的长程关联",{"5":{"92":2,"153":2}}],["能够充分利用内存带宽",{"5":{"92":2,"153":2}}],["能够处理训练时未见过的长度",{"5":{"92":2,"153":2}}],["能够处理各种类型的依赖关系",{"5":{"92":2,"153":2}}],["能量下界",{"5":{"142":2}}],["能量",{"5":{"90":2,"142":2}}],["能量集中",{"5":{"90":2}}],["能量集中在有限的频率点",{"5":{"90":2}}],["能力涌现和多模态信息融合方面",{"5":{"137":2}}],["能力",{"5":{"91":2}}],["解可能不是真正的极小值",{"5":{"144":2}}],["解决高维联合分布建模问题的关键在于概率论中的链式法则",{"5":{"140":2}}],["解决方案",{"5":{"44":1,"134":2}}],["解为",{"5":{"134":2}}],["解码器对输出",{"5":{"133":1}}],["解码器对输出的预测分布",{"5":{"133":1}}],["解码器预测",{"5":{"133":2}}],["解码器",{"5":{"133":2}}],["解码器和重参数化技巧三个核心组件",{"5":{"133":2}}],["解码器信息流",{"5":{"132":2}}],["解码器的信息瓶颈约束体现了信息效率的优化目标",{"5":{"132":2}}],["解码器通过跨注意力从编码器注入信息",{"5":{"132":2}}],["解码器架构中",{"5":{"132":2}}],["解码器结构",{"5":{"132":2}}],["解码器可以看到前缀的所有token",{"5":{"101":2}}],["解码过程",{"5":{"132":2}}],["解析解",{"5":{"101":2}}],["解这个不等式得到两个解",{"5":{"41":2}}],["解释性",{"5":{"143":1}}],["解释了训练过程中可能出现的有界周期行为",{"5":{"136":2}}],["解释了深度网络训练中的优化困难",{"5":{"134":2}}],["解释了为什么某些任务更容易涌现",{"5":{"138":1}}],["解释了为什么低维度使用低频",{"5":{"90":2}}],["解释了为什么纯注意力机制需要位置编码来感知序列顺序",{"5":{"91":2}}],["解释",{"5":{"134":4,"137":2}}],["解释为伯努利分布的成功概率",{"5":{"71":1}}],["控制了允许的专业化程度",{"5":{"158":2}}],["控制了输出激活值的尺度",{"5":{"144":2}}],["控制正则化的强度",{"5":{"144":2}}],["控制概率分布的平滑程度",{"5":{"137":2}}],["控制方法",{"5":{"136":2}}],["控制预测能力的保持程度",{"5":{"133":2}}],["控制压缩强度",{"5":{"133":2}}],["控制压缩程度与信息保留之间的权衡",{"5":{"133":2}}],["控制探索",{"5":{"132":1}}],["控制信息通过率",{"5":{"132":3}}],["控制信息传递的远程范围",{"5":{"132":2}}],["控制相似度分数的",{"5":{"69":2}}],["控制",{"5":{"71":1}}],["控制着注意力分布的",{"5":{"95":1}}],["应最小化信息损失与稀疏性之间的权衡",{"5":{"132":1}}],["应该按比例同时增长的观点",{"5":{"159":2}}],["应该升高",{"5":{"158":2}}],["应该降低以最小化损失",{"5":{"158":2}}],["应该与",{"5":{"158":2}}],["应该移动多远",{"5":{"154":2}}],["应该向哪个方向移动",{"5":{"154":2}}],["应该将多少资源分配给增加参数量",{"5":{"138":2}}],["应该尽可能均匀分布",{"5":{"132":1}}],["应该最大化",{"5":{"71":1}}],["应该增加其被选中的概率",{"5":{"155":2}}],["应该增加模型容量",{"5":{"51":2}}],["应该增加正则化或收集更多数据",{"5":{"51":2}}],["应该设置为略大于训练时的最大序列长度",{"5":{"89":2}}],["应用dropout",{"5":{"145":3}}],["应用dropout后",{"5":{"145":2}}],["应用dropout后为",{"5":{"145":4}}],["应用dropout后的期望损失为",{"5":{"144":2}}],["应用门控",{"5":{"132":2}}],["应用",{"5":{"61":2,"132":14,"133":10,"137":18}}],["应用链式法则",{"5":{"61":2}}],["应用于位置",{"5":{"88":2}}],["应用sigmoid",{"5":{"42":2}}],["应用softmax和value加权策略二",{"5":{"88":1}}],["应用softmax和value加权",{"5":{"88":1}}],["应用rope时",{"5":{"88":1}}],["应当与内容编码保持适当的数学关系",{"5":{"91":2}}],["杰弗里",{"5":{"44":2}}],["辛顿",{"5":{"44":2}}],["鲁梅尔哈特",{"5":{"44":2}}],["威廉姆斯",{"5":{"44":2}}],["杂志上发表了关于反向传播算法的开创性论文",{"5":{"44":2}}],["推向零",{"5":{"142":1}}],["推动人工智能系统向更智能",{"5":{"132":2}}],["推论",{"5":{"132":10,"133":12,"134":12,"135":12,"136":14,"137":12,"139":2,"140":6,"141":4,"142":2,"143":2,"144":4,"149":2}}],["推论2",{"5":{"44":5,"45":2}}],["推开",{"5":{"69":2}}],["推拉",{"5":{"69":2}}],["推力的大小与",{"5":{"69":1}}],["推力的大小与对该负样本的关注程度成正比",{"5":{"69":1}}],["推力",{"5":{"69":2,"149":4}}],["推理不对称性要求实现中仔细管理噪声的开关状态",{"5":{"159":2}}],["推理时需要关闭以获得确定性的路由决策",{"5":{"159":2}}],["推理时需要处理的位置为​",{"5":{"89":1}}],["推理时需要处理的位置为",{"5":{"89":1}}],["推理时每个输入实际只需要计算",{"5":{"159":2}}],["推理稳定",{"5":{"146":1}}],["推理阶段的数学处理",{"2":{"146":1},"5":{"146":1}}],["推理阶段直接使用",{"5":{"145":2}}],["推理阶段",{"5":{"145":3}}],["推理",{"5":{"44":2}}],["推断",{"5":{"88":2}}],["推导其基本性质",{"5":{"149":2}}],["推导了标量输入和向量输入的方差表达式",{"5":{"145":2}}],["推导了最优编码的条件和求解方法",{"5":{"133":2}}],["推导而来",{"5":{"61":2}}],["推导和性质",{"5":{"91":2}}],["路径的效率优势",{"5":{"132":2}}],["路径无关的梯度流",{"5":{"132":2}}],["路径归一化与深度网络稳定性",{"5":{"41":2}}],["路径归一化",{"5":{"41":2}}],["路径归一化确保所有路径的深度相近",{"5":{"41":2}}],["路径深度",{"5":{"41":2}}],["路径传递到前一层",{"5":{"41":1}}],["路径长度相比",{"5":{"132":2}}],["路径长度的减少意味着信息流动效率的显著提升",{"5":{"132":2}}],["路径长度指的是信息从一个位置传递到另一个位置所需经过的中间节点数",{"5":{"92":2,"153":2}}],["路径长度是路径中边的数量",{"5":{"92":2,"153":2}}],["路径长度为",{"5":{"92":2,"153":2}}],["路径长度恒为1",{"5":{"92":2,"153":2}}],["路径长度与依赖建模",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["路径长度与依赖关系的跨度成正比",{"5":{"92":2,"153":2}}],["路径长度与序列长度成正比",{"5":{"95":2}}],["路径为",{"5":{"92":2,"153":2}}],["路由得分的分布和特性直接影响专家分配的模式",{"5":{"158":2}}],["路由得分的计算是路由概率的基础",{"5":{"158":2}}],["路由概率受softmax归一化限制",{"5":{"158":2}}],["路由概率的标准差",{"5":{"158":2}}],["路由概率的均衡确保了每个专家都有机会被选中",{"5":{"158":2}}],["路由概率的方差定义为",{"5":{"158":2}}],["路由概率的条件期望也是重要的分析工具",{"5":{"158":2}}],["路由概率的归一化通过softmax函数实现",{"5":{"158":2}}],["路由概率的统计分析",{"5":{"158":2}}],["路由概率是门控得分的归一化结果",{"5":{"158":2}}],["路由概率是门控机制的输出结果",{"5":{"158":2}}],["路由概率与专家分配",{"2":{"158":1},"5":{"158":1}}],["路由和融合的系统",{"5":{"132":2}}],["路由和融合的数学本质",{"5":{"132":2}}],["路由决策",{"5":{"132":2}}],["路由",{"5":{"44":2}}],["返回得分最高的",{"5":{"159":2}}],["返回第",{"5":{"157":2}}],["返回的标量反馈信号",{"5":{"156":2}}],["返回步骤2",{"5":{"155":2}}],["返回",{"5":{"44":2,"143":2}}],["返回分数最高的个索引",{"5":{"86":1,"151":1}}],["返回分数最高的",{"5":{"86":1,"151":1}}],["约2d次",{"5":{"146":1}}],["约4d次",{"5":{"146":1}}],["约有一半的神经元输出为零",{"5":{"71":2}}],["约为2倍前向传播",{"5":{"44":2}}],["约128mb",{"5":{"95":2}}],["充分利用硬件并行性",{"5":{"44":2}}],["误差从第",{"5":{"148":1}}],["误差从第层反向传播到第",{"5":{"148":1}}],["误差会累积",{"5":{"142":2}}],["误差累积分析",{"5":{"142":2}}],["误差完全由被丢弃的奇异值决定",{"5":{"142":2}}],["误差界的分解",{"5":{"142":2}}],["误差为",{"5":{"70":2}}],["误差信号的传播",{"2":{"44":1},"5":{"44":1}}],["误差信号",{"5":{"44":4}}],["误差信号包含了损失函数对该层激活值变化的敏感程度信息",{"5":{"44":2}}],["误差信号为",{"5":{"44":2}}],["误差信号有不同的简化形式",{"5":{"44":2}}],["误差信号从输出层向输入层逐层传播",{"5":{"44":2}}],["误差信号从前一层传播到当前层",{"5":{"44":2}}],["误差信号从前向传播的最后一层",{"5":{"44":2}}],["误差信号会被衰减",{"5":{"44":2}}],["误差信号矩阵需要除以批量大小",{"5":{"44":2}}],["误差信号传播",{"5":{"44":2}}],["误差信号逐层反向传播",{"5":{"51":2}}],["误差传播",{"5":{"44":1}}],["误差最小化",{"5":{"51":2}}],["误差向量与预测空间正交",{"5":{"51":1}}],["误差向量",{"5":{"51":1}}],["误差很小",{"5":{"87":2,"152":2}}],["症状",{"5":{"44":1}}],["损失项",{"5":{"158":2}}],["损失会惩罚这种不平衡",{"5":{"158":2}}],["损失会非常大",{"5":{"70":2}}],["损失曲面接近球形",{"5":{"144":2}}],["损失曲面的等高线从原始的椭圆变为",{"5":{"144":2}}],["损失曲面的几何性质与低秩解的形成密切相关",{"5":{"142":2}}],["损失曲面的几何与低秩解",{"2":{"142":1},"5":{"142":1}}],["损失主要由参数量决定",{"5":{"138":2}}],["损失主要由数据量决定",{"5":{"138":2}}],["损失主导",{"5":{"70":2}}],["损失由两部分主导",{"5":{"138":2}}],["损失变化足够小",{"5":{"136":2}}],["损失监测",{"5":{"136":2}}],["损失值的变化相对不敏感",{"5":{"149":2}}],["损失值对参数变化更敏感",{"5":{"149":2}}],["损失值趋于稳定",{"5":{"134":2}}],["损失值或验证性能动态调整",{"5":{"101":2}}],["损失出发",{"5":{"70":2}}],["损失",{"2":{"135":1},"5":{"70":2,"135":1,"136":4,"158":4}}],["损失的可微代理",{"5":{"70":2}}],["损失是不可微的",{"5":{"70":2}}],["损失增长越来越快",{"5":{"70":2}}],["损失越大",{"5":{"70":2}}],["损失也不会趋于",{"5":{"70":2}}],["损失函数定义为",{"5":{"154":2}}],["损失函数定义了优化的目标",{"5":{"101":2}}],["损失函数变为",{"5":{"144":2}}],["损失函数作为",{"5":{"134":2}}],["损失函数作为模型参数的标量函数",{"5":{"51":1}}],["损失函数前沿与未来方向",{"2":{"101":1},"5":{"101":1}}],["损失函数对参数的梯度可表示为",{"5":{"135":2}}],["损失函数对不同位置误差的惩罚会通过这些正交基函数传播",{"5":{"70":2}}],["损失函数对权重的梯度可以分解为损失函数对净输入的梯度",{"5":{"44":2}}],["损失函数​",{"5":{"70":1}}],["损失函数选择的原则",{"2":{"70":1},"5":{"70":1}}],["损失函数梯度计算",{"5":{"45":1}}],["损失函数",{"5":{"41":1,"44":3,"48":1,"51":1,"70":1,"97":1,"134":2,"136":2,"137":2,"149":2,"157":2}}],["损失函数值出现nan或inf",{"5":{"41":2}}],["损失函数关于第一层权重的梯度为",{"5":{"41":1}}],["损失函数关于网络参数的梯度需要通过激活函数的导数逐层传递",{"5":{"42":2}}],["损失函数关于模型参数的梯度是反向传播算法的基础",{"5":{"61":2}}],["损失函数关于输入的梯度涉及注意力矩阵的逆或其伪逆",{"5":{"87":2,"152":2}}],["损失函数关于logits的梯度具有极其简洁的形式",{"5":{"96":2}}],["损失函数关于注意力参数",{"5":{"97":2}}],["损失函数度量预测与真实标签之间的差异",{"5":{"44":1}}],["损失函数为交叉熵",{"5":{"147":2}}],["损失函数为",{"5":{"44":2,"89":2}}],["损失函数数学性质的系统性对比",{"2":{"70":1},"5":{"70":1}}],["损失函数数学结构对比<",{"5":{"131":1}}],["损失函数数学结构对比",{"0":{"70":1},"4":{"70":1},"5":{"70":1,"131":4}}],["损失函数数学",{"2":{"131":1},"4":{"51":1,"61":1,"69":1,"70":1,"97":1,"101":1},"5":{"131":13}}],["损失函数会以多快的速度变化",{"5":{"48":2}}],["损失函数衡量输出与目标之间的差异",{"5":{"48":1}}],["损失函数的导数",{"5":{"147":2}}],["损失函数的局部极小值满足额外的约束",{"5":{"147":2}}],["损失函数的",{"5":{"147":2}}],["损失函数的平坦最小值定义为",{"5":{"135":2}}],["损失函数的理论极限",{"2":{"101":1},"5":{"101":1}}],["损失函数的数学结构与注意力机制",{"5":{"70":2}}],["损失函数的组合方式直接影响注意力层的学习",{"5":{"70":2}}],["损失函数的选择应遵循以下原则",{"5":{"70":2}}],["损失函数的几何分析与位置编码的几何分析在方法论上是相通的",{"5":{"70":2}}],["损失函数的边界行为决定了训练的后期动态",{"5":{"70":2}}],["损失函数的曲率特性直接影响优化的收敛速度和稳定性",{"5":{"70":2}}],["损失函数的hessian通常具有极值的特征值分布",{"5":{"48":2}}],["损失函数的等值面在高维空间中可能比直觉上预期的更加连通",{"5":{"48":2}}],["损失函数的优化性质",{"0":{"97":1},"4":{"97":1},"5":{"97":1,"131":4}}],["损失函数的优化性质<",{"5":{"131":1}}],["损失函数的优化性质不仅决定了训练过程的效率和稳定性",{"5":{"97":2}}],["损失函数的凸性是优化理论中最重要的概念之一",{"5":{"97":2}}],["损失函数的梯度可能非常小",{"5":{"97":2}}],["损失函数的收敛速度和最终性能之间存在复杂的权衡关系",{"5":{"97":2}}],["损失函数包含重构项和kl正则项",{"5":{"96":2}}],["损失函数扮演着衡量模型预测与真实目标之间差距的核心角色",{"5":{"51":2}}],["损失函数将变为",{"5":{"61":2}}],["损失函数优化性质的比较分析",{"2":{"97":1},"5":{"97":1}}],["损失函数与模型架构的协同设计",{"2":{"101":1},"5":{"101":1}}],["损失nan",{"5":{"44":1}}],["损失都是正的",{"5":{"51":2}}],["损失为",{"5":{"51":2}}],["损失景观中存在大量的",{"5":{"51":2}}],["损失景观中鞍点和局部极小值的分布直接影响优化的难度和最终解的质量",{"5":{"97":2}}],["损失景观中良好的区域",{"5":{"97":2}}],["损失景观通常非常复杂",{"5":{"97":2}}],["损失景观的平滑化",{"5":{"147":2}}],["损失景观的宽碗",{"5":{"97":2}}],["损失景观的形状决定了局部极小值的质量",{"5":{"97":2}}],["损失景观的全局结构",{"5":{"97":2}}],["损失通常是非凸的",{"5":{"97":2}}],["损失缩放通过在反向传播前将损失乘以一个缩放因子来解决这个问题",{"5":{"97":1}}],["损失缩放通过在反向传播前将损失乘以一个缩放因子",{"5":{"97":1}}],["损失感知",{"5":{"101":2}}],["myopic",{"5":{"156":2}}],["mdp的核心是",{"5":{"156":1}}],["mdp的核心是马尔可夫性质",{"5":{"156":1}}],["mdp",{"2":{"156":1},"5":{"156":3}}],["mdl原理",{"5":{"139":2}}],["mdl",{"5":{"137":16}}],["mjx",{"2":{"154":25,"155":33,"156":65}}],["mhc也能通过多尺度设计实现很大的有效感受野",{"5":{"150":2}}],["mhc的核心思想是在一个层内并行或级联地应用多个不同尺度的变换",{"5":{"150":2}}],["mhc",{"2":{"150":1},"5":{"150":1}}],["mn",{"5":{"143":2}}],["m2",{"5":{"143":2}}],["m1",{"5":{"143":2}}],["m^min",{"5":{"135":2}}],["m^max",{"5":{"135":2}}],["mcallester",{"5":{"133":2}}],["mcculloch",{"5":{"47":4}}],["mean",{"5":{"41":2,"51":4,"132":2,"133":4,"136":2,"137":10,"145":4,"146":2}}],["metric",{"5":{"70":2,"87":2,"152":2}}],["method",{"5":{"87":2,"134":2,"152":2}}],["mercer定理",{"5":{"87":2,"152":2}}],["memory",{"5":{"89":2}}],["mechanism",{"5":{"93":2,"95":2}}],["m",{"5":{"69":5,"97":13,"132":10,"135":4,"137":4,"146":5}}],["mlm损失为",{"5":{"101":2}}],["mlm损失函数",{"5":{"101":2}}],["mlm同时利用左右上下文来预测被掩码的token",{"5":{"101":2}}],["mlp输出等",{"5":{"69":2}}],["mle估计",{"5":{"140":1}}],["mle估计以概率收敛到真实参数",{"5":{"140":1}}],["mle具有一致性和渐近最优性",{"5":{"139":2}}],["mle与最小化交叉熵在统计意义上等价",{"5":{"139":2}}],["mle与交叉熵的等价性",{"5":{"139":2}}],["mle的一致性和渐近正态性",{"5":{"139":2}}],["mle的存在性和唯一性条件",{"5":{"139":2}}],["mle",{"5":{"51":2,"61":2,"96":2,"139":2,"140":2}}],["mle是训练的标准方法",{"5":{"96":2}}],["mle是相合的",{"5":{"96":2}}],["müssen",{"2":{"21":1,"131":1},"5":{"21":1,"131":1}}],["maddow",{"5":{"137":2}}],["margin参数",{"5":{"158":2}}],["margin",{"2":{"156":1}}],["marginal",{"5":{"132":4}}],["markov",{"5":{"137":4,"156":4}}],["manifold",{"5":{"70":2,"136":2}}],["manimce",{"5":{"47":8,"48":4}}],["mask=none",{"5":{"132":1}}],["masking后的mlm损失",{"5":{"101":2}}],["masking",{"5":{"101":6}}],["masked",{"5":{"101":2,"132":2}}],["mask",{"5":{"91":2,"95":4,"132":15,"145":2}}],["mass",{"5":{"96":2}}],["maximum",{"5":{"61":2,"96":2,"139":2}}],["max",{"5":{"42":6,"135":6,"136":12,"142":2,"144":2}}],["matmul",{"5":{"132":4}}],["matrix",{"5":{"45":2,"48":2,"50":2,"86":2,"87":14,"95":6,"149":2,"151":2,"152":14}}],["match",{"5":{"95":2}}],["math><",{"2":{"154":1,"155":1,"156":3}}],["mathjax",{"2":{"154":1,"155":1,"156":3}}],["mathematical",{"5":{"137":2}}],["math",{"2":{"154":3,"155":3,"156":9},"5":{"132":2}}],["mathbb",{"5":{"42":4,"141":2}}],["mathbf",{"5":{"46":44,"141":80,"142":66,"143":94,"144":6}}],["mathcal",{"5":{"97":2,"142":26,"143":2,"144":26,"146":6}}],["mahalanobis",{"5":{"96":2}}],["mae",{"5":{"51":2,"70":4}}],["mae对异常值的惩罚较轻",{"5":{"51":2}}],["mae对异常值的敏感性较低",{"5":{"51":2}}],["mae为",{"5":{"51":2}}],["mae或huber损失可能是更好的选择",{"5":{"51":2}}],["map",{"5":{"86":2,"151":2}}],["magnitude",{"5":{"88":2}}],["mamba等架构采用这类方法",{"5":{"88":2}}],["msup><mjx",{"2":{"155":2,"156":4}}],["msub><mjx",{"2":{"154":2}}],["mse",{"0":{"51":1},"4":{"51":1},"5":{"51":11,"61":4,"70":12,"131":5}}],["mse以其优雅的数学形式和清晰的几何解释",{"5":{"51":2}}],["mse的偏差",{"5":{"139":2}}],["mse的核心思想非常直观",{"5":{"51":2}}],["mse的统计学意义可以从最优预测理论的角度来理解",{"5":{"51":2}}],["mse的梯度特性",{"2":{"51":1},"5":{"51":1}}],["mse的梯度计算是深度学习反向传播的基础",{"5":{"51":2}}],["mse的几何解释是理解其数学本质的关键视角",{"5":{"51":2}}],["mse的几何解释是本节的核心内容之一",{"5":{"51":2}}],["mse可以用向量形式更加简洁地表示",{"5":{"51":2}}],["mse可以分为总体mse和样本mse两种形式",{"5":{"51":2}}],["mse与其他损失函数的联系",{"2":{"51":1},"5":{"51":1}}],["mse与高斯分布之间存在深刻的联系",{"5":{"51":2}}],["mse虽然是回归任务的标准损失",{"5":{"51":2}}],["mse正是这两点之间欧几里得距离的平方除以样本数",{"5":{"51":2}}],["mse度量的是预测向量与目标向量之间的",{"5":{"51":2}}],["mse最优解的几何解释是其最深刻的结果之一",{"5":{"51":2}}],["mse对异常值",{"5":{"51":2}}],["mse是合适的选择",{"5":{"51":2}}],["mse定义为",{"5":{"51":2}}],["mse梯度的一个特点是梯度幅度与误差大小成正比",{"5":{"51":2}}],["mse作为的函数",{"5":{"51":1}}],["mse作为",{"5":{"51":1}}],["mse关于是严格的凸函数",{"5":{"51":1}}],["mse关于是凸函数吗",{"5":{"51":1}}],["mse关于",{"5":{"51":2}}],["mse不是从信息论原理导出的损失函数",{"5":{"61":2}}],["msg",{"5":{"45":4,"96":4}}],["mi><",{"2":{"154":1,"155":1,"156":3}}],["mi><mjx",{"2":{"154":2,"155":3,"156":5}}],["mi",{"2":{"154":3,"155":4,"156":8},"5":{"137":8}}],["miller",{"5":{"137":2}}],["mine",{"5":{"137":8}}],["minimum",{"5":{"137":2}}],["minima",{"5":{"136":2}}],["mini",{"5":{"41":1,"44":2,"149":2}}],["min",{"5":{"42":2,"132":8,"133":2,"135":6,"136":2,"141":2,"142":4,"144":2}}],["mid",{"5":{"46":2,"69":4,"141":2,"142":2}}],["mirsky定理",{"5":{"50":2,"143":2}}],["micro",{"5":{"50":2}}],["mixing",{"5":{"159":2}}],["mixture",{"5":{"93":2,"159":4}}],["mixed",{"5":{"97":2}}],["mo><",{"2":{"154":1,"155":1,"156":2}}],["mo><mjx",{"2":{"154":1,"155":2,"156":3}}],["mo",{"2":{"154":2,"155":3,"156":5}}],["moving",{"5":{"146":2}}],["monte",{"5":{"145":2}}],["monro",{"5":{"136":2}}],["momentum",{"5":{"48":2,"134":2,"144":2}}],["moment",{"5":{"48":2,"96":4}}],["mobilenet中的深度可分离卷积可以用kronecker积来表示",{"5":{"50":2}}],["module",{"5":{"132":2}}],["modulation",{"5":{"88":2}}],["modeling",{"5":{"101":2}}],["models",{"5":{"101":2}}],["model",{"5":{"69":2,"101":2,"132":34,"154":4,"155":4}}],["mode",{"5":{"44":4,"45":4,"136":6}}],["moe退化为随机混合专家",{"5":{"159":2}}],["moe的计算量是稠密模型的",{"5":{"159":2}}],["moe的组合结构提供了一种",{"5":{"159":2}}],["moe架构通过改变参数利用效率来突破这一缩放瓶颈",{"5":{"159":2}}],["moe架构也带来了稠密模型中不存在的新挑战",{"5":{"159":2}}],["moe架构引入了独特的函数逼近特性",{"5":{"159":2}}],["moe架构引入了一种革命性的条件计算范式",{"5":{"159":2}}],["moe将条件计算的思想具体化为",{"5":{"159":2}}],["moe核心思想",{"2":{"159":1},"5":{"159":1}}],["moe层的计算量也可以保持在与稠密层相当的水平",{"5":{"159":2}}],["moe层的最终输出是专家输出的加权组合",{"5":{"158":2}}],["moe层的前向传播可以分解为两个阶段",{"5":{"157":2}}],["moe层通常替代标准transformer中的前馈网络层",{"5":{"158":2}}],["moe模型",{"5":{"159":1}}],["moe模型可以拥有16倍于稠密模型的参数量",{"5":{"159":2}}],["moe模型可以支持的参数量为",{"5":{"159":2}}],["moe模型能够取得更低的损失",{"5":{"159":2}}],["moe模型相比同等计算量的稠密模型能够取得显著的性能提升",{"5":{"159":2}}],["moe模型的损失多了一个常数因子",{"5":{"159":2}}],["moe模型的预期损失为",{"5":{"159":2}}],["moe模型的显存消耗可以分为三个主要部分",{"5":{"157":2}}],["moe模型的大参数量和稀疏激活模式带来了独特的显存和通信挑战",{"5":{"157":2}}],["moe模型的大规模和稀疏激活特性为训练和推理带来了独特的挑战",{"5":{"157":2}}],["moe模型的端到端训练需要同时优化门控网络参数和专家网络参数",{"5":{"157":2}}],["moe模型需要在多个设备间有效地并行化计算",{"5":{"157":2}}],["moe端到端训练梯度计算",{"2":{"157":1},"5":{"157":1}}],["moe概述与门控机制<",{"5":{"131":1}}],["moe概述与门控机制",{"0":{"159":1},"4":{"106":1,"159":1},"5":{"131":4,"159":1}}],["moe",{"5":{"93":2,"159":2}}],["moe通常需要为每个输入样本选择活跃的专家子集",{"5":{"93":2}}],["moa",{"5":{"93":2}}],["moa使用一个门控网络为每个输入动态地选择",{"5":{"93":2}}],["mgf",{"5":{"96":2}}],["mu",{"5":{"144":2,"146":4}}],["multi",{"2":{"150":1},"5":{"46":2,"70":2,"86":2,"93":6,"101":2,"150":3,"151":2}}],["multinomial",{"5":{"69":2,"96":2}}],["multiqueryattention",{"5":{"86":2,"151":2}}],["multiple",{"5":{"93":2}}],["mutual",{"5":{"69":2,"96":2,"132":8,"137":2}}],["mqa",{"5":{"86":2,"151":2}}],["mb的存储",{"5":{"86":1,"151":1}}],["mb",{"5":{"86":1,"151":1}}],["mt",{"5":{"97":1}}],["l1",{"5":{"147":6}}],["l1正则化通过次梯度计算和硬阈值效应诱导参数稀疏性",{"5":{"144":2}}],["l1正则化的软阈值是硬阈值的凸松弛",{"5":{"144":2}}],["l2通过显式惩罚大权重引入显式正则化",{"5":{"145":2}}],["l2",{"2":{"145":1},"5":{"144":1,"145":1,"147":16}}],["l2正则化则通过限制模型复杂度来实现类似的泛化提升",{"5":{"145":2}}],["l2正则化的效果被削弱",{"5":{"144":2}}],["l2正则化的梯度计算",{"5":{"144":2}}],["l2正则化的梯度表达式",{"2":{"144":1},"5":{"144":1}}],["l2正则化对所有奇异值施加相同的惩罚",{"5":{"144":2}}],["l2正则化实际上是frobenius范数的正则化",{"5":{"144":2}}],["l2正则化线性回归",{"5":{"144":2}}],["l2正则化通过显式惩罚大权重实现同样的目标",{"5":{"145":2}}],["l2正则化通过",{"5":{"144":2}}],["l2正则化项的梯度为",{"5":{"144":2}}],["l2正则化",{"5":{"144":4}}],["l2正则化与权重衰减的梯度分析",{"2":{"144":1},"5":{"144":1}}],["l2正则化在损失函数中添加编码范数的惩罚",{"5":{"89":2}}],["l2正则化倾向于学习较小的编码值",{"5":{"89":2}}],["lyapunov",{"5":{"134":12,"136":10}}],["len",{"5":{"132":16}}],["length",{"5":{"50":2,"91":2,"136":2,"137":2}}],["leftarrow",{"5":{"146":2}}],["left",{"2":{"156":1},"5":{"42":22,"46":8,"61":14,"69":6,"86":2,"97":2,"141":8,"142":2,"143":2,"144":2,"151":2}}],["left|",{"5":{"42":4}}],["leq",{"5":{"42":8,"69":2,"141":2,"142":2,"144":4}}],["leaky",{"5":{"41":2}}],["leakyrelu",{"5":{"42":4}}],["learnable",{"5":{"132":10}}],["learn",{"5":{"45":4}}],["learning",{"5":{"48":2,"51":2,"69":2,"87":2,"93":2,"101":4,"136":2,"152":2,"154":2,"155":2,"156":2}}],["learned",{"5":{"88":2,"89":2,"91":2}}],["leibler",{"5":{"96":2,"137":2}}],["leibler散度",{"2":{"61":1},"5":{"61":3}}],["l",{"5":{"42":14,"46":30,"61":18,"97":4,"132":18,"133":48,"137":4,"142":12,"144":26}}],["lipschitz常数与对抗鲁棒性之间的联系",{"5":{"144":2}}],["lipschitz的",{"5":{"144":2}}],["lipschitz",{"2":{"144":1,"147":1},"5":{"135":4,"136":10,"144":1,"147":17}}],["list",{"5":{"132":4}}],["likelihood",{"5":{"61":12,"96":2,"139":2}}],["linformer证明了可以将",{"5":{"141":1}}],["linformer证明了可以将的注意力矩阵近似为的低秩矩阵",{"5":{"141":1}}],["linformer提出用低秩矩阵近似注意力",{"5":{"141":2}}],["linformer的近似误差界",{"5":{"141":2}}],["linformer的复杂度分析",{"5":{"141":2}}],["linformer的低秩分解",{"5":{"141":2}}],["linformer的数学框架",{"2":{"141":1},"5":{"141":1}}],["lindenstrauss引理",{"5":{"141":2}}],["lindenstrauss引理的应用",{"2":{"141":1},"5":{"141":1}}],["lindenstrauss",{"5":{"141":2}}],["link",{"5":{"21":10,"131":51}}],["linearattention",{"5":{"141":4}}],["linear",{"5":{"42":2,"86":8,"87":2,"88":4,"132":12,"136":2,"151":8,"152":2}}],["linearly",{"5":{"47":2}}],["linearinseparablescene",{"5":{"47":4}}],["limit",{"5":{"96":2,"136":2}}],["ln",{"5":{"41":12,"146":4,"150":6}}],["ln结构能够更好地稳定训练过程",{"5":{"150":2}}],["ln结构在数学上的优势在于",{"5":{"146":2}}],["ln结构在训练初期可能导致较大的梯度波动",{"5":{"146":2}}],["ln结构更有利于训练非常深的网络",{"5":{"41":2}}],["ln结构中",{"5":{"41":2,"146":2}}],["ln的梯度稳定性质",{"5":{"146":2}}],["ln的数学优势",{"5":{"41":2}}],["ln的挑战",{"5":{"41":2}}],["lt",{"5":{"42":2,"133":2,"136":3,"144":2}}],["ldots",{"5":{"42":2,"69":2,"86":2,"151":2}}],["l层前馈网络",{"5":{"46":2}}],["load",{"5":{"159":2}}],["loaded",{"2":{"154":1,"155":1,"156":3}}],["lora变体的数学对比",{"5":{"142":1}}],["lora变体的数学对比定义",{"5":{"142":1}}],["lora提出后",{"5":{"142":2}}],["lora定义的参数流形为",{"5":{"142":2}}],["lora流形",{"5":{"142":2}}],["lora将权重增量参数化为两个低秩矩阵的乘积",{"5":{"142":2}}],["lora的优化过程倾向于让",{"5":{"142":1}}],["lora的优化过程倾向于让的奇异值与的奇异值对齐",{"5":{"142":1}}],["lora的优化视角",{"2":{"142":1},"5":{"142":1}}],["lora的梯度等价性",{"5":{"142":2}}],["lora的低秩约束定义了一个特定的参数化流形",{"5":{"142":2}}],["lora的参数量为",{"5":{"142":2}}],["lora的参数化",{"5":{"142":2}}],["lora的变体与扩展",{"2":{"142":1},"5":{"142":1}}],["lora的数学框架",{"2":{"142":1},"5":{"142":1}}],["lora",{"2":{"142":1},"5":{"142":5}}],["lora等高效变体提供了坚实的数学基础",{"5":{"141":2}}],["logσ",{"5":{"133":4}}],["log",{"2":{"61":1},"5":{"61":13,"71":2,"97":12,"132":4,"133":8,"137":10,"139":2,"142":2,"145":3}}],["logits",{"5":{"61":12,"69":5,"70":10,"71":6,"96":5}}],["logits空间",{"5":{"42":2}}],["logits可以理解为",{"5":{"71":2}}],["logits与概率的变换",{"5":{"71":2}}],["logits与概率分布的生成",{"2":{"96":1},"5":{"96":1}}],["logits这一术语源自统计学中的logit函数",{"5":{"96":2}}],["logits向量具有两个显著特点",{"5":{"96":2}}],["logits的分量之间是相对的",{"5":{"96":2}}],["logits的生成过程如下",{"5":{"96":2}}],["logits的定义域是整个实数空间",{"5":{"96":2}}],["logit函数",{"5":{"42":2}}],["logit",{"5":{"61":2,"69":2}}],["logical",{"5":{"47":2}}],["logistic",{"5":{"47":2}}],["logsumexp技巧的数学原理",{"2":{"61":1},"5":{"61":1}}],["logsumexp",{"5":{"61":10}}],["loss",{"5":{"47":2,"61":2,"70":8,"132":10,"136":6,"146":2,"154":2,"157":2}}],["lower",{"5":{"137":2}}],["low",{"5":{"50":2,"87":2,"91":2,"132":2,"142":4,"152":2}}],["local",{"5":{"86":2,"151":2}}],["long",{"5":{"88":2,"92":2,"153":2}}],["lambda",{"5":{"142":2,"144":40}}],["lasalle",{"5":{"134":2}}],["langevin",{"5":{"149":2}}],["language",{"5":{"101":6}}],["lanczos",{"5":{"135":6}}],["landscape",{"2":{"135":1},"5":{"134":2,"135":9,"136":6,"146":4,"147":2}}],["lagrange",{"5":{"133":2}}],["layernorm",{"5":{"132":1,"146":6}}],["layers",{"5":{"132":4,"133":10}}],["layer",{"2":{"146":1,"150":1},"5":{"41":4,"46":12,"132":8,"133":2,"143":2,"146":7,"147":2,"150":19}}],["layer或fully",{"5":{"46":2}}],["label",{"5":{"96":2,"137":2,"144":2}}],["laws",{"5":{"159":2}}],["law",{"5":{"96":2,"138":4}}],["large",{"5":{"96":2,"101":2}}],["llama等大型transformer模型中",{"5":{"146":2}}],["llama等数十亿参数模型",{"5":{"146":2}}],["llama",{"5":{"71":2,"88":2,"92":2,"93":2,"153":2}}],["llama系列是rope的典型使用者",{"5":{"88":1}}],["llama系列",{"5":{"88":1}}],["llm",{"5":{"50":4}}],["lu分解的主要用途是高效求解线性方程组",{"5":{"50":2}}],["lu分解将矩阵分解为下三角矩阵和上三角矩阵的乘积",{"5":{"50":1}}],["lu分解将矩阵分解为下三角矩阵",{"5":{"50":1}}],["lstm中的遗忘门和输入门控制着信息保留和添加的比例",{"5":{"150":2}}],["lstm",{"5":{"150":2}}],["lstm和gru通过门控机制设计",{"5":{"50":2}}],["lse",{"5":{"61":2}}],["ls",{"5":{"97":2}}],["cumulative",{"5":{"156":2}}],["current",{"5":{"136":8}}],["curriculum",{"5":{"51":2}}],["curvature",{"5":{"48":2,"135":2}}],["c2c",{"2":{"155":1,"156":1}}],["c29",{"2":{"154":1,"155":1,"156":2}}],["c28",{"2":{"154":1,"155":1,"156":2}}],["c1d6fe",{"2":{"156":1}}],["c1d70b",{"2":{"155":1,"156":2}}],["c1d703",{"2":{"154":1}}],["c1d444",{"2":{"156":1}}],["c1d449",{"2":{"156":1}}],["c1d44e",{"2":{"155":1,"156":1}}],["c1d460",{"2":{"155":1,"156":2}}],["c1d461",{"2":{"154":1}}],["c1d434",{"2":{"155":1}}],["c1d45f",{"2":{"154":1}}],["c><",{"2":{"154":5,"155":7,"156":13}}],["cycling",{"5":{"136":2}}],["cyclic",{"5":{"92":2,"153":2}}],["cycle",{"5":{"136":2}}],["centering",{"5":{"146":2}}],["center",{"5":{"134":1,"136":2}}],["central",{"5":{"96":2}}],["cnn",{"5":{"132":3,"143":2,"145":2,"146":2}}],["cdot",{"5":{"42":14,"61":18,"141":2,"142":2,"143":4,"144":2,"146":4}}],["cdots",{"5":{"46":4,"143":8}}],["cot才展现出其优势",{"5":{"138":2}}],["cot要求模型学习两个条件概率分布",{"5":{"138":2}}],["cot",{"5":{"138":2}}],["count",{"5":{"137":4}}],["cost",{"5":{"142":2}}],["cos",{"5":{"132":2}}],["cosine",{"5":{"48":2,"97":2}}],["coding",{"5":{"69":2}}],["container><",{"2":{"154":1,"155":1,"156":3}}],["container",{"2":{"154":1,"155":1,"156":3}}],["contiguous",{"5":{"132":2}}],["contraction",{"5":{"50":2,"93":2}}],["contrastive",{"5":{"61":2,"69":6,"137":2}}],["concat",{"5":{"69":2,"86":2,"132":4,"151":2}}],["concatenation",{"5":{"91":2}}],["connection是highway",{"5":{"150":2}}],["connection继承了残差连接的梯度传播优势",{"5":{"150":2}}],["connection可以被视为lstm思想的简化版本",{"5":{"150":2}}],["connection中的变换门和携带门执行着类似的功能",{"5":{"150":2}}],["connection的进一步扩展",{"5":{"150":2}}],["connection的理论优势",{"5":{"150":2}}],["connection的形式与长短期记忆网络",{"5":{"150":2}}],["connection的深层理论基础",{"5":{"150":2}}],["connection的关键创新",{"5":{"150":2}}],["connection的核心数学形式可以表示为",{"5":{"150":2}}],["connection使用神经网络学习的门来动态决定多少信息应该通过",{"5":{"150":2}}],["connection",{"2":{"150":2},"5":{"41":2,"92":2,"147":2,"150":4,"153":2}}],["connected",{"5":{"46":2,"92":2,"153":2}}],["conditional",{"5":{"96":2,"159":2}}],["condition",{"5":{"51":2,"87":2,"148":2,"152":2}}],["constant",{"5":{"89":2}}],["convergence",{"5":{"136":10}}],["convergent",{"5":{"136":2}}],["convex",{"5":{"92":2,"153":2}}],["convolutional",{"5":{"46":2}}],["convolution",{"5":{"87":2,"152":2}}],["coordinates",{"5":{"46":2,"47":2}}],["communication",{"5":{"137":2}}],["compare",{"5":{"148":4}}],["compact",{"5":{"70":2}}],["computation",{"5":{"159":2}}],["computational",{"5":{"45":2}}],["compute",{"5":{"138":2}}],["compression",{"5":{"132":8}}],["comp",{"5":{"45":4}}],["compositional",{"5":{"71":2}}],["composite",{"5":{"45":2}}],["component",{"5":{"50":2}}],["completion",{"5":{"50":2}}],["complete",{"5":{"92":2,"153":2}}],["combinedscene",{"5":{"48":4}}],["combination",{"5":{"92":2,"153":2}}],["covariate",{"5":{"41":2,"146":4}}],["covariance",{"5":{"96":2}}],["coefficient",{"5":{"96":2}}],["collapse",{"5":{"158":2,"159":2}}],["column",{"5":{"50":2}}],["colorcoded",{"5":{"61":4}}],["corr",{"5":{"137":4}}],["correlation",{"5":{"96":2}}],["core",{"5":{"86":2,"151":2}}],["core的更充分利用",{"5":{"86":2,"151":2}}],["core进行矩阵乘法加速",{"5":{"93":2}}],["core对半精度矩阵乘法的加速使得多头注意力的计算效率大幅提升",{"5":{"93":2}}],["c",{"2":{"154":5,"155":7,"156":13},"5":{"42":2,"97":2,"141":4,"143":18}}],["clip目标",{"5":{"154":2}}],["clip目标函数",{"2":{"154":1},"5":{"154":1}}],["clip因其稳定性和易用性而被更广泛采用",{"5":{"154":2}}],["clip因其简单有效而被更广泛采用",{"5":{"154":2}}],["clip通过硬截断直接限制比率的变化范围",{"5":{"154":2}}],["clip通过截断",{"5":{"154":2}}],["clip与ppo",{"5":{"154":2}}],["clip之外",{"5":{"154":2}}],["clip实现了",{"5":{"154":2}}],["clip的优化行为",{"5":{"154":1}}],["clip的优化行为可以这样总结",{"5":{"154":1}}],["clip的目标函数定义为",{"5":{"154":2}}],["clip是ppo算法最常用的变体",{"5":{"154":2}}],["clip和ppo",{"5":{"154":2}}],["clip",{"5":{"136":10,"137":4}}],["clipping",{"5":{"44":2,"48":2,"147":12,"154":2}}],["clr",{"5":{"136":2}}],["clt",{"5":{"96":2}}],["clamp",{"5":{"136":2}}],["claude",{"5":{"61":2,"137":4}}],["class=",{"2":{"154":13,"155":17,"156":35},"5":{"21":10,"131":51}}],["class",{"5":{"61":4,"132":2}}],["carry",{"5":{"150":2}}],["carlo",{"5":{"145":4}}],["cat",{"5":{"132":2}}],["categorical",{"5":{"96":2}}],["candidates",{"5":{"132":4}}],["candecomp",{"5":{"50":2,"143":2}}],["calculus",{"5":{"47":2}}],["causal",{"5":{"91":2,"95":2}}],["cases",{"5":{"42":24,"61":4,"144":12}}],["casual",{"5":{"91":2}}],["chtml",{"2":{"154":1,"155":1,"156":3}}],["chinchilla论文进一步精细化了计算最优缩放的理论",{"5":{"159":2}}],["chinchilla论文提出了一个新的标度律",{"5":{"138":2}}],["chinchilla标度律与数据最优策略",{"5":{"138":1}}],["chinchilla标度律与数据最优策略是对早期标度律研究的重要修正",{"5":{"138":1}}],["chinchilla的研究表明",{"5":{"138":2}}],["chinchilla在多项基准测试上显著优于参数量更大的gopher",{"5":{"138":2}}],["chinchilla",{"5":{"138":2}}],["chinchilla团队训练了一个名为",{"5":{"138":2}}],["channel",{"5":{"146":2}}],["change",{"5":{"136":6}}],["chain",{"5":{"48":2,"138":2,"140":2}}],["cholesky分解是lu分解的特例",{"5":{"50":2}}],["cholesky分解的数值稳定性好",{"5":{"50":2}}],["checking",{"5":{"44":2}}],["checkpointing",{"5":{"50":2,"157":2}}],["critic网络持续评估策略的价值",{"5":{"155":2}}],["critic算法可以在每一步交互后就进行参数更新",{"5":{"155":2}}],["critic算法的完整流程如下",{"5":{"155":2}}],["critic架构相对于纯reinforce算法的优势在于",{"5":{"155":2}}],["critic架构结合了策略梯度方法和价值函数逼近方法的优点",{"5":{"155":2}}],["critic架构",{"5":{"155":2}}],["critic",{"5":{"154":2,"155":2}}],["critic框架中",{"5":{"154":2,"155":2}}],["critic框架",{"2":{"155":1},"5":{"154":2,"155":1}}],["critic方法<",{"5":{"131":1}}],["critic方法",{"0":{"155":1},"4":{"110":1,"155":1},"5":{"131":4,"155":1}}],["critical",{"5":{"48":2}}],["cross",{"5":{"47":2,"61":2,"70":2,"86":2,"87":2,"93":2,"96":2,"151":2,"152":2}}],["crossentropy联合优化",{"2":{"61":1},"5":{"61":1}}],["crossentropy",{"5":{"61":12,"97":2}}],["crossentropyloss",{"5":{"61":2}}],["cp分解将张量表示为秩一张量的和",{"5":{"143":2}}],["cp分解后参数量为",{"5":{"143":2}}],["cp分解用于cnn压缩",{"5":{"143":2}}],["cp分解更合适",{"5":{"143":2}}],["cp分解与tucker分解的对比",{"5":{"143":1}}],["cp分解与tucker分解的对比定理",{"5":{"143":1}}],["cp分解与tucker分解的比较",{"2":{"143":1},"5":{"143":1}}],["cp分解和tucker分解是两种主要的张量分解方法",{"5":{"143":2}}],["cp分解假设基张量是秩一张量的和",{"5":{"143":2}}],["cp分解的数学形式",{"2":{"143":1},"5":{"143":1}}],["cp分解<",{"5":{"131":1}}],["cp分解",{"0":{"143":1},"2":{"143":1},"4":{"143":1},"5":{"131":4,"143":8}}],["cp分解可以将一个的三阶张量表示为个秩一张量的和",{"5":{"50":1}}],["cp分解可以将一个",{"5":{"50":1}}],["cp",{"5":{"50":4,"143":1}}],["circle",{"5":{"90":2}}],["c^",{"5":{"97":8}}],["数学原理和门控机制的设计",{"5":{"159":2}}],["数学定义为",{"5":{"156":4}}],["数学定义与基本性质",{"2":{"149":1},"5":{"149":1}}],["数学定义如下",{"5":{"86":2,"151":2}}],["数学推导",{"5":{"148":4}}],["数学表达为",{"5":{"146":6}}],["数学表达式为",{"5":{"145":2}}],["数学表达式更加紧凑",{"5":{"90":2}}],["数学公式推导",{"2":{"146":2},"5":{"146":2}}],["数学特点",{"5":{"142":1}}],["数学形式统一",{"5":{"101":2}}],["数学本质",{"5":{"70":2}}],["数学基础",{"4":{"48":1,"50":1,"96":1},"5":{"21":6,"131":6}}],["数学评估指标",{"5":{"41":2}}],["数学上的统一性",{"5":{"70":2}}],["数学上的这种统一性并非巧合",{"5":{"61":2}}],["数学上的优雅性是首要因素",{"5":{"88":1}}],["数学上的优雅性",{"5":{"88":1}}],["数学上",{"5":{"47":1,"50":6,"86":2,"91":2,"92":4,"93":2,"95":2,"138":4,"142":2,"147":4,"150":4,"151":2,"153":4,"157":6,"158":6,"159":16}}],["数学的自然结果就是平移等变性",{"5":{"88":2}}],["数学最优解",{"5":{"88":2}}],["数学最优",{"5":{"88":2}}],["数学结构",{"5":{"90":2}}],["数据量大",{"5":{"157":1}}],["数据并行",{"5":{"157":1}}],["数据并行在批次维度上进行",{"5":{"157":2}}],["数据并行和专家并行的组合是大规模moe训练的典型配置",{"5":{"157":2}}],["数据增强通过增加训练数据的多样性来正则化模型",{"5":{"145":2}}],["数据增强和正则化技术被用来引导模型学习有意义的依赖关系",{"5":{"92":2,"153":2}}],["数据形式",{"5":{"143":1}}],["数据矩阵的秩远小于其维度",{"5":{"143":2}}],["数据冗余度",{"5":{"143":2}}],["数据的内在低维性",{"5":{"143":2}}],["数据的似然函数为",{"5":{"51":2}}],["数据稀疏性",{"5":{"140":2}}],["数据最优策略不仅仅是增加数据量",{"5":{"138":2}}],["数据最优",{"5":{"138":2}}],["数据标度指数",{"5":{"138":2}}],["数据处理不等式阐明了信息流动的基本规律",{"5":{"137":2}}],["数据处理不等式",{"5":{"137":2}}],["数据",{"5":{"133":2}}],["数据变化敏感的方向",{"5":{"70":2}}],["数据上的平均损失",{"5":{"51":2}}],["数据层面",{"5":{"61":2}}],["数据被投影到方差最大的正交方向上",{"5":{"94":2}}],["数据特性",{"5":{"97":2}}],["数据分布",{"5":{"157":1}}],["数据分布的mle估计",{"5":{"149":2}}],["数据分布的不可知性",{"5":{"140":2}}],["数据分布的统计特性与涌现的关系",{"5":{"138":1}}],["数据分布的统计特性与涌现的关系解释了为什么某些任务更容易涌现",{"5":{"138":1}}],["数据分布的复杂性和噪声水平",{"5":{"97":2}}],["数据分布为两个交织的螺旋形状",{"5":{"71":2}}],["数十亿甚至万亿级",{"5":{"48":2}}],["数值方法基础",{"5":{"135":2}}],["数值稳定性考虑",{"2":{"139":1},"5":{"139":1}}],["数值稳定性与实现细节",{"2":{"44":1},"5":{"44":1}}],["数值稳定性是反向传播实现中的关键考虑",{"5":{"44":2}}],["数值稳定性",{"5":{"47":2,"97":1}}],["数值稳定性问题与logsumexp技巧",{"2":{"61":1},"5":{"61":1}}],["数值稳定的softmax实现",{"5":{"44":1}}],["数值稳定",{"5":{"87":2,"152":2}}],["数值上更加稳定",{"5":{"61":2}}],["数值不稳定",{"5":{"87":2,"152":2}}],["数值计算不稳定",{"5":{"87":2,"152":2}}],["数值计算可能不稳定",{"5":{"89":2}}],["数值计算更稳定",{"5":{"89":2}}],["数值示例与可视化",{"2":{"91":1},"5":{"91":1}}],["分母为",{"5":{"146":1}}],["分母为而非",{"5":{"146":1}}],["分岔与混沌视角",{"5":{"136":2}}],["分岔和极限环理论",{"5":{"136":2}}],["分岔图包含以下典型区域",{"5":{"136":2}}],["分岔图是系统行为随参数变化的定性描述",{"5":{"136":2}}],["分岔图是可视化系统行为随参数变化的重要工具",{"5":{"136":2}}],["分岔图的定义",{"5":{"136":2}}],["分岔行为分析",{"5":{"136":2}}],["分岔理论提供了理解超参数变化如何影响训练行为的数学框架",{"5":{"136":2}}],["分岔边界曲线",{"5":{"136":2}}],["分岔边界",{"5":{"136":2}}],["分岔的判据",{"5":{"136":2}}],["分岔的定义",{"5":{"136":2}}],["分岔",{"5":{"136":8}}],["分岔是产生极限环的主要机制之一",{"5":{"136":2}}],["分岔后",{"5":{"136":2}}],["分岔前",{"5":{"136":2}}],["分类预测的不确定性量化",{"5":{"137":2}}],["分类",{"5":{"134":2}}],["分类任务中的温度参数",{"5":{"70":2}}],["分类任务中的交叉熵损失",{"5":{"69":2}}],["分类任务选择交叉熵或其变体",{"5":{"70":2}}],["分类任务的输出空间是",{"5":{"70":1}}],["分类任务的输出空间是概率单纯形",{"5":{"70":1}}],["分类任务的数学结构",{"5":{"70":2}}],["分类任务的最大似然推导",{"2":{"61":1},"5":{"61":1}}],["分类任务在分类任务中",{"5":{"69":1}}],["分类任务",{"5":{"69":2,"101":2}}],["分层裁剪提供了灵活性",{"5":{"147":2}}],["分层裁剪的范数上界",{"5":{"147":2}}],["分层裁剪的优势在于它可以根据每一层的特性设置不同的阈值",{"5":{"147":2}}],["分层裁剪",{"5":{"147":2}}],["分层压缩的必要性",{"5":{"133":2}}],["分层信息瓶颈考虑每层的信息压缩",{"5":{"133":2}}],["分层信息瓶颈对每一层进行独立的信息约束",{"5":{"133":2}}],["分层信息瓶颈目标",{"5":{"133":2}}],["分层位置编码",{"5":{"89":2}}],["分辨率",{"5":{"132":2,"150":2}}],["分数作为信息路由信号的本质",{"5":{"132":2}}],["分数缩放的影响",{"5":{"132":2}}],["分数差异与熵的关系",{"5":{"132":2}}],["分数均匀分布时",{"5":{"132":2}}],["分词后的mlm损失",{"5":{"101":2}}],["分词器的选择直接影响损失函数的结构和模型的学习行为",{"5":{"101":2}}],["分词器对损失函数的影响",{"2":{"101":1},"5":{"101":1}}],["分割到设备",{"5":{"157":4}}],["分割到多个计算设备上",{"5":{"45":2}}],["分割函数的对偶",{"5":{"70":2}}],["分布和专家分配的模式",{"5":{"158":2}}],["分布漂移",{"5":{"154":2,"155":2}}],["分布一致性",{"5":{"140":2}}],["分布趋向于均匀分布",{"5":{"140":2}}],["分布趋向于确定性的one",{"5":{"140":2}}],["分布趋于平坦",{"5":{"95":2}}],["分布趋于尖锐",{"5":{"95":2}}],["分布替换为",{"5":{"137":1}}],["分布等价于能量模型中的",{"5":{"132":2}}],["分布外泛化",{"5":{"154":2}}],["分布外",{"5":{"101":2}}],["分布锐度效应",{"5":{"69":2}}],["分布",{"5":{"69":2,"95":2,"132":4,"137":1}}],["分布越",{"5":{"96":4}}],["分布越均匀",{"5":{"61":2}}],["分布越集中",{"5":{"61":2}}],["分布式训练",{"5":{"41":2}}],["分布用pdf表示",{"5":{"96":2}}],["分布间差异的信息度量",{"2":{"61":1},"5":{"61":1}}],["分布集中在区间内",{"5":{"95":1}}],["分布集中在",{"5":{"95":1}}],["分布变得更加尖锐",{"5":{"96":2}}],["分析的不动点稳定性",{"5":{"136":1}}],["分析震荡机制和探索周期行为的形成机理",{"5":{"136":2}}],["分析噪声驱动的概率转移",{"5":{"135":2}}],["分析随机网络的信息容量",{"5":{"133":2}}],["分析",{"2":{"135":3},"5":{"133":2,"135":3,"136":3}}],["分析信息瓶颈目标关于时间的导数",{"5":{"133":2}}],["分析瓶颈位置",{"5":{"132":2}}],["分析编码器各层表示的信息含量变化",{"5":{"132":2}}],["分析现代llm训练中使用的各种损失函数的数学本质及其设计原理",{"5":{"101":2}}],["分析了它们在连续概率分布下的数学特性",{"5":{"145":2}}],["分析了它与正弦余弦编码的逼近关系",{"5":{"89":2}}],["分析了周期轨道的",{"5":{"136":2}}],["分析了周期倍增分岔的机理",{"5":{"136":2}}],["分析了尺度律分布",{"5":{"135":2}}],["分析了全局感受野的信息论意义",{"5":{"132":2}}],["分析了",{"5":{"61":2}}],["分析了softmax变换对秩的影响",{"5":{"87":2,"152":2}}],["分析了其基本性质",{"5":{"87":2,"152":2}}],["分析了其参数空间和自由度",{"5":{"89":2}}],["分析了两种范式的数学差异和适用场景",{"5":{"89":2}}],["分析了各个参数",{"5":{"91":2}}],["分析了各自的优缺点和适用场景",{"5":{"91":2}}],["分析了隐式正则化效应",{"5":{"97":2}}],["分析其对模型输出的影响",{"5":{"145":2}}],["分析其几何意义和梯度特性",{"5":{"42":2}}],["分析其表达能力",{"5":{"87":2,"152":2}}],["分析单个头的信息容量",{"5":{"93":2}}],["分析不同激活函数对互信息的影响",{"5":{"133":2}}],["分析不同层之间隐藏状态的相关性",{"5":{"96":2}}],["分析不同头之间的信息互补性",{"5":{"93":2}}],["分析模型行为",{"5":{"95":2}}],["分段线性的突破",{"2":{"148":1},"5":{"148":1}}],["分段线性",{"5":{"71":3}}],["分别为特征图的高度和宽度",{"5":{"146":2}}],["分别为第",{"5":{"46":1}}],["分别满足一阶和二阶矩的演化方程",{"5":{"134":2}}],["分别是该批次数据的均值和方差向量",{"5":{"148":1}}],["分别是该层的输入和输出维度",{"5":{"148":1}}],["分别是模1",{"5":{"143":1}}],["分别是教师和学生模型的软化概率分布",{"5":{"137":1}}],["分别是键和值的维度",{"5":{"132":1}}],["分别是",{"5":{"41":1,"135":2}}],["分别是输入和输出的维度",{"5":{"41":1}}],["分别是两个头的输出",{"5":{"93":1}}],["分别编码粗粒度和细粒度的位置关系",{"5":{"71":2}}],["分别编码",{"5":{"71":1}}],["分别编码各层次的位置信息",{"5":{"88":2}}],["分别出现在不同的项中",{"5":{"88":1}}],["分别左乘这三个投影矩阵",{"5":{"94":1}}],["分别定义了键空间和值空间的投影",{"5":{"94":1}}],["分别表示键空间和值空间的维度",{"5":{"95":1}}],["分片",{"5":{"45":2}}],["分解形式",{"5":{"143":2}}],["分解方法",{"5":{"143":1}}],["分解方法的层次结构",{"5":{"143":1}}],["分解方法的层次结构理解这些分解技术的数学本质",{"5":{"143":1}}],["分解是tucker分解的另一种推广形式",{"5":{"143":2}}],["分解的选择原则",{"5":{"143":2}}],["分解的目标是找到一组",{"5":{"143":2}}],["分解技术的核心思想正是利用这种冗余性",{"5":{"143":2}}],["分解",{"5":{"50":4}}],["分解将一个张量表示为若干个秩一张量的和",{"5":{"50":2}}],["分解则将张量表示为一个核心张量与多个因子矩阵在各模上的乘积",{"5":{"50":2}}],["分解为低秩矩阵的乘积",{"5":{"87":2,"152":2}}],["分离度",{"5":{"70":2,"97":2}}],["分离",{"5":{"88":2}}],["分散",{"5":{"89":2}}],["分而治之",{"5":{"93":2,"159":2}}],["分块对角结构意味着不同头的投影在输入空间中是不相交的",{"5":{"93":2}}],["分组查询注意力的平衡设计",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["分组查询注意力",{"5":{"86":2,"151":2}}],["分组查询注意力是多查询注意力的折中方案",{"5":{"93":2}}],["分组多头注意力",{"5":{"93":2}}],["分组多头注意力的计算可以表示为",{"5":{"93":2}}],["分组注意力将头分为若干组",{"5":{"93":2}}],["分组注意力",{"5":{"93":2}}],["反而可以通过局部",{"5":{"147":2}}],["反而可能是有益的正则化",{"5":{"94":2}}],["反例",{"5":{"137":2}}],["反复展开",{"5":{"136":1}}],["反映任务重要性",{"5":{"133":1}}],["反映各任务的相对重要性",{"5":{"133":2}}],["反映了在长期运行中专家",{"5":{"158":2}}],["反映了专家对当前输入的",{"5":{"158":2}}],["反映了强化学习中两种不同的约束策略更新的思路",{"5":{"154":1}}],["反映了各个因素对模型性能的边际贡献",{"5":{"138":1}}],["反映了类别不平衡程度",{"5":{"137":1}}],["反映了注意力分布的可能性范围",{"5":{"132":2}}],["反映了不同头之间信息共享的程度",{"5":{"132":2}}],["反映了一种深刻的设计哲学",{"5":{"69":2}}],["反映了增长的速率",{"5":{"48":2}}],["反映了行随机矩阵的基本性质",{"5":{"87":2,"152":2}}],["反向传播时重新计算专家内部的中间激活",{"5":{"157":2}}],["反向传播时",{"5":{"150":2}}],["反向传播时对权重和偏置的梯度更新可能使用不同的学习率或正则化系数",{"5":{"46":2}}],["反向传播中的梯度范数由雅可比矩阵乘积的范数决定",{"5":{"148":2}}],["反向传播中的梯度计算",{"2":{"94":1},"5":{"94":1}}],["反向传播过程中计算得到的梯度在到达浅层网络时往往会变得极小",{"5":{"148":2}}],["反向传播特性及推理时的行为差异",{"5":{"146":2}}],["反向传播梯度推导",{"0":{"44":1},"4":{"44":1},"5":{"21":4,"44":1,"131":4}}],["反向传播梯度推导<",{"5":{"21":1,"131":1}}],["反向传播后",{"5":{"41":1}}],["反向传播与自动微分的关系",{"2":{"44":1},"5":{"44":1}}],["反向传播",{"5":{"44":3,"133":2}}],["反向传播本质上是链式法则在复合函数梯度计算中的高效应用",{"5":{"44":2}}],["反向传播通过巧妙地利用链式法则和动态规划思想",{"5":{"44":2}}],["反向传播的链式法则与雅可比矩阵",{"2":{"148":1},"5":{"148":1}}],["反向传播的",{"2":{"135":1},"5":{"135":1}}],["反向传播的梯度可以表示为",{"5":{"150":2}}],["反向传播的梯度满足",{"5":{"41":2}}],["反向传播的梯度满足某种形式的尺度约束",{"5":{"41":2}}],["反向传播的梯度为​",{"5":{"41":1}}],["反向传播的梯度为",{"5":{"41":1,"148":2}}],["反向传播的flops",{"5":{"44":2}}],["反向传播的主要计算包括两部分",{"5":{"44":2}}],["反向传播的总flops约为前向传播的两倍",{"5":{"44":2}}],["反向传播的核心递推关系为",{"5":{"148":2}}],["反向传播的核心数学工具是链式法则",{"5":{"44":2}}],["反向传播的核心是链式法则的高效应用",{"5":{"44":2}}],["反向传播的核心",{"5":{"44":1,"45":1}}],["反向传播的计算复杂度约为前向传播的两倍",{"5":{"44":2}}],["反向传播算法",{"5":{"44":2}}],["反向传播算法可以描述为",{"5":{"44":2}}],["反向传播算法对应于反向模式自动微分在神经网络中的应用",{"5":{"44":2}}],["反向传播算法沿着计算图反向遍历",{"5":{"48":2}}],["反向传播循环",{"5":{"44":2}}],["反向传播和参数更新",{"5":{"44":2}}],["反向传播是自动微分",{"5":{"44":2}}],["反向传播是反向模式自动微分",{"5":{"44":2}}],["反向传播正是沿此图的反向边计算各节点对损失的梯度",{"5":{"44":2}}],["反向传播从输出层开始",{"5":{"48":2}}],["反向传播遵循链式法则",{"5":{"51":2}}],["反向模式",{"5":{"44":2,"45":1}}],["反向模式自动微分从输出节点开始",{"5":{"44":2}}],["反向模式自动微分",{"5":{"44":1}}],["反向模式自动微分是深度学习中反向传播算法的理论基础",{"5":{"45":2}}],["反向模式的微分计算",{"5":{"45":2}}],["反向模式的核心是链式法则​",{"5":{"45":1}}],["反向模式的核心是链式法则",{"5":{"45":1}}],["反幂迭代",{"5":{"50":2}}],["反之亦然",{"5":{"41":2,"69":2,"71":2,"93":2}}],["反之",{"5":{"61":2,"90":2,"136":2,"138":4,"148":2,"149":2,"154":2,"155":4,"158":2}}],["自动均衡",{"5":{"159":1}}],["自动适应训练动态",{"5":{"147":1}}],["自动调整学习率",{"5":{"135":2}}],["自动微分的分类",{"5":{"44":2}}],["自动微分主要分为两种模式",{"5":{"44":2}}],["自动微分",{"5":{"45":4,"48":2,"133":2}}],["自动微分利用链式法则和计算图的结构",{"5":{"45":2}}],["自动微分分为前向模式",{"5":{"45":2}}],["自动微分将函数分解为基本操作的序列",{"5":{"48":2}}],["自动微分结合了数值计算的效率和符号计算的精确性",{"5":{"48":2}}],["自回归生成",{"5":{"140":2}}],["自回归与注意力的统一",{"5":{"140":2}}],["自回归似然函数",{"5":{"140":2}}],["自回归语言模型",{"5":{"140":2}}],["自回归语言建模",{"5":{"101":2}}],["自回归语言建模的目标是建模序列的联合概率分布",{"5":{"101":2}}],["自回归语言建模的形式化定义",{"2":{"101":1},"5":{"101":1}}],["自回归语言建模是大语言模型预训练的核心任务",{"5":{"101":2}}],["自回归",{"5":{"140":2}}],["自回归公式作为现代大模型的核心数学原理",{"5":{"140":2}}],["自回归公式",{"0":{"140":1},"4":{"140":1},"5":{"131":5,"140":1}}],["自回归分解形式",{"5":{"140":2}}],["自回归分解",{"5":{"101":2}}],["自回归或掩码方式",{"5":{"101":2}}],["自回归模型的一个关键优势是训练和推理阶段的高度一致性",{"5":{"140":2}}],["自回归模型的数学表述",{"2":{"140":1},"5":{"140":1}}],["自回归模型",{"5":{"41":2}}],["自然语言中的长程语义依赖",{"5":{"150":2}}],["自然语言中词语共现的模式",{"5":{"143":2}}],["自然梯度更新",{"5":{"137":1}}],["自然梯度更新在",{"5":{"137":1}}],["自然梯度相比于普通梯度",{"5":{"137":2}}],["自然梯度的几何意义",{"5":{"137":2}}],["自然涌现的结构",{"5":{"70":2}}],["自然",{"5":{"41":1,"44":2}}],["自然对数",{"5":{"61":2}}],["自化",{"5":{"42":4}}],["自身表示导数",{"5":{"42":1}}],["自适应kl惩罚系数",{"5":{"154":2}}],["自适应优化器中的等效噪声",{"5":{"149":2}}],["自适应优化算法的收敛性质",{"2":{"97":1},"5":{"97":1}}],["自适应优化算法",{"5":{"97":4}}],["自适应优化算法根据历史梯度信息为每个参数调整学习率",{"5":{"97":2}}],["自适应阈值",{"5":{"147":1}}],["自适应阈值策略",{"5":{"147":2}}],["自适应学习",{"5":{"145":1}}],["自适应学习率调整策略",{"5":{"136":2}}],["自适应学习率方法为每个参数单独调整学习率",{"5":{"48":2}}],["自适应学习率等技术有助于跳出较差的局部最小值",{"5":{"51":2}}],["自适应重要性加权",{"5":{"142":1}}],["自适应调度的收敛保证",{"5":{"136":2}}],["自适应调整公式通常为",{"5":{"154":2}}],["自适应调整每个参数的学习率",{"5":{"134":2}}],["自适应调整",{"5":{"133":1}}],["自适应调整参数以平衡学习速度和压缩程度",{"5":{"133":1}}],["自适应方法的",{"2":{"135":1},"5":{"135":1}}],["自适应方法的动态特性",{"2":{"134":1},"5":{"134":1}}],["自适应",{"5":{"133":2}}],["自适应多任务损失",{"5":{"101":2}}],["自适应损失函数可以写为",{"5":{"101":2}}],["自适应损失函数",{"2":{"101":1},"5":{"101":1}}],["自适应激活函数",{"5":{"41":2}}],["自适应稀疏模式",{"5":{"86":2,"151":2}}],["自助法",{"5":{"96":2}}],["自助法可用于估计困惑度",{"5":{"96":2}}],["自信息的概率论定义",{"2":{"61":1},"5":{"61":1}}],["自信息的数学定义为",{"5":{"61":2}}],["自信息",{"5":{"61":3,"137":2}}],["自由度从减少到",{"5":{"87":1,"152":1}}],["自由度从",{"5":{"87":1,"152":1}}],["自注意力提供多条信息路径",{"5":{"132":2}}],["自注意力一步完成全局聚合",{"5":{"132":2}}],["自注意力机制计算",{"5":{"140":2}}],["自注意力机制使得任意位置对",{"5":{"132":1}}],["自注意力机制使得任意位置对的可以非零",{"5":{"132":1}}],["自注意力机制的核心特性是任意位置可以直接与所有其他位置交互",{"5":{"132":2}}],["自注意力机制的信息流分析",{"2":{"132":1},"5":{"132":1}}],["自注意力机制可以被解释为一种",{"5":{"69":1}}],["自注意力机制可以被解释为一种互信息最大化的机制",{"5":{"69":1}}],["自注意力机制通过",{"5":{"69":1}}],["自注意力机制通过softmax权重的",{"5":{"69":2}}],["自注意力机制通过计算每个位置的输出",{"5":{"69":1}}],["自注意力机制需要计算查询向量与键向量的相似度",{"5":{"50":2}}],["自注意力机制",{"5":{"88":2,"141":2}}],["自注意力的全局传递",{"5":{"132":2}}],["自注意力的信息流特性",{"5":{"132":2}}],["自注意力的信息处理可视为信息瓶颈过程",{"5":{"132":2}}],["自注意力的信息路径长度为",{"5":{"132":2}}],["自注意力的信息覆盖范围定义为该位置能够有效访问的其他位置集合",{"5":{"132":2}}],["自注意力的核心运算是缩放后的点积注意力",{"5":{"88":2}}],["自注意力的输出为",{"5":{"92":2,"153":2}}],["自注意力中的softmax操作使得注意力权重对所有的和为1",{"5":{"97":1}}],["自注意力中的softmax操作使得注意力权重",{"5":{"97":1}}],["自注意力",{"5":{"101":2}}],["从模型表达能力的角度分析",{"5":{"159":2}}],["从模型输出到最终损失值的完整数学流程可以表示为以下变换链",{"5":{"61":2}}],["从延迟的",{"5":{"156":2}}],["从人类反馈中学习的强化学习",{"5":{"154":2}}],["从sgd的噪声模型出发",{"5":{"149":2}}],["从softmax统一性看transformer的设计哲学",{"2":{"69":1},"5":{"69":1}}],["从隐式正则化的角度看",{"5":{"147":2}}],["从动力系统的角度分析梯度裁剪可以帮助理解其行为模式",{"5":{"147":2}}],["从动力系统的角度来看",{"5":{"147":2}}],["从2015年批归一化",{"5":{"146":2}}],["从2017年vaswani等人提出的原始正弦位置编码",{"5":{"88":2}}],["从优化角度看",{"5":{"145":2,"147":2}}],["从优化的角度理解",{"5":{"142":2}}],["从优化的角度看",{"5":{"96":2,"158":2,"159":4}}],["从优化的角度",{"5":{"91":2,"93":2}}],["从优化的角度来看",{"5":{"94":2}}],["从优化的视角审视",{"5":{"96":2}}],["从下一层的角度来看",{"5":{"145":2}}],["从连续时间视角分析",{"5":{"144":2}}],["从连续时间动力学的角度分析梯度下降有助于理解其收敛行为",{"5":{"97":2}}],["从深度学习应用的角度",{"5":{"143":2}}],["从经典的学习理论观点看",{"5":{"149":2}}],["从经典的奇异值分解",{"5":{"143":2}}],["从经验风险最小化的角度",{"5":{"61":2}}],["从主成分分析到张量分解",{"2":{"143":1},"5":{"143":1}}],["从简单统计到复杂语义理解",{"5":{"140":1}}],["从简单统计到复杂语义理解本节从概率论的基本原理出发",{"5":{"140":1}}],["从简单的曲线到高维空间中的复杂流形",{"5":{"71":2}}],["从小规模统计模型到数十亿参数",{"5":{"140":2}}],["从顺序处理到并行计算",{"5":{"140":2}}],["从固定短上下文到可处理超长上下文",{"5":{"140":2}}],["从n",{"5":{"140":2}}],["从nce到infonce的理论演进",{"2":{"69":1},"5":{"69":1}}],["从二元语法到gpt",{"2":{"140":1},"5":{"140":1}}],["从二阶条件",{"5":{"89":2}}],["从统计可靠性角度理解涌现",{"5":{"138":1}}],["从统计可靠性角度理解涌现揭示了其本质是学习精度的量化跃升",{"5":{"138":1}}],["从统计学角度",{"5":{"69":2}}],["从统计学角度来看",{"5":{"51":2}}],["从统计学中的最小二乘法到深度学习中的回归任务",{"5":{"51":2}}],["从统计学习理论的角度",{"5":{"87":2,"142":2,"152":2}}],["从统计学的角度",{"5":{"92":2,"153":2}}],["从无到有",{"5":{"138":2}}],["从百万到十亿级别",{"5":{"138":2}}],["从llama到各类开源模型",{"5":{"138":2}}],["从logits到概率的完整流程",{"2":{"61":1},"5":{"61":1}}],["从logits到概率分布的转换通过softmax函数实现",{"5":{"96":1}}],["从logits到概率分布的转换通过",{"5":{"96":1}}],["从gpt系列到claude",{"5":{"138":2}}],["从视角看与从视角看是不同的",{"5":{"137":1}}],["从分岔理论视角理解超参数变化导致的行为跃迁",{"5":{"136":2}}],["从分辨率的角度",{"5":{"90":4}}],["从更广阔的视角来看",{"5":{"149":2}}],["从更多初始点可以到达平坦最小值",{"5":{"136":2}}],["从更高的视角来看",{"5":{"88":2}}],["从中心极限定理的角度来看",{"5":{"149":2}}],["从中心极限定理的角度看",{"5":{"96":2}}],["从中出发的轨道最终收敛到极限环",{"5":{"136":2}}],["从初始状态",{"5":{"134":2}}],["从数据压缩和信息提取的角度解释了为什么深度网络能够学习有效的表示",{"5":{"133":2}}],["从数学原理到工程实现",{"5":{"157":2}}],["从数学定义出发",{"5":{"155":2}}],["从数学角度看",{"5":{"47":2,"71":2,"86":2,"88":2,"139":2,"147":2,"148":2,"151":2,"158":2}}],["从数学角度来看",{"5":{"44":2,"45":2,"89":2,"93":2}}],["从数学角度",{"5":{"86":2,"89":2,"151":2}}],["从数学角度分析两者的协同作用",{"5":{"147":2}}],["从数学角度分析这种分工现象",{"5":{"93":2}}],["从数学角度分析",{"5":{"93":2}}],["从数学上分析",{"5":{"71":4}}],["从数学上看",{"5":{"96":2,"159":2}}],["从标准正态分布采样噪声",{"5":{"133":2}}],["从特征中学习信息过滤",{"5":{"132":2}}],["从特征学习的角度来看",{"5":{"47":2}}],["从离散分布中采样",{"5":{"132":2}}],["从互信息的角度分析",{"5":{"132":2}}],["从个位置中选择个最重要位置",{"5":{"132":1}}],["从dpo的目标函数可以反推出一个隐式的奖励函数",{"5":{"101":2}}],["从均方误差的信息论起源开始",{"5":{"101":2}}],["从均匀分布中采样每个编码元素",{"5":{"89":1}}],["从均匀分布",{"5":{"89":1}}],["从1减小到0时",{"5":{"70":1}}],["从1到0",{"5":{"70":2}}],["从损失函数的角度",{"5":{"70":2}}],["从损失函数的定义到模型性能的评价",{"5":{"96":2}}],["从infonce的统一视角",{"5":{"69":2}}],["从infonce到注意力的数学桥梁",{"2":{"69":1},"5":{"69":1}}],["从几何和概率的角度分析最小值",{"5":{"135":2}}],["从几何上看",{"5":{"42":2}}],["从几何角度理解",{"5":{"41":2,"71":4}}],["从几何角度看",{"5":{"42":2,"45":2,"46":2,"47":2,"69":2,"141":2,"146":2,"147":4}}],["从几何角度分析",{"5":{"42":2}}],["从几何角度来看",{"5":{"48":2,"50":2,"91":2,"94":2}}],["从几何角度",{"5":{"90":2,"91":2}}],["从几何视角来看",{"5":{"47":2}}],["从几何视角理解",{"5":{"61":2}}],["从导数公式可以看出",{"5":{"41":1}}],["从导数公式",{"5":{"41":1}}],["从极限的角度分析sigmoid函数的边界行为",{"5":{"42":2}}],["从某种意义上",{"5":{"42":2}}],["从变换的角度",{"5":{"42":2}}],["从而促进专家分配的多样性",{"5":{"159":2}}],["从而促进专家的专业化",{"5":{"159":2}}],["从而长期被冷落",{"5":{"159":2}}],["从而充分利用模型的全部容量",{"5":{"159":2}}],["从而确保专家的长期使用频率与其被选中的概率一致",{"5":{"158":2}}],["从而确保位置只能关注位置到",{"5":{"95":1}}],["从而确保位置",{"5":{"95":1}}],["从而减少了总计算量",{"5":{"158":2}}],["从而保留专家专业化的空间",{"5":{"158":2}}],["从而保留了门控权重梯度与专家贡献之间的关系",{"5":{"157":2}}],["从而保持模型的表达能力",{"5":{"146":2}}],["从而计算出优势函数",{"5":{"155":2}}],["从而得到梯度的无偏估计",{"5":{"155":2}}],["从而阻止了策略过度偏离旧策略",{"5":{"154":2}}],["从而捕捉输入数据的不同方面",{"5":{"150":2}}],["从而捕获更抽象的语义信息",{"5":{"45":2}}],["从而缓解深度网络的训练困难",{"5":{"150":2}}],["从而缓解了梯度消失问题",{"5":{"50":2}}],["从而提供了最有效的逃离",{"5":{"149":2}}],["从而提高泛化性能",{"5":{"137":2}}],["从而提高mlm的预测准确性",{"5":{"101":2}}],["从而提高数值稳定性",{"5":{"61":2}}],["从而维持稳定的梯度流",{"5":{"148":2}}],["从而将计算复杂度从",{"5":{"159":2}}],["从而将计算量从",{"5":{"159":2}}],["从而将通信负载均匀分布到所有设备上",{"5":{"157":2}}],["从而将梯度从掩码传递到原始门控得分",{"5":{"157":2}}],["从而将参数限制在有界区域内",{"5":{"147":2}}],["从而将复杂的多变量期望分解为单变量期望的乘积",{"5":{"145":2}}],["从而控制模型复杂度",{"5":{"147":2}}],["从而控制输入扰动对输出的影响",{"5":{"144":2}}],["从而防止参数跳入不稳定的区域",{"5":{"147":2}}],["从而防止神经元之间过度协同适应",{"5":{"145":2}}],["从而防止梯度爆炸导致的训练发散",{"5":{"136":2}}],["从而间接限制了梯度的尺度",{"5":{"144":2}}],["从而间接影响损失函数的收敛速度和最终性能",{"5":{"70":2}}],["从而改善了深层网络的训练动态",{"5":{"150":2}}],["从而改善泛化性能",{"5":{"144":2}}],["从而改善外推性能",{"5":{"88":2}}],["从而严格遵循自回归的数学框架",{"5":{"140":2}}],["从而指导我们设计更高效",{"5":{"137":2}}],["从而指导实际的模型训练过程",{"5":{"136":2}}],["从而调节信息选择的集中程度",{"5":{"132":2}}],["从而调整的结构",{"5":{"94":1}}],["从而调整",{"5":{"94":1}}],["从而导致策略优化走向错误的方向",{"5":{"101":2}}],["从而打破了上述对称性",{"5":{"101":2}}],["从而传递更多信息",{"5":{"69":2}}],["从而影响泛化性能",{"5":{"147":2}}],["从而影响学习动态和最终表示的质量",{"5":{"69":2}}],["从而影响模型对位置模式的捕获能力",{"5":{"71":2}}],["从而影响各种损失函数的收敛速度和最终性能",{"5":{"101":2}}],["从而通过stone",{"5":{"71":2}}],["从而逼近任意形状的函数曲面",{"5":{"71":2}}],["从而更准确地估计和最大化互信息",{"5":{"71":2}}],["从而开启了深度学习的现代时代",{"5":{"44":2}}],["从而解决异或等线性不可分问题",{"5":{"47":2}}],["从而大幅降低计算和存储成本",{"5":{"48":2}}],["从而使得机器学习算法能够在这些向量上进行运算和优化",{"5":{"50":2}}],["从而约束了网络函数的lipschitz常数",{"5":{"50":2}}],["从而在固定的计算预算下获得更强的模型",{"5":{"159":2}}],["从而在相同的计算预算下支持更大的模型容量",{"5":{"159":2}}],["从而在实际应用中更有效地使用它",{"5":{"154":2}}],["从而在实际应用中做出更好的选择",{"5":{"89":2}}],["从而在某些方向上完全阻断梯度流",{"5":{"148":2}}],["从而在反向传播时为梯度提供了一条恒等映射路径",{"5":{"50":2}}],["从而在多个计算设备上分布式执行",{"5":{"50":2}}],["从而对应不同的注意力行为",{"5":{"86":2,"151":2}}],["从而加速训练过程并提升模型性能",{"5":{"146":2}}],["从而加速收敛",{"5":{"135":2}}],["从而加速收敛并减少震荡",{"5":{"48":2}}],["从而加速注意力计算",{"5":{"86":2,"151":2}}],["从而避免下溢",{"5":{"44":2}}],["从而避免条件数过大",{"5":{"87":4,"152":4}}],["从而正确处理词序承载的语法和语义信息",{"5":{"91":2}}],["从而实现更加精细的函数逼近",{"5":{"159":2}}],["从而实现更好的训练效果和泛化性能",{"5":{"136":2}}],["从而实现动态秩调整",{"5":{"142":2}}],["从而实现了真正的全上下文依赖",{"5":{"140":2}}],["从而实现高效的存储和计算",{"5":{"50":2}}],["从而实现信息聚合",{"5":{"92":2,"153":2}}],["从而实现灵活的注意力计算",{"5":{"94":2}}],["从而极大地增强了模型的表达能力",{"5":{"93":2}}],["从而产生不同的泛化行为",{"5":{"147":2}}],["从而产生低秩解",{"5":{"144":2}}],["从而产生更大的梯度来",{"5":{"69":2}}],["从而产生较高的注意力分数",{"5":{"95":2}}],["从而允许我们使用概率论的统一工具进行分析和优化",{"5":{"69":2}}],["从而允许使用更大的学习率和更稳定的收敛过程",{"5":{"150":2}}],["从而允许使用更大的学习率",{"5":{"97":2}}],["从直观上看",{"5":{"71":2}}],["从贝叶斯推断的角度看",{"5":{"147":2}}],["从贝叶斯推断的角度",{"5":{"71":2,"90":2}}],["从多项式分布的角度",{"5":{"71":2}}],["从链式法则的矩阵形式出发",{"5":{"44":2}}],["从前向传播的数学描述到实际代码实现",{"5":{"45":1}}],["从神经元到层",{"2":{"46":1},"5":{"46":1}}],["从生物神经元到数学抽象",{"2":{"47":1},"5":{"47":1}}],["从麦肯罗皮层神经元的二值模型出发",{"5":{"47":2}}],["从概率的角度理解",{"5":{"69":2}}],["从概率论角度",{"5":{"71":2}}],["从概率论视角审视自信息的定义",{"5":{"61":2}}],["从概率论的角度来看",{"5":{"138":2}}],["从概率论的角度",{"5":{"71":4}}],["从概率论的视角",{"5":{"61":2}}],["从概率论的视角来看",{"5":{"95":2}}],["从概率角度看",{"5":{"47":2}}],["从",{"5":{"47":1,"70":3,"92":1,"132":5,"135":2,"137":1,"153":1}}],["从很小的值线性增长到目标学习率",{"5":{"48":2}}],["从梯度传播的角度看",{"5":{"148":2}}],["从梯度公式",{"5":{"69":1}}],["从梯度公式可以看出",{"5":{"69":1}}],["从梯度角度分析",{"5":{"71":2,"144":4}}],["从梯度计算的期望到模型不确定性的估计",{"5":{"96":2}}],["从梯度流动角度",{"5":{"91":2}}],["从梯度分析的角度来看",{"5":{"92":2,"153":2}}],["从梯度的角度",{"5":{"92":2,"153":2}}],["从应用上看",{"5":{"96":2}}],["从公式可以看出",{"5":{"96":2}}],["从transformer架构的注意力机制到词嵌入的向量表示",{"5":{"50":2}}],["从矩阵乘法的并行计算到张量运算的维度变换",{"5":{"50":2}}],["从矩阵运算的角度来看",{"5":{"51":2}}],["从矩阵分析的角度",{"5":{"61":2}}],["从矩阵计算的角度来看",{"5":{"95":2}}],["从张量网络的角度来看",{"5":{"50":2}}],["从张量的角度来看",{"5":{"50":2}}],["从张量代数的角度来看",{"5":{"93":2}}],["从拉普拉斯分布假设出发",{"5":{"51":2}}],["从它的数学定义",{"5":{"51":2}}],["从kl散度的角度看",{"5":{"96":2}}],["从kl散度视角的再解释",{"2":{"61":1},"5":{"61":1}}],["从期望的形式来看",{"5":{"61":2}}],["从根本上解决了数值溢出问题",{"5":{"61":2,"97":2}}],["从融合计算的角度",{"5":{"61":2}}],["从偏微分的角度分析",{"5":{"61":2}}],["从不同角度对标准注意力进行了改进或扩展",{"5":{"86":2,"151":2}}],["从线性变换的角度看",{"5":{"143":2}}],["从线性映射的局限性出发",{"5":{"71":2}}],["从线性代数的角度审视",{"5":{"158":2}}],["从线性代数的角度来看",{"5":{"87":2,"94":2,"152":2}}],["从线性代数的角度",{"5":{"90":2,"91":4,"93":2}}],["从线性代数的视角来看",{"5":{"95":2}}],["从核方法",{"5":{"87":2,"152":2}}],["从谱性质的角度",{"5":{"87":2,"152":2}}],["从谱分析的角度",{"5":{"87":2,"152":2}}],["从幅度到相位的范式转换",{"2":{"88":1},"5":{"88":1}}],["从抽象代数的角度看",{"5":{"88":2}}],["从频域分析的角度看",{"5":{"88":2}}],["从频谱分析的角度看",{"5":{"88":2}}],["从频谱的角度",{"5":{"90":2}}],["从频率分析的角度",{"5":{"90":2}}],["从频率域的角度",{"5":{"90":6}}],["从频率域看",{"5":{"90":2}}],["从工程角度看",{"5":{"88":2}}],["从历史角度看",{"5":{"88":2}}],["从4k到32k",{"5":{"88":2}}],["从信息几何的角度",{"5":{"70":2}}],["从信息论的视角审视注意力机制",{"5":{"132":2}}],["从信息论的角度",{"5":{"69":2,"71":4,"89":4,"90":2}}],["从信息论的角度看",{"5":{"88":6,"145":2}}],["从信息论的角度来看",{"5":{"92":2,"95":2,"153":2}}],["从信息论角度分析了编码器",{"5":{"132":2}}],["从信息论角度分析长程依赖的建立机制",{"5":{"132":2}}],["从信息论角度理解这些扩展有助于更好地设计和优化注意力机制",{"5":{"132":2}}],["从信息论角度看",{"5":{"132":6}}],["从信息论角度",{"5":{"41":2,"61":2,"71":8,"101":2}}],["从信息论视角",{"5":{"61":2}}],["从信息编码的角度",{"5":{"61":2}}],["从信息传播的角度",{"5":{"87":2,"152":2}}],["从信息瓶颈角度",{"5":{"132":2}}],["从信息瓶颈",{"5":{"87":2,"152":2}}],["从信息融合角度",{"5":{"91":2}}],["从信息聚合的角度来看",{"5":{"92":2,"153":2}}],["从信息流的角度",{"5":{"92":2,"153":2}}],["从信号处理的角度重新思考位置编码",{"5":{"88":1}}],["从微分几何的角度",{"5":{"89":2}}],["从秩的角度",{"5":{"89":2}}],["从参数利用效率的角度分析",{"5":{"159":2}}],["从参数量的角度分析",{"5":{"158":2}}],["从参数化的角度看",{"5":{"146":2}}],["从参数化角度",{"5":{"89":2,"91":2}}],["从参数效率的角度",{"5":{"89":4,"90":2}}],["从参数效率角度",{"5":{"91":2}}],["从函数逼近论的角度",{"5":{"71":2}}],["从函数逼近的角度",{"5":{"87":2,"89":2,"152":2}}],["从函数逼近的角度来看",{"5":{"92":2,"150":2,"153":2}}],["从函数逼近理论的角度分析",{"5":{"93":2}}],["从函数空间角度",{"5":{"89":2}}],["从计算到函数逼近",{"2":{"45":1},"5":{"45":1}}],["从计算角度看",{"5":{"158":2}}],["从计算角度",{"5":{"89":2}}],["从计算复杂度的角度分析",{"5":{"159":4}}],["从计算复杂度的角度",{"5":{"92":2,"153":2}}],["从计算复杂度的角度看",{"5":{"93":2}}],["从实验结果来看",{"5":{"89":2}}],["从正态分布中采样",{"5":{"89":1}}],["从正态分布",{"5":{"89":1}}],["从对数频率的角度来看",{"5":{"90":2}}],["从傅里叶分析的角度来看",{"5":{"90":2}}],["从傅里叶分析的角度",{"5":{"90":2}}],["从相位角度理解位置编码",{"5":{"90":2}}],["从群论的角度",{"5":{"90":2}}],["从覆盖范围的角度",{"5":{"90":2}}],["从维度角度",{"5":{"91":2}}],["从0开始或从1开始",{"5":{"91":2}}],["从0到",{"5":{"91":2,"136":1}}],["从另一个角度看",{"5":{"147":2}}],["从另一个角度",{"5":{"91":2}}],["从这个表达式可以清晰地看到",{"5":{"148":2}}],["从这个关系可以看出",{"5":{"138":2}}],["从这个点向预测直线作垂线",{"5":{"51":2}}],["从这个示例可以看出",{"5":{"91":2}}],["从位置域看",{"5":{"90":2}}],["从位置差0时的",{"5":{"91":2}}],["从位置到位置",{"5":{"92":1,"153":1}}],["从位置到位置的信息传递路径是中的一条有向路径",{"5":{"92":1,"153":1}}],["从位置",{"5":{"92":3,"153":3}}],["从泛化角度",{"5":{"89":2}}],["从泛化能力角度",{"5":{"91":2}}],["从归纳偏置角度",{"5":{"91":2}}],["从图论的角度来看",{"5":{"92":2,"153":2}}],["从到的",{"5":{"70":1}}],["从到的仿射变换定义为",{"5":{"47":1}}],["从到",{"5":{"92":1,"153":1}}],["从整个序列聚合的信息",{"5":{"92":2,"153":2}}],["从单头到多头的动机",{"2":{"93":1},"5":{"93":1}}],["从重参数化",{"5":{"93":2}}],["从输入embedding通过线性变换得到",{"5":{"88":2}}],["从输入空间到查询空间的映射可以写为",{"5":{"93":2}}],["从理论上讲",{"5":{"147":2}}],["从理论上分析",{"5":{"101":2}}],["从理论上看",{"5":{"96":2}}],["从理论分析的角度",{"5":{"93":2}}],["从表中可以看出",{"5":{"146":1}}],["从表达能力角度",{"5":{"91":2}}],["从表达能力的角度分析",{"5":{"93":2}}],["从表示学习的角度来看",{"5":{"94":2}}],["从文章中提取相关信息",{"5":{"94":2}}],["从原始输入空间",{"5":{"47":1}}],["从原始的嵌入空间",{"5":{"94":1}}],["从雅可比矩阵的结构可以看出",{"5":{"95":2}}],["从预训练阶段的语言建模损失和掩码语言建模损失",{"5":{"101":2}}],["表9",{"5":{"148":2}}],["表",{"5":{"140":2,"142":2,"143":6,"144":2,"145":6,"146":8,"147":2}}],["表示专家",{"5":{"158":2}}],["表示将矩阵",{"5":{"157":2}}],["表示轨迹是根据策略",{"5":{"155":2}}],["表示智能体与环境交互产生的一条轨迹",{"5":{"155":2}}],["表示人类对该回复的偏好程度",{"5":{"154":2}}],["表示允许的策略变化幅度",{"5":{"154":2}}],["表示向量的第",{"5":{"149":1}}],["表示向量的第个分量",{"5":{"149":1}}],["表示任意相容的矩阵范数",{"5":{"148":1}}],["表示欧几里得投影",{"5":{"147":2}}],["表示层归一化",{"5":{"146":1}}],["表示逐元素",{"5":{"158":2}}],["表示逐元素hadamard乘法",{"5":{"145":1}}],["表示逐元素乘法",{"5":{"44":1,"48":2,"51":2,"97":1,"146":2,"159":2}}],["表示每个神经元被保留的概率",{"5":{"145":2}}],["表示每个头被选中的概率或权重",{"5":{"93":2}}],["表示能力的增强",{"5":{"140":2}}],["表示为",{"5":{"138":2}}],["表示训练计算量",{"5":{"138":2}}],["表示训练数据量",{"5":{"138":2}}],["表示模型参数量",{"5":{"138":2}}],["表示模型对所有类别一视同仁",{"5":{"137":2}}],["表示模型对第",{"5":{"61":1}}],["表示测试损失",{"5":{"138":2}}],["表示和输出之间的信息联系",{"5":{"137":2}}],["表示和",{"5":{"137":2}}],["表示学习的互信息下界",{"5":{"137":2}}],["表示存在主导类别",{"5":{"137":2}}],["表示函数",{"5":{"136":1}}],["表示损失值随训练时间的变化",{"5":{"134":2}}],["表示所有可能状态的集合",{"5":{"134":2}}],["表示丢失关键信息",{"5":{"133":2}}],["表示包含过多输入细节",{"5":{"133":2}}],["表示的信息量受限",{"5":{"133":2}}],["表示的形式",{"5":{"42":1}}],["表示注意力越集中在少数位置",{"5":{"132":2}}],["表示注意力越分散在多个位置",{"5":{"132":2}}],["表示除位置",{"5":{"101":1}}],["表示两个在语义上相关的样本",{"5":{"69":2}}],["表示位置是填充token",{"5":{"101":1}}],["表示位置",{"5":{"92":2,"101":2,"153":2}}],["表示sigmoid曲线在各点的斜率",{"5":{"42":2}}],["表示样本属于第",{"5":{"71":1}}],["表示空间的层次结构",{"5":{"45":2}}],["表示数学运算",{"5":{"45":2}}],["表示数据",{"5":{"45":2}}],["表示​",{"5":{"45":1}}],["表示​范数",{"5":{"51":1}}],["表示",{"5":{"45":1,"50":2,"51":2,"96":1,"136":1,"137":2,"155":2}}],["表示取特定值的概率",{"5":{"96":1}}],["表示生成该词的可能性",{"5":{"96":2}}],["表示线性无关",{"5":{"96":2}}],["表示完全负相关",{"5":{"96":2}}],["表示完全正相关",{"5":{"96":1}}],["表示矩阵转置",{"5":{"51":2}}],["表示期望运算",{"5":{"51":2}}],["表示对训练数据集的期望",{"5":{"51":2}}],["表示对给定的的期望",{"5":{"51":1}}],["表示对给定",{"5":{"51":1}}],["表示第",{"5":{"51":1,"90":1,"92":1,"95":2,"134":2,"146":2,"149":1,"153":1}}],["表示第个参数方向上的梯度方差",{"5":{"149":1}}],["表示第个样本的特征向量的中心位置",{"5":{"146":1}}],["表示第个查询向量",{"5":{"95":1}}],["表示第个键向量",{"5":{"95":1}}],["表示该事件的发生几乎不提供新的信息",{"5":{"61":2}}],["表示在当前批次中",{"5":{"158":2}}],["表示在给定提示",{"5":{"154":2}}],["表示在",{"5":{"140":1}}],["表示在sigmoid曲线上各点的斜率",{"5":{"42":2}}],["表示在计算第",{"5":{"61":2}}],["表示在第步使用的学习率",{"5":{"97":1}}],["表示在第",{"5":{"97":1}}],["表示使用基于分布",{"5":{"61":2}}],["表示从到经过步间接传播的比例",{"5":{"87":1,"152":1}}],["表示从",{"5":{"87":1,"152":1}}],["表示维度索引",{"5":{"91":2}}],["表示序列中的第",{"5":{"140":1}}],["表示序列中的位置索引",{"5":{"91":2}}],["表示序列中各位置之间的信息流动关系",{"5":{"92":2,"153":2}}],["表示输入在特定子空间中的关联模式",{"5":{"93":2}}],["表示互信息",{"5":{"93":2}}],["表示分块对角矩阵",{"5":{"93":1}}],["表示熵",{"5":{"93":1}}],["表示信息需求的抽象表示",{"5":{"94":2}}],["表示各类别样本数量均衡",{"5":{"137":2}}],["表示各个信息片段的标识符",{"5":{"94":2}}],["表示各片段携带的实际信息内容",{"5":{"94":2}}],["表示键矩阵",{"5":{"95":2}}],["表示值矩阵",{"5":{"95":2}}],["表示查询位置对键位置的注意力权重",{"5":{"95":1}}],["表示查询位置",{"5":{"95":1}}],["表4",{"5":{"44":6,"45":2}}],["表2",{"5":{"45":2}}],["表达式是线性变换与平移的组合",{"5":{"47":1}}],["表达式",{"5":{"47":1}}],["表达能力更强",{"5":{"71":2}}],["表达能力更强的激活函数可能提供更紧的下界",{"5":{"71":2}}],["表达能力的理论界限",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["表达能力",{"5":{"87":2,"152":2,"159":2}}],["表达能力分析",{"2":{"89":1},"5":{"89":1}}],["表达能力最强",{"5":{"89":2}}],["表达能力受限",{"5":{"89":2}}],["表达能力越强",{"5":{"89":2}}],["表达能力越弱",{"5":{"89":2}}],["表达能力与计算能力界限",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["表达能力来源和计算特性",{"5":{"93":2}}],["表明只有被激活的专家才会接收到梯度来更新参数",{"5":{"157":2,"158":2}}],["表明其发生带来了显著的信息增益",{"5":{"61":2}}],["表明",{"5":{"61":3,"87":2,"150":1,"152":2}}],["表明如果",{"5":{"61":1}}],["表明高层表示更加",{"5":{"87":2,"152":2}}],["表明高层表示更加紧凑",{"5":{"87":2,"152":2}}],["表明它是输入",{"5":{"71":1}}],["表明它们在",{"5":{"92":4,"153":4}}],["表明注意力机制的优化与分类任务的优化具有相似的数学性质",{"5":{"97":2}}],["表面上看",{"5":{"88":2}}],["多数专家的",{"5":{"159":2}}],["多数特征值很小",{"5":{"48":2}}],["多尺度特征提取的数学原理",{"5":{"150":1}}],["多尺度特征提取的数学原理源于信号处理中的多分辨率分析理论",{"5":{"150":1}}],["多尺度设计不可避免地增加了计算量",{"5":{"150":2}}],["多尺度频率结构可以看作是一种高效的编码方案",{"5":{"90":2}}],["多因素标度律的形式",{"5":{"138":1}}],["多因素标度律的形式需要考虑参数规模",{"5":{"138":1}}],["多步计算",{"5":{"138":2}}],["多模态模型的信息论分析和涌现能力的信息论解释",{"5":{"137":2}}],["多模态学习",{"5":{"41":2}}],["多智能体协作信息论",{"5":{"137":2}}],["多条路径确保即使部分路径阻塞",{"5":{"132":2}}],["多变量互信息的链式分解",{"5":{"132":2}}],["多样性注意力正则化",{"5":{"132":2}}],["多样性注意力的信息论基础",{"5":{"132":2}}],["多样性选择带来信息增益",{"5":{"132":2}}],["多样性子注意力鼓励选择信息互补的位置",{"5":{"132":2}}],["多任务场景时特别有效",{"5":{"159":2}}],["多任务与迁移学习",{"5":{"133":2}}],["多任务信息瓶颈的目标是对所有任务进行联合优化",{"5":{"133":2}}],["多任务信息瓶颈目标",{"5":{"133":2}}],["多任务损失",{"5":{"101":2}}],["多任务学习中的知识迁移可以通过互信息进行量化评估",{"5":{"133":2}}],["多任务学习中的损失函数组合",{"2":{"70":1,"101":1},"5":{"70":1,"101":1}}],["多任务学习的总体损失为",{"5":{"101":2}}],["多任务学习的数学框架",{"2":{"101":1},"5":{"101":1}}],["多任务学习",{"5":{"101":2}}],["多组并行的",{"5":{"69":1}}],["多分类任务",{"5":{"44":2}}],["多分类任务与softmax函数",{"2":{"61":1},"5":{"61":1}}],["多分类任务要求模型为每个样本预测其在所有个类别上的概率分布",{"5":{"61":1}}],["多分类任务要求模型为每个样本预测其在所有",{"5":{"61":1}}],["多分类情况的证明类似",{"5":{"44":2}}],["多分类交叉熵损失的完整推导",{"2":{"61":1},"5":{"61":1}}],["多分类交叉熵损失为",{"5":{"61":2}}],["多个计算节点贡献的梯度具有什么样的协方差结构",{"5":{"149":2}}],["多个这样的超平面组合",{"5":{"71":2}}],["多个神经元的组合形成了一个丰富的特征基",{"5":{"47":2}}],["多个样本的信息被平均",{"5":{"47":2}}],["多个注意力层被堆叠在一起",{"5":{"92":2,"153":2}}],["多个层的组合则构成了这些映射的复合",{"5":{"45":2}}],["多个层面理解信息",{"5":{"93":2}}],["多个头的梯度会复合",{"5":{"41":2}}],["多个头可以通过各自的非线性变换组合出更丰富的交互模式",{"5":{"93":2}}],["多层网络",{"5":{"133":2}}],["多层网络的梯度是各层雅可比矩阵的连乘",{"5":{"148":2}}],["多层网络的flops",{"5":{"45":2}}],["多层网络的矩阵形式",{"2":{"46":1},"5":{"46":1}}],["多层网络的表达能力超越了单层网络",{"5":{"46":2}}],["多层网络的复合函数表示",{"5":{"46":2}}],["多层神经网络",{"5":{"46":2}}],["多层神经网络可以学习复杂的非线性决策边界",{"5":{"47":2}}],["多层神经网络叠加",{"5":{"97":2}}],["多层线性网络的恒等性",{"5":{"47":2}}],["多层感知机",{"5":{"47":2}}],["多层堆叠使得网络能够表示极其复杂的函数",{"5":{"71":2}}],["多层堆叠中的表示变换",{"2":{"94":1},"5":{"94":1}}],["多层叠加和梯度存储会使内存压力进一步增大",{"5":{"95":2}}],["多gpu",{"5":{"45":1}}],["多节点",{"5":{"45":1}}],["多元函数梯度与hessian",{"2":{"48":1},"5":{"48":1}}],["多元高斯分布是一维高斯分布向多维空间的推广",{"5":{"96":2}}],["多元高斯分布的pdf为",{"5":{"96":2}}],["多元高斯分布具有许多重要的性质",{"5":{"96":2}}],["多项logit模型",{"5":{"69":2}}],["多项分布的共轭先验是dirichlet分布",{"5":{"96":2}}],["多项式分布的似然函数为",{"5":{"71":2}}],["多项式特征映射",{"5":{"86":2,"151":2}}],["多项式核可以捕获特征之间的高阶交互",{"5":{"86":2,"151":2}}],["多重比较问题是在进行多个假设检验时需要考虑的重要问题",{"5":{"96":2}}],["多重比较谬误",{"5":{"96":2}}],["多少信息应该通过",{"5":{"150":2}}],["多少资源分配给增加数据量",{"5":{"138":2}}],["多少",{"5":{"61":2}}],["多查询注意力",{"5":{"86":4,"93":2,"151":4}}],["多查询注意力的数学定义",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["多查询注意力的query投影参数量为",{"5":{"86":2,"151":2}}],["多查询注意力的参数数量从减少到​",{"5":{"93":1}}],["多查询注意力的参数数量从",{"5":{"93":1}}],["多查询注意力可以将参数量减少约",{"5":{"86":2,"151":2}}],["多查询注意力与分组查询注意力",{"2":{"93":1},"5":{"93":1}}],["多查询注意力等",{"5":{"93":2}}],["多次加载到sram",{"5":{"86":2,"151":2}}],["多头冗余度分析",{"5":{"132":2}}],["多头注意力子层的输出为",{"5":{"158":2}}],["多头注意力机制中的多个头可以看作是张量分解的一种形式",{"5":{"143":2}}],["多头注意力层",{"5":{"132":4}}],["多头注意力",{"5":{"69":2,"93":2,"132":2}}],["多头注意力的分解",{"5":{"143":2}}],["多头注意力的信息分解",{"5":{"132":2}}],["多头注意力的梯度复合",{"5":{"41":2}}],["多头注意力的矩阵推导与表达能力分析",{"0":{"93":1},"4":{"93":1},"5":{"93":1,"131":4}}],["多头注意力的矩阵推导与表达能力分析<",{"5":{"131":1}}],["多头注意力的表达能力",{"5":{"86":2,"151":2}}],["多头注意力的表达能力提升可以从子空间学习的角度来理解",{"5":{"93":2}}],["多头注意力的数学结构",{"2":{"69":1},"5":{"69":1}}],["多头注意力的数学定义",{"2":{"93":1},"5":{"93":1}}],["多头注意力的数学定义如公式",{"5":{"93":2}}],["多头注意力的张量计算形式",{"2":{"93":1},"5":{"93":1}}],["多头注意力的复杂度",{"2":{"93":1},"5":{"93":1}}],["多头注意力的函数逼近视角",{"2":{"93":1},"5":{"93":1}}],["多头注意力的投影计算可以通过矩阵运算高效地并行完成",{"5":{"93":2}}],["多头注意力的输出可以被视为在多个子空间中进行注意力计算的融合结果",{"5":{"93":2}}],["多头注意力的时间复杂度分析需要分别考虑各个计算阶段",{"5":{"93":2}}],["多头注意力的总时间复杂度为",{"5":{"93":2}}],["多头注意力的空间复杂度分析与时间复杂度类似",{"5":{"93":2}}],["多头注意力的空间复杂度为",{"5":{"93":2}}],["多头注意力的设计是transformer架构的核心创新之一",{"5":{"93":2}}],["多头注意力的设计天然适合并行计算",{"5":{"93":2}}],["多头注意力的基函数是从数据中学习的",{"5":{"93":2}}],["多头注意力的",{"5":{"93":2}}],["多头注意力中的不同头可以专门化于处理不同类型的任务",{"5":{"101":2}}],["多头注意力中的多个头可以学习互补的低秩结构",{"5":{"87":2,"152":2}}],["多头注意力中的拼接和输出投影操作",{"5":{"93":2}}],["多头注意力中的",{"5":{"93":2}}],["多头注意力则可以被视为多个核函数的组合",{"5":{"93":2}}],["多头注意力引入了额外的参数",{"5":{"93":2}}],["多头注意力可以优雅地表示为一个紧凑的运算序列",{"5":{"93":2}}],["多头注意力可以自然地被分解为块级别的计算",{"5":{"93":2}}],["多头注意力可以被解释为",{"5":{"69":1}}],["多头注意力可以被解释为多组并行的",{"5":{"69":1}}],["多头注意力可以被视为一种",{"5":{"93":2}}],["多头注意力可以被视为对动态注意力矩阵的一种",{"5":{"93":2}}],["多头注意力与mixture",{"5":{"93":2}}],["多头注意力与moe也有所不同",{"5":{"93":2}}],["多头注意力与卷积运算的关系",{"2":{"93":1},"5":{"93":1}}],["多头注意力与卷积运算之间存在着深刻的数学联系",{"5":{"93":2}}],["多头注意力通过设置多个独立的注意力头来解决这个问题",{"5":{"93":2}}],["多头注意力通过学习不同的投影矩阵",{"5":{"93":2}}],["多头注意力输出中的信息流动可以通过信息论的工具来分析",{"5":{"93":2}}],["多头机制与子空间学习",{"2":{"93":1},"5":{"93":1}}],["多头机制在保持参数总量不变的情况下",{"5":{"93":2}}],["多核学习",{"5":{"93":2}}],["单样本梯度的协方差确实与",{"5":{"149":2}}],["单样本梯度为",{"5":{"149":2}}],["单样本ok",{"5":{"146":1}}],["单调下降",{"5":{"147":2}}],["单调性",{"5":{"47":2,"61":1,"69":2,"155":2}}],["单调性表明如果",{"5":{"61":1}}],["单个专家的参数量为",{"5":{"158":2}}],["单个专家",{"5":{"158":2}}],["单个样本",{"5":{"41":1}}],["单个样本的注意力计算涉及形状为的查询",{"5":{"50":1}}],["单个样本的注意力计算涉及形状为",{"5":{"50":1}}],["单个噪声样本不太可能同时激活多个稀疏神经元",{"5":{"71":2}}],["单个感知机只能解决线性可分的问题",{"5":{"46":2}}],["单个神经元的能力是有限的",{"5":{"46":2}}],["单个神经元只形成了一个超平面决策边界",{"5":{"46":2}}],["单个神经元可以模拟任意单变量布尔函数",{"5":{"47":2}}],["单个神经元执行的是一个超平面分类",{"5":{"47":2}}],["单个神经元",{"5":{"47":2}}],["单个神经元无法找到正确的分类边界",{"5":{"47":2}}],["单个神经元无法解决异或问题",{"5":{"47":2}}],["单个神经元实现了一个超平面分类器",{"5":{"47":2}}],["单个麦肯罗皮层神经元可以实现所有的基本逻辑运算",{"5":{"47":2}}],["单个事件的自信息仅描述了单一结果的信息量",{"5":{"61":2}}],["单隐藏层神经网络就可以以任意精度逼近任意连续函数",{"5":{"71":2}}],["单次参数更新就可能将权重推向无穷大或负无穷",{"5":{"41":2}}],["单次反向传播的浮点运算数为",{"5":{"44":2}}],["单次前向传播的flops为",{"5":{"45":2}}],["单次前向传播的flops约为量级",{"5":{"45":1}}],["单次前向传播的flops约为",{"5":{"45":1}}],["单层线性网络",{"5":{"133":2}}],["单层神经网络可以逼近任意连续函数到任意精度",{"5":{"71":2}}],["单层flops",{"5":{"44":1}}],["单层前向传播的flops",{"5":{"45":2}}],["单层transformer的flops约为",{"5":{"45":2}}],["单层网络的计算图",{"5":{"45":2}}],["单层网络的矩阵表示",{"2":{"46":1},"5":{"46":1}}],["单层注意力可能无法精确表示",{"5":{"87":2,"152":2}}],["单层注意力就能提供全局的感受野",{"5":{"92":2,"153":2}}],["单层注意力即可完成",{"5":{"92":2,"153":2}}],["单层注意力能够建模任意复杂的长程依赖关系吗",{"5":{"92":2,"153":2}}],["单层注意力能够表示的函数类受到低秩结构的限制",{"5":{"87":2,"152":2}}],["单层注意力能够表示这个函数吗",{"5":{"92":2,"153":2}}],["单层的限制",{"5":{"87":2,"152":2}}],["单位圆变为椭圆",{"5":{"143":2}}],["单位时间内完成的完整震荡周期数",{"5":{"136":2}}],["单位压缩获得的信息量",{"5":{"133":2}}],["单位为比特",{"5":{"61":2,"96":2}}],["单位为纳特",{"5":{"61":2,"137":2}}],["单位为哈特利",{"5":{"61":2}}],["单位元",{"5":{"88":2}}],["单位矩阵",{"5":{"88":2,"89":2}}],["单纯增加上下文长度会导致性能下降",{"5":{"88":2}}],["单一的注意力头在表达能力上存在固有的局限性",{"5":{"93":2}}],["单一的注意力头难以同时捕获所有这些不同层面的关联信息",{"5":{"93":2}}],["单头注意力可以被视为一个特定的核函数",{"5":{"93":2}}],["而moe架构之所以具有吸引力",{"5":{"159":2}}],["而多数专家几乎不被激活",{"5":{"159":2}}],["而多数专家几乎不被激活的情况",{"5":{"159":2}}],["而多个负样本",{"5":{"69":1}}],["而多个负样本散布在周围",{"5":{"69":1}}],["而推理成本仅线性于",{"5":{"159":2}}],["而处理代码补全的输入则需要模型的程序理解能力",{"5":{"159":2}}],["而自然语言token被路由到语言理解专家",{"5":{"158":2}}],["而自回归模型允许",{"5":{"140":1}}],["而自回归模型允许依赖于完整的上下文",{"5":{"140":1}}],["而专家集合则构成了一个函数库",{"5":{"158":2}}],["而负载均衡则是确保moe模型高效训练的关键挑战",{"5":{"158":2}}],["而一个较高的",{"5":{"156":2}}],["而与之前的状态和动作历史无关",{"5":{"156":2}}],["而与它们在序列中的绝对位置无关",{"5":{"88":2}}],["而actor",{"5":{"155":2}}],["而batch",{"5":{"150":2}}],["而layer",{"5":{"150":4}}],["而linformer的复杂度为",{"5":{"141":2}}],["而highway",{"5":{"150":2}}],["而hessian特征值的提升直接改善了条件数",{"5":{"144":2}}],["而网络的实际输出则为",{"5":{"150":2}}],["而归一化技术则通过规范化激活值的分布来稳定训练过程",{"5":{"150":2}}],["而sgd则能够借助梯度噪声继续探索",{"5":{"149":2}}],["而沿椭球的短轴方向",{"5":{"149":2}}],["而大多数特征值相对较小",{"5":{"149":2}}],["而大模型使用cot提示时性能显著提升",{"5":{"138":2}}],["而理解这种噪声的本质特征对于把握优化动力学至关重要",{"5":{"149":2}}],["而使用xavier初始化的网络在较浅的层数就开始出现梯度消失",{"5":{"148":2}}],["而正是这些连乘项决定了梯度的命运",{"5":{"148":2}}],["而正规方程的求解等价于计算矩阵的伪逆",{"5":{"50":2}}],["而梯度裁剪约束参数更新的幅度",{"5":{"147":2}}],["而梯度饱和则关注导数趋近于零导致的梯度消失问题",{"5":{"41":2}}],["而梯度饱和是激活函数导数的行为",{"5":{"41":2}}],["而范数裁剪得到",{"5":{"147":2}}],["而二阶矩",{"5":{"145":1}}],["而谱范数正则化只惩罚最大奇异值",{"5":{"144":2}}],["而曲率由hessian矩阵的特征值描述",{"5":{"144":2}}],["而标签平滑损失对正确类别的梯度被",{"5":{"144":2}}],["而标准的多头注意力在所有头上都进行完整的计算",{"5":{"93":2}}],["而名义秩可以达到",{"5":{"141":2}}],["而忽略或错误地模拟深层依赖",{"5":{"138":2}}],["而某些复杂能力",{"5":{"138":2}}],["而简单地增加参数量而忽视数据量会导致收益递减",{"5":{"138":2}}],["而训练数据量也需要足够大以充分利用模型容量",{"5":{"138":2}}],["而均方误差的裁剪是基于梯度的范数约束",{"5":{"101":2}}],["而均方误差则无法享受这一特性",{"5":{"70":2}}],["而目标部分使用因果注意力",{"5":{"101":2}}],["而交叉熵认为",{"5":{"70":1}}],["而交叉熵认为比严重得多",{"5":{"70":1}}],["而预测",{"5":{"70":2}}],["而不会出现性能退化现象",{"5":{"150":2}}],["而不应用于循环连接",{"5":{"145":2}}],["而不预先设定固定的训练步数或数据遍历次数",{"5":{"138":2}}],["而不仅仅是点估计",{"5":{"70":2}}],["而不是自适应调整",{"5":{"145":2}}],["而不是伯努利分布",{"5":{"145":2}}],["而不是依赖词内的子词相关性",{"5":{"101":2}}],["而不是独立地掩码单个token",{"5":{"101":2}}],["而不是点估计",{"5":{"96":2}}],["而不是最小化全局kl散度",{"5":{"96":2}}],["而不是高斯消元的",{"5":{"50":1}}],["而不是",{"5":{"61":2}}],["而不是每层独立学习",{"5":{"89":2}}],["而不显式地计算和存储完整的hessian矩阵",{"5":{"48":2}}],["而不需要重新与环境交互采样",{"5":{"154":2}}],["而不需要重新训练或复杂的插值策略",{"5":{"88":2}}],["而不需要重新训练或特殊的插值策略",{"5":{"88":2}}],["而不需要维护全局统计量",{"5":{"146":2}}],["而不需要显式地计算逆操作",{"5":{"146":2}}],["而不需要显式地平均多个dropout样本",{"5":{"145":2}}],["而不需要显式的奖励模型",{"5":{"101":2}}],["而不需要显式计算",{"5":{"61":2}}],["而不需要存储",{"5":{"61":2}}],["而不增加计算成本",{"5":{"93":2}}],["而不重要的特征或噪声会对应于较小的奇异值",{"5":{"94":2}}],["而",{"5":{"48":1,"50":2,"51":1,"61":1,"70":4,"91":1,"97":2,"145":1,"148":2,"155":2,"159":2}}],["而当注意力分布趋于one",{"5":{"41":2}}],["而非由于优化器动量",{"5":{"149":2}}],["而非的一半",{"5":{"148":1}}],["而非相对尺度",{"5":{"147":2}}],["而非提升收敛速度",{"5":{"147":2}}],["而非加速技术",{"5":{"147":2}}],["而非作为方差的加性项",{"5":{"146":2}}],["而非其相对于均值的离散程度",{"5":{"146":2}}],["而非协变量偏移的减少",{"5":{"146":2}}],["而非精确的",{"5":{"145":2}}],["而非单调递减到最小值",{"5":{"136":2}}],["而非单一数值",{"5":{"70":2}}],["而非具体序列顺序的函数",{"5":{"101":2}}],["而非对每个特征",{"5":{"146":1}}],["而非对每个特征的所有样本求和",{"5":{"146":1}}],["而非对",{"5":{"70":2}}],["而非对特定位置的精确记忆",{"5":{"90":2}}],["而非对角元素也非常小",{"5":{"41":1}}],["而非对角元素",{"5":{"41":1}}],["而非对角线元素则捕捉了不同参数方向上梯度噪声的协方差",{"5":{"149":1}}],["而非对角线元素也接近0",{"5":{"95":1}}],["而非对角线元素",{"5":{"95":1,"149":1}}],["而非批维度",{"5":{"41":2}}],["而非预先指定的",{"5":{"45":2}}],["而非",{"5":{"51":2,"101":1,"146":1,"148":1,"149":2,"159":2}}],["而非所有位置",{"5":{"86":2,"151":2}}],["而非幅度变化中",{"5":{"88":2}}],["而非这种混合方案",{"5":{"88":2}}],["而非经验性的观察",{"5":{"88":2}}],["而非经验性的",{"5":{"88":2}}],["而非手工设计",{"5":{"88":2}}],["而非仅由它们的差决定",{"5":{"88":1}}],["而非仅由它们的差",{"5":{"88":1}}],["而非过拟合训练位置的特殊模式",{"5":{"89":2}}],["而非完整",{"5":{"89":1}}],["而非完全自由的",{"5":{"90":2}}],["而非训练位置的",{"5":{"90":2}}],["而非可学习的参数",{"5":{"91":2}}],["而非卷积那种",{"5":{"92":2,"153":2}}],["而非绝对位置的函数",{"5":{"90":2}}],["而非绝对位置",{"5":{"92":2,"153":2}}],["而非它们在序列中的具体位置",{"5":{"91":2}}],["而非它们的绝对位置",{"5":{"92":2,"153":2}}],["而非显式地依赖于输入",{"5":{"93":2}}],["而非总是选择最确定的答案",{"5":{"96":2}}],["而深度网络仅需多项式规模",{"5":{"71":2}}],["而各种信息量",{"5":{"70":2,"71":2}}],["而是使用单步td误差",{"5":{"155":2}}],["而是使用不同频率的正弦波叠加来表示位置",{"5":{"90":2}}],["而是随着参数位置和数据分布的变化而演化",{"5":{"149":2}}],["而是在噪声较强的方向上有更大的探索幅度",{"5":{"149":2}}],["而是在奖励和策略多样性之间进行权衡",{"5":{"101":2}}],["而是主要集中在某些特定的方向上",{"5":{"149":2}}],["而是被标准化到稳定的范围",{"5":{"148":2}}],["而是被正则化项",{"5":{"147":2}}],["而是源于深度网络梯度计算的数学结构本身",{"5":{"148":2}}],["而是对每个样本的所有特征维度计算统计量",{"5":{"146":2}}],["而是服从某种连续分布",{"5":{"145":2}}],["而是集中在某个低维流形附近或在某些方向上具有高度相关性",{"5":{"143":2}}],["而是蕴含着深刻的数学原理和理论基础",{"5":{"138":2}}],["而是历史梯度的加权平均",{"5":{"136":2}}],["而是提取与任务最相关的本质特征",{"5":{"133":2}}],["而是有选择地关注最相关的部分",{"5":{"132":2}}],["而是作用于整个生成策略",{"5":{"101":2}}],["而是从机器学习的基本原则",{"5":{"70":2}}],["而是由统计学习理论所保证的内在性质",{"5":{"149":2}}],["而是由",{"5":{"70":2}}],["而是由fisher信息加权的距离",{"5":{"71":2}}],["而是将所有位置作为候选",{"5":{"69":2}}],["而是一种非线性变换",{"5":{"69":2}}],["而是取连续的实数值",{"5":{"50":2}}],["而是缩放因子",{"5":{"51":1}}],["而是让复杂性自然涌现",{"5":{"88":2}}],["而是让每个位置对应一个旋转操作",{"5":{"88":1}}],["而是让每个位置对应一个",{"5":{"88":1}}],["而是具有协方差矩阵",{"5":{"149":2}}],["而是具有特定几何结构的协方差场",{"5":{"149":2}}],["而是具有深刻的物理直觉",{"5":{"88":2}}],["而是具有某种对数或平方根性质",{"5":{"88":2}}],["而是经过深思熟虑的数学设计",{"5":{"71":2}}],["而是经过深思熟虑的设计",{"5":{"88":2}}],["而是经过精心设计的频率成分",{"5":{"90":2}}],["而是不同频率正弦波的叠加",{"5":{"90":2}}],["而是直接选择能带来最大价值的动作",{"5":{"156":2}}],["而是直接叠加到原始输入的梯度上",{"5":{"146":2}}],["而是直接计算低秩近似下的输出",{"5":{"87":2,"152":2}}],["而是直接指定了频率分布",{"5":{"90":2}}],["而是直接修改注意力计算",{"5":{"91":2}}],["而是固定的常数",{"5":{"91":1}}],["而是遍历整个球面",{"5":{"91":2}}],["而是通过架构设计",{"5":{"88":2}}],["而是通过可学习的线性变换从输入数据中投射到不同的",{"5":{"94":2}}],["而是可以通过投影变换到隐空间中形成有意义的匹配",{"5":{"94":2}}],["而是数学必然",{"5":{"88":2}}],["而是数据驱动的",{"5":{"94":2}}],["而是首先构建一个未归一化的实数向量",{"5":{"96":2}}],["而​",{"5":{"48":1}}],["而连续空间中的嵌入表示和隐藏状态则是连续的",{"5":{"96":2}}],["而真实分布是",{"5":{"96":2}}],["而强大数定律保证样本均值几乎必然收敛于期望",{"5":{"96":2}}],["而反向则鼓励找到一个主要的模式并紧密地拟合它",{"5":{"96":1}}],["而反向",{"5":{"96":1}}],["而零空间则包含了可能被",{"5":{"50":2}}],["而奇异值分解适用于任意矩阵",{"5":{"50":2}}],["而数据并行",{"5":{"50":2}}],["而且在实践中也被广泛应用于分析sgd的极限行为",{"5":{"149":2}}],["而且在实际计算中也更加高效",{"5":{"51":2}}],["而且与交叉熵损失函数等价",{"5":{"140":2}}],["而且并非所有方阵都有逆矩阵",{"5":{"50":2}}],["而其他样本的总误差约为",{"5":{"51":2}}],["而对于概率极小的事件",{"5":{"61":2}}],["而香农熵",{"5":{"61":1}}],["而分类任务使用softmax将logits转换为类别概率分布",{"5":{"61":2}}],["而稀疏矩阵运算可能无法充分利用硬件并行能力",{"5":{"86":2,"151":2}}],["而复数的模长",{"5":{"88":2}}],["而内积运算本身就会",{"5":{"88":2}}],["而最高频的维度",{"5":{"88":2}}],["而相对位置的范围在推理时与训练时是相同的",{"5":{"88":2}}],["而相位是循环的",{"5":{"88":2}}],["而绝对位置编码提供额外的绝对位置信息",{"5":{"88":2}}],["而ntk",{"5":{"88":2}}],["而中间相隔数十个词",{"5":{"88":2}}],["而可学习编码的函数空间由训练数据决定",{"5":{"89":2}}],["而可学习编码可能在外推时表现不佳",{"5":{"89":2}}],["而rope正是这一探索的集大成之作",{"5":{"88":2}}],["而rnn的建造成本与依赖跨度成正比",{"5":{"92":2,"153":2}}],["而这些方向恰好是曲率较大的方向",{"5":{"149":2}}],["而这些区域通常对应更好的泛化性能",{"5":{"147":2}}],["而这个映射的几何性质决定了神经网络的学习动态和表达能力",{"5":{"71":2}}],["而这个最小化过程正是通过梯度下降等优化算法实现的",{"5":{"51":2}}],["而这个旋转完全由相对位置",{"5":{"88":2}}],["而这取决于训练数据",{"5":{"92":2,"153":2}}],["而在于内存带宽",{"5":{"159":2}}],["而在理想的均衡状态下",{"5":{"158":2}}],["而在highway网络中",{"5":{"150":2}}],["而在正交的子空间中噪声几乎为零",{"5":{"149":2}}],["而在推理时使用完整的网络进行预测",{"5":{"145":2}}],["而在推理时进行缩放补偿",{"5":{"145":2}}],["而在",{"5":{"132":2}}],["而在极端值",{"5":{"42":2}}],["而在标准的多头注意力中",{"5":{"93":2}}],["而在多查询注意力中",{"5":{"93":2}}],["而在注意力机制中",{"5":{"94":2}}],["而gpt",{"5":{"93":2}}],["而注意力机制则引入了动态的信息筛选机制",{"5":{"132":2}}],["而注意力机制的投影是有监督的",{"5":{"94":2}}],["而注意力的路径长度恒为1",{"5":{"92":2,"153":2}}],["而注意力的",{"5":{"92":2,"153":2}}],["而注意力的感受野是全局的",{"5":{"93":2}}],["而注意力矩阵是数据依赖的动态矩阵",{"5":{"93":2}}],["而value则承载了实际的信息内容",{"5":{"94":2}}],["而矩阵乘法的梯度为",{"5":{"94":2}}],["而缩放确保了这种对齐程度的度量不受维度数量的影响",{"5":{"95":2}}],["而缩放操作可以保持点积的方差稳定",{"5":{"95":2}}],["而键向量在同一维度上的分量也较大",{"5":{"95":1}}],["而键向量",{"5":{"95":1}}],["而后期的大学习率",{"5":{"97":2}}],["而动态损失缩放根据梯度范数自适应调整",{"5":{"97":2}}],["熵为量化随机性和不确定性提供了严格的数学框架",{"5":{"137":2}}],["熵为0",{"5":{"96":2}}],["熵估计",{"5":{"137":2}}],["熵估计的收敛性",{"5":{"137":2}}],["熵估计值",{"5":{"137":4}}],["熵可能进一步降低甚至出现过度自信现象",{"5":{"137":2}}],["熵可以被视为概率单纯形",{"5":{"61":2}}],["熵较低",{"5":{"137":2}}],["熵较高",{"5":{"137":2}}],["熵逐渐降低",{"5":{"137":2}}],["熵满足",{"5":{"137":2}}],["熵被广泛应用于不确定性估计",{"5":{"137":2}}],["熵与温度的信息论解释",{"5":{"132":2}}],["熵与散度的数学定义",{"2":{"61":1},"5":{"61":1}}],["熵正则化的效果",{"5":{"132":2}}],["熵正则化",{"5":{"132":2}}],["熵的上下界和非负性为理解模型行为提供了理论基础",{"5":{"137":2}}],["熵的上界",{"5":{"137":2}}],["熵的非负性",{"5":{"137":2}}],["熵的单位为比特",{"5":{"137":2}}],["熵的概念源于物理学中的热力学熵",{"5":{"137":2}}],["熵的范围",{"5":{"132":2}}],["熵的若干重要性质值得深入探讨",{"5":{"61":2}}],["熵定义为",{"5":{"96":2,"137":2}}],["熵衡量了随机变量不确定性的大小",{"5":{"96":2}}],["熵越大",{"5":{"96":2}}],["熵越小",{"5":{"96":2}}],["熵代表了分布的",{"5":{"61":2}}],["熵值越大",{"5":{"61":2}}],["熵值越小",{"5":{"61":2}}],["熵实际上是自信息的数学期望",{"5":{"61":2}}],["熵函数在这个流形上的等高线描述了不同概率分布的",{"5":{"61":2}}],["熵",{"5":{"61":2,"70":2,"132":4,"137":4}}],["熵具有非负性",{"5":{"61":1}}],["熵具有",{"5":{"61":1}}],["熵在均匀分布时取得最大值",{"5":{"61":1}}],["熵在",{"5":{"61":1}}],["熵和互信息的数学框架",{"5":{"71":2}}],["熵和kl散度的定义都依赖于对数运算",{"5":{"61":2}}],["熵度量了注意力分布的",{"5":{"92":2,"153":2}}],["熵最大",{"5":{"92":2,"153":2}}],["熵最小",{"5":{"92":2,"153":2}}],["或专家选择的组合",{"5":{"159":2}}],["或2×",{"5":{"157":2}}],["或完整的句子",{"5":{"156":2}}],["或完全",{"5":{"70":2}}],["或动作价值",{"5":{"155":2}}],["或简称为",{"5":{"149":2}}],["或极大",{"5":{"148":4}}],["或值裁剪",{"5":{"147":2}}],["或基于高阶正交迭代",{"5":{"143":2}}],["或反射",{"5":{"143":4}}],["或列",{"5":{"143":2}}],["或离散时间系统",{"5":{"136":1}}],["或使用二阶方法",{"5":{"134":2}}],["或使用可学习的因果位置编码来实现",{"5":{"91":2}}],["或直接偏好优化损失",{"5":{"101":2}}],["或排名损失",{"5":{"70":2}}],["或者说",{"5":{"159":2}}],["或者说是在某些条件下的加权平均",{"5":{"155":2}}],["或者序列长度增加到8196",{"5":{"157":2}}],["或者按学习率的比例调整阈值",{"5":{"147":2}}],["或者",{"5":{"147":2}}],["或者使用一个学习到的",{"5":{"155":1}}],["或者使用一个学习到的价值函数近似器来作为基线",{"5":{"155":1}}],["或者使用空间dropout",{"5":{"145":2}}],["或者使用有放缩版本",{"5":{"145":2}}],["或者深度神经网络的多维权重张量",{"5":{"143":2}}],["或者在批量形式下",{"5":{"70":2}}],["或者修改为",{"5":{"69":2}}],["或者等效地",{"5":{"69":2}}],["或者更能够代表目标任务的需求",{"5":{"51":2}}],["或者端到端联合训练两者",{"5":{"89":2}}],["或同一图像的不同增强视图",{"5":{"69":2}}],["或",{"5":{"41":3,"44":3,"45":2,"50":2,"51":1,"61":4,"69":1,"70":8,"71":4,"86":1,"87":2,"88":2,"89":2,"91":1,"92":4,"93":4,"95":2,"96":3,"97":2,"133":2,"136":1,"137":3,"138":2,"143":3,"145":2,"146":3,"147":2,"150":4,"151":1,"152":2,"153":4,"159":4}}],["或之后",{"5":{"41":2}}],["或过小",{"5":{"41":2}}],["或层次化结构",{"5":{"71":2}}],["或行",{"5":{"45":2}}],["或行向量",{"5":{"50":2}}],["或回归",{"5":{"47":2}}],["或等效的范数",{"5":{"148":2}}],["或等价地",{"5":{"45":2,"133":2}}],["或等价地表示为",{"5":{"61":2}}],["或等值线",{"5":{"48":2}}],["或经过dropout操作后某个神经元是否被激活等",{"5":{"96":2}}],["或均方根为1",{"5":{"41":2}}],["或均方根",{"5":{"41":2}}],["或均匀分布",{"5":{"96":2}}],["或奈特",{"5":{"96":2}}],["或证据",{"5":{"96":2}}],["或平均值",{"5":{"96":2}}],["或和",{"5":{"96":2}}],["或更宽",{"5":{"41":1}}],["或更低精度",{"5":{"50":2}}],["或超曲面",{"5":{"51":2}}],["或其变体",{"5":{"69":2,"70":2}}],["或其中一个维度为",{"5":{"50":2}}],["或其他能够生成凸预测空间的模型",{"5":{"51":2}}],["或在最后一维单独处理",{"5":{"88":2}}],["或狄拉克函数",{"5":{"90":1}}],["或狄拉克",{"5":{"90":1}}],["或​维的空间",{"5":{"91":1}}],["或4096",{"5":{"91":1}}],["或距离呈某种函数关系",{"5":{"92":2,"153":2}}],["或接近序列结尾",{"5":{"92":2,"153":2}}],["或对于分类任务",{"5":{"44":1}}],["或对称位置编码",{"5":{"92":2,"153":2}}],["或glorot初始化",{"5":{"94":2}}],["或称单位高斯分布",{"5":{"96":2}}],["或称he初始化",{"5":{"94":2}}],["或学习率衰减",{"5":{"97":2}}],["或收敛过慢",{"5":{"97":2}}],["且使用负对数似然损失函数",{"5":{"149":2}}],["且其他雅可比矩阵的范数下界不为零",{"5":{"148":2}}],["且其梯度",{"5":{"136":1}}],["且其梯度满足",{"5":{"136":1}}],["且忽略偏置项和激活函数",{"5":{"148":2}}],["且在某些任务中可能不如层归一化稳定",{"5":{"146":2}}],["且在小批量情况下可能不如批归一化有效",{"5":{"146":2}}],["且在rnn和在线学习场景中表现不佳",{"5":{"146":2}}],["且在长上下文外推上表现出色",{"5":{"88":2}}],["且似然函数",{"5":{"139":1}}],["且似然函数关于连续",{"5":{"139":1}}],["且随着网络容量和样本量增加而收敛到真实值",{"5":{"137":2}}],["且梯度",{"5":{"136":2}}],["且的各分量独立",{"5":{"145":1}}],["且的泛化误差上界最小",{"5":{"133":1}}],["且的每一行只依赖于对应的查询行和整个键矩阵",{"5":{"92":1,"153":1}}],["且应该尽可能均匀分布",{"5":{"132":1}}],["且满足归一化条件",{"5":{"132":2}}],["且满足分解式",{"5":{"45":2}}],["且损失值被有效位置的数量归一化",{"5":{"101":2}}],["且可以任意大",{"5":{"70":2}}],["且任何局部最小值都是全局最小值",{"5":{"70":2}}],["且优化的收敛行为可以精确预测",{"5":{"70":2}}],["且具有唯一的全局最小值",{"5":{"70":2}}],["且",{"5":{"42":1,"45":2,"61":3,"70":2,"71":2,"87":1,"92":7,"96":1,"132":2,"133":3,"134":6,"135":4,"136":3,"137":1,"140":1,"145":3,"149":4,"152":1,"153":7,"154":8,"156":2,"158":2,"159":8}}],["且最优解集是凸集",{"5":{"48":2}}],["且同样唯一确定概率分布",{"5":{"96":2}}],["且当且仅当时kl散度为0",{"5":{"96":1}}],["且当且仅当",{"5":{"96":1}}],["且特征向量矩阵是正交的",{"5":{"50":2}}],["且这些局部最小值之间由平缓的区域连接",{"5":{"51":2}}],["且通常归一化使得或",{"5":{"51":1}}],["且通常归一化使得",{"5":{"51":1}}],["且加速效果随序列长度增加而增大",{"5":{"86":2,"151":2}}],["且对应的特征向量为",{"5":{"87":2,"152":2}}],["且所有特征值的模不超过1",{"5":{"87":2,"152":2}}],["且便于进行复变函数分析",{"5":{"90":2}}],["且计算难以并行化",{"5":{"91":2}}],["且由于需要顺序计算",{"5":{"92":2,"153":2}}],["且不易被内容信息所干扰",{"5":{"88":2}}],["且不依赖于词序",{"5":{"92":2,"153":2}}],["且它们之间存在依赖",{"5":{"92":1,"153":1}}],["且局部极小值的数量随参数数量指数级增长",{"5":{"97":2}}],["则带噪声的门控得分为",{"5":{"159":2}}],["则带有位置信息的输入表示为",{"5":{"91":2}}],["则top",{"5":{"159":2}}],["则稀疏门控可以写为",{"5":{"159":2}}],["则稀疏门控权重为",{"5":{"159":2}}],["则稀疏度",{"5":{"159":2}}],["则稀疏注意力矩阵定义为",{"5":{"86":1,"151":1}}],["则稀疏注意力矩阵",{"5":{"86":1,"151":1}}],["则稀疏注意力矩阵为",{"5":{"87":2,"152":2}}],["则模型规模翻倍只能使损失降低约",{"5":{"159":2}}],["则模型利用了更多的参数自由度",{"5":{"50":2}}],["则集中度可以定义为前",{"5":{"158":2}}],["则分配概率的方差为",{"5":{"158":2}}],["则损失关于门控权重",{"5":{"158":2}}],["则负载不均衡度为",{"5":{"157":2}}],["则新样本被路由到专家",{"5":{"157":2}}],["则新位置的编码可以通过已学习位置的线性组合来构造",{"5":{"89":2}}],["则激活显存从",{"5":{"157":2}}],["则通信效率可以定义为",{"5":{"157":2}}],["则通过优化过程的动态特性来引导解的性质",{"5":{"147":2}}],["则流水线化的总时间为",{"5":{"157":2}}],["则设备",{"5":{"157":4}}],["则专家",{"5":{"158":2}}],["则专家权重的显存消耗为",{"5":{"157":2}}],["则专家并行的划分方式为",{"5":{"157":2}}],["则专家参数",{"5":{"157":2}}],["则前向传播的计算图可以表示为",{"5":{"157":2}}],["则会鼓励模型进行更长远的规划",{"5":{"156":2}}],["则会在较小张量的左侧自动补",{"5":{"50":2}}],["则融合后的输出",{"5":{"150":2}}],["则融合过程可以表示为",{"5":{"150":2}}],["则捕捉了不同参数方向上梯度噪声的协方差",{"5":{"149":1}}],["则样本损失",{"5":{"149":2}}],["则样本均值依概率收敛于对任意成立",{"5":{"96":1}}],["则样本均值",{"5":{"96":1}}],["则单专家计算量是稠密模型的",{"5":{"159":2}}],["则单次前向传播的总计算量",{"5":{"159":2}}],["则单个样本的损失为",{"5":{"149":2}}],["则单层雅可比矩阵的范数上界为",{"5":{"148":2}}],["则随机梯度定义为",{"5":{"149":2}}],["则随指数增长",{"5":{"41":1}}],["则随指数衰减",{"5":{"41":1}}],["则净输入的方差为",{"5":{"148":2}}],["则经过relu后",{"5":{"148":2}}],["则经过100层后",{"5":{"148":2}}],["则谱半径定义为",{"5":{"148":2}}],["则从第",{"5":{"148":1}}],["则从第层到第层的累积雅可比矩阵为",{"5":{"148":1}}],["则从整体上刻画了一个概率分布的信息特征",{"5":{"61":2}}],["则网络的前向传播可表示为",{"5":{"148":2}}],["则网络的前向传播可以表示为两个连续的线性变换",{"5":{"71":2}}],["则一个标准transformer前馈层的计算可以表示为",{"5":{"159":2}}],["则一个合理的裁剪阈值可以是",{"5":{"147":2}}],["则一个批次的表示就是形状为",{"5":{"50":2}}],["则序列",{"5":{"147":2}}],["则使用较严格的裁剪",{"5":{"147":2}}],["则使用最优学习率",{"5":{"136":1}}],["则使用最优学习率时",{"5":{"136":1}}],["则均方根统计量也乘以",{"5":{"146":2}}],["则均方误差定义为",{"5":{"70":2}}],["则归一化后的输出保持不变",{"5":{"150":2}}],["则归一化后的输出",{"5":{"146":2}}],["则归一化后的注意力权重为",{"5":{"92":2,"153":2}}],["则循环连接的稳定性得以保持",{"5":{"145":2}}],["则应该减少其概率",{"5":{"155":2}}],["则应该降低选择该动作的概率",{"5":{"155":2}}],["则应该优先增加数据量",{"5":{"138":2}}],["则应用dropout后期望输出为",{"5":{"145":2}}],["则期望保持性质为",{"5":{"145":2}}],["则期望的掩码比例为",{"5":{"101":2}}],["则裁剪后的梯度为",{"5":{"144":2}}],["则等效的正则化系数约为",{"5":{"144":2}}],["则正则化后为",{"5":{"144":2}}],["则l2正则化后的总损失函数为",{"5":{"144":2}}],["则lu分解存在",{"5":{"50":2}}],["则卷积运算",{"5":{"143":1}}],["则卷积运算可以分解为个小卷积的和",{"5":{"143":1}}],["则卷积矩阵是一个带状矩阵",{"5":{"87":1,"152":1}}],["则卷积矩阵",{"5":{"87":1,"152":1}}],["则实际秩会更小",{"5":{"141":2}}],["则实部和虚部正好对应编码的奇数维度和偶数维度",{"5":{"90":2}}],["则整体梯度范数的上界为",{"5":{"147":2}}],["则整体误差约为",{"5":{"138":2}}],["则整个特征图通道要么全部保留",{"5":{"145":2}}],["则整个网络的期望输出缩放因子为",{"5":{"145":2}}],["则整个网络等价于一个单一的仿射变换",{"5":{"47":2}}],["则整个数据集的负对数似然",{"5":{"61":2}}],["则更多的计算资源应该分配给增加参数量",{"5":{"138":2}}],["则更可能收敛到泛化性能好的解",{"5":{"136":2}}],["则泛化误差可由互信息上界",{"5":{"137":2}}],["则泛化误差上界为",{"5":{"133":2}}],["则二元熵函数定义为",{"5":{"137":2}}],["则周期轨道不稳定",{"5":{"136":2}}],["则当样本数量",{"5":{"140":1}}],["则当样本数量时",{"5":{"140":1}}],["则当",{"5":{"136":1,"148":1}}],["则当时",{"5":{"136":1,"148":1}}],["则当且仅当两个输入均为1时",{"5":{"47":2}}],["则幂迭代线性收敛到该特征值",{"5":{"135":2}}],["则线性化方法无法判断稳定性",{"5":{"135":2}}],["则不动点",{"5":{"135":2}}],["则不显式编码绝对位置",{"5":{"91":2}}],["则复合映射的",{"5":{"135":2}}],["则复制整个模型在多个数据批次上并行训练",{"5":{"50":2}}],["则系统存在唯一不动点",{"5":{"136":2}}],["则系统在",{"5":{"135":2}}],["则系统渐近收敛到平衡点",{"5":{"134":2}}],["则系统是稳定的",{"5":{"50":2}}],["则系统是不稳定的",{"5":{"50":2}}],["则信息容量满足以下近似关系",{"5":{"133":2}}],["则信息传递效率随距离呈指数衰减",{"5":{"132":2}}],["则熵的上界与分数差异相关",{"5":{"132":2}}],["则头的冗余度定义为条件熵度量的信息重叠程度",{"5":{"132":2}}],["则多头注意力输出为各头信息的融合",{"5":{"132":2}}],["则多头注意力的计算过程为",{"5":{"93":2}}],["则wwm的掩码策略为",{"5":{"101":2}}],["则moe模型的表达能力可以达到或超过同等容量的稠密模型",{"5":{"159":2}}],["则moe层的输出为专家输出的加权和",{"5":{"158":2}}],["则moe层的输出为",{"5":{"158":2}}],["则mle唯一",{"5":{"139":1}}],["则mle唯一证明框架",{"5":{"139":1}}],["则mle存在",{"5":{"139":1}}],["则mle存在2",{"5":{"139":1}}],["则mlm损失变为",{"5":{"101":2}}],["则mlm的目标是预测被掩码的token",{"5":{"101":2}}],["则mse为",{"5":{"51":2}}],["则仅是位置的token集合的函数",{"5":{"101":1}}],["则条件众数预测也是最优的",{"5":{"70":2}}],["则误差仅为0",{"5":{"70":2}}],["则交叉熵定义为",{"5":{"70":2}}],["则交叉熵损失相对于的梯度为​",{"5":{"96":1}}],["则交叉熵损失相对于",{"5":{"96":1}}],["则交叉熵损失关于的偏导数为",{"5":{"96":1}}],["则交叉熵损失",{"5":{"96":1}}],["则位置",{"5":{"69":1}}],["则位置的输出为",{"5":{"69":1}}],["则位置感知的嵌入表示为",{"5":{"71":2}}],["则位置编码可以表示为",{"5":{"91":2}}],["则",{"5":{"41":12,"42":3,"44":9,"45":8,"47":8,"48":6,"50":1,"51":2,"61":6,"69":1,"70":2,"87":13,"89":3,"90":6,"92":4,"93":5,"94":4,"96":10,"101":5,"133":1,"135":20,"136":3,"137":4,"139":2,"142":3,"143":5,"144":2,"145":4,"146":2,"147":4,"148":24,"149":6,"150":2,"152":13,"153":4,"157":4,"158":6,"159":6}}],["则​",{"5":{"41":1,"69":1,"87":1,"93":1,"152":1}}],["则​​可以表示为前个基向量的线性组合",{"5":{"89":1}}],["则的rademacher复杂度满足",{"5":{"142":1}}],["则的特征值接近1",{"5":{"41":1}}],["则的各元素方差为",{"5":{"41":1}}],["则的梯度为​",{"5":{"45":1}}],["则的秩与相同",{"5":{"87":1,"152":1}}],["则和",{"5":{"41":1}}],["则反向传播的梯度变换为​",{"5":{"41":1}}],["则反向传播的梯度变换为",{"5":{"41":1}}],["则只能为正或零",{"5":{"42":1}}],["则深度神经网络在表示效率上可能显著优于浅层网络",{"5":{"71":2}}],["则层次化特征学习可以表示为",{"5":{"71":2}}],["则对应的对角导数矩阵",{"5":{"148":2}}],["则对应的概率为",{"5":{"71":2}}],["则对每一层的梯度单独进行裁剪",{"5":{"147":2}}],["则对于输入",{"5":{"158":2}}],["则对于有放缩版本的dropout",{"5":{"145":2}}],["则对于任意函数",{"5":{"145":2}}],["则对于任意位置置换",{"5":{"101":2}}],["则对于任意有限的点集",{"5":{"86":2,"151":2}}],["则对于任意",{"5":{"94":2}}],["则对于任意的",{"5":{"95":2}}],["则对所有",{"5":{"134":2}}],["则对的梯度为",{"5":{"44":1}}],["则对的导数为",{"5":{"45":1}}],["则对的雅可比矩阵为",{"5":{"45":1}}],["则对任意范数都有",{"5":{"148":2}}],["则对任意矩阵",{"5":{"96":2}}],["则对任意整数平移量",{"5":{"88":2}}],["则按比例缩放",{"5":{"44":2}}],["则全连接层可以表示为",{"5":{"46":2}}],["则仿射变换可以统一表示为",{"5":{"47":2}}],["则将整个超平面平移",{"5":{"47":1}}],["则提供了灵活的阈值调节能力",{"5":{"47":1}}],["则第层的输出为",{"5":{"47":1}}],["则第",{"5":{"47":1}}],["则关于变量在点处的偏导数定义为",{"5":{"48":1}}],["则关于的偏导数为",{"5":{"48":1}}],["则梯度协方差矩阵定义为",{"5":{"149":2}}],["则梯度范数的主要决定因素是",{"5":{"148":2}}],["则梯度爆炸对应于",{"5":{"147":1}}],["则梯度爆炸对应于随迭代次数急剧增长的现象",{"5":{"147":1}}],["则梯度很小",{"5":{"69":2}}],["则梯度是一个与形状相同的三阶张量",{"5":{"48":1}}],["则梯度",{"5":{"48":1}}],["则梯度的分布趋向于高斯分布",{"5":{"96":2}}],["则梯度会按指数级衰减",{"5":{"42":2}}],["则梯度会均匀地流向所有位置",{"5":{"89":2}}],["则梯度主要流向被",{"5":{"89":2}}],["则在参数",{"5":{"149":2}}],["则在某些方向上放大信号",{"5":{"148":1}}],["则在梯度裁剪后的空间中",{"5":{"147":2}}],["则在以下意义上具有最优的泛化性能",{"5":{"133":1}}],["则在该区域输入的饱和称为输出饱和",{"5":{"41":2}}],["则在该区域输入的微小变化不会引起输出的显著变化",{"5":{"41":2,"42":2}}],["则在点处的梯度定义为",{"5":{"48":1}}],["则在这个映射下计算输入的某种",{"5":{"87":2,"152":2}}],["则沿对应特征方向的迭代不会以线性速率收敛",{"5":{"136":2}}],["则沿网络深度存在梯度消失趋势",{"5":{"135":2}}],["则沿时间步反向传播后",{"5":{"41":1}}],["则沿时间步",{"5":{"41":1}}],["则沿的方向导数为",{"5":{"48":1}}],["则hessian矩阵是对称的",{"5":{"48":2}}],["则该序列具有以下重要性质",{"5":{"136":2}}],["则该词的所有子词都被掩码",{"5":{"101":2}}],["则该层的输出为",{"5":{"47":2}}],["则该点是局部极小值",{"5":{"48":2}}],["则该点是局部极大值",{"5":{"48":2}}],["则该点是鞍点",{"5":{"48":2}}],["则该维度可以广播",{"5":{"50":2}}],["则该头的注意力权重矩阵有个自由度",{"5":{"93":1}}],["则该头的注意力权重矩阵有",{"5":{"93":1}}],["则其hessian也与fisher信息相关",{"5":{"70":2}}],["则其pdf为当",{"5":{"96":2}}],["则其pdf为",{"5":{"96":2}}],["则其和为向量加法满足交换律和结合律",{"5":{"50":1}}],["则其和为",{"5":{"50":1}}],["则其元素可以表示为​",{"5":{"50":1}}],["则其元素可以表示为",{"5":{"50":1}}],["则其香农熵定义为",{"5":{"61":2}}],["则记为",{"5":{"96":2}}],["则拒绝原假设",{"5":{"96":2}}],["则任意分量服从一维高斯分布",{"5":{"96":1}}],["则任意分量",{"5":{"96":1}}],["则给定时的条件分布仍然是高斯分布",{"5":{"96":1}}],["则给定",{"5":{"96":1}}],["则鼓励",{"5":{"96":1}}],["则称训练过程中出现",{"5":{"148":2}}],["则称训练过程中出现梯度爆炸现象",{"5":{"148":1}}],["则称训练过程中出现梯度消失现象",{"5":{"148":1}}],["则称优化过程处于梯度爆炸临界状态",{"5":{"147":2}}],["则称数据具有高冗余度",{"5":{"143":2}}],["则称为混沌",{"5":{"136":2}}],["则称为极限环",{"5":{"136":1}}],["则称为标准正交基",{"5":{"50":2}}],["则称序列二次收敛",{"5":{"136":2}}],["则称序列超线性收敛",{"5":{"136":2}}],["则称序列次线性收敛",{"5":{"136":2}}],["则称序列线性收敛",{"5":{"136":2}}],["则称系统具有全局收敛性",{"5":{"136":2}}],["则称系统收敛到不动点",{"5":{"136":2}}],["则称系统在该邻域内收敛",{"5":{"136":2}}],["则称该网络存在梯度爆炸问题",{"5":{"41":2}}],["则称它们是正交的",{"5":{"50":2}}],["则称",{"5":{"50":1,"88":1}}],["则称是的特征向量",{"5":{"50":1}}],["则称是等变的",{"5":{"88":1}}],["则plu分解总是存在",{"5":{"50":2}}],["则参数量从减少到",{"5":{"50":1}}],["则参数量从",{"5":{"50":1}}],["则两者的mse可以表示为",{"5":{"51":2}}],["则上式直接给出了加权平均的平方误差",{"5":{"51":2}}],["则需要进一步分析",{"5":{"48":2}}],["则需要除以权重总和",{"5":{"51":2}}],["则需要额外的编码开销",{"5":{"61":2}}],["则需要",{"5":{"92":2,"153":2}}],["则加权均方误差定义为",{"5":{"51":2}}],["则加权mse可以写为",{"5":{"51":2}}],["则预测向量为",{"5":{"51":2}}],["则kl散度定义为",{"5":{"61":2}}],["则这种编码浪费的信息越少",{"5":{"61":2}}],["则雅可比矩阵",{"5":{"61":2}}],["则最终输出",{"5":{"143":2}}],["则最终输出的近似误差满足",{"5":{"142":2}}],["则最终的输入表示为",{"5":{"94":2}}],["则最优配置满足边际贡献最大化",{"5":{"132":2}}],["则最大似然估计等价于最小化mae",{"5":{"51":2}}],["则最小化交叉熵",{"5":{"61":2}}],["则属于负类",{"5":{"61":2}}],["则邻域定义为",{"5":{"86":2,"151":2}}],["则总参数量",{"5":{"159":2}}],["则总损失",{"5":{"41":2}}],["则总复杂度为",{"5":{"86":2,"151":2}}],["则总编码容量约为比特",{"5":{"89":1}}],["则总编码容量约为",{"5":{"89":1}}],["则原始注意力分数矩阵为",{"5":{"87":2,"152":2}}],["则是moe架构的决策中心",{"5":{"159":2}}],["则是",{"5":{"143":1}}],["则是一个的列向量",{"5":{"50":1}}],["则是低秩矩阵",{"5":{"87":1,"152":1}}],["则注意力矩阵",{"5":{"141":1}}],["则注意力矩阵的第行为",{"5":{"141":1}}],["则注意力矩阵是核矩阵经过行归一化得到的",{"5":{"87":2,"152":2}}],["则注意力输出满足",{"5":{"87":1,"152":1}}],["则注意力输出",{"5":{"87":1,"152":1}}],["则注意力输出为输入信息的加权聚合",{"5":{"132":2}}],["则注意力输出为",{"5":{"92":2,"153":2}}],["则注意力分数可以重新写为",{"5":{"91":2}}],["则可以认为反向传播实现正确",{"5":{"44":2}}],["则可以理解为在​空间中的表示",{"5":{"45":1}}],["则可学习位置编码矩阵定义为",{"5":{"89":2}}],["则质心位于原点",{"5":{"89":2}}],["则协方差矩阵为",{"5":{"89":2}}],["则存在某种范数使得",{"5":{"148":2}}],["则存在某个隐空间使得注意力计算等价于该隐空间中的线性运算",{"5":{"86":2,"151":2}}],["则存在正交矩阵",{"5":{"143":2}}],["则存在子列使得梯度范数趋于零",{"5":{"136":2}}],["则存在收敛子列收敛到临界点",{"5":{"136":2}}],["则存在个正交基向量",{"5":{"89":1}}],["则存在",{"5":{"89":1}}],["则存在信息冗余",{"5":{"93":2}}],["则编码矩阵可以表示为",{"5":{"89":2}}],["则编码向量的期望范数为",{"5":{"89":2}}],["则傅里叶变换变为",{"5":{"90":2}}],["则平移序列的dft为",{"5":{"90":1}}],["则平移序列",{"5":{"90":1}}],["则各元素的方差为",{"5":{"148":1}}],["则各层的计算定义为",{"5":{"46":2}}],["则各维度对应的频率参数为",{"5":{"91":1}}],["则各维度对应的频率参数",{"5":{"91":1}}],["则各部分的参数数量分析如下",{"5":{"93":2}}],["则query张量的计算为",{"5":{"93":2}}],["则它们的线性组合可以逼近任意连续函数",{"5":{"71":2}}],["则它们的联合信息量是各自信息量之和",{"5":{"93":2}}],["则输出退化为仅对权重最高的两个专家求和",{"5":{"159":2}}],["则输出公式变为",{"5":{"158":2}}],["则输出向量为",{"5":{"71":2}}],["则输出为",{"5":{"86":2,"145":2,"151":2}}],["则输入嵌入矩阵可以表示为",{"5":{"94":1}}],["则输入嵌入矩阵",{"5":{"94":1}}],["则每层前馈网络的计算量约为",{"5":{"159":2}}],["则每层的梯度传递因子约为",{"5":{"41":2}}],["则每个设备的参数量约为",{"5":{"159":2}}],["则每个设备需要发送和接收的数据量为",{"5":{"157":2}}],["则每个设备处理的数据量为",{"5":{"157":2}}],["则每个专家接收到的梯度样本数为",{"5":{"157":2}}],["则每个头的投影矩阵维度为",{"5":{"94":2}}],["则每个元素的方差初始化为",{"5":{"94":2}}],["则代表了源语言各个词语所携带的实际信息内容",{"5":{"95":2}}],["则矩阵的frobenius范数为",{"5":{"89":2}}],["则矩阵乘法的结果为",{"5":{"95":2}}],["则标准化和的分布趋向于标准正态分布",{"5":{"96":1}}],["则标准化和",{"5":{"96":1}}],["则标准差为1",{"5":{"95":2}}],["则softmax操作定义为",{"5":{"95":2}}],["有利于防止少数专家被过度使用",{"5":{"159":2}}],["有利于梯度下降的收敛",{"5":{"71":2}}],["有远见",{"5":{"156":2}}],["有多好",{"5":{"156":4}}],["有多种理论解释",{"5":{"48":2}}],["有意义的文本",{"5":{"154":2}}],["有能力的",{"5":{"154":2}}],["有约束的策略优化",{"5":{"154":2}}],["有关",{"5":{"148":1}}],["有放缩",{"5":{"145":1}}],["有放缩版本的dropout在训练阶段的期望输出等于原始输入",{"5":{"145":2}}],["有放缩版本的dropout在训练阶段就保证了输出期望的一致性",{"5":{"145":2}}],["有放缩版本的dropout在期望意义上保持了原始输入",{"5":{"145":2}}],["有放缩版本在训练时就进行缩放",{"5":{"145":1}}],["有放缩版本",{"5":{"145":4}}],["有放缩与无放缩的实现差异",{"2":{"145":1},"5":{"145":1}}],["有一个零特征值",{"5":{"136":2}}],["有价值的特征",{"5":{"133":1}}],["有各自的特点",{"5":{"101":2}}],["有其独立的query",{"5":{"69":1}}],["有助于跳出次优点",{"5":{"136":2}}],["有助于逃离",{"5":{"136":2}}],["有助于逃离平坦区域",{"5":{"48":2}}],["有助于模型学习边界情况",{"5":{"70":2}}],["有助于学习清晰的表示空间",{"5":{"69":2}}],["有助于捕获全局依赖",{"5":{"86":2,"151":2}}],["有助于数值稳定性",{"5":{"86":2,"151":2}}],["有以下效果",{"5":{"69":2}}],["有",{"5":{"44":2,"45":4,"47":2,"48":2,"51":4,"61":8,"70":8,"87":2,"88":4,"90":2,"91":1,"96":5,"97":4,"101":2,"134":4,"135":8,"136":3,"137":6,"139":4,"140":2,"141":2,"143":2,"145":2,"147":2,"149":4,"152":2}}],["有效计算量",{"5":{"159":2}}],["有效缓解了这个问题",{"5":{"150":2}}],["有效缓解了深层网络的梯度消失问题",{"5":{"150":2}}],["有效势能不仅包含原始损失函数",{"5":{"149":2}}],["有效放大因子",{"5":{"148":1}}],["有效性能接近随机",{"5":{"138":2}}],["有效条件数随模型规模增长",{"5":{"135":2}}],["有效临界学习率约为",{"5":{"135":2}}],["有效访问",{"5":{"132":2}}],["有效避免了梯度消失问题",{"5":{"70":2}}],["有效地",{"5":{"69":1}}],["有效秩决定了网络能够捕获的信息维度",{"5":{"41":2}}],["有效秩",{"5":{"41":2,"87":2,"143":2,"152":2}}],["有效秩定义为",{"5":{"41":2}}],["有效秩衡量了矩阵中",{"5":{"41":2}}],["有效秩与信息传递",{"2":{"41":1},"5":{"41":1}}],["有效秩与梯度衰减",{"5":{"41":2}}],["有效秩与衰减奇异值",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["有效秩的定义基于奇异值的分布",{"5":{"87":2,"152":2}}],["有效秩度量了需要多少个奇异值才能",{"5":{"87":2,"152":2}}],["有效自由度",{"5":{"50":2}}],["有效批大小",{"5":{"50":2}}],["有效复杂度",{"5":{"92":4,"153":4}}],["有更强的梯度信号",{"5":{"42":2}}],["有界区域",{"5":{"147":2}}],["有界性",{"5":{"136":2,"137":2}}],["有界振荡性",{"5":{"71":2}}],["有界",{"5":{"71":2,"133":2,"135":2,"136":1}}],["有损编码",{"5":{"71":1}}],["有限",{"5":{"44":1,"45":1,"96":3}}],["有限样本量带来的统计波动自然地起到了正则化的作用",{"5":{"61":2}}],["有些样本更关键",{"5":{"51":2}}],["有副作用的",{"5":{"88":2}}],["有可训练参数的",{"5":{"89":2}}],["有时我们需要的是",{"5":{"70":2}}],["有时我们需要的是区分性的预测",{"5":{"70":1}}],["有时我们需要的是校准的预测概率",{"5":{"70":1}}],["有时略优",{"5":{"89":2}}],["有深刻的联系",{"5":{"90":2}}],["有​",{"5":{"91":1}}],["有着有趣的类比",{"5":{"94":2}}],["与此同时",{"5":{"159":2}}],["与此相比",{"5":{"146":2}}],["与稠密模型相比",{"5":{"159":2}}],["与计算量",{"5":{"159":4}}],["与直接惩罚分配频率偏离的损失函数不同",{"5":{"158":2}}],["与均匀分布的kl散度的",{"5":{"158":2}}],["与均方误差和交叉熵的比较",{"5":{"69":2}}],["与均方误差的比较",{"5":{"61":2}}],["与过去无关",{"5":{"156":2}}],["与监督学习不同",{"5":{"156":2}}],["与动作价值函数",{"2":{"156":1},"5":{"156":1}}],["与环境交互",{"5":{"155":4}}],["与1偏离太远",{"5":{"154":2}}],["与1的相对关系",{"5":{"148":2}}],["与lstm的内在联系",{"5":{"150":1}}],["与lstm的内在联系揭示了highway",{"5":{"150":1}}],["与batch",{"5":{"150":4}}],["与残差连接中简单的加法操作不同",{"5":{"150":2}}],["与fisher信息矩阵",{"5":{"149":1}}],["与全批量梯度下降不同",{"5":{"149":2}}],["与全局最小值相近",{"5":{"51":2}}],["与朗之万动力学的联系",{"2":{"149":1},"5":{"149":1}}],["与hessian矩阵的联系",{"2":{"149":1},"5":{"149":1}}],["与裁剪密切相关",{"5":{"147":2}}],["与层归一化不同",{"5":{"146":2}}],["与层归一化相比",{"5":{"41":2}}],["与批量中其他样本无关",{"5":{"146":2}}],["与批归一化不同",{"5":{"41":2,"146":2}}],["与批归一化的比较",{"5":{"41":2}}],["与一个均匀分布之间进行插值",{"5":{"144":2}}],["与普通最小二乘解",{"5":{"144":2}}],["与各mode因子矩阵的乘积",{"5":{"143":2}}],["与各任务",{"5":{"133":1}}],["与推理时的条件分布完全相同",{"5":{"140":1}}],["与通过链式法则计算的时间复杂度对比",{"5":{"140":1}}],["与信息论的深层联系",{"2":{"139":1},"5":{"139":1}}],["与信息瓶颈理论的统一框架",{"5":{"133":2}}],["与泛化性能正相关",{"5":{"137":1}}],["与数据量",{"5":{"137":1}}],["与数值微分",{"5":{"45":2,"48":2}}],["与从",{"5":{"137":1}}],["与贝叶斯推断的关系",{"5":{"137":2}}],["与减小学习率",{"5":{"134":2}}],["与共享参数的依赖越强",{"5":{"133":1}}],["与训练数据",{"5":{"133":1}}],["与训练数据之间的互信息",{"5":{"133":1}}],["与标签",{"5":{"133":2}}],["与标准softmax注意力不同",{"5":{"87":2,"152":2}}],["与标准注意力框架兼容",{"5":{"91":2}}],["与条件熵的关系",{"5":{"133":2}}],["与传统的稠密模型相比",{"5":{"157":2}}],["与传统的分类或回归问题不同",{"5":{"140":2}}],["与传统的权重正则化",{"5":{"133":2}}],["与传统距离度量不同",{"5":{"61":2}}],["与线性的信息论权衡",{"5":{"132":2}}],["与最大熵分布",{"5":{"132":2}}],["与最相似的键获得接近1的概率",{"5":{"69":1}}],["与infonce的联系",{"5":{"101":2}}],["与自回归语言建模不同",{"5":{"101":2}}],["与自然语言中位置依赖的统计特性相符",{"5":{"90":2}}],["与具有相同的函数形式",{"5":{"101":1}}],["与参数",{"5":{"70":1,"97":1}}],["与参数无关",{"5":{"70":1,"97":1}}],["与任务类型匹配",{"5":{"70":2}}],["与键",{"5":{"69":1,"132":2}}],["与某个负样本",{"5":{"69":1}}],["与某种",{"5":{"69":2}}],["与每个",{"5":{"69":1}}],["与真实值存在显著差距",{"5":{"133":2}}],["与真实互信息之间的关系",{"5":{"69":1}}],["与真实分布",{"5":{"61":1}}],["与所有其他样本",{"5":{"69":1}}],["与位置",{"5":{"69":1,"132":1}}],["与位置语义相关",{"5":{"69":1}}],["与位置编码的联合设计",{"5":{"41":2}}],["与位置编码的信息论联系",{"5":{"71":2}}],["与位置编码的协同",{"5":{"101":2}}],["与位置之间的距离存在特定的数学关系",{"5":{"91":2}}],["与位置无关",{"5":{"91":2}}],["与位置距离的直觉一致",{"5":{"91":2}}],["与位置距离无关",{"5":{"92":2,"153":2}}],["与查询更相似的键获得更大的权重",{"5":{"69":2}}],["与交叉熵的联系",{"5":{"101":2}}],["与交叉熵的等价性",{"5":{"69":2}}],["与交叉熵组合的梯度计算",{"5":{"61":2}}],["与下游任务的契合",{"2":{"41":1},"5":{"41":1}}],["与sigmoid相比",{"5":{"41":2,"42":4}}],["与sigmoid导数的比较",{"5":{"42":2}}],["与sigmoid导数在形式上非常相似",{"5":{"42":1}}],["与sigmoid导数",{"5":{"42":1}}],["与sigmoid的数学关系",{"2":{"42":1},"5":{"42":1}}],["与sigmoid的梯度比较",{"5":{"42":2}}],["与梯度饱和相反",{"5":{"41":2}}],["与梯度消失问题的联系",{"5":{"42":2}}],["与第五章注意力的联系",{"5":{"71":2}}],["与第四章损失函数的联系",{"5":{"71":2}}],["与第四章第一节讨论的均方误差不同",{"5":{"61":2}}],["与第二项转置相关",{"5":{"91":2}}],["与损失函数的联系",{"5":{"71":2}}],["与前向传播相同",{"5":{"44":1,"45":1}}],["与前向传播相反",{"5":{"44":1,"45":1}}],["与其直接对策略参数进行无约束的梯度更新",{"5":{"154":2}}],["与其上下文之间的相对关系",{"5":{"101":1}}],["与其他头的重叠程度",{"5":{"132":2}}],["与其他头的信息重叠",{"5":{"132":2}}],["与其他位置之间的互信息",{"5":{"69":1}}],["与其他选项无关",{"5":{"69":2}}],["与其他行无关",{"5":{"45":2}}],["与其通过标准正态累积分布函数",{"5":{"71":1}}],["与其手工设计位置编码的形式",{"5":{"89":2}}],["与权重初始化的关系",{"5":{"41":2}}],["与权重矩阵区别对待",{"5":{"46":2}}],["与麦肯罗皮层神经元使用阶跃函数不同",{"5":{"47":2}}],["与完整的hessian相比",{"5":{"48":2}}],["与凸优化问题不同",{"5":{"48":2}}],["与目标",{"5":{"48":1}}],["与pmf不同",{"5":{"96":2}}],["与向量的大小无关",{"5":{"50":2}}],["与随机高斯初始化或xavier初始化不同",{"5":{"50":2}}],["与mse相比",{"5":{"51":2}}],["与预测空间",{"5":{"51":1}}],["与",{"2":{"145":1},"5":{"61":7,"69":1,"88":4,"92":5,"93":2,"94":2,"95":2,"101":1,"132":4,"135":2,"145":1,"153":5,"158":2}}],["与模型参数量",{"5":{"159":2}}],["与模型无关",{"5":{"61":2}}],["与模型分布",{"5":{"61":2}}],["与欧几里得距离不同",{"5":{"61":2}}],["与gram矩阵的比较",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["与核矩阵的比较",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["与初始位置无关",{"5":{"87":2,"152":2}}],["与起始位置无关",{"5":{"87":1,"152":1}}],["与起始位置",{"5":{"87":1,"152":1}}],["与序列长度无关",{"5":{"86":2,"87":1,"151":2,"152":1}}],["与序列长度",{"5":{"87":1,"152":1}}],["与绝对位置",{"5":{"88":2,"90":1,"91":1}}],["与绝对位置和无关",{"5":{"90":1,"91":1}}],["与绝对位置无关",{"5":{"90":2}}],["与正弦编码的比较分析",{"2":{"89":1},"5":{"89":1}}],["与正弦余弦编码不同",{"5":{"89":2}}],["与正弦余弦编码相比",{"5":{"89":2}}],["与正弦余弦编码的多尺度频率结构有某种对应关系",{"5":{"89":2}}],["与正弦余弦编码的平滑性质一致",{"5":{"89":2}}],["与正弦余弦编码各有优劣",{"5":{"89":2}}],["与频率域之间的深刻对偶关系",{"5":{"90":2}}],["与频率域中频率分辨率",{"5":{"90":2}}],["与注意力熵单调相关",{"5":{"132":1}}],["与注意力机制中缩放因子的比较",{"5":{"69":2}}],["与注意力机制的数学联系",{"2":{"70":1},"5":{"70":1}}],["与注意力机制的梯度流",{"2":{"41":1},"5":{"41":1}}],["与注意力机制的设计协同",{"2":{"41":1},"5":{"41":1}}],["与注意力机制的联系",{"5":{"61":4}}],["与注意力分数的交互分析",{"2":{"91":1},"5":{"91":1}}],["与假设矛盾",{"5":{"91":2}}],["与后面的",{"5":{"92":2,"153":2}}],["与它们在序列中的距离无关",{"5":{"92":2,"153":2}}],["与rope结合的可能性",{"5":{"88":2}}],["与rnn的迭代依赖结构不同",{"5":{"92":2,"153":2}}],["与rnn相比",{"5":{"92":2,"153":2}}],["与硬选择",{"5":{"92":2,"153":2}}],["与之对应的是不同",{"5":{"92":2,"153":2}}],["与内容向量相加或拼接",{"5":{"88":2}}],["与内容编码兼容",{"5":{"92":2,"153":2}}],["与卷积矩阵的比较",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["与卷积网络相比",{"5":{"92":2,"153":2}}],["与混合专家模型的类比",{"2":{"93":1},"5":{"93":1}}],["与相乘",{"5":{"93":1}}],["与输入向量",{"5":{"145":1}}],["与输入向量的外积",{"5":{"44":2}}],["与输入",{"5":{"133":2}}],["与输入序列长度和模型维度成正比",{"5":{"93":2}}],["与固定形式的正弦余弦位置编码不同",{"5":{"89":2}}],["与固定的基函数",{"5":{"93":2}}],["与头数无关",{"5":{"93":2}}],["与堆叠后的投影矩阵相乘",{"5":{"93":1}}],["与单独使用可学习编码相同",{"5":{"89":2}}],["与单头配置的参数数量相同",{"5":{"94":2}}],["与query更相似的key获得更高的权重",{"5":{"69":2}}],["与query和key不同",{"5":{"94":2}}],["与非凸损失函数的对比",{"5":{"97":2}}],["与利普希茨常数",{"5":{"97":1}}],["与transformer架构的联系",{"5":{"71":2}}],["与transformer架构的协同",{"5":{"101":2}}],["k会导致梯度无法反向传播到门控网络",{"5":{"159":2}}],["k位置为1",{"5":{"159":2}}],["k掩码向量为",{"5":{"159":2}}],["k的离散选择",{"5":{"158":2}}],["k稀疏路由",{"2":{"159":1},"5":{"159":3}}],["k稀疏路由中",{"5":{"158":2}}],["k稀疏路由的离散性",{"5":{"157":2}}],["k路由的稀疏门控权重需要重新归一化",{"5":{"159":2}}],["k路由的核心思想是只选择门控权重最大的",{"5":{"159":2}}],["k路由的数学原理和实现细节对于理解moe的工作机制至关重要",{"5":{"159":2}}],["k路由策略实现",{"5":{"159":2}}],["k路由策略选择的专家索引集合为",{"5":{"158":2}}],["k路由",{"5":{"158":2}}],["k路由设置下",{"5":{"158":2}}],["k选择操作可以数学化为",{"5":{"159":2}}],["k选择操作是恒等映射",{"5":{"157":2}}],["k选择的一个微妙之处在于它破坏了梯度流的连续性",{"5":{"159":2}}],["k选择的专家索引",{"5":{"158":2}}],["k选择的掩码向量",{"5":{"157":2}}],["k选择",{"5":{"157":2,"159":6}}],["k是分段常数函数",{"5":{"157":2}}],["k操作本身是不可微的",{"5":{"159":2}}],["k操作原语",{"5":{"159":2}}],["k操作通常结合一个",{"5":{"159":2}}],["k操作的离散性使得",{"5":{"157":2}}],["k操作",{"5":{"157":2}}],["khatri",{"5":{"143":3}}],["k$列",{"5":{"143":2}}],["kb",{"5":{"143":1}}],["k+1",{"5":{"143":2}}],["k|",{"5":{"143":2}}],["kaplan等人的原始标度律研究",{"5":{"138":1}}],["kaplan等人的原始标度律研究为这一领域奠定了基础",{"5":{"138":1}}],["kaiming初始化",{"5":{"89":2,"148":2}}],["kde",{"5":{"137":4}}],["kr",{"5":{"143":4}}],["krylov",{"5":{"135":2}}],["kronecker",{"5":{"48":2,"92":2,"135":4,"153":2}}],["kronecker积",{"5":{"143":1}}],["kronecker积与向量化",{"2":{"50":1},"5":{"50":1}}],["kronecker积是两个矩阵之间的特殊二元运算",{"5":{"50":2}}],["kronecker积具有许多有用的代数性质",{"5":{"50":2}}],["kronecker积的主要应用包括权重共享",{"5":{"50":2}}],["kronecker积可以帮助我们将大矩阵运算分解为多个小矩阵运算的组合",{"5":{"50":2}}],["k相似度",{"5":{"69":1}}],["k^",{"5":{"69":6,"141":2}}],["k个最大的分数",{"5":{"101":2}}],["k个非零权重",{"5":{"69":2}}],["k个位置",{"5":{"86":2,"151":2}}],["k",{"5":{"42":78,"46":6,"48":3,"61":76,"86":17,"94":2,"132":22,"133":14,"135":8,"137":6,"141":26,"142":10,"143":11,"151":17,"157":2,"159":11}}],["kullback",{"2":{"61":1},"5":{"61":3,"96":2,"137":2}}],["kurtosis",{"5":{"96":2}}],["kl约束项使得优化后的策略不是简单地选择奖励最高的动作",{"5":{"101":2}}],["kl散度惩罚项的引入部分是为了缓解这个问题",{"5":{"154":2}}],["kl散度惩罚项",{"5":{"154":2}}],["kl散度会是一个较大的正值",{"5":{"154":2}}],["kl散度为零",{"5":{"154":2}}],["kl散度约束与自适应惩罚",{"2":{"154":1},"5":{"154":1}}],["kl散度约束的数学分析",{"2":{"101":1},"5":{"101":1}}],["kl散度约束的引入增加了优化目标复杂性",{"5":{"97":2}}],["kl散度度量了概率分布之间的差异",{"5":{"139":2}}],["kl散度度量了两个概率分布之间的",{"5":{"61":2}}],["kl散度项的作用",{"5":{"101":2}}],["kl散度",{"5":{"61":4,"69":2,"70":3,"71":2,"96":2,"139":2}}],["kl散度与交叉熵",{"2":{"96":1},"5":{"96":1}}],["kl散度衡量两个概率分布之间的",{"5":{"96":2}}],["kl散度是非负的",{"5":{"96":2}}],["kl散度有多种重要应用",{"5":{"96":2}}],["kl散度用于限制策略更新的幅度",{"5":{"96":2}}],["kl散度用于衡量学生模型和教师模型输出分布之间的差异",{"5":{"96":2}}],["kl散度在不同场景下的不同形式反映了其灵活性",{"5":{"96":2}}],["kl散度可以理解为使用分布来编码来自分布的样本时所需要的额外信息量",{"5":{"96":1}}],["kl散度可以理解为使用分布",{"5":{"96":1}}],["kl散度不对称",{"5":{"96":2}}],["kl散度不满足对称性和三角不等式",{"5":{"61":2}}],["kl散度的实际计算",{"5":{"154":1}}],["kl散度的实际计算在大规模应用中可能比较昂贵",{"5":{"154":1}}],["kl散度的数学意义",{"5":{"154":1}}],["kl散度的数学意义在于它衡量了两个概率分布之间的",{"5":{"154":1}}],["kl散度的非负性",{"5":{"139":2}}],["kl散度的类比",{"5":{"70":2}}],["kl散度的几何意义",{"5":{"70":2}}],["kl散度的几个关键性质对其应用至关重要",{"5":{"61":2}}],["kl散度的定义为",{"5":{"96":2}}],["kl散度的物理意义可以从",{"5":{"61":2}}],["kl散度越小",{"5":{"61":2}}],["kl散度定义为",{"5":{"96":2}}],["kl散度定义了所谓的",{"5":{"61":2}}],["kl散度诱导的几何结构是黎曼几何结构",{"5":{"61":1}}],["kl散度诱导的几何结构是",{"5":{"61":1}}],["kl",{"2":{"137":1},"5":{"61":4,"70":14,"97":2,"133":18,"137":57}}],["keep",{"5":{"145":2}}],["keepdim",{"5":{"132":1}}],["keepdim=true",{"5":{"132":1}}],["key相似度分数转换为注意力权重",{"5":{"69":2}}],["key相似度分数转换为概率分布",{"5":{"71":2}}],["key",{"0":{"94":1},"4":{"94":1},"5":{"61":2,"69":2,"70":4,"71":2,"86":6,"88":2,"91":4,"93":18,"94":21,"95":6,"97":4,"131":5,"132":10,"151":6}}],["key和value的缓存也可以采用稀疏格式",{"5":{"86":2,"151":2}}],["key和value投影各为",{"5":{"86":2,"151":2}}],["key和value共享一个​维投影",{"5":{"93":1}}],["key和value共享一个",{"5":{"93":1}}],["key矩阵",{"5":{"87":2,"152":2}}],["key矩阵和value矩阵",{"5":{"94":2}}],["key维度限制",{"5":{"87":2,"152":2}}],["key内积",{"5":{"88":6}}],["key内积自动地只与相对位置有关",{"5":{"88":2}}],["key内积运算",{"5":{"88":2}}],["key内积的差异",{"5":{"88":2}}],["key内积就自然地只依赖于相对位置",{"5":{"88":2}}],["key向量进行逐元素乘法具体而言",{"5":{"88":1}}],["key向量进行逐元素乘法",{"5":{"88":1}}],["key投影和value投影的参数数量也各为",{"5":{"93":2}}],["key提供了信息片段的标识符",{"5":{"94":2}}],["key空间有不同的维度",{"5":{"94":2}}],["kernel",{"5":{"86":2,"87":6,"88":2,"93":2,"151":2,"152":6}}],["knowledge",{"5":{"87":2,"152":2}}],["k采样和top",{"5":{"96":2}}],["因子",{"5":{"145":1}}],["因子收缩参数范数",{"5":{"144":2}}],["因子矩阵",{"5":{"143":2}}],["因子矩阵可以通过对张量各模的切片矩阵进行svd得到",{"5":{"143":2}}],["因为两个簇在输入空间中可能以非线性方式交织",{"5":{"159":2}}],["因为两个点和通过线性关系连接",{"5":{"51":1}}],["因为两个点",{"5":{"51":1}}],["因为top",{"5":{"157":2}}],["因为路由决策",{"5":{"157":2}}],["因为需要遍历整个动作空间计算期望",{"5":{"154":2}}],["因为分母只有",{"5":{"148":2}}],["因为relu的零导数特性没有被充分考虑",{"5":{"148":2}}],["因为归一化操作本身就限制了激活值和梯度的尺度",{"5":{"147":2}}],["因为归一化操作已经处理了输入的尺度变化",{"5":{"146":2}}],["因为通常较大",{"5":{"146":1}}],["因为在mle点附近",{"5":{"149":2}}],["因为在深度学习实践中",{"5":{"146":2}}],["因为在数值上计算高阶多项式的根非常困难",{"5":{"50":2}}],["因为循环结构会导致时间步之间的信息累积",{"5":{"145":2}}],["因为较大的lipschitz常数意味着较小的输入扰动可能导致较大的输出变化",{"5":{"144":2}}],["因为这需要掌握所有可能的文本序列的概率",{"5":{"140":2}}],["因为这些领域的",{"5":{"89":2}}],["因为趋于",{"5":{"137":1}}],["因为自然对数的导数形式更为简洁",{"5":{"137":2}}],["因为不同层在不同时间进入压缩阶段",{"5":{"133":2}}],["因为不依赖于",{"5":{"89":1}}],["因为互信息在指数族分布中与内积相关",{"5":{"132":2}}],["因为真实类别是1",{"5":{"70":2}}],["因为真实分布固定",{"5":{"96":2}}],["因为被",{"5":{"70":1}}],["因为我们在推导中直接对",{"5":{"70":2}}],["因为我们可以将期望操作与梯度操作交换次序",{"5":{"96":2}}],["因为",{"5":{"41":3,"44":2,"61":4,"70":1,"87":2,"89":1,"90":2,"91":1,"95":2,"137":1,"140":2,"145":2,"146":1,"152":2,"158":2}}],["因为或",{"5":{"41":1}}],["因为层归一化位于关键路径上",{"5":{"41":2}}],["因为语言",{"5":{"71":2}}],["因为语言中的许多规律具有位置平移不变性",{"5":{"88":2}}],["因为它在效率和效果之间取得了良好的平衡",{"5":{"159":2}}],["因为它可以强制某些专家处理特定类型的输入",{"5":{"159":2}}],["因为它可以将不同样本的计算分配到不同的处理单元上同时执行",{"5":{"50":2}}],["因为它约束的是梯度的绝对尺度",{"5":{"147":2}}],["因为它对偏离均值的平方进行累加",{"5":{"158":2}}],["因为它对接近零的参数施加恒定的惩罚",{"5":{"147":2}}],["因为它对频繁出现的特征使用较小的学习率",{"5":{"97":2}}],["因为它保留了梯度的主要方向信息",{"5":{"41":2,"97":2}}],["因为它的稀疏激活特性和线性区域的恒定导数有利于深层网络的训练",{"5":{"71":2}}],["因为它的数学形式最为清晰",{"5":{"46":2}}],["因为它不满足对称性和三角不等式",{"5":{"96":2}}],["因为它决定了信息放大或衰减的上界",{"5":{"50":2}}],["因为它将复杂的矩阵运算转化为标准的矩阵",{"5":{"50":2}}],["因为它考虑到了概率分布的归一化约束和概率质量在各个类别间的分配方式",{"5":{"61":2}}],["因为它已经与内容信息纠缠在一起",{"5":{"88":2}}],["因为它们在推理过程中不变",{"5":{"89":2}}],["因为它们的位置差都是1",{"5":{"91":2}}],["因为它为每个绝对位置生成一个独立的编码向量",{"5":{"91":1}}],["因为它为每个绝对位置",{"5":{"91":1}}],["因为它能够捕捉语义方向的相似性而不受词频等因素的影响",{"5":{"50":2}}],["因为它能够适应训练过程中梯度分布的变化",{"5":{"97":2}}],["因为异或问题的正例点位于对角位置",{"5":{"47":2}}],["因为数值稳定性较差且计算成本高昂",{"5":{"50":2}}],["因为正交矩阵的奇异值全部为1",{"5":{"50":2}}],["因为正规方程的解等价于求解",{"5":{"50":1}}],["因为正规方程",{"5":{"50":1}}],["因为正弦函数对所有实数都有定义",{"5":{"88":2}}],["因为长序列可能消耗大量内存",{"5":{"50":2}}],["因为梯度是线性的",{"5":{"50":2}}],["因为前向和后向替换的复杂度是而不是高斯消元的",{"5":{"50":1}}],["因为前向和后向替换的复杂度是",{"5":{"50":1}}],["因为其使用绝对值而非平方",{"5":{"51":2}}],["因为对于任意向量",{"5":{"61":2}}],["因为行和为1且元素非负",{"5":{"87":2,"152":2}}],["因为1是特征值",{"5":{"87":2,"152":2}}],["因为指数函数是单调变换",{"5":{"87":2,"152":2}}],["因为每个设备既是发送者也是接收者",{"5":{"157":2}}],["因为每个头的投影矩阵维度为",{"5":{"86":2,"151":2}}],["因为每一层的贡献被独立限制",{"5":{"147":2}}],["因为每层可以学习不同的低秩变换",{"5":{"87":2,"152":2}}],["因为模型从未学习过如何处理那种位置与内容的新组合",{"5":{"88":2}}],["因为所有其他位置都参与比较",{"5":{"69":2}}],["因为所有位置编码向量都位于由前​个基向量张成的子空间中",{"5":{"89":1}}],["因为所有位置编码向量都位于由前",{"5":{"89":1}}],["因为非零奇异值较少",{"5":{"89":2}}],["因为公式定义在所有整数上",{"5":{"89":2,"91":2}}],["因为维度限制",{"5":{"89":2}}],["因为编码不使用极高频率",{"5":{"90":2}}],["因为是固定的",{"5":{"91":1}}],["因为一边没有足够的",{"5":{"92":2,"153":2}}],["因为key和value的缓存可以大幅减少",{"5":{"93":2}}],["因为假设均值为0",{"5":{"95":2}}],["因为负曲率方向上的梯度通常很小",{"5":{"97":2}}],["因此没有信息来指导其改进",{"5":{"157":2}}],["因此没有被广泛采用",{"5":{"88":2}}],["因此网络可以学习在不同的输入和不同的深度位置采用不同的信息流动策略",{"5":{"150":2}}],["因此公式简化为",{"5":{"150":2}}],["因此具有更好的泛化边界",{"5":{"149":2}}],["因此具有更好的泛化性质",{"5":{"142":2}}],["因此深层网络仍然可能面临梯度消失问题",{"5":{"148":2}}],["因此深入掌握线性代数的基本概念和运算规则是理解模型工作原理的必要前提",{"5":{"50":2}}],["因此无法给出简单的解析关系",{"5":{"147":2}}],["因此算法收敛到某个",{"5":{"143":2}}],["因此训练",{"5":{"138":2}}],["因此不同样本的损失值差异更大",{"5":{"149":2}}],["因此不影响泛化",{"5":{"147":2}}],["因此不是度量",{"5":{"137":2}}],["因此不如策略一常用",{"5":{"88":2}}],["因此​",{"5":{"101":1}}],["因此受到更强的",{"5":{"69":2}}],["因此",{"5":{"42":6,"44":10,"45":8,"46":2,"47":6,"51":2,"61":4,"69":2,"70":4,"71":8,"86":2,"87":9,"88":8,"89":2,"90":12,"91":8,"92":9,"93":2,"95":4,"101":3,"134":2,"138":4,"139":2,"140":2,"141":4,"143":2,"144":4,"145":12,"147":2,"148":12,"149":2,"151":2,"152":9,"153":9,"155":4,"156":2,"157":4,"158":4}}],["因此需要使用更大的权重方差来保持激活的稳定性",{"5":{"41":1}}],["因此需要使用更大的权重方差",{"5":{"41":1}}],["因此在实践中表现出更强的表示能力和泛化性能",{"5":{"71":2}}],["因此得名",{"5":{"46":2}}],["因此共轭梯度法和拟牛顿法等方法在处理大规模优化问题时具有优势",{"5":{"48":2}}],["因此标准化操作是合理的",{"5":{"96":2}}],["因此现代gpu都针对矩阵运算进行了专门的硬件优化",{"5":{"50":2}}],["因此单个层的计算量是相当可观的",{"5":{"50":2}}],["因此敏感性较低",{"5":{"51":2}}],["因此它不是真正意义上的",{"5":{"61":2}}],["因此我们可以将数据生成过程建模为",{"5":{"61":2}}],["因此可以进行特征分解",{"5":{"143":2}}],["因此可以使用更大的学习率进行训练",{"5":{"41":2}}],["因此可以视为一种",{"5":{"87":1,"152":1}}],["因此可以学习到不同的特征表示和关联模式",{"5":{"93":2}}],["因此可以直接适应特定任务的需求",{"5":{"93":2}}],["因此可以保留更多的原始信息",{"5":{"94":2}}],["因此特征值可以是复数",{"5":{"87":2,"152":2}}],["因此秩最多为​",{"5":{"87":1,"152":1}}],["因此秩最多为",{"5":{"87":1,"152":1}}],["因此点积较大",{"5":{"91":2}}],["因此是理论上的最优复杂度",{"5":{"92":1,"153":1}}],["因此从到的路径长度恒为1",{"5":{"92":1,"153":1}}],["因此从",{"5":{"92":1,"153":1}}],["因此输出饱和往往伴随梯度饱和",{"5":{"41":2}}],["因此输出维度为",{"5":{"93":2}}],["因此最小值是第二部分",{"5":{"154":4}}],["因此最小化交叉熵等价于最小化kl散度",{"5":{"96":2}}],["因此最优解具有最优的泛化保证",{"5":{"133":2}}],["因此最终输出中各头的信息是相互交织的",{"5":{"93":2}}],["因此增加头数意味着减小每个头的维度",{"5":{"93":2}}],["因此均方误差的",{"5":{"97":2}}],["因其在微分运算中的便利性而被广泛采用",{"5":{"61":2}}],["因果注意力掩码确保位置",{"5":{"101":1}}],["因果注意力掩码确保位置只能关注位置的token",{"5":{"101":1}}],["因果性",{"5":{"91":2}}],["因果位置编码",{"5":{"91":2}}],["因果位置编码通过某种方式打破这种对称性",{"5":{"91":2}}],["因果掩码",{"5":{"95":2}}],["因果掩码的数学形式",{"2":{"95":1},"5":{"95":1}}],["因果掩码的引入使得注意力计算仍然保持矩阵形式",{"5":{"95":2}}],["因式分解",{"5":{"93":2}}],["距离",{"5":{"51":2,"61":4,"70":2,"71":2,"90":2,"96":2,"154":2}}],["距离最小的点",{"5":{"51":1}}],["距离越远的token之间的依赖越弱",{"5":{"88":2}}],["距离衰减",{"5":{"91":2}}],["距离近的位置倾向于有更高的注意力权重",{"5":{"91":2}}],["梯度越大",{"5":{"158":2}}],["梯度聚合",{"5":{"157":1}}],["梯度和优化器状态的一部分使用fp32",{"5":{"157":2}}],["梯度和优化器状态的显存消耗通常与参数数量成正比",{"5":{"157":2}}],["梯度和优化器状态",{"5":{"157":2}}],["梯度和hessian等概念",{"5":{"48":2}}],["梯度通信和梯度计算也可以流水线化",{"5":{"157":4}}],["梯度方向只提供了",{"5":{"154":2}}],["梯度就能够有效地在变换路径和恒等路径之间分配",{"5":{"150":2}}],["梯度分解为",{"5":{"150":2}}],["梯度协方差的各向异性结构对sgd的优化动力学产生了深刻影响",{"5":{"149":2}}],["梯度协方差矩阵扮演着",{"5":{"149":1}}],["梯度协方差矩阵扮演着扩散张量",{"5":{"149":1}}],["梯度协方差矩阵提供了理解这一现象的理论框架",{"5":{"149":2}}],["梯度协方差矩阵与hessian矩阵的关系",{"5":{"149":2}}],["梯度协方差矩阵与fisher信息矩阵的关系",{"5":{"149":2}}],["梯度协方差矩阵与fisher信息矩阵存在近似关系",{"5":{"149":2}}],["梯度协方差矩阵与损失函数的二阶曲率之间存在深刻而优美的联系",{"5":{"149":2}}],["梯度协方差矩阵往往是秩亏的或接近秩亏的",{"5":{"149":2}}],["梯度协方差矩阵通常表现出以下频谱特征",{"5":{"149":2}}],["梯度协方差矩阵的一个核心特征是其高度",{"5":{"149":1}}],["梯度协方差矩阵的一个核心特征是其高度各向异性",{"5":{"149":1}}],["梯度协方差矩阵的研究仍在深入发展",{"5":{"149":2}}],["梯度协方差矩阵的严格定义如下",{"5":{"149":2}}],["梯度协方差矩阵也被称为",{"5":{"149":2}}],["梯度协方差矩阵正是描述这种噪声二阶统计特性的核心数学工具",{"5":{"149":2}}],["梯度协方差矩阵<",{"5":{"131":1}}],["梯度协方差矩阵",{"0":{"149":1},"4":{"112":1,"149":1},"5":{"131":4,"149":4}}],["梯度将指数级消失",{"5":{"148":2}}],["梯度将膨胀到原来的上万倍",{"5":{"148":2}}],["梯度放大因子为",{"5":{"148":2}}],["梯度以指数速率",{"5":{"148":2}}],["梯度以指数速率爆炸",{"5":{"148":1}}],["梯度以指数速率消失",{"5":{"148":1}}],["梯度必然消失",{"5":{"148":2}}],["梯度几乎完全消失",{"5":{"148":2}}],["梯度几乎为零",{"5":{"42":2}}],["梯度需要通过链式法则跨越所有中间层传播回来",{"5":{"148":2}}],["梯度需要沿时间步反向传播",{"5":{"41":2}}],["梯度稳定性由以下因素共同决定",{"5":{"148":2}}],["梯度稳定性分析",{"5":{"41":2}}],["梯度稳定化和有效正则化的重要性将更加凸显",{"5":{"147":2}}],["梯度增大",{"5":{"136":2}}],["梯度足够小",{"5":{"136":2}}],["梯度噪声与曲率正相关在深度网络优化的整个过程中都具有一定的指导意义",{"5":{"149":2}}],["梯度噪声在曲率大的方向上更强",{"5":{"149":2}}],["梯度噪声在负特征值方向提供逃逸力",{"5":{"135":2}}],["梯度噪声协方差",{"5":{"149":2}}],["梯度噪声大",{"5":{"134":2}}],["梯度作为",{"5":{"135":2}}],["梯度减小",{"5":{"134":2}}],["梯度估计相对稳定",{"5":{"149":2}}],["梯度估计更准确",{"5":{"134":2}}],["梯度估计的方差减小",{"5":{"96":2}}],["梯度估计的方差较大",{"5":{"96":2}}],["梯度更新",{"5":{"133":2,"137":2}}],["梯度信息就被",{"5":{"148":2}}],["梯度信息流",{"5":{"132":2}}],["梯度信号会指数级衰减",{"5":{"92":2,"153":2}}],["梯度信号会高度相似",{"5":{"93":2}}],["梯度解释",{"5":{"101":2}}],["梯度被放大了约7",{"5":{"148":2}}],["梯度被放大了约2",{"5":{"148":2}}],["梯度被放大",{"5":{"70":2}}],["梯度被压缩",{"5":{"70":2}}],["梯度为",{"5":{"70":4}}],["梯度为零的点",{"5":{"97":2,"136":2}}],["梯度仍能传递",{"5":{"132":2}}],["梯度仍保持与",{"5":{"70":1}}],["梯度仍保持与成正比",{"5":{"70":1}}],["梯度仍为",{"5":{"70":2}}],["梯度仍然可以通过路径传递到前一层",{"5":{"41":1}}],["梯度仍然可以通过",{"5":{"41":1}}],["梯度仍然需要经过次矩阵乘法的累积",{"5":{"92":1,"153":1}}],["梯度仍然需要经过",{"5":{"92":1,"153":1}}],["梯度在概率饱和区域被过度压缩",{"5":{"70":2}}],["梯度还受到",{"5":{"70":2}}],["梯度结构的对比分析",{"2":{"70":1},"5":{"70":1}}],["梯度归一化的推广",{"5":{"101":2}}],["梯度归一化的更新方向为",{"5":{"101":2}}],["梯度归一化方法的一个理论优势是",{"5":{"101":2}}],["梯度归一化方法通过调整梯度范数来实现任务间的平衡",{"5":{"101":2}}],["梯度归一化方法通过调整每个任务的梯度范数来实现平衡",{"5":{"70":2}}],["梯度归一化与帕累托优化",{"2":{"101":1},"5":{"101":1}}],["梯度归一化",{"5":{"70":2,"101":2}}],["梯度",{"5":{"44":2,"95":1,"133":2,"136":2,"150":2,"157":2}}],["梯度饱和特性",{"2":{"70":1},"5":{"70":1}}],["梯度饱和的数学机制",{"2":{"41":1},"5":{"41":1}}],["梯度饱和与梯度爆炸的数学根源",{"0":{"41":1},"4":{"41":1},"5":{"21":4,"41":1,"131":4}}],["梯度饱和与梯度爆炸的数学根源<",{"5":{"21":1,"131":1}}],["梯度饱和与梯度爆炸是深度学习训练中的两大核心挑战",{"5":{"41":2}}],["梯度饱和是深度学习训练中的核心挑战之一",{"5":{"41":2,"70":2}}],["梯度饱和",{"5":{"41":2,"42":2,"97":1}}],["梯度饱和可以在输出尚未饱和时发生",{"5":{"41":2}}],["梯度饱和意味着信息在反向传播过程中丢失",{"5":{"41":2}}],["梯度衰减为原来的",{"5":{"41":2,"42":2}}],["梯度衰减为",{"5":{"41":2,"42":2}}],["梯度爆炸以及性能退化等一系列问题",{"5":{"150":2}}],["梯度爆炸通常需要所有雅可比矩阵都具有一定的",{"5":{"148":2}}],["梯度爆炸风险依然存在",{"5":{"147":2}}],["梯度爆炸临界状态",{"5":{"147":2}}],["梯度爆炸是导致训练不稳定的主要威胁之一",{"5":{"147":2}}],["梯度爆炸是深度学习训练中的常见问题",{"5":{"97":2}}],["梯度爆炸时的正则化削弱",{"5":{"144":2}}],["梯度爆炸等问题的根本原因",{"5":{"135":2}}],["梯度爆炸的下界分析比上界更为复杂",{"5":{"148":2}}],["梯度爆炸的充分条件",{"5":{"148":2}}],["梯度爆炸的数学定义",{"5":{"148":2}}],["梯度爆炸的数学表征",{"2":{"147":1},"5":{"147":1}}],["梯度爆炸的数学机制",{"2":{"41":1},"5":{"41":1}}],["梯度爆炸的判据",{"5":{"135":2}}],["梯度爆炸的现象描述",{"2":{"41":1},"5":{"41":1}}],["梯度爆炸的阈值与裁剪策略",{"2":{"41":1},"5":{"41":1}}],["梯度爆炸的表现包括",{"5":{"41":2}}],["梯度爆炸指的是在反向传播过程中梯度值变得非常大",{"5":{"41":2}}],["梯度爆炸在循环神经网络和transformer中尤为突出",{"5":{"41":2}}],["梯度爆炸",{"5":{"41":2,"44":1,"148":7}}],["梯度爆炸可能通过多个渠道发生",{"5":{"41":2}}],["梯度至少可以通过恒等路径",{"5":{"41":2}}],["梯度高速公路",{"5":{"41":2}}],["梯度裁剪通过直接约束梯度范数来防止训练过程中的梯度爆炸",{"5":{"147":2}}],["梯度裁剪通过限制梯度范数来防止训练过程中的不稳定性",{"5":{"144":2}}],["梯度裁剪通常与其他稳定化技术配合使用",{"5":{"147":2}}],["梯度裁剪阈值选择策略对比",{"5":{"147":2}}],["梯度裁剪阈值的选取是实践中的关键问题",{"5":{"147":2}}],["梯度裁剪则在梯度较大的区域",{"5":{"147":2}}],["梯度裁剪也产生隐式正则化效应",{"5":{"147":2}}],["梯度裁剪等价于在原始损失函数中添加一个",{"5":{"147":2}}],["梯度裁剪隐式地引入了一个局部",{"5":{"147":2}}],["梯度裁剪可以帮助避免训练过程中的不稳定",{"5":{"147":2}}],["梯度裁剪可以在不同的粒度上实施",{"5":{"147":2}}],["梯度裁剪实际上削弱了l2正则化的效果",{"5":{"144":2}}],["梯度裁剪修改梯度为",{"5":{"144":2}}],["梯度裁剪将有效学习率限制在安全范围内",{"5":{"136":2}}],["梯度裁剪将梯度范数限制在范围内",{"5":{"44":1}}],["梯度裁剪将梯度范数限制在",{"5":{"44":1}}],["梯度裁剪的实践考量",{"2":{"147":1},"5":{"147":1}}],["梯度裁剪的理论分析",{"2":{"147":1},"5":{"147":1}}],["梯度裁剪的数学框架",{"2":{"147":1},"5":{"147":1}}],["梯度裁剪的稳定性上界",{"5":{"136":2}}],["梯度裁剪的稳定性保证",{"5":{"136":2}}],["梯度裁剪的边界效应",{"5":{"44":2}}],["梯度裁剪与正则化的协同",{"2":{"144":1},"5":{"144":1}}],["梯度裁剪与正则化<",{"5":{"131":1}}],["梯度裁剪与正则化",{"0":{"147":1},"4":{"114":1,"147":1},"5":{"131":4,"147":1}}],["梯度裁剪与爆炸抑制",{"2":{"97":1},"5":{"97":1}}],["梯度裁剪有两种主要形式",{"5":{"41":2,"97":2}}],["梯度裁剪",{"5":{"44":7,"48":2,"97":2,"147":2,"154":2}}],["梯度裁剪就是将梯度向量的模长限制在某个阈值之内",{"5":{"50":2}}],["梯度裁剪是否影响模型的泛化性能",{"5":{"147":2}}],["梯度裁剪是否会损害优化算法的收敛性是一个重要的理论问题",{"5":{"147":2}}],["梯度裁剪是最常用的策略",{"5":{"41":2}}],["梯度裁剪是一种有效的抑制梯度爆炸的技术",{"5":{"97":2}}],["梯度裁剪本质上限制了有效学习率的上界",{"5":{"97":2}}],["梯度裁剪需要考虑这些特殊结构",{"5":{"97":2}}],["梯度流改善",{"5":{"150":1}}],["梯度流改善是归一化技术的共同优势",{"5":{"150":1}}],["梯度流分析",{"5":{"150":2}}],["梯度流分析表明",{"5":{"150":1}}],["梯度流分析是理解残差连接优势的关键",{"5":{"150":1}}],["梯度流",{"5":{"41":2,"97":2,"148":2}}],["梯度流的信息瓶颈",{"5":{"41":2}}],["梯度流稳定",{"5":{"41":2}}],["梯度流更加稳定",{"5":{"41":2}}],["梯度流通过注意力权重矩阵进行",{"5":{"41":1}}],["梯度流通过注意力权重矩阵",{"5":{"41":1}}],["梯度流动更加顺畅",{"5":{"146":2}}],["梯度流动更加稳定和鲁棒",{"5":{"132":2}}],["梯度流动保证损失值不增",{"5":{"134":2}}],["梯度流动系统沿轨迹单调减少损失值",{"5":{"134":2}}],["梯度流动的收敛性",{"5":{"134":2}}],["梯度流动的连续时间极限",{"2":{"134":1},"5":{"134":1}}],["梯度流动可类比于物理中的",{"5":{"134":2}}],["梯度流动",{"5":{"89":2,"134":2}}],["梯度流与优化数学",{"2":{"131":1},"4":{"112":1,"113":1,"114":1,"147":1,"148":1,"149":1},"5":{"131":7}}],["梯度流与信息保持",{"2":{"41":1},"5":{"41":1}}],["梯度流与离散动力系统",{"2":{"97":1},"5":{"97":1}}],["梯度流是梯度下降的连续极限",{"5":{"97":2}}],["梯度流方程为",{"5":{"97":2}}],["梯度范数会指数级地消失或爆炸",{"5":{"148":2}}],["梯度范数保持恒定",{"5":{"148":2}}],["梯度范数监测",{"5":{"136":2}}],["梯度范数演化",{"5":{"135":2}}],["梯度范数约为",{"5":{"41":2}}],["梯度范数与奇异值",{"5":{"41":2}}],["梯度范数可能保持在",{"5":{"148":1}}],["梯度范数可能保持在范围内",{"5":{"148":1}}],["梯度范数可能指数增长",{"5":{"148":2}}],["梯度范数可能放大",{"5":{"41":2}}],["梯度范数可能衰减",{"5":{"41":2}}],["梯度范数边界",{"5":{"41":2}}],["梯度范数的可能变化范围越大",{"5":{"41":2}}],["梯度的稳定性不仅取决于单层的雅可比矩阵性质",{"5":{"148":1}}],["梯度的传播可以表示为",{"5":{"148":1}}],["梯度的传播可以表示为​",{"5":{"148":1}}],["梯度的衰减因子为",{"5":{"148":2}}],["梯度的方向更加一致",{"5":{"144":2}}],["梯度的方向更加稳定",{"5":{"96":2}}],["梯度的可分解性",{"5":{"140":2}}],["梯度的统计性质",{"5":{"134":2}}],["梯度的雅可比矩阵",{"5":{"70":2,"97":2}}],["梯度的显式计算",{"2":{"69":1},"5":{"69":1}}],["梯度的累积效应可能导致不稳定",{"5":{"41":2}}],["梯度的大小也更加稳定",{"5":{"41":2}}],["梯度的性质对于理解优化过程至关重要",{"5":{"48":2}}],["梯度的范数满足",{"5":{"41":2}}],["梯度的范数缩放约为",{"5":{"92":2,"153":2}}],["梯度主要集中在少数方向",{"5":{"41":2}}],["梯度尺度的归一化",{"5":{"41":2}}],["梯度尺度归一化",{"5":{"41":2}}],["梯度也可能逐渐消失",{"5":{"148":2}}],["梯度也不会因为激活值的尺度变化而放大或缩小",{"5":{"41":2}}],["梯度也被限制在这个流形的切空间中",{"5":{"41":2}}],["梯度关于的雅可比矩阵的范数满足",{"5":{"41":1}}],["梯度关于",{"5":{"41":1}}],["梯度消失比梯度爆炸更容易发生",{"5":{"148":2}}],["梯度消失与梯度爆炸并非互斥的两种情况",{"5":{"148":2}}],["梯度消失与梯度爆炸是深度神经网络训练中最核心",{"5":{"148":2}}],["梯度消失与梯度爆炸的数学分析",{"2":{"135":1},"5":{"135":1}}],["梯度消失与梯度爆炸的数学条件<",{"5":{"131":1}}],["梯度消失与梯度爆炸的数学条件",{"0":{"148":1},"4":{"113":1,"148":1},"5":{"131":4,"148":1}}],["梯度消失的充分条件",{"5":{"148":2}}],["梯度消失的数学定义",{"5":{"148":2}}],["梯度消失的典型案例",{"2":{"148":1},"5":{"148":1}}],["梯度消失的判据",{"5":{"135":2}}],["梯度消失",{"5":{"42":2,"44":1,"94":2,"95":2,"140":1,"148":7}}],["梯度消失或梯度爆炸等问题",{"5":{"44":2}}],["梯度检查",{"5":{"44":4}}],["梯度检查点技术通过在前向传播时只保存部分激活值",{"5":{"48":2}}],["梯度检查点",{"5":{"50":2,"157":2}}],["梯度可以更容易地在多层之间传播",{"5":{"148":2}}],["梯度可以更直接地反向传播",{"5":{"132":2}}],["梯度可以有效地在时间步之间传播",{"5":{"145":2}}],["梯度可以简洁地写为",{"5":{"69":2}}],["梯度可以稳定传播",{"5":{"41":2}}],["梯度可以正可以负",{"5":{"42":2}}],["梯度可以直接计算为",{"5":{"61":2}}],["梯度可能指数级爆炸",{"5":{"148":2}}],["梯度可能沿",{"5":{"144":2}}],["梯度可能因精度限制而下溢",{"5":{"44":2}}],["梯度可能变得很大或很小",{"5":{"87":2,"152":2}}],["梯度缩放",{"5":{"44":3}}],["梯度缩放等技术都是保证大规模神经网络训练稳定性的重要手段",{"5":{"44":2}}],["梯度计算为",{"5":{"157":2}}],["梯度计算与优化",{"2":{"139":1},"5":{"139":1}}],["梯度计算",{"5":{"44":1}}],["梯度计算的优化策略",{"5":{"61":2}}],["梯度值接近0",{"5":{"44":1}}],["梯度值溢出",{"5":{"44":1}}],["梯度值在fp16下为0",{"5":{"44":1}}],["梯度是多元函数最陡上升方向的向量",{"5":{"48":2}}],["梯度向量就是误差信号本身",{"5":{"44":2}}],["梯度向量指向函数增长最快的方向",{"5":{"48":2}}],["梯度向量指导着参数的更新方向",{"5":{"48":2}}],["梯度向量与等值面垂直",{"5":{"48":2}}],["梯度与雅可比矩阵",{"5":{"44":2}}],["梯度与等值面",{"5":{"48":2}}],["梯度下降可以看作朗之万动力学的离散化",{"5":{"144":2}}],["梯度下降可能收敛到较差的解",{"5":{"51":2}}],["梯度下降更新",{"5":{"136":2}}],["梯度下降性质",{"5":{"136":2}}],["梯度下降算法满足以下全局收敛界",{"5":{"136":2}}],["梯度下降算法可能沿着这个方向逃离鞍点",{"5":{"97":2}}],["梯度下降是最基本的优化算法",{"5":{"136":2}}],["梯度下降是最基本也最广泛使用的优化算法",{"5":{"48":2}}],["梯度下降系统在不动点处的",{"5":{"136":2}}],["梯度下降系统线性收敛的特征值条件",{"5":{"136":2}}],["梯度下降",{"5":{"134":4}}],["梯度下降的收敛速度与条件数成反比",{"5":{"144":2}}],["梯度下降的收敛速度为",{"5":{"97":2}}],["梯度下降的累积梯度范数满足",{"5":{"136":2}}],["梯度下降的稳定性条件",{"5":{"135":2}}],["梯度下降的",{"2":{"135":1},"5":{"135":1}}],["梯度下降的线性收敛速度由条件数决定",{"5":{"134":2}}],["梯度下降的线性稳定性条件",{"5":{"134":2}}],["梯度下降的离散更新趋近于连续时间动态",{"5":{"134":2}}],["梯度下降与凸优化基础",{"2":{"48":1},"5":{"48":1}}],["梯度下降在凸优化中的收敛性有完善的理论保证",{"5":{"48":2}}],["梯度下降等一阶优化算法在凸函数上具有良好的收敛保证",{"5":{"51":2}}],["梯度下降以线性速率收敛",{"5":{"136":2}}],["梯度下降以线性速度收敛",{"5":{"48":2}}],["梯度下降以的速度收敛到最优解附近",{"5":{"51":1}}],["梯度下降以",{"5":{"51":1}}],["梯度下降动力学与收敛分析",{"2":{"97":1},"5":{"97":1}}],["梯度下降动力学",{"5":{"97":2}}],["梯度累积效应",{"5":{"41":2}}],["梯度累积是另一种与批处理相关的技术",{"5":{"50":2}}],["梯度累积在数学上等价于使用更大的批次进行训练",{"5":{"50":2}}],["梯度较小",{"5":{"51":2}}],["梯度较大",{"5":{"51":2}}],["梯度特性",{"5":{"61":1}}],["梯度会被收缩",{"5":{"144":2}}],["梯度会被放大",{"5":{"144":2}}],["梯度会均匀分布到所有位置",{"5":{"41":2}}],["梯度会指数级衰减",{"5":{"92":2,"153":2}}],["梯度会指数级增长",{"5":{"92":2,"153":2}}],["梯度会变得非常小",{"5":{"95":1}}],["梯度从输出层向输入层传播",{"5":{"41":2}}],["梯度从输出层向输入层传播时",{"5":{"42":2}}],["梯度从输出端通过value矩阵反向传播回query投影",{"5":{"94":2}}],["梯度都能稳定地流动",{"5":{"94":2}}],["梯度无法有效地反向传播回前层网络",{"5":{"95":2}}],["梯度能够稳定地回传",{"5":{"95":2}}],["0或1",{"5":{"148":2,"150":2}}],["0次",{"5":{"146":3}}],["09",{"5":{"145":2}}],["0||",{"5":{"135":4}}],["01",{"5":{"70":4,"133":2,"148":2}}],["07",{"5":{"69":2}}],["0",{"2":{"154":1,"155":1,"156":3},"5":{"42":42,"47":6,"61":8,"69":4,"70":18,"132":18,"133":14,"134":8,"135":20,"136":14,"137":4,"138":4,"141":2,"144":28,"145":12,"148":3}}],["044715x^3",{"5":{"42":2}}],["059em",{"2":{"156":1}}],["05",{"5":{"96":2,"136":2,"138":2}}],["时为1",{"5":{"159":2}}],["时才施加惩罚",{"5":{"158":2}}],["时最小",{"5":{"158":6,"159":2}}],["时序差分学习",{"5":{"155":1}}],["时梯度较小",{"5":{"144":2}}],["时发生分岔",{"5":{"136":1}}],["时刻的系统状态",{"5":{"136":1}}],["时严格递减",{"5":{"134":2}}],["时间索引集",{"5":{"134":2}}],["时间复杂度",{"5":{"44":1,"45":1,"93":2}}],["时间复杂度为",{"5":{"92":2,"93":2,"153":2}}],["时达到平衡",{"5":{"134":2}}],["时达到",{"5":{"132":2}}],["时达到这一下界",{"5":{"61":2}}],["时",{"5":{"41":13,"42":14,"45":1,"46":4,"47":8,"50":1,"61":11,"69":3,"70":13,"71":2,"86":2,"87":7,"88":2,"89":5,"90":5,"92":10,"94":1,"95":5,"96":9,"132":2,"133":4,"134":18,"135":6,"136":24,"137":4,"138":10,"140":7,"141":5,"142":2,"143":3,"144":6,"145":22,"146":4,"147":4,"148":6,"149":3,"150":10,"151":2,"152":7,"153":10,"154":16,"155":4,"156":6,"157":4,"158":8,"159":8}}],["时变化最慢",{"5":{"42":2}}],["时可归入任一模式",{"5":{"45":1}}],["时kl散度为0",{"5":{"96":1}}],["时取严格小于号",{"5":{"48":1}}],["时取等号",{"5":{"61":4,"137":4,"139":1}}],["时仍能保持较大的梯度值",{"5":{"61":2}}],["时域和频域之间的对偶关系",{"5":{"90":2}}],["时首先考虑的特征",{"5":{"94":2}}],["时的2倍",{"5":{"95":1}}],["时增加",{"5":{"97":1}}],["时减少",{"5":{"97":1}}],["由ioffe和szegedy于2015年提出",{"5":{"148":2}}],["由ioffe和szegedy于2015年在论文",{"5":{"146":2}}],["由he等人于2015年提出",{"5":{"148":2}}],["由glorot和bengio于2010年提出",{"5":{"148":2}}],["由期望保持性质",{"5":{"145":2}}],["由srivastava等人于2014年在论文",{"5":{"145":2}}],["由softmax给出时",{"5":{"71":2}}],["由softmax函数给出",{"5":{"61":1}}],["由最大化以下目标给出",{"5":{"137":2}}],["由和对称性得证",{"5":{"137":1}}],["由定义的对称性显然成立",{"5":{"137":2}}],["由定义出发",{"5":{"137":2}}],["由周期",{"5":{"136":1}}],["由周期点生成的轨道",{"5":{"136":1}}],["由时间平移不变性保证",{"5":{"136":2}}],["由条件1可知损失函数序列单调递减且有下界",{"5":{"136":2}}],["由函数值递减性质",{"5":{"136":2}}],["由推论",{"5":{"135":2}}],["由独立随机变量和的方差公式",{"5":{"135":2}}],["由特征值的加法性质和三角不等式可得",{"5":{"135":2}}],["由特定频率的正弦余弦函数张成",{"5":{"89":2}}],["由递推关系",{"5":{"135":2}}],["由线性系统的谱分解可得",{"5":{"135":2}}],["由泰勒展开",{"5":{"135":2}}],["由混合偏导数的",{"5":{"135":2}}],["由微分中值定理",{"5":{"134":2}}],["由优化算法定义",{"5":{"134":2}}],["由状态转移函数的半群性质可得",{"5":{"134":2}}],["由广义特征值问题确定",{"5":{"133":1}}],["由互信息的定义展开",{"5":{"133":1}}],["由互信息的定义展开和",{"5":{"133":1}}],["由数据处理不等式和信息链式法则递归可得",{"5":{"133":2}}],["由数据处理不等式决定",{"5":{"133":2}}],["由数据固有噪声决定的不可减少误差",{"5":{"51":2}}],["由熵的上界",{"5":{"133":1}}],["由熵的上界和可得",{"5":{"133":1}}],["由熵的定义和性质",{"5":{"132":2}}],["由信息熵的上界公式",{"5":{"132":2}}],["由限制信息维度",{"5":{"132":1}}],["由决定信息流向哪",{"5":{"132":1}}],["由自注意力的对称性",{"5":{"101":2}}],["由于路由决策的稀疏性",{"5":{"159":2}}],["由于每个输入只需要",{"5":{"159":2}}],["由于每个输入只激活",{"5":{"159":2}}],["由于门控机制倾向于将相似输入分配给相似的专家",{"5":{"159":2}}],["由于稀疏激活的存在",{"5":{"158":2}}],["由于可能需要保存所有",{"5":{"157":2}}],["由于可以近似表示为低秩分解",{"5":{"87":1,"152":1}}],["由于可以取任意值",{"5":{"91":1}}],["由于top",{"5":{"157":2,"159":2}}],["由于真实的",{"5":{"155":2}}],["由于回报值的大小取决于具体的任务和奖励函数设置",{"5":{"155":2}}],["由于奖励模型是基于有限的人类偏好数据训练的",{"5":{"154":2}}],["由于我们无法为语言模型生成的每一个输出提供精确的数值奖励",{"5":{"154":2}}],["由于优势函数为负",{"5":{"154":2}}],["由于两个策略的分布不同",{"5":{"154":2}}],["由于恒等矩阵",{"5":{"150":2}}],["由于梯度为零",{"5":{"149":2}}],["由于fisher信息矩阵的负期望等于hessian矩阵的期望",{"5":{"149":2}}],["由于训练数据中存在的冗余性以及神经网络参数化方式的特点",{"5":{"149":2}}],["由于训练数据通常被组织为序列",{"5":{"96":2}}],["由于批量内各样本是独立采样的",{"5":{"149":2}}],["由于其参数对梯度爆炸不太敏感",{"5":{"147":2}}],["由于仿射变换参数",{"5":{"146":1}}],["由于仿射变换参数被移除",{"5":{"146":1}}],["由于前一层参数的更新",{"5":{"146":2}}],["由于使用小批量数据的样本均值和样本方差",{"5":{"146":2}}],["由于卷积层具有空间结构",{"5":{"145":2}}],["由于掩码",{"5":{"145":3}}],["由于掩码的各分量独立",{"5":{"145":1}}],["由于行向量只有",{"5":{"141":1}}],["由于行向量只有种可能",{"5":{"141":1}}],["由于链式法则的自回归分解",{"5":{"140":2}}],["由于函数在上单调递增",{"5":{"139":1}}],["由于损失函数的特殊结构",{"5":{"136":2}}],["由于的特征向量与",{"5":{"149":1}}],["由于的连续性",{"5":{"136":1}}],["由于的每一行是一个概率分布",{"5":{"87":1,"152":1}}],["由于约一半的神经元输出为零",{"5":{"41":2}}],["由于归一化使用mini",{"5":{"41":2}}],["由于激活值的分布更加稳定",{"5":{"41":2}}],["由于",{"5":{"44":1,"45":1,"61":6,"70":4,"71":1,"87":9,"88":2,"91":1,"92":1,"93":6,"95":1,"97":2,"136":1,"137":2,"139":1,"141":2,"144":4,"145":3,"148":2,"149":8,"150":2,"152":9,"153":1,"154":4,"155":4,"159":4}}],["由于神经网络是一个深度复合函数",{"5":{"44":2}}],["由于是各向异性的",{"5":{"149":1}}],["由于是非线性函数",{"5":{"71":1}}],["由于是逐元素函数",{"5":{"44":1}}],["由于是行随机矩阵",{"5":{"87":4,"152":4}}],["由于是对角非奇异矩阵",{"5":{"87":1,"152":1}}],["由于本身是线性的",{"5":{"45":1}}],["由于任意布尔函数都可以由and",{"5":{"47":2}}],["由于参数数量巨大",{"5":{"48":2}}],["由于交叉项的期望为零",{"5":{"51":2}}],["由于连乘在数值计算上容易产生下溢",{"5":{"61":2}}],["由于在大多数学习任务中",{"5":{"61":2}}],["由于核函数的限制",{"5":{"87":2,"152":2}}],["由于旋转操作是周期性的",{"5":{"88":2}}],["由于通常不是对称矩阵",{"5":{"87":1,"152":1}}],["由于通常",{"5":{"89":2}}],["由于正弦编码的秩为",{"5":{"89":2}}],["由于正弦编码矩阵​的秩不超过",{"5":{"89":1}}],["由于正弦编码矩阵",{"5":{"89":1}}],["由于这些区别",{"5":{"87":2,"152":2}}],["由于这是一个单位复指数",{"5":{"90":2}}],["由于编码矩阵只学习了前​行的参数",{"5":{"89":1}}],["由于编码矩阵只学习了前",{"5":{"89":1}}],["由于编码包含多个频率成分",{"5":{"90":2}}],["由于编码包含多个维度",{"5":{"91":2}}],["由于余弦函数的周期性",{"5":{"90":2}}],["由于频率​是互不相同的无理数倍数",{"5":{"90":1}}],["由于频率",{"5":{"90":1}}],["由于各频率互不相同",{"5":{"90":1}}],["由于各频率",{"5":{"90":1}}],["由于除以了很大的数",{"5":{"91":2}}],["由于​且",{"5":{"87":1,"152":1}}],["由于​通常不会精确为零",{"5":{"92":1,"153":1}}],["由于​是独立学习的",{"5":{"93":1}}],["由于​是随机矩阵",{"5":{"93":1}}],["由于有个头",{"5":{"94":1}}],["由于有",{"5":{"94":1}}],["由于和独立",{"5":{"95":1,"145":1}}],["由于对于任意向量",{"5":{"97":2}}],["由于hessian是常数",{"5":{"97":2}}],["由于hessian依赖于预测概率",{"5":{"97":2}}],["由链式法则和联合熵的非负性得",{"5":{"137":1}}],["由链式法则和矩阵范数的次可乘性",{"5":{"135":2}}],["由链式法则和矩阵微积分的基本规则可得",{"5":{"135":2}}],["由链式法则和雅可比矩阵的性质",{"5":{"44":2}}],["由链式法则",{"5":{"44":2,"137":1}}],["由矩阵幂次的谱分解和",{"5":{"134":2}}],["由矩阵描述",{"5":{"71":1}}],["由矩阵",{"5":{"71":1}}],["由向量函数的链式法则直接得出",{"5":{"135":2}}],["由向量描述",{"5":{"71":1}}],["由向量",{"5":{"71":1}}],["由导数的定义",{"5":{"45":2}}],["由描述",{"5":{"45":2}}],["由激活模式定义",{"5":{"45":2}}],["由",{"5":{"45":2,"92":1,"132":4,"133":2,"134":2,"135":10,"136":2,"137":9,"153":1}}],["由多个全连接层堆叠而成",{"5":{"46":2}}],["由细胞体",{"5":{"47":2}}],["由超平面所界定",{"5":{"47":1}}],["由超平面",{"5":{"47":1}}],["由神经元的数量决定",{"5":{"47":1}}],["由三阶中心矩归一化得到",{"5":{"96":2}}],["由四阶中心矩归一化得到",{"5":{"96":2}}],["由jensen不等式保证",{"5":{"96":2}}],["由此可以得到",{"5":{"51":2}}],["由训练数据近似",{"5":{"61":2}}],["由训练数据决定",{"5":{"61":2}}],["由kl散度度量",{"5":{"61":2}}],["由真实标签",{"5":{"61":1}}],["由预测概率决定",{"5":{"61":1}}],["由预测概率",{"5":{"61":1}}],["由不同频率的正弦余弦函数张成",{"5":{"89":2}}],["由点积给出",{"5":{"90":2}}],["由余弦函数给出",{"5":{"91":2}}],["由后续的线性变换自动学习",{"5":{"91":2}}],["由正弦函数给出",{"5":{"91":2}}],["由正弦等式可得",{"5":{"91":2}}],["由公式完全指定的",{"5":{"88":2}}],["由公式",{"5":{"91":2}}],["由相同的词元组成",{"5":{"92":2,"153":2}}],["由​参数化",{"5":{"92":1,"153":1}}],["由卷积核大小决定",{"5":{"93":2}}],["函数在",{"5":{"139":1}}],["函数在注意力机制中扮演着核心角色",{"5":{"61":2}}],["函数值的连续性",{"5":{"136":2}}],["函数值随某一特定变量变化的速率",{"5":{"48":2}}],["函数从相似度分数转换而来",{"5":{"132":2}}],["函数是注意力机制的核心组件",{"5":{"132":2}}],["函数是凸的",{"5":{"70":1,"97":1}}],["函数是凸函数的充要条件是对于任意和任意",{"5":{"48":1}}],["函数将任意实数分数转换为概率分布",{"5":{"132":2}}],["函数将相似度分数转换为概率",{"5":{"132":2}}],["函数逼近基石",{"5":{"71":2}}],["函数",{"5":{"48":2,"61":4,"70":1,"90":2,"92":2,"97":1,"134":10,"145":2,"153":2}}],["函数的雅可比矩阵具有特殊的结构",{"5":{"61":2}}],["函数的二阶泰勒展开为",{"5":{"48":1}}],["函数的定义为",{"5":{"61":2}}],["函数的几个关键性质值得强调",{"5":{"61":2}}],["函数与交叉熵损失的数学联系",{"5":{"61":2}}],["函数定义为",{"5":{"61":2}}],["bytes",{"5":{"157":4}}],["burn",{"5":{"136":2}}],["bug",{"5":{"50":2}}],["broken",{"5":{"138":2}}],["broadcasting",{"5":{"50":2}}],["break",{"5":{"135":2,"136":4}}],["bptt",{"5":{"41":2}}],["bn",{"5":{"41":2}}],["bellman",{"5":{"156":6}}],["between",{"5":{"146":2}}],["betaβ",{"5":{"146":1}}],["beta先验和dirichlet先验可用于建模词汇概率的不确定性",{"5":{"96":2}}],["beta",{"5":{"97":11}}],["beta1",{"5":{"97":1}}],["begin",{"5":{"42":20,"61":8,"86":4,"97":2,"143":4,"144":6,"151":4}}],["bert的15",{"5":{"101":2}}],["bert",{"5":{"71":2}}],["bert等",{"5":{"47":2}}],["bernoulli",{"5":{"47":2,"96":2,"132":2,"145":1}}],["balancing",{"5":{"159":2}}],["ball",{"5":{"134":2}}],["baseline",{"5":{"155":2}}],["baseline与方差缩减",{"2":{"155":1},"5":{"155":1}}],["based",{"5":{"147":2,"155":2}}],["basin",{"5":{"135":2}}],["basis",{"5":{"93":2}}],["bayes",{"5":{"133":6,"137":2}}],["batchnorm通过将激活值标准化到单位方差",{"5":{"144":2}}],["batchnorm对hessian矩阵有两个主要影响",{"5":{"144":2}}],["batchnorm对hessian的影响",{"5":{"144":2}}],["batchnorm的梯度计算涉及输入梯度的标准化",{"5":{"144":2}}],["batchnorm的梯度流改善",{"5":{"144":2}}],["batchnorm的数学效应可以分解为两步",{"5":{"144":2}}],["batchnorm的平滑效应以及标签平滑对梯度分布的影响",{"5":{"144":2}}],["batchnorm的平滑效应",{"2":{"144":1},"5":{"144":1}}],["batchnorm虽然在概念上不属于",{"5":{"144":2}}],["batchnorm",{"5":{"132":1,"146":6}}],["batch",{"2":{"146":1},"5":{"41":5,"44":2,"47":2,"50":6,"94":2,"96":4,"132":18,"133":2,"145":2,"146":7,"147":4,"148":6,"149":6,"150":2}}],["batch中的激活值为",{"5":{"41":2}}],["batch的统计量而非全局统计量",{"5":{"41":2}}],["batch依赖",{"5":{"41":1}}],["batch内的样本",{"5":{"45":1}}],["backward",{"5":{"44":4}}],["backpropagation",{"5":{"44":2}}],["backprop",{"5":{"48":4}}],["band",{"5":{"86":2,"87":2,"151":2,"152":2}}],["b",{"5":{"46":10,"132":2,"133":2,"137":4,"141":2,"142":8,"143":53,"146":6}}],["bigram",{"5":{"140":2}}],["bigbird等模型中被广泛使用",{"5":{"86":2,"151":2}}],["bigbird",{"5":{"87":2,"152":2}}],["bifurcation",{"5":{"136":4}}],["bialek",{"5":{"133":2}}],["bias",{"5":{"47":2,"50":2,"51":8,"92":2,"137":4,"142":2,"146":2,"153":2,"155":2}}],["biases",{"5":{"88":4}}],["biological",{"5":{"47":2}}],["bit",{"5":{"96":2,"142":1}}],["bits",{"5":{"61":2,"137":2}}],["binomial",{"5":{"96":2}}],["binary",{"5":{"61":2}}],["bidirectional",{"5":{"86":2,"151":2}}],["boldsymbol",{"5":{"144":56}}],["bound",{"5":{"137":2}}],["boundary",{"5":{"47":2,"92":2,"153":2}}],["bottleneck",{"5":{"71":2,"87":2,"132":6,"133":2,"152":2}}],["bootstrap",{"5":{"96":2}}],["b测试是互联网公司和研究机构常用的实验方法",{"5":{"96":2}}],["b测试的核心是随机分组和控制变量",{"5":{"96":2}}],["b测试可以用于评估模型对用户交互的实际影响",{"5":{"96":2}}],["block",{"5":{"86":4,"93":2,"151":4}}],["bf16",{"5":{"93":2}}],["zy",{"5":{"133":8}}],["zou",{"5":{"133":2}}],["zip",{"5":{"132":4}}],["zig",{"5":{"42":2}}],["z",{"5":{"42":81,"46":11,"61":28,"97":12,"132":18,"133":32,"137":10,"141":8}}],["zk",{"5":{"42":1,"61":2}}],["zag",{"5":{"42":2}}],["z^",{"5":{"46":4}}],["zj",{"5":{"46":2}}],["==",{"5":{"132":2,"136":4}}],["=",{"5":{"42":98,"46":38,"61":48,"69":10,"86":18,"97":20,"101":2,"132":104,"133":46,"135":4,"137":6,"141":14,"142":20,"143":14,"144":26,"151":18,"157":4}}],["equation",{"5":{"156":6}}],["equivariant",{"5":{"88":2}}],["early",{"5":{"144":2}}],["eckart",{"5":{"143":2}}],["emergent",{"5":{"138":2}}],["embedding",{"0":{"88":1},"4":{"88":1},"5":{"88":5,"92":2,"94":2,"131":5,"153":2}}],["evidence",{"5":{"137":2}}],["est",{"5":{"133":4,"135":4,"137":16}}],["estimator",{"5":{"157":2}}],["estimate",{"5":{"132":12,"133":8}}],["estimation",{"5":{"48":2,"61":4,"69":4,"96":2,"137":4,"139":2,"155":2}}],["epoch",{"5":{"133":2}}],["epsilon",{"5":{"97":6}}],["environment",{"5":{"156":4}}],["ensemble",{"5":{"145":2}}],["enc",{"5":{"132":4}}],["encoding",{"5":{"88":10,"89":2,"91":12,"92":8,"94":2,"153":8}}],["engineering",{"5":{"70":2}}],["entropy",{"5":{"47":2,"61":4,"70":2,"92":2,"96":8,"132":14,"153":2}}],["end",{"5":{"42":20,"61":8,"86":4,"97":2,"132":14,"133":16,"135":8,"136":14,"137":4,"143":4,"144":6,"151":4}}],["e",{"5":{"42":8,"133":6,"141":2,"142":10}}],["erf",{"5":{"42":2}}],["error",{"5":{"44":2,"51":8,"156":2}}],["edge",{"5":{"45":2}}],["e^x",{"5":{"42":4}}],["e^",{"5":{"46":4,"141":4}}],["elbo",{"5":{"133":2,"137":2}}],["elu",{"5":{"41":2,"42":6,"86":2,"151":2}}],["elu特征映射",{"5":{"86":2,"151":2}}],["ellipsoid",{"5":{"149":2}}],["ell=1",{"5":{"46":10}}],["ell",{"5":{"46":26}}],["element",{"5":{"91":2,"147":2}}],["eff",{"5":{"141":2,"142":6}}],["efficiency",{"5":{"133":6}}],["efficient",{"5":{"50":2}}],["effect",{"5":{"92":2,"96":2,"153":2}}],["effective",{"5":{"87":2,"92":2,"152":2,"153":2}}],["exp技巧",{"5":{"139":4}}],["exp",{"5":{"42":56,"61":10,"97":6,"133":2,"137":8,"142":2}}],["exponentially",{"5":{"134":2}}],["exponential",{"5":{"48":2,"86":6,"96":2,"151":6}}],["expert",{"5":{"157":4,"158":4,"159":4}}],["experts",{"5":{"93":2,"159":6}}],["expectation",{"5":{"96":4,"145":2,"156":2}}],["expansion",{"5":{"93":2}}],["extension",{"5":{"88":2}}],["extrapolation",{"5":{"91":2}}],["eta",{"5":{"97":2,"142":6,"144":10}}],["jax=",{"2":{"154":1,"155":1,"156":3}}],["jacobian",{"0":{"135":1},"2":{"135":9},"4":{"135":1},"5":{"45":2,"48":2,"131":5,"133":2,"134":6,"135":86,"136":24}}],["jq",{"5":{"143":2}}],["jr",{"5":{"143":2}}],["jl",{"5":{"141":2}}],["jl引理的数学陈述",{"2":{"141":1},"5":{"141":1}}],["j≠i",{"5":{"137":2}}],["jensen",{"5":{"137":2}}],["j^2",{"5":{"142":3}}],["j^",{"5":{"141":4}}],["j^min",{"5":{"135":2}}],["j^max",{"5":{"135":2}}],["j^t",{"5":{"135":2}}],["j+1",{"5":{"135":2}}],["johnson",{"2":{"141":1},"5":{"141":5}}],["jordan",{"5":{"135":2}}],["joint",{"5":{"96":2}}],["js",{"5":{"133":6}}],["j",{"5":{"42":39,"46":9,"61":8,"97":10,"135":24,"137":12,"141":10,"143":6,"144":6}}],["j=1",{"5":{"42":6,"46":2,"141":8,"142":4}}],["ji",{"5":{"46":8}}],["rl",{"5":{"155":2}}],["rlhf训练中的关键挑战",{"5":{"154":1}}],["rlhf训练中的关键挑战包括",{"5":{"154":1}}],["rlhf仍然是将大语言模型与人类价值观和偏好对齐的最有效方法之一",{"5":{"154":2}}],["rlhf中的ppo目标函数通常包含三个组成部分",{"5":{"154":2}}],["rlhf中的奖励模型与策略优化",{"2":{"154":1},"5":{"154":1}}],["rlhf中的策略优化问题可以通过重新参数化直接求解",{"5":{"101":2}}],["rlhf损失函数中的kl散度项",{"5":{"101":1}}],["rlhf损失函数中的kl散度项是一个关键的数学组件",{"5":{"101":1}}],["rlhf依赖于人类偏好数据的质量和一致性",{"5":{"101":2}}],["rlhf的训练流程通常包含三个阶段",{"5":{"154":2}}],["rlhf的理论挑战",{"5":{"101":2}}],["rlhf的目标可以写为",{"5":{"101":2}}],["rlhf的三阶段流程",{"5":{"101":2}}],["rlhf的核心思想是",{"5":{"101":2}}],["rlhf的数学框架",{"2":{"101":1},"5":{"101":1}}],["rlhf",{"2":{"101":1},"5":{"97":3,"101":3,"154":2}}],["rlhf使用人类偏好的正样本和负样本",{"5":{"101":2}}],["rm",{"5":{"154":4}}],["rms维度",{"5":{"146":1}}],["rmsnorm首先计算第",{"5":{"146":1}}],["rmsnorm首先计算第个样本的均方根统计量",{"5":{"146":1}}],["rmsnorm已经逐渐取代层归一化",{"5":{"146":2}}],["rmsnorm可以达到与层归一化相当甚至更好的性能",{"5":{"146":2}}],["rmsnorm仍然保持了某种程度的归一化性质",{"5":{"146":2}}],["rmsnorm通过移除去均值化步骤",{"5":{"146":2}}],["rmsnorm不需要计算",{"5":{"146":2}}],["rmsnorm不需要计算方差",{"5":{"146":2}}],["rmsnorm不需要计算均值",{"5":{"41":2,"146":2}}],["rmsnorm相比层归一化具有更低的计算复杂度",{"5":{"146":2}}],["rmsnorm移除平移参数",{"5":{"146":1}}],["rmsnorm移除平移参数的数学依据是",{"5":{"146":1}}],["rmsnorm移除了去均值化步骤",{"5":{"146":2}}],["rmsnorm保留了仿射变换参数",{"5":{"146":2}}],["rmsnorm对输入进行归一化",{"5":{"146":2}}],["rmsnorm在保持甚至提升性能的同时",{"5":{"146":2}}],["rmsnorm在保持性能的同时减少了计算量",{"5":{"41":2}}],["rmsnorm的优点是计算效率高",{"5":{"146":2}}],["rmsnorm的设计基于对层归一化去均值化步骤的深入分析",{"5":{"146":2}}],["rmsnorm的计算量约为层归一化的一半",{"5":{"146":2}}],["rmsnorm的输出满足",{"5":{"146":2}}],["rmsnorm的缩放不变性",{"5":{"146":2}}],["rmsnorm的统计量计算与层归一化有本质区别",{"5":{"146":2}}],["rmsnorm的核心洞见是",{"5":{"146":2}}],["rmsnorm的理论基础",{"2":{"146":1},"5":{"146":1}}],["rmsnorm的数学动机是",{"5":{"41":2}}],["rmsnorm的数学性质",{"5":{"41":2}}],["rmsnorm与计算优化",{"2":{"41":1},"5":{"41":1}}],["rmsnorm",{"2":{"146":1},"5":{"41":6,"132":1,"146":10}}],["rmsnorm定义为",{"5":{"41":2}}],["rmsnorm具有以下优势",{"5":{"41":2}}],["rms范数是更简单的统计量",{"5":{"41":2}}],["rmsprop等自适应优化器中的梯度归一化和动量项可以等效为某种修改后的噪声结构",{"5":{"149":2}}],["rmsprop是adagrad的改进版本",{"5":{"48":2}}],["rmsprop",{"5":{"97":4,"134":8,"135":2}}],["r=1",{"5":{"143":4}}],["r^",{"5":{"142":2}}],["r",{"5":{"142":17,"143":8}}],["ridge回归的闭式解揭示了极小值向原点的选择性收缩",{"5":{"144":2}}],["ridge回归的闭式解",{"5":{"144":2}}],["ritz",{"5":{"135":4}}],["riemannian",{"5":{"70":4}}],["right",{"5":{"42":22,"46":8,"61":14,"69":6,"86":2,"97":2,"141":8,"142":2,"143":2,"144":2,"151":2}}],["right|",{"5":{"42":4}}],["reinforce算法必须等到轨迹结束后才能计算回报并进行更新",{"5":{"155":2}}],["reinforce算法的一个显著优点是它的简洁性和理论基础",{"5":{"155":2}}],["reinforce算法的完整流程如下",{"5":{"155":2}}],["reinforce算法的更新公式为",{"5":{"155":2}}],["reinforce算法的核心思想可以概括为",{"5":{"155":2}}],["reinforce算法是williams于1992年提出的",{"5":{"155":2}}],["reinforce算法",{"2":{"155":1},"5":{"155":1}}],["reinforcement",{"5":{"101":2,"154":2}}],["reward",{"5":{"154":6,"155":2,"156":6}}],["regularization",{"5":{"144":2,"145":2}}],["reg",{"5":{"144":8}}],["regime",{"5":{"133":2}}],["regression",{"5":{"47":2,"51":2,"143":2}}],["reduce",{"5":{"157":1}}],["reduce通信",{"5":{"157":4}}],["reduction",{"5":{"146":2}}],["reduction=",{"5":{"61":2}}],["reducing",{"5":{"146":2}}],["redundancy",{"5":{"132":4}}],["returns",{"5":{"132":2}}],["return",{"5":{"132":12,"133":8,"135":4,"136":4,"137":8,"156":2}}],["rel=",{"5":{"21":10,"131":51}}],["relu问题",{"5":{"148":2}}],["relu通过其分段线性的性质在正半轴保留梯度传输",{"5":{"148":2}}],["relu通过分段线性的设计实现了这一点",{"5":{"41":2}}],["relu现象",{"5":{"148":2}}],["relu引入了新的问题",{"5":{"148":2}}],["relu",{"5":{"41":3,"42":6,"47":2,"71":2,"133":2,"135":2,"148":3}}],["relu的优势在于它",{"5":{"148":2}}],["relu的导数为1",{"5":{"148":2}}],["relu的导数只有两个可能值",{"5":{"148":2}}],["relu的负区域完全抑制了信号",{"5":{"41":2}}],["relu的计算仅涉及比较和乘法",{"5":{"41":2}}],["relu的数学形式极其简单",{"5":{"42":2}}],["relu的信息选择性",{"5":{"71":2}}],["relu的稀疏激活特性可能更有价值",{"5":{"41":2}}],["relu的稀疏激活特性",{"5":{"71":2}}],["relu函数族的导数与性质",{"2":{"42":1},"5":{"42":1}}],["relu函数的定义与导数",{"2":{"42":1},"5":{"42":1}}],["relu函数",{"2":{"148":1},"5":{"42":2,"71":2,"148":1}}],["relu函数定义为",{"5":{"42":2,"148":2}}],["relu及其变体",{"5":{"42":2}}],["relu虽然数学形式简单",{"5":{"71":2}}],["relu可以被解释为一种",{"5":{"71":2}}],["relu激活",{"5":{"44":1}}],["relu激活的几何效果",{"5":{"45":2}}],["relu激活函数将空间按个坐标轴分割成个象限",{"5":{"45":1}}],["relu激活函数将",{"5":{"45":1}}],["relu网络表现为一个线性函数",{"5":{"45":2}}],["relu对每个坐标独立操作",{"5":{"45":2}}],["relu特征映射",{"5":{"86":2,"151":2}}],["relation",{"5":{"96":8}}],["relative",{"5":{"88":2,"91":2,"92":2,"153":2}}],["reverse",{"5":{"44":2,"45":2}}],["recent",{"5":{"136":6}}],["receptive",{"5":{"92":2,"153":2}}],["recon",{"5":{"133":4}}],["recomputation",{"5":{"86":2,"151":2}}],["rectified",{"5":{"42":2,"48":2}}],["recurrent",{"5":{"46":2,"92":2,"95":2,"153":2}}],["recursive",{"5":{"86":2,"151":2}}],["resnet",{"5":{"142":2,"147":2}}],["residual",{"5":{"41":2,"92":2,"147":2,"150":2,"153":2}}],["reshape",{"5":{"50":2,"93":2}}],["representation",{"5":{"69":2}}],["reparameterization",{"5":{"93":2,"145":2,"146":2}}],["rnn中dropout的位置选择",{"5":{"145":2}}],["rnn",{"5":{"41":2,"50":2,"91":2,"92":2,"95":2,"132":8,"140":1,"145":4,"146":5,"153":2}}],["rnn的隐藏状态更新为",{"5":{"41":2}}],["rnn的梯度范数增长",{"5":{"41":2}}],["rnn的复杂度只能建模",{"5":{"92":1,"153":1}}],["rnn的函数类是",{"5":{"92":2,"153":2}}],["rnn的",{"5":{"92":1,"153":1}}],["rnn为",{"5":{"92":2,"153":2}}],["rnn建模长程依赖的",{"5":{"92":2,"153":2}}],["rnn实际上需要进行步的顺序计算",{"5":{"92":1,"153":1}}],["rnn实际上需要进行",{"5":{"92":1,"153":1}}],["routing",{"5":{"157":2}}],["rope等位置编码的相对位置敏感性",{"5":{"101":2}}],["rope",{"0":{"88":1},"4":{"88":1},"5":{"70":4,"88":7,"89":2,"90":4,"91":2,"92":2,"97":2,"101":2,"131":5,"153":2}}],["rope的深层结构",{"2":{"88":1},"5":{"88":1}}],["rope的工程实现",{"2":{"88":1},"5":{"88":1}}],["rope的变体与扩展",{"2":{"88":1},"5":{"88":1}}],["rope的洞察始于一个看似简单却极其深刻的问题",{"5":{"88":2}}],["rope的位置编码是确定性的",{"5":{"88":2}}],["rope的数学形式化",{"2":{"88":1},"5":{"88":1}}],["rope的数学结构可以用群论的语言来描述",{"5":{"88":2}}],["rope的数学定义为",{"5":{"92":2,"153":2}}],["rope的第一步是构建位置索引",{"5":{"88":2}}],["rope的核心思想",{"2":{"88":1},"5":{"88":1}}],["rope的核心思想可以概括为",{"5":{"88":2}}],["rope的核心公式是",{"5":{"88":2}}],["rope的核心性质是相对位置的内积不变性",{"5":{"92":2,"153":2}}],["rope的平移等变性是严格可证的",{"5":{"88":2}}],["rope的外推能力源于其旋转结构",{"5":{"88":2}}],["rope的外推能力使得这种需求更容易满足",{"5":{"88":2}}],["rope的外推能力使其成为处理长上下文的有力工具",{"5":{"88":2}}],["rope的线性旋转结构可能无法最优地捕捉这类复杂的依赖关系",{"5":{"88":2}}],["rope的精妙之处在于它将位置编码问题转化为旋转群上的问题",{"5":{"88":2}}],["rope的实现简洁高效",{"5":{"88":2}}],["rope的旋转编码是一种相位调制",{"5":{"88":1}}],["rope的旋转编码是一种",{"5":{"88":1}}],["rope的相对位置内积结构只依赖于相对位置",{"5":{"88":1}}],["rope的相对位置内积结构",{"5":{"88":1}}],["rope的这一性质使得transformer能够自然地处理超出训练序列长度的输入",{"5":{"92":2,"153":2}}],["rope最神奇的性质是",{"5":{"88":2}}],["rope操作由",{"5":{"88":2}}],["rope公式中频率参数",{"5":{"88":2}}],["rope能够同时捕捉这两种尺度的位置依赖",{"5":{"88":2}}],["rope平移等变性",{"5":{"88":2}}],["rope将位置信息编码为相位",{"5":{"88":2}}],["rope与正弦位置编码的对比分析",{"2":{"88":1},"5":{"88":1}}],["rope与自注意力的集成有两种常见策略",{"5":{"88":2}}],["rope是一种针对长上下文外推的优化技术",{"5":{"88":2}}],["rope建议使用",{"5":{"88":2}}],["rope提出了关键性的设计",{"5":{"88":2}}],["rope提供良好的相对位置感知",{"5":{"88":2}}],["rope不仅仅是一种新的位置编码技术",{"5":{"88":2}}],["rope不需要额外的可学习参数",{"5":{"88":2}}],["rope在大语言模型中的应用",{"2":{"88":1},"5":{"88":1}}],["rope在各种语言建模任务上与或超越其他位置编码方法",{"5":{"88":2}}],["rope可以缓解这一问题",{"5":{"88":2}}],["rope代表了位置编码设计从",{"5":{"88":2}}],["rope则提出了一个根本不同的视角",{"5":{"88":2}}],["rope则建立在坚实的群论基础上",{"5":{"88":2}}],["rope及其变体为应对这些挑战提供了有力的工具",{"5":{"88":2}}],["rope中使用的群结构是的次直积",{"5":{"88":1}}],["rope中使用的群结构是",{"5":{"88":1}}],["rope采用的是乘法变换的设计哲学",{"5":{"88":1}}],["rope采用的是",{"5":{"88":1}}],["rope正是将这种几何直觉推广到高维向量空间",{"5":{"90":2}}],["root",{"5":{"41":2,"146":2}}],["ronald",{"5":{"44":2}}],["rosenblatt",{"5":{"47":2}}],["rotary",{"0":{"88":1},"4":{"88":1},"5":{"88":3,"92":2,"131":5,"153":2}}],["running",{"5":{"146":6}}],["russo",{"5":{"133":2}}],["rumelhart",{"5":{"44":2}}],["rule",{"5":{"48":2,"140":2}}],["rao积",{"5":{"143":1}}],["rao积等基本工具",{"5":{"143":2}}],["rayleigh",{"5":{"135":2}}],["ratio",{"5":{"132":2,"133":10,"136":2,"154":2}}],["rate",{"5":{"48":2,"132":8,"136":2}}],["rate控制",{"5":{"96":2}}],["ranked",{"5":{"132":4,"133":6}}],["rank",{"5":{"50":2,"87":6,"141":2,"142":8,"143":2,"152":6,"154":2}}],["random",{"5":{"86":2,"145":2,"151":2,"159":2}}],["range",{"5":{"92":2,"132":2,"153":2}}],["rademacher",{"5":{"133":2}}],["rademacher复杂度等",{"5":{"87":2,"152":2}}],["radam",{"5":{"48":2}}],["radius",{"5":{"87":2,"148":2,"152":2}}],["r为矩阵的秩",{"5":{"50":2}}],["rwkv",{"5":{"88":2}}],["^2",{"5":{"42":8,"46":2,"61":2}}],["^c",{"5":{"42":10,"61":10}}],["^t",{"5":{"42":4,"46":6}}],["^",{"5":{"46":30,"141":32,"142":20,"143":24,"144":2}}],["+=",{"5":{"132":4}}],["+",{"5":{"42":26,"46":20,"61":14,"69":2,"97":12,"101":4,"132":10,"133":10,"136":2,"137":6,"142":6,"144":22,"146":2}}],["是退火时间常数",{"5":{"159":2}}],["是初始噪声标准差",{"5":{"159":2}}],["是初始学习率",{"5":{"97":1}}],["是选中的专家索引",{"5":{"159":2}}],["是预定义的激活专家数量",{"5":{"159":2}}],["是预测误差",{"5":{"70":1}}],["是预测空间",{"5":{"51":1}}],["是每参数的浮点运算数",{"5":{"159":2}}],["是每个专家的隐藏维度",{"5":{"159":2}}],["是每个专家的原始得分",{"5":{"158":2}}],["是每个训练样本所需的训练步数",{"5":{"138":2}}],["是每个batch中样本的数量",{"5":{"69":2}}],["是稠密模型的参数量",{"5":{"159":2}}],["是比例常数",{"5":{"159":2}}],["是比传统",{"5":{"133":1}}],["是实际分配频率",{"5":{"158":2}}],["是实际应用中必须考虑的问题",{"5":{"138":1,"150":1}}],["是排序后的专家分配概率",{"5":{"158":2}}],["是否被激活的指示随机变量",{"5":{"158":2}}],["是否是最优的",{"5":{"90":2}}],["是门控权重最大的",{"5":{"159":2}}],["是门控函数",{"5":{"158":2}}],["是门控向量",{"5":{"101":2}}],["是专家维度缩减因子",{"5":{"159":2}}],["是专家",{"5":{"158":14,"159":4}}],["是专家的中间隐藏表示",{"5":{"158":2}}],["是专家网络的隐藏维度",{"5":{"158":2}}],["是流水线深度",{"5":{"157":2}}],["是设备数量",{"5":{"157":4}}],["是设计矩阵",{"5":{"51":1,"70":1,"97":1}}],["是稀疏化操作的梯度",{"5":{"157":2}}],["是稀疏注意力研究的重要方向",{"5":{"87":2,"152":2}}],["是指示函数",{"5":{"157":2,"158":4,"159":2}}],["是指学习算法对解空间的先验假设",{"5":{"92":2,"153":2}}],["是强化学习的基石",{"5":{"156":2}}],["是整个轨迹累积回报的随机采样",{"5":{"155":2}}],["是整数加法群",{"5":{"88":2}}],["是采样的轨迹数量",{"5":{"155":2}}],["是这种方法的关键技巧",{"5":{"154":1}}],["是这种中间表示的标准形式",{"5":{"45":2}}],["是价值函数损失",{"5":{"154":2}}],["是ppo",{"5":{"154":2}}],["是kl惩罚系数",{"5":{"154":2}}],["是kl散度度量",{"5":{"154":2}}],["是kl散度惩罚系数",{"5":{"154":2}}],["是kl散度区别于传统距离的关键特征",{"5":{"61":1}}],["是失败者",{"5":{"154":2}}],["是胜出者",{"5":{"154":2}}],["是调整速率参数",{"5":{"154":2}}],["是归一化技术的共同优势",{"5":{"150":1}}],["是归一化常数",{"5":{"101":1}}],["是layer",{"5":{"150":2}}],["是moe训练中的典型失效模式",{"5":{"158":2}}],["是moe架构中执行具体计算任务的功能模块",{"5":{"158":2}}],["是mhc区别于简单并联结构的关键",{"5":{"150":1}}],["是mhc设计中的一个重要考量",{"5":{"150":1}}],["是mse的",{"5":{"51":4}}],["是highway",{"5":{"150":1}}],["是残差连接的另一个数学贡献",{"5":{"150":1}}],["是残差函数的",{"5":{"135":2}}],["是残差函数的雅可比矩阵",{"5":{"48":1}}],["是尺度的数量",{"5":{"150":2}}],["是携带门",{"5":{"150":2}}],["是温度",{"5":{"149":1}}],["是温度参数",{"5":{"69":2,"71":2,"86":2,"101":2,"137":1,"151":2,"157":2,"159":2}}],["是从输入",{"5":{"159":2}}],["是从gumbel分布采样的噪声",{"5":{"157":2,"159":2}}],["是从时间步",{"5":{"156":2}}],["是从时刻",{"5":{"155":2}}],["是从",{"5":{"149":1}}],["是维纳过程",{"5":{"149":2}}],["是维度索引",{"5":{"92":2,"153":2}}],["是放大系数",{"5":{"147":1}}],["是截断函数",{"5":{"147":2}}],["是精确计算而非估计时成立",{"5":{"146":2}}],["是空间掩码",{"5":{"145":2}}],["是保留概率",{"5":{"145":3}}],["是当前批次",{"5":{"158":2,"159":2}}],["是当前主流深度学习框架",{"5":{"145":2}}],["是当前研究的热点问题",{"5":{"138":1}}],["是关于权重矩阵的损失函数",{"5":{"144":2}}],["是类别数量",{"5":{"144":2}}],["是平均分配概率",{"5":{"159":2}}],["是平均路由概率关于门控参数的梯度",{"5":{"158":2}}],["是平均路由概率",{"5":{"158":2}}],["是平滑系数",{"5":{"144":2}}],["是平移向量",{"5":{"71":2}}],["是岭回归的核心优势",{"5":{"144":2}}],["是frobenius范数",{"5":{"143":1}}],["是旋转",{"5":{"143":1}}],["是沿第",{"5":{"143":1}}],["是沿第模的投影矩阵",{"5":{"143":1}}],["是沿坐标轴的缩放",{"5":{"143":2}}],["是应用低秩近似技术的基础",{"5":{"142":2}}],["是低秩维度",{"5":{"141":2}}],["是低秩矩阵",{"5":{"87":3,"152":3}}],["是上三角掩码矩阵",{"5":{"140":1}}],["是该序列的长度",{"5":{"140":2}}],["是在状态",{"5":{"155":2}}],["是在更新过程中计算的平均kl散度",{"5":{"154":2}}],["是在给定历史上下文",{"5":{"140":1}}],["是在给定历史上下文条件下输出词元的概率分布",{"5":{"140":1}}],["是在训练过程中动态调整学习率的策略",{"5":{"48":2}}],["是紧致的",{"5":{"139":1}}],["是紧集",{"5":{"71":2}}],["是涌现能力的数学基础",{"5":{"138":1}}],["是期望的输出",{"5":{"138":2}}],["是临界参数和数据量",{"5":{"138":2}}],["是标量函数",{"5":{"147":2}}],["是标度律研究的核心应用问题",{"5":{"138":2}}],["是标度指数",{"5":{"138":2,"159":2}}],["是标准化后的特征",{"5":{"144":2}}],["是标准化的协方差",{"5":{"96":2}}],["是标准差",{"5":{"96":2}}],["是标准正态分布的cdf",{"5":{"96":1}}],["是标准多头注意力的参数共享变体",{"5":{"86":2,"151":2}}],["是标准多头注意力和多查询注意力的折中方案",{"5":{"86":2,"151":2}}],["是与范数选择相关的常数",{"5":{"148":1}}],["是与dropout保留概率",{"5":{"144":2}}],["是与",{"5":{"141":1}}],["是与任务相关的常数",{"5":{"138":2}}],["是与参数",{"5":{"61":2}}],["是均匀分布",{"5":{"137":1,"144":2}}],["是均值",{"5":{"41":1,"150":2}}],["是均值为0",{"5":{"96":2}}],["是均值参数",{"5":{"96":1}}],["是均值向量",{"5":{"96":1}}],["是变换门",{"5":{"150":2}}],["是变换函数",{"5":{"150":2}}],["是变换矩阵",{"5":{"71":1}}],["是变分推断",{"5":{"137":2}}],["是压缩系数",{"5":{"137":2}}],["是反馈增益矩阵",{"5":{"136":1}}],["是增强系统的",{"5":{"136":1}}],["是增加",{"5":{"101":1}}],["是增加似然的梯度",{"5":{"101":1}}],["是不可约损失",{"5":{"138":2,"159":2}}],["是不同的",{"5":{"137":1}}],["是不稳定的",{"5":{"136":1}}],["是不动点",{"5":{"135":2}}],["是稳定训练和促进专家多样性的关键技巧",{"5":{"159":2}}],["是稳定结点",{"5":{"136":1}}],["是稳定的极限环",{"5":{"136":1}}],["是周期",{"5":{"136":1}}],["是周期为",{"5":{"90":1}}],["是全局最小点",{"5":{"136":1}}],["是全1向量",{"5":{"47":2,"87":4,"89":1,"152":4}}],["是状态空间中的一个目标点",{"5":{"136":1}}],["是控制输入",{"5":{"136":2}}],["是系统动力学的典型特征",{"5":{"136":2}}],["是系统的更新映射函数",{"5":{"136":2}}],["是激活的专家数量",{"5":{"158":2}}],["是激活的协方差矩阵",{"5":{"135":2}}],["是激活专家数",{"5":{"157":2,"158":2}}],["是激活函数",{"5":{"48":1,"71":1,"135":2,"144":2,"145":1,"158":2,"159":2}}],["是鞍点",{"5":{"135":4}}],["是严格局部最大值",{"5":{"135":2}}],["是严格局部最小值",{"5":{"135":2}}],["是严格的凸函数",{"5":{"51":1}}],["是净输入",{"5":{"135":2}}],["是净输入矩阵",{"5":{"47":2}}],["是数值稳定项",{"5":{"134":2}}],["是数据最优策略必须考虑的因素",{"5":{"138":1}}],["是数据集大小",{"5":{"134":2}}],["是数据的真实联合分布",{"5":{"51":2}}],["是数据矩阵",{"5":{"87":1,"152":1}}],["是动量变量",{"5":{"134":2}}],["是动量系数",{"5":{"48":2,"134":2,"136":1}}],["是基于随机样本",{"5":{"134":2}}],["是优化问题的临界点",{"5":{"134":2}}],["是优势函数的估计值",{"5":{"154":2,"155":2}}],["是优势函数",{"5":{"101":2,"155":2}}],["是离散优化的核心数学形式",{"5":{"134":2}}],["是教师表示",{"5":{"133":1}}],["是表示空间与数据流形拓扑的差异度量",{"5":{"133":1}}],["是潜在变量",{"5":{"133":1}}],["是信息论中最重要的概念之一",{"5":{"137":2}}],["是信息论中的核心概念",{"5":{"96":2}}],["是信息瓶颈最优解",{"5":{"133":1}}],["是假设",{"5":{"133":1}}],["是负样本",{"5":{"137":1}}],["是负样本数量",{"5":{"133":1}}],["是负样本表示",{"5":{"133":1}}],["是负样本的数量",{"5":{"69":1,"71":1}}],["是鲁棒性系数",{"5":{"133":2}}],["是学生表示",{"5":{"133":2}}],["是学习率参数",{"5":{"136":1}}],["是学习率",{"5":{"48":2,"134":2,"136":2,"149":2,"155":2}}],["是学习到的位置编码矩阵",{"5":{"89":1}}],["是重构输出",{"5":{"133":2}}],["是重要性采样比率",{"5":{"101":2}}],["是相似度函数",{"5":{"137":2}}],["是相邻层之间的互信息",{"5":{"133":2}}],["是相应的偏置向量",{"5":{"71":2}}],["是谱半径",{"5":{"133":2}}],["是三变量互信息",{"5":{"132":1}}],["是三个核心概念",{"5":{"94":2}}],["是三个可学习的投影矩阵",{"5":{"94":2}}],["是各向异性的",{"5":{"149":1}}],["是各维度的可能取值数",{"5":{"132":1}}],["是各频率的周期",{"5":{"92":2,"153":2}}],["是时间",{"5":{"101":1}}],["是减少",{"5":{"101":1}}],["是减少似然的梯度",{"5":{"101":1}}],["是softmax的雅可比矩阵",{"5":{"157":2}}],["是sigmoid函数",{"5":{"101":2,"154":2}}],["是span的长度",{"5":{"101":2}}],["是任务权重",{"5":{"133":1}}],["是任务",{"5":{"101":2,"133":1}}],["是任意非常数的有界连续函数",{"5":{"71":1}}],["是共享参数",{"5":{"101":1}}],["是子词单元",{"5":{"101":1}}],["是被路由到专家",{"5":{"157":2}}],["是被掩码的span集合",{"5":{"101":1}}],["是被选中的样本索引",{"5":{"69":2}}],["是填充token",{"5":{"101":1}}],["是有效token",{"5":{"101":1}}],["是有效秩",{"5":{"87":1,"152":1}}],["是生成目标",{"5":{"101":1}}],["是前缀",{"5":{"101":1}}],["是来自词汇表",{"5":{"101":1}}],["是近年来提出的一种简化的对齐方法",{"5":{"101":2}}],["是近年来广泛使用的位置编码方案",{"5":{"92":2,"153":2}}],["是裁剪参数",{"5":{"101":2}}],["是裁剪阈值",{"5":{"41":1}}],["是rlhf中最常用的策略优化算法",{"5":{"101":2}}],["是rope的主要竞争者之一",{"5":{"88":2}}],["是rope的典型使用者",{"5":{"88":1}}],["是帕累托最优的",{"5":{"101":2}}],["是bert等模型采用的预训练任务",{"5":{"101":2}}],["是黎曼度量张量",{"5":{"70":1}}],["是凸的",{"5":{"70":2,"97":3}}],["是凸函数的充要条件是对于任意",{"5":{"48":1}}],["是凸函数",{"5":{"51":1,"70":2,"97":1,"136":1}}],["是凸函数吗",{"5":{"51":1}}],["是噪声的标准差",{"5":{"159":2}}],["是噪声向量",{"5":{"159":2}}],["是噪声项",{"5":{"70":2}}],["是噪声样本与正样本的比例",{"5":{"69":1}}],["是研究概率分布空间几何性质的理论框架",{"5":{"70":2}}],["是参数量",{"5":{"138":2}}],["是参数化概率族",{"5":{"137":1}}],["是参数依赖的",{"5":{"70":1,"97":2}}],["是参数向量",{"5":{"51":2}}],["是描述强化学习问题的标准数学框架",{"5":{"156":2}}],["是描述曲率的标准工具",{"5":{"70":2}}],["是描述离散随机变量概率分布的函数",{"5":{"96":2}}],["是描述损失函数曲率的标准工具",{"5":{"97":2}}],["是查询",{"5":{"69":1,"132":2}}],["是查询向量",{"5":{"69":1}}],["是infonce中的一个关键超参数",{"5":{"69":1}}],["是样本数量",{"5":{"137":1}}],["是样本",{"5":{"69":1,"157":2}}],["是样本与所有其他样本",{"5":{"69":1}}],["是样本级别的交叉熵损失",{"5":{"61":2}}],["是正确的",{"5":{"149":2}}],["是正则化函数",{"5":{"147":2}}],["是正则化项的贡献",{"5":{"144":2}}],["是正则化系数",{"5":{"133":1,"147":2}}],["是正样本对之间的相似度分数",{"5":{"69":2}}],["是正交特征向量矩阵",{"5":{"149":1}}],["是正交的",{"5":{"143":1}}],["是正交矩阵",{"5":{"45":1,"94":1,"143":5}}],["是正交投影的核心定义",{"5":{"51":2}}],["是正定的",{"5":{"86":1,"151":1}}],["是位置",{"5":{"69":1,"86":1,"89":1,"91":3,"92":1,"101":4,"151":1,"153":1,"158":2}}],["是位置的双向上下文表示",{"5":{"101":1}}],["是位置的邻域集合",{"5":{"86":1,"151":1}}],["是位置的位置query",{"5":{"91":1}}],["是位置的内容query",{"5":{"91":1}}],["是位置的内容key",{"5":{"91":1}}],["是位置的值向量",{"5":{"92":1,"153":1}}],["是位置编码矩阵",{"5":{"71":1,"89":2,"92":2,"153":2}}],["是位置编码矩阵的前行",{"5":{"89":1}}],["是位置编码矩阵的前",{"5":{"89":1}}],["是位置编码研究中的重要范式之一",{"5":{"89":2}}],["是位置编码能够有效支持位置相关建模的关键",{"5":{"90":2,"91":2}}],["是位置与内容之间的交叉项",{"5":{"91":1}}],["是位置到向量的映射",{"5":{"91":1}}],["是位置索引",{"5":{"92":2,"153":2}}],["是",{"5":{"41":3,"42":4,"45":1,"47":1,"50":3,"61":2,"69":1,"70":5,"71":1,"87":5,"88":1,"89":1,"92":3,"93":5,"96":3,"133":2,"134":4,"135":10,"136":4,"137":1,"143":3,"144":2,"146":1,"147":4,"152":5,"153":3,"158":2}}],["是防止除零的小常数",{"5":{"41":2}}],["是层归一化的一种简化变体",{"5":{"41":2}}],["是饱和阈值",{"5":{"41":2}}],["是批次大小",{"5":{"158":4}}],["是批方差",{"5":{"41":2}}],["是批均值",{"5":{"41":1}}],["是批量预测矩阵",{"5":{"44":1}}],["是批量隐藏激活矩阵",{"5":{"44":1}}],["是批量大小",{"5":{"50":1,"134":2}}],["是批处理的一个变体",{"5":{"50":2}}],["是批大小",{"5":{"50":1}}],["是另一种负载均衡的正则化方法",{"5":{"158":2}}],["是另一种重要的显存优化技术",{"5":{"157":2}}],["是另一种近似推断方法",{"5":{"96":2}}],["是另一种基于对比学习的损失函数",{"5":{"61":2}}],["是另一个旋转",{"5":{"143":2}}],["是另一个经典的激活函数",{"5":{"42":2}}],["是另一个重要的矩阵结构",{"5":{"87":2,"152":2}}],["是另一个理论考虑",{"5":{"88":1}}],["是双曲正割函数",{"5":{"42":1}}],["是隐藏状态",{"5":{"41":1}}],["是隐藏状态向量",{"5":{"41":1}}],["是隐藏层到输出层的权重矩阵",{"5":{"71":2}}],["是隐藏维度",{"5":{"50":2,"159":2}}],["是输出处的误差信号",{"5":{"157":2}}],["是输出关于参数的",{"5":{"135":2}}],["是输出层的偏置向量",{"5":{"71":2}}],["是输出权重",{"5":{"71":2}}],["是输出矩阵",{"5":{"47":2}}],["是输出",{"5":{"48":2}}],["是输出投影矩阵",{"5":{"93":2}}],["是输入激活",{"5":{"159":2}}],["是输入分布的支持域",{"5":{"41":1}}],["是输入层到隐藏层的权重矩阵",{"5":{"71":1}}],["是输入权重向量",{"5":{"71":1}}],["是输入",{"5":{"45":1,"48":2,"138":2,"158":2}}],["是输入神经元的数量",{"5":{"96":1}}],["是输入特征",{"5":{"51":1,"145":2}}],["是输入嵌入",{"5":{"87":1,"152":1}}],["是输入依赖的注意力矩阵",{"5":{"92":2,"153":2}}],["是输入序列",{"5":{"93":2}}],["是组合偏置向量",{"5":{"71":2}}],["是因为它与注意力的概率输出特性相契合",{"5":{"41":2}}],["是因为在适当条件下",{"5":{"71":2}}],["是偏置",{"5":{"71":2}}],["是偏置向量",{"5":{"48":2,"158":2,"159":2}}],["是top",{"5":{"157":2,"158":2}}],["是token嵌入矩阵",{"5":{"71":2}}],["是td误差",{"5":{"155":2}}],["是transformer中缓解梯度问题的重要技术",{"5":{"41":2}}],["是transformer中使用的标准归一化技术",{"5":{"41":2}}],["是transformer原始论文",{"5":{"91":2}}],["是transformer架构的核心组件",{"5":{"141":2}}],["是transformer架构成功的关键因素之一",{"5":{"95":2}}],["是transformer架构可训练性的关键保障",{"5":{"95":2}}],["是非负对角矩阵",{"5":{"143":2}}],["是非负行随机矩阵",{"5":{"41":1}}],["是非线性激活函数",{"5":{"71":1}}],["是非线性函数",{"5":{"71":1}}],["是第一层权重",{"5":{"159":2}}],["是第一层权重矩阵",{"5":{"158":2}}],["是第一模态的独立贡献",{"5":{"137":2}}],["是第二层权重",{"5":{"159":2}}],["是第二层权重矩阵",{"5":{"158":2}}],["是第二模态在第一模态基础上的增量信息",{"5":{"137":2}}],["是第二个因素",{"5":{"88":1}}],["是第",{"5":{"48":1,"61":1,"70":1,"71":1,"88":1,"90":1,"93":2,"96":1,"133":2,"135":4,"136":1,"137":1,"138":2,"139":2,"140":2,"146":1,"147":2,"149":2,"150":2,"155":4,"159":2}}],["是第三个因素",{"5":{"88":1}}],["是第四个因素",{"5":{"88":1}}],["是第个样本对应的损失函数",{"5":{"149":1}}],["是第个样本的真实类别",{"5":{"61":1}}],["是第个训练序列的真实词元",{"5":{"140":1}}],["是第个频率成分对应的旋转因子",{"5":{"90":1}}],["是第个头的query",{"5":{"93":1}}],["是第个头定义的",{"5":{"93":1}}],["是gumbel噪声",{"5":{"71":1}}],["是真实梯度",{"5":{"149":1}}],["是真实数据分布",{"5":{"140":1}}],["是真实的目标函数",{"5":{"70":1}}],["是真实的数据分布",{"5":{"61":2}}],["是真实标签",{"5":{"44":2}}],["是真实词元",{"5":{"96":2}}],["是真实分布而",{"5":{"96":1}}],["是网络对第",{"5":{"69":1}}],["是网络的原始输出",{"5":{"71":1}}],["是网络预测",{"5":{"44":1}}],["是缩放因子",{"5":{"44":1,"51":1}}],["是缩放后的相似度分数",{"5":{"132":1}}],["是缩放后损失的梯度",{"5":{"44":2}}],["是缩放后注意力分数矩阵",{"5":{"94":1}}],["是神经网络信息流动的基本方式",{"5":{"45":2}}],["是由门控机制分配的概率权重",{"5":{"159":2}}],["是由特征值组成的对角矩阵",{"5":{"149":2}}],["是由激活函数在各位置的导数组成的对角矩阵",{"5":{"148":1}}],["是由公式",{"5":{"146":1}}],["是由hu等人于2021年提出的一种参数高效微调方法",{"5":{"142":2}}],["是由transformer参数化的复杂函数",{"5":{"140":1}}],["是由参数",{"5":{"140":1,"159":2}}],["是由softmax定义的概率分布",{"5":{"69":1}}],["是由gutmann和hyvarinen于2010年提出的一种参数密度估计方法",{"5":{"69":2}}],["是由网络参数定义的函数映射",{"5":{"45":1}}],["是由数据本身的固有噪声决定的",{"5":{"51":2}}],["是由训练数据经验估计的",{"5":{"61":2}}],["是线性的",{"5":{"70":2,"71":1}}],["是线性变换与平移的组合",{"5":{"47":1}}],["是线性代数中最重要的研究对象之一",{"5":{"50":2}}],["是线性注意力研究的核心问题",{"5":{"87":2,"152":2}}],["是仿射变换算子",{"5":{"47":1}}],["是深度学习领域最具影响力的架构创新之一",{"5":{"159":2}}],["是深度学习成功的关键因素之一",{"5":{"144":2}}],["是深度学习中参数学习的核心方法",{"5":{"139":2}}],["是深度学习中最重要的归一化技术之一",{"5":{"150":2}}],["是深度学习中最重要的技术之一",{"5":{"96":2}}],["是深度学习中最常用的正则化技术之一",{"5":{"144":2}}],["是深度学习中最核心的算法之一",{"5":{"44":2}}],["是深度学习优化中最重要的进展之一",{"5":{"48":2}}],["是深度学习训练的标准范式",{"5":{"47":2}}],["是深度学习训练和推理的基本范式",{"5":{"50":2}}],["是深入掌握深度学习优化算法的必要基础",{"5":{"48":2}}],["是微积分中最重要的法则之一",{"5":{"48":2}}],["是一类直接针对这一问题的技术",{"5":{"147":2}}],["是一类直接作用于权重矩阵奇异值或特征值的正则化技术",{"5":{"144":2}}],["是一类通过替换softmax来降低计算复杂度的注意力变体",{"5":{"87":2,"152":2}}],["是一条直线",{"5":{"138":1}}],["是一种根据负载动态分配计算资源的策略",{"5":{"157":2}}],["是一种降低单设备显存压力的技术",{"5":{"157":2}}],["是一种专门针对moe架构设计的并行策略",{"5":{"157":2}}],["是一种不需要显式修改损失函数的正则化技术",{"5":{"144":2}}],["是一种针对分类任务的新型正则化技术",{"5":{"144":2}}],["是一种实用的互信息下界估计方法",{"5":{"137":2}}],["是一种自监督表示学习方法",{"5":{"133":2}}],["是一种验证反向传播实现正确性的数值方法",{"5":{"44":2}}],["是一种防止梯度爆炸的技术",{"5":{"44":2}}],["是一种利用计算图高效计算导数的技术",{"5":{"45":2}}],["是一种二值神经元模型",{"5":{"47":2}}],["是一种用于深度学习的hessian近似方法",{"5":{"48":2}}],["是一种通过调整权重来稳定训练的方法",{"5":{"41":2}}],["是一种通过重采样来估计统计量方差的非参数方法",{"5":{"96":2}}],["是一种非参数方法",{"5":{"96":2}}],["是一种常用的正则化技术",{"5":{"96":2}}],["是一种常用的权重归一化技术",{"5":{"50":2}}],["是一种常用的内存优化技术",{"5":{"50":2}}],["是一种重要的权重归一化技术",{"5":{"50":2}}],["是一种io感知的注意力计算算法",{"5":{"86":2,"151":2}}],["是一种典型的变体",{"5":{"93":2}}],["是一个重要的超参数",{"5":{"159":2}}],["是一个重要的数学工具",{"5":{"145":2}}],["是一个通用逼近器",{"5":{"159":2}}],["是一个参数化为",{"5":{"159":2}}],["是一个无限项级数",{"5":{"156":2}}],["是一个从状态到动作的映射",{"5":{"156":2}}],["是一个超参数",{"5":{"154":2}}],["是一个融合函数",{"5":{"150":2}}],["是一个取值在",{"5":{"150":2}}],["是一个小常数",{"5":{"150":2}}],["是一个小的扰动",{"5":{"148":2}}],["是一个小型多层感知机",{"5":{"93":2}}],["是一个不可约的常数项",{"5":{"138":2}}],["是一个关键的数学组件",{"5":{"101":1}}],["是一个的矩阵",{"5":{"48":1,"50":1}}],["是一个多元函数",{"5":{"48":1}}],["是一个与",{"5":{"48":1}}],["是一个",{"5":{"48":1,"50":2,"88":1,"95":2,"96":2,"141":1,"148":1}}],["是一个单位向量",{"5":{"48":1}}],["是一个one",{"5":{"61":1}}],["是一个可微函数",{"5":{"48":1}}],["是一个可学习的评分函数",{"5":{"86":2,"151":2}}],["是一个可学习的参数",{"5":{"88":1}}],["是一个可学习的线性变换",{"5":{"93":2}}],["是一个特征映射函数",{"5":{"86":2,"151":2}}],["是一个随机矩阵",{"5":{"87":1,"92":1,"152":1,"153":1}}],["是一个带状矩阵",{"5":{"87":1,"152":1}}],["是一个群",{"5":{"88":2}}],["是一个集合",{"5":{"88":2}}],["是一个映射",{"5":{"88":2}}],["是一个非线性函数",{"5":{"88":1}}],["是一个缩放因子",{"5":{"88":1}}],["是一个理论上的局限",{"5":{"88":1}}],["是一个实践中的挑战",{"5":{"88":1}}],["是一个复指数序列",{"5":{"90":1}}],["是一个离散序列",{"5":{"90":1}}],["是一个经验性选择的常数基准",{"5":{"91":2}}],["是一个等范数向量集合",{"5":{"91":2}}],["是一个有向图",{"5":{"92":1,"153":1}}],["是一个半负定矩阵",{"5":{"97":2}}],["是一次性建模所有位置对之间的依赖",{"5":{"92":1,"153":1}}],["是训练步数",{"5":{"159":2}}],["是训练前若干步中梯度范数的最大值",{"5":{"147":2}}],["是训练这些模型的主要挑战之一",{"5":{"41":2}}],["是训练神经网络的关键技术基础",{"5":{"48":2}}],["是训练超大规模语言模型的必要技术",{"5":{"50":2}}],["是加速梯度下降的重要技术",{"5":{"48":2}}],["是最常见的设置",{"5":{"159":2}}],["是最常用的梯度裁剪策略",{"5":{"147":2}}],["是最经典的降维方法",{"5":{"143":2}}],["是最基本的分岔类型",{"5":{"136":2}}],["是最基本也是最重要的参数估计方法",{"5":{"96":2}}],["是最优的",{"5":{"70":2}}],["是最流行的自适应优化算法",{"5":{"48":2}}],["是最简单的离散分布",{"5":{"96":2}}],["是最简单的连续分布",{"5":{"96":2}}],["是最简单的一类",{"5":{"88":2}}],["是最后一层的线性输出",{"5":{"51":2}}],["是最直接的选择",{"5":{"86":2,"151":2}}],["是最早也是最经典的策略梯度算法之一",{"5":{"155":2}}],["是最早提出且应用广泛的归一化技术",{"5":{"41":2}}],["是最早的解决方案",{"5":{"88":2}}],["是最小学习率",{"5":{"97":2}}],["是外层函数关于中间变量的导数",{"5":{"48":1}}],["是梯度的全局上界",{"5":{"147":2}}],["是梯度的协方差矩阵",{"5":{"135":2}}],["是梯度关于参数的",{"5":{"135":2}}],["是梯度噪声的协方差矩阵",{"5":{"134":2}}],["是梯度为零向量的点",{"5":{"48":2}}],["是梯度与方向",{"5":{"48":1}}],["是速度向量",{"5":{"48":1}}],["是逐元素函数",{"5":{"44":1}}],["是逐元素累积的梯度平方",{"5":{"48":1}}],["是逐元素乘方",{"5":{"86":1,"151":1}}],["是逐元素乘法",{"5":{"87":2,"132":1,"152":2}}],["是掌握语言模型概率本质的必要前提",{"5":{"96":2}}],["是n次独立伯努利试验中成功次数的分布",{"5":{"96":2}}],["是伯努利分布向多值情况的推广",{"5":{"96":2}}],["是随机噪声源",{"5":{"71":1}}],["是随机变量取值的加权平均",{"5":{"96":2}}],["是随机变量幂次期望的统称",{"5":{"96":2}}],["是随机采样的子集",{"5":{"86":1,"151":1}}],["是概率分布",{"5":{"140":1}}],["是概率单纯形中的点",{"5":{"71":1}}],["是概率单纯形",{"5":{"71":1}}],["是概率论中最重要的连续分布",{"5":{"96":2}}],["是概率论中最深刻的定理之一",{"5":{"96":2}}],["是协方差矩阵",{"5":{"96":2}}],["是协方差矩阵的行列式",{"5":{"96":2}}],["是协方差矩阵的逆矩阵",{"5":{"96":2}}],["是常数矩阵",{"5":{"70":1}}],["是常数",{"5":{"70":3,"86":1,"96":2,"97":1,"151":1}}],["是边际似然",{"5":{"96":2}}],["是成功概率",{"5":{"96":1}}],["是率参数",{"5":{"96":1}}],["是模型中的一组子网络",{"5":{"159":2}}],["是模型参数向量",{"5":{"149":1}}],["是模型参数总数",{"5":{"134":2}}],["是模型参数",{"5":{"101":1}}],["是模型对第个样本的预测值",{"5":{"51":1}}],["是模型对第",{"5":{"51":1}}],["是模型预测概率",{"5":{"139":2}}],["是模型预测的概率分布",{"5":{"144":2}}],["是模型预测的条件概率分布",{"5":{"101":2}}],["是模型预测的期望",{"5":{"51":1}}],["是模型预测分布时",{"5":{"96":1}}],["是模型输出的概率分布",{"5":{"61":2}}],["是模型的预测均值",{"5":{"51":2}}],["是模型的线性输出",{"5":{"61":2}}],["是模型的隐藏维度",{"5":{"95":1}}],["是似然函数",{"5":{"96":1}}],["是序列长度",{"5":{"50":3,"141":1,"158":2,"159":2}}],["是矩阵奇异值之和",{"5":{"144":2}}],["是矩阵的秩",{"5":{"144":2}}],["是矩阵的最大奇异值",{"5":{"50":4}}],["是矩阵分解最基本",{"5":{"143":2}}],["是矩阵概念在更高维度的延伸",{"5":{"50":2}}],["是矩阵所有列向量的线性组合构成的空间",{"5":{"50":2}}],["是矩阵所有奇异值的和",{"5":{"50":2}}],["是矩阵",{"5":{"51":1}}],["是所有可能动作的集合",{"5":{"156":2}}],["是所有可能状态的集合",{"5":{"156":2}}],["是所有被矩阵映射到零向量的输入向量构成的空间",{"5":{"50":2}}],["是所谓的",{"5":{"42":2}}],["是对应的投影矩阵",{"5":{"159":2}}],["是对应的计算量",{"5":{"159":2}}],["是对应的特征值",{"5":{"50":2}}],["是对应的特征向量",{"5":{"87":2,"152":2}}],["是对层归一化的重要改进",{"5":{"146":2}}],["是对总体均值和方差的有偏估计",{"5":{"146":2}}],["是对早期标度律研究的重要修正",{"5":{"138":1}}],["是对抗攻击扰动",{"5":{"133":1}}],["是对角近似",{"5":{"135":2}}],["是对角矩阵",{"5":{"45":2,"94":2,"135":2,"143":4}}],["是对角非奇异矩阵",{"5":{"87":1,"152":1}}],["是的线性变换",{"5":{"133":1}}],["是的伪逆",{"5":{"45":1}}],["是的特征值的平方根",{"5":{"50":1}}],["是的矩阵",{"5":{"50":1}}],["是的最大奇异值",{"5":{"87":1,"152":1}}],["是的平稳分布",{"5":{"87":1,"152":1}}],["是的截断svd",{"5":{"87":1,"152":1}}],["是的前列",{"5":{"89":1}}],["是的第行",{"5":{"92":1,"153":1}}],["是将权重矩阵按行优先顺序展平后的参数",{"5":{"144":2}}],["是将人类偏好融入模型训练的关键技术",{"5":{"101":2}}],["是将任意矩阵分解为三个矩阵乘积的强大工具",{"5":{"50":2}}],["是将矩阵转换为列向量的操作",{"5":{"50":2}}],["是连续型目标变量",{"5":{"51":2}}],["是衡量负载均衡的重要指标",{"5":{"158":2}}],["是衡量两个概率分布之间差异的重要工具",{"5":{"137":2}}],["是衡量计算复杂度的标准指标",{"5":{"45":2}}],["是衡量模型真实性能的理论上指标",{"5":{"51":2}}],["是衡量数值稳定性的关键指标",{"5":{"87":2,"152":2}}],["是我们通常在训练中实际计算的损失",{"5":{"51":2}}],["是估计量的方差",{"5":{"51":2}}],["是高斯噪声的方差",{"5":{"51":2}}],["是给定一个随机变量时另一个随机变量的熵",{"5":{"96":2}}],["是给定的最优预测",{"5":{"51":1}}],["是给定",{"5":{"51":1}}],["是机器学习中最基础",{"5":{"51":2}}],["是机器学习中最基本的权衡关系之一",{"5":{"51":2}}],["是迭代次数",{"5":{"51":2}}],["是目前深度学习中最广泛使用的激活函数",{"5":{"42":2}}],["是目标向量",{"5":{"51":1}}],["是投影矩阵",{"5":{"51":1}}],["是垂直的",{"5":{"51":1}}],["是以",{"5":{"61":2}}],["是半正定的",{"5":{"70":2,"86":1,"97":2,"151":1}}],["是半正定对称矩阵",{"5":{"45":1}}],["是半负定的",{"5":{"61":2}}],["是待估计的参数向量",{"5":{"61":2}}],["是定义损失函数和分析模型行为的重要工具",{"5":{"96":2}}],["是定义在同一概率空间上的两个离散概率分布",{"5":{"61":1}}],["是one",{"5":{"61":3}}],["是阈值",{"5":{"86":2,"151":2}}],["是频率参数",{"5":{"86":1,"151":1}}],["是特征图",{"5":{"145":2}}],["是特征值密度",{"5":{"135":2}}],["是特征值对角矩阵",{"5":{"87":2,"152":2}}],["是特征映射函数",{"5":{"132":1}}],["是特征映射对应的核函数",{"5":{"86":1,"151":1}}],["是特征维度",{"5":{"86":1,"150":2,"151":1}}],["是特征向量矩阵",{"5":{"87":2,"152":2}}],["是置换矩阵",{"5":{"87":3,"152":3}}],["是秩不超过",{"5":{"142":1}}],["是秩1矩阵",{"5":{"61":2,"87":2,"152":2}}],["是秩",{"5":{"87":2,"152":2}}],["是奇异值",{"5":{"87":2,"152":2}}],["是满秩的",{"5":{"87":2,"152":2}}],["是容忍阈值",{"5":{"87":2,"152":2}}],["是互信息",{"5":{"87":2,"152":2}}],["是权衡压缩和预测的超参数",{"5":{"137":1}}],["是权衡参数",{"5":{"87":2,"152":2}}],["是权重向量",{"5":{"145":2}}],["是权重张量",{"5":{"143":1}}],["是权重因子",{"5":{"101":2}}],["是权重矩阵",{"5":{"48":2,"135":2,"159":2}}],["是列正交矩阵",{"5":{"87":2,"152":2}}],["是分岔参数",{"5":{"136":1}}],["是分析梯度爆炸的经典模型",{"5":{"41":2}}],["是分析梯度饱和的经典案例",{"5":{"41":1}}],["是分解秩",{"5":{"87":2,"152":2}}],["是行向量还是列向量需要注意维度匹配",{"5":{"44":1}}],["是行随机矩阵",{"5":{"87":4,"152":4}}],["是度量学习",{"5":{"87":1,"152":1}}],["是自注意力且",{"5":{"87":1,"152":1}}],["是核张量",{"5":{"143":1}}],["是核方法中的核心概念",{"5":{"87":2,"152":2}}],["是核函数",{"5":{"87":1,"152":1}}],["是某层某头的注意力矩阵",{"5":{"87":1,"152":1}}],["是某个特征映射函数",{"5":{"87":2,"152":2}}],["是某个可学习的距离函数",{"5":{"92":1,"153":1}}],["是绝对位置",{"5":{"88":2}}],["是query投影和key投影的",{"5":{"87":2,"152":2}}],["是query投影和key投影的某种",{"5":{"94":1}}],["是query",{"5":{"88":2}}],["是阿贝尔群",{"5":{"88":2}}],["是虚数单位",{"5":{"88":1,"90":2}}],["是角度为",{"5":{"88":1}}],["是未知参数",{"5":{"96":1}}],["是未编码位置的原始向量",{"5":{"88":1}}],["是单位矩阵",{"5":{"50":1}}],["是单位元",{"5":{"88":1}}],["是首要因素",{"5":{"88":1}}],["是内层函数关于自变量的导数",{"5":{"48":2}}],["是内容嵌入矩阵",{"5":{"89":2}}],["是内容与位置之间的交叉项",{"5":{"91":1}}],["是注意力衰减长度",{"5":{"132":1}}],["是注意力输出",{"5":{"132":1}}],["是注意力输出关于输入的雅可比矩阵",{"5":{"89":2}}],["是注意力阈值",{"5":{"132":1}}],["是注意力头的数量",{"5":{"50":1}}],["是注意力权重矩阵",{"5":{"97":1}}],["是损失函数的一个局部极小值",{"5":{"142":1}}],["是损失函数的利普希茨常数",{"5":{"97":1}}],["是损失函数在当前参数处的梯度",{"5":{"48":2}}],["是损失函数",{"5":{"89":2,"134":2}}],["是固定的",{"5":{"61":2,"91":1}}],["是固定的常数",{"5":{"91":1}}],["是固定正弦编码",{"5":{"89":2}}],["是计算dft的高效算法",{"5":{"90":2}}],["是小的距离差",{"5":{"90":2}}],["是足够的",{"5":{"90":2}}],["是克罗内克函数",{"5":{"90":1,"159":2}}],["是克罗内克",{"5":{"90":1}}],["是带有位置信息的输入表示",{"5":{"91":2}}],["是针对解码器设计的位置编码变体",{"5":{"91":2}}],["是两个权重矩阵的乘积",{"5":{"71":1}}],["是两个随机变量的联合分布的熵",{"5":{"96":2}}],["是两种主要的编码范式",{"5":{"91":2}}],["是两种针对key和value的变体设计",{"5":{"93":2}}],["是原始门控得分",{"5":{"159":2}}],["是原始的策略价值函数",{"5":{"154":2}}],["是原始的query和key向量",{"5":{"88":1}}],["是原始损失的hessian",{"5":{"144":2}}],["是原始内容之间的注意力分数",{"5":{"91":1}}],["是纯粹的位置",{"5":{"91":1}}],["是向量空间",{"5":{"88":2}}],["是向量的第",{"5":{"92":1,"153":1}}],["是理想的",{"5":{"148":1}}],["是理想的临界状态",{"5":{"148":1}}],["是理解大语言模型能力增长规律的重要理论框架",{"5":{"159":2}}],["是理解layer",{"5":{"150":1}}],["是理解残差连接优势的关键",{"5":{"150":1}}],["是理解梯度稳定性的核心概念",{"5":{"148":2}}],["是理解深度学习中随机正则化机制的核心范式",{"5":{"145":2}}],["是理解dropout行为的核心数学原理",{"5":{"145":2}}],["是理解模型行为的重要工具",{"5":{"137":2}}],["是理解其他层类型的基础",{"5":{"46":2}}],["是理解牛顿法等二阶优化方法的基础",{"5":{"48":2}}],["是理解机器学习模型泛化误差的核心理论工具",{"5":{"51":2}}],["是理论上的最优复杂度",{"5":{"92":1,"153":1}}],["是主语",{"5":{"93":2}}],["是谓语",{"5":{"93":2}}],["是独立同分布的均匀随机变量",{"5":{"71":2}}],["是独立同分布的随机变量",{"5":{"96":2}}],["是独立的",{"5":{"93":1}}],["是卷积核大小",{"5":{"87":2,"152":2}}],["是卷积核",{"5":{"93":1}}],["是卷积矩阵",{"5":{"93":1}}],["是多项式度数",{"5":{"86":2,"151":2}}],["是多头注意力中的头数",{"5":{"94":1}}],["是方差",{"5":{"41":2,"150":2}}],["是方差参数",{"5":{"96":2}}],["是方阵还是长矩阵取决于维度的选择",{"5":{"94":1}}],["是可学习的权重矩阵",{"5":{"159":2}}],["是可学习的融合权重",{"5":{"150":2}}],["是可学习的缩放和偏移参数",{"5":{"41":1,"150":2}}],["是可学习的逐元素缩放和偏移参数",{"5":{"41":1}}],["是可逆的当且仅当其行列式",{"5":{"94":1}}],["是值向量的第行",{"5":{"95":1}}],["是值向量",{"5":{"95":1}}],["是现代深度学习框架的核心功能",{"5":{"48":2}}],["是现代深度学习",{"5":{"95":2}}],["是现代大规模语言模型能够稳定训练的重要数学基础",{"5":{"96":2}}],["是衰减率",{"5":{"134":2}}],["是衰减系数",{"5":{"48":1}}],["是衰减因子",{"5":{"97":2}}],["是衰减周期",{"5":{"97":2}}],["是函数",{"5":{"97":1}}],["是总训练步数",{"5":{"97":1}}],["是通用的",{"5":{"101":2}}],["雅可比矩阵是高度稀疏的块循环矩阵",{"5":{"148":2}}],["雅可比矩阵是一个",{"5":{"148":1}}],["雅可比矩阵",{"5":{"41":1,"44":1,"45":2,"48":2,"61":2,"92":1,"148":2,"153":1}}],["雅可比矩阵向量积",{"5":{"44":1,"45":1}}],["雅可比矩阵包含所有一阶偏导数",{"5":{"48":2}}],["雅可比矩阵的谱半径",{"5":{"148":2}}],["雅可比矩阵的谱半径也满足",{"5":{"148":1}}],["雅可比矩阵的对角元素非常小",{"5":{"41":1}}],["雅可比矩阵的结构分析",{"2":{"61":1},"5":{"61":1}}],["雅可比矩阵​是对角矩阵",{"5":{"44":1}}],["雅可比矩阵​​的范数决定了梯度在传递过程中的缩放程度",{"5":{"92":1,"153":1}}],["d次除法",{"5":{"146":2}}],["d次减法",{"5":{"146":1}}],["ddots",{"5":{"143":2}}],["dora",{"5":{"142":1}}],["donsker",{"5":{"137":6}}],["dots",{"5":{"141":2,"143":8}}],["dot",{"0":{"95":1},"4":{"95":1},"5":{"61":6,"69":4,"71":2,"86":2,"87":2,"88":2,"93":4,"94":2,"95":17,"131":5,"132":2,"151":2,"152":2}}],["dynamics",{"5":{"149":2}}],["dynamic",{"5":{"101":2,"157":2}}],["dynamical",{"5":{"50":2}}],["dpo训练的策略会改变这些注意力模式",{"5":{"101":2}}],["dpo可以被视为一种",{"5":{"101":2}}],["dpo损失关于",{"5":{"101":1}}],["dpo损失关于的梯度为",{"5":{"101":1}}],["dpo损失在形式上是二元交叉熵损失的一种变体",{"5":{"101":2}}],["dpo目标关于",{"5":{"101":1}}],["dpo目标关于的梯度与ppo目标的梯度在期望意义下是相近的",{"5":{"101":1}}],["dpo目标",{"5":{"101":2}}],["dpo的优化过程实际上是在调整策略",{"5":{"101":2}}],["dpo的一个重要理论贡献是揭示了策略比率与隐式奖励之间的联系",{"5":{"101":2}}],["dpo的梯度结构与infonce",{"5":{"101":2}}],["dpo的梯度",{"5":{"101":2}}],["dpo的梯度分析",{"2":{"101":1},"5":{"101":1}}],["dpo的最优解与rlhf的最优解是等价的",{"5":{"101":2}}],["dpo的损失函数为",{"5":{"101":2}}],["dpo的核心洞察是",{"5":{"101":2}}],["dpo的数学推导",{"2":{"101":1},"5":{"101":1}}],["dpo",{"2":{"101":1},"5":{"101":3}}],["dpo直接优化正样本相对于负样本的概率优势",{"5":{"101":2}}],["damping",{"5":{"136":2}}],["data",{"5":{"21":10,"50":2,"131":51,"138":2}}],["david",{"2":{"21":1,"131":1},"5":{"21":1,"44":2,"131":1}}],["dag",{"5":{"44":2,"45":2}}],["d",{"5":{"42":4,"69":2,"86":4,"96":4,"132":36,"133":12,"141":8,"144":2,"151":4}}],["dx",{"5":{"42":4}}],["dead",{"5":{"148":3}}],["description",{"5":{"137":2}}],["descending",{"5":{"132":4}}],["descent",{"5":{"44":2,"70":2}}],["deepmind的研究人员分析了更多的模型训练配置",{"5":{"138":2}}],["deep",{"5":{"133":4,"137":4,"146":2}}],["def",{"5":{"132":6}}],["definite",{"5":{"86":2,"151":2}}],["derivative",{"5":{"42":2,"48":2}}],["dendrites",{"5":{"47":2}}],["dense",{"5":{"46":2,"159":2}}],["density",{"5":{"96":2}}],["decreasing",{"5":{"136":2}}],["decision",{"5":{"47":2,"156":2}}],["decay",{"5":{"48":4,"50":2,"97":2}}],["decomposition",{"5":{"50":4,"51":2,"94":2,"143":2}}],["decoder",{"5":{"91":2}}],["deviation",{"5":{"96":2}}],["dependency",{"5":{"92":4,"153":4}}],["delta",{"5":{"92":2,"141":2,"142":6,"153":2}}],["difference",{"5":{"155":2}}],["differentiation",{"5":{"44":2,"45":2,"48":2}}],["diffusion",{"5":{"149":2}}],["dim",{"5":{"132":16}}],["dim=1",{"5":{"132":2}}],["dim=",{"5":{"132":4}}],["dimension",{"5":{"50":2,"149":2}}],["diversity",{"5":{"132":8}}],["divergence",{"5":{"96":2,"137":2}}],["direct",{"5":{"101":2,"132":4}}],["directional",{"5":{"48":2}}],["diag",{"5":{"42":8}}],["dismiss",{"5":{"148":4}}],["distinct",{"5":{"137":2}}],["distillation",{"5":{"87":2,"152":2}}],["distribution",{"5":{"47":2,"87":2,"96":16,"101":2,"152":2,"154":4}}],["dist",{"5":{"96":8}}],["distance",{"5":{"88":2,"96":2}}],["discount",{"5":{"156":2}}],["discovery",{"5":{"96":2}}],["discrete",{"5":{"90":2}}],["dilated",{"5":{"86":2,"151":2}}],["dropconnect",{"5":{"145":2}}],["drop",{"5":{"145":2}}],["dropping",{"5":{"133":2}}],["dropout概率的自动搜索",{"5":{"145":2}}],["dropout概率的选择",{"2":{"145":1},"5":{"145":1}}],["dropout几乎不起作用",{"5":{"145":2}}],["dropout保留概率",{"5":{"145":2}}],["dropout技术利用了dropout引入的随机性来估计预测的不确定性",{"5":{"145":2}}],["dropout只在循环连接之外应用",{"5":{"145":2}}],["dropout后的期望为",{"5":{"145":2}}],["dropout可以在batch",{"5":{"145":2}}],["dropout可以看作是训练大量共享参数的子网络",{"5":{"145":2}}],["dropout可以被解释为对神经网络进行高斯近似",{"5":{"96":2}}],["dropout与batch",{"5":{"145":2}}],["dropout与数据增强",{"5":{"145":2}}],["dropout与l2的组合通常优于单独使用其中任何一种",{"5":{"145":2}}],["dropout与l2正则化",{"5":{"145":2}}],["dropout与l2正则化的等价性是在特定假设下推导得到的",{"5":{"145":2}}],["dropout与l2正则化的等价性",{"5":{"145":2}}],["dropout与l2权重衰减之间存在深刻的数学联系",{"5":{"145":2}}],["dropout与其他正则化技术的组合",{"2":{"145":1},"5":{"145":1}}],["dropout引入的随机性在期望意义上等价于对权重施加l2正则化",{"5":{"145":2}}],["dropout引入的随机性使得每次迭代的梯度都略有不同",{"5":{"144":2}}],["dropout引入的噪声很小",{"5":{"145":2}}],["dropout引入的噪声方差达到最大值",{"5":{"145":4}}],["dropout引入的噪声强度与输入信号强度的平方成正比",{"5":{"145":2}}],["dropout引入的绝对噪声也较大",{"5":{"145":2}}],["dropout输出协方差矩阵的对角性",{"5":{"145":2}}],["dropout输出的协方差矩阵是对角矩阵",{"5":{"145":2}}],["dropout输出的协方差矩阵为",{"5":{"145":2}}],["dropout输出的方差为",{"5":{"145":2}}],["dropout输出方差的表达式",{"5":{"145":2}}],["dropout通常应用于隐藏层",{"5":{"145":2}}],["dropout通常应用在激活函数之后",{"5":{"145":2}}],["dropout通过增加模型的多样性来正则化",{"5":{"145":2}}],["dropout通过随机噪声引入隐式正则化",{"5":{"145":2}}],["dropout通过随机丢弃神经元引入噪声",{"5":{"145":2}}],["dropout通过引入一个独立的伯努利随机向量",{"5":{"145":2}}],["dropout显著改变了输出的方差分布",{"5":{"145":2}}],["dropout有两种主要的实现方式",{"5":{"145":2}}],["dropout是深度学习中最具影响力的正则化技术之一",{"5":{"145":2}}],["dropout的应用方式与全连接网络有所不同",{"5":{"145":2}}],["dropout的随机噪声和l2的正则化项都倾向于将权重推向原点附近",{"5":{"145":2}}],["dropout的期望效应在层与层之间线性传播",{"5":{"145":2}}],["dropout的期望保持性质<",{"5":{"131":1}}],["dropout的期望保持性质",{"0":{"145":1},"4":{"145":1},"5":{"131":4,"145":3}}],["dropout的线性期望传递",{"5":{"145":2}}],["dropout的核心机制可以用随机掩码",{"5":{"145":2}}],["dropout的数学本质远不止于随机丢弃",{"5":{"145":2}}],["dropout的隐式l2正则化系数为",{"5":{"144":2}}],["dropout的隐式正则化强度",{"5":{"144":2}}],["dropout的隐式正则化",{"2":{"144":1},"5":{"144":1}}],["dropout",{"2":{"145":4},"5":{"96":2,"136":2,"145":20}}],["dropout噪声等",{"5":{"96":2}}],["dropout等效于将输入乘以一个随机缩放因子",{"5":{"145":1}}],["dropout等效于将输入乘以一个随机缩放因子​",{"5":{"145":1}}],["dropout等价于对权重施加l2正则化",{"5":{"145":2}}],["dropout等价于在权重上施加l2正则化",{"5":{"145":2}}],["dropout等",{"5":{"87":2,"152":2}}],["drawio",{"5":{"44":4,"45":8,"48":4,"50":8,"96":12}}],["dct",{"5":{"87":2,"152":2}}],["dft",{"5":{"90":2}}],["dft将离散序列从时域",{"5":{"90":2}}],["中提取的先验特征",{"5":{"159":2}}],["中提出",{"5":{"145":2,"146":6}}],["中提出了infonce",{"5":{"69":2}}],["中提出的位置编码方案",{"5":{"91":2}}],["中独立同分布采样的索引",{"5":{"149":2}}],["中独立采样的",{"5":{"61":2}}],["中具有重要意义",{"5":{"146":2}}],["中等梯度消失",{"5":{"148":1}}],["中等压缩",{"5":{"148":1}}],["中等",{"5":{"145":2}}],["中应用dropout需要特别小心",{"5":{"145":2}}],["中发现",{"5":{"145":2}}],["中都得到了广泛验证",{"5":{"142":2}}],["中首次系统化阐述",{"5":{"137":2}}],["中所有临界点都是稳定的",{"5":{"136":2}}],["中期",{"5":{"134":2}}],["中期使用大学习率快速收敛",{"5":{"97":2}}],["中局部感受野的限制",{"5":{"132":2}}],["中对应方向的曲率变小",{"5":{"70":2}}],["中识别出与",{"5":{"69":1}}],["中识别出与匹配的正样本​",{"5":{"69":1}}],["中",{"5":{"41":2,"50":6,"51":3,"61":4,"70":2,"86":2,"92":2,"95":1,"97":1,"101":2,"132":5,"133":1,"134":2,"137":2,"145":3,"146":2,"151":2,"153":2,"158":2}}],["中的每一个元素都会被用于计算",{"5":{"159":2}}],["中的减法和除法操作",{"5":{"146":2}}],["中的表现形式",{"5":{"144":2}}],["中的表示",{"5":{"45":2}}],["中的单位圆",{"5":{"143":1}}],["中的历史上下文",{"5":{"140":1}}],["中的应用",{"5":{"133":2}}],["中的",{"5":{"133":2,"142":1}}],["中的互信息正则化",{"5":{"133":2}}],["中的信息流动",{"2":{"132":1},"5":{"132":1}}],["中的生成机制相契合",{"5":{"71":2}}],["中的点",{"5":{"71":1}}],["中的所有元素都是可学习的参数",{"5":{"89":1}}],["中的一条有向路径",{"5":{"92":1,"153":1}}],["中的自注意力机制引入了额外的非线性",{"5":{"97":2}}],["中的自注意力机制对学习率也有特殊要求",{"5":{"97":2}}],["中有其独特优势",{"5":{"45":2}}],["中间嵌入一个非线性激活函数",{"5":{"158":2}}],["中间激活的显存消耗是moe训练中的特殊挑战",{"5":{"157":2}}],["中间值过大或过小",{"5":{"44":1}}],["中间变量的伴随值为",{"5":{"45":1}}],["中间变量",{"5":{"45":1}}],["中最大值的指数增长占主导",{"5":{"71":2}}],["中最常用的输出激活函数",{"5":{"47":2}}],["中任意点变为",{"5":{"47":1}}],["中任意点",{"5":{"47":1}}],["中心",{"5":{"96":2,"134":1}}],["中心极限定理表明大量独立随机变量之和趋向于高斯分布",{"5":{"96":2}}],["中心极限定理",{"5":{"96":2}}],["中心极限定理的重要性在于它表明正态分布在自然界中无处不在",{"5":{"96":2}}],["中心极限定理提供了重要的洞见",{"5":{"96":2}}],["中心趋势",{"5":{"89":2}}],["中心语依赖等",{"5":{"92":2,"153":2}}],["中寻找与目标向量",{"5":{"51":1}}],["中建立了信息量的严格数学描述",{"5":{"61":2}}],["中满足",{"5":{"61":1}}],["中得到了充分的应用",{"5":{"47":2}}],["中得到更为系统的阐述",{"5":{"61":2}}],["中采样每个编码元素",{"5":{"89":1}}],["中采样",{"5":{"89":1}}],["中更为明显",{"5":{"91":2}}],["中较为平缓",{"5":{"91":2}}],["中继",{"5":{"92":2,"153":2}}],["中​​这一缩放因子是scaled",{"5":{"95":1}}],["作弊",{"5":{"154":2}}],["作用于通道维度",{"5":{"146":2}}],["作用于矩阵而非向量",{"5":{"61":2}}],["作用下循环出现",{"5":{"136":1}}],["作正交投影",{"5":{"51":2}}],["作为辅助损失加入总损失函数",{"5":{"159":2}}],["作为基线",{"5":{"155":2}}],["作为优势函数的估计值来计算策略梯度",{"5":{"155":2}}],["作为一个基于蒙特卡洛的方法",{"5":{"155":2}}],["作为一种高效",{"5":{"95":2}}],["作为critic",{"2":{"155":1},"5":{"155":1}}],["作为折中方案",{"5":{"148":2}}],["作为整体的不确定性",{"5":{"137":1}}],["作为分母",{"5":{"69":2}}],["作为近似",{"5":{"41":1}}],["作为模型参数",{"5":{"51":1}}],["作为",{"5":{"61":2,"92":1,"153":1}}],["作为query时",{"5":{"94":2}}],["作为key时",{"5":{"94":2}}],["但每次前向传播只计算其中一个",{"5":{"159":2}}],["但每个输入只动态选择其中一小部分参与计算",{"5":{"159":2}}],["但每个频率成分乘以一个相位因子",{"5":{"90":4}}],["但每个头的维度变小",{"5":{"93":2}}],["但每个头的参数数量减少",{"5":{"93":2}}],["但随着模型规模的增长",{"5":{"159":2}}],["但随机梯度",{"5":{"149":1}}],["但随机梯度的噪声分量以非零均值的随机力扰动参数",{"5":{"149":2}}],["但随机梯度并不为零",{"5":{"149":1}}],["但揭示了moe计算的核心结构",{"5":{"158":2}}],["但参数量和过拟合风险也相应增加",{"5":{"159":2}}],["但参数",{"5":{"157":2}}],["但能够提供有意义的更新方向",{"5":{"157":2}}],["但能提升表示的丰富性",{"5":{"93":2}}],["但方差显著降低",{"5":{"155":2}}],["但方差可能较大",{"5":{"155":2}}],["但方向性强",{"5":{"92":2,"153":2}}],["但需要仔细调节惩罚系数",{"5":{"154":2}}],["但需要额外的训练策略来确保长度外推的性能",{"5":{"92":2,"153":2}}],["但改进的幅度是受限的",{"5":{"154":2}}],["但改进幅度可能很小以至于在实际应用中可以忽略",{"5":{"96":2}}],["但减少幅度最多为",{"5":{"154":2}}],["但减少方差",{"5":{"144":2}}],["但增加幅度最多为",{"5":{"154":2}}],["但增加的幅度被限制在",{"5":{"154":2}}],["但增加的速度取决于各自的标度指数",{"5":{"138":2}}],["但通过合理设计各尺度的通道数和操作类型",{"5":{"150":2}}],["但通常可以忽略",{"5":{"135":2}}],["但通常不用于位置编码的分析",{"5":{"90":2}}],["但符号相反",{"5":{"149":2}}],["但平均<1",{"5":{"148":1}}],["但引入了dead",{"5":{"148":2}}],["但引入了新的梯度挑战",{"5":{"41":2}}],["但难以维持",{"5":{"148":2}}],["但下界的估计则更为复杂",{"5":{"148":2}}],["但捕捉到了梯度消失与爆炸问题的核心数学机制",{"5":{"148":2}}],["但机制不同",{"5":{"147":2}}],["但只有权重最高的",{"5":{"159":2}}],["但只计算均方根统计量",{"5":{"146":2}}],["但只保留缩放参数",{"5":{"146":2}}],["但只对目标token计算损失",{"5":{"101":2}}],["但整体分布被标准化",{"5":{"146":2}}],["但消除了个体间",{"5":{"146":2}}],["但批归一化在处理不同位置的归一化时也存在问题",{"5":{"146":2}}],["但保持通道维度不变",{"5":{"145":2}}],["但dropout对输出方差的影响同样重要",{"5":{"145":2}}],["但张量秩的np困难性给实际计算带来挑战",{"5":{"143":2}}],["但精确的低秩分解可能包含大量接近零的奇异值",{"5":{"143":2}}],["但更重要的是",{"5":{"141":2}}],["但受限于马尔可夫假设",{"5":{"140":2}}],["但牺牲了模型的表达能力",{"5":{"140":2}}],["但准确预测某个任务在什么规模下会涌现仍然困难",{"5":{"138":2}}],["但数学分析的基本框架是类似的",{"5":{"148":2}}],["但数据质量同样至关重要",{"5":{"138":2}}],["但数值上更加稳定",{"5":{"61":2}}],["但使用了4倍的数据量",{"5":{"138":2}}],["但使用各自独立的投影矩阵",{"5":{"93":2}}],["但对任意",{"5":{"136":1}}],["但信息容量可能受限",{"5":{"132":2}}],["但计算复杂度",{"5":{"132":1}}],["但计算开销相同",{"5":{"89":2}}],["但提供了良好的初始化",{"5":{"143":2}}],["但提供不同尺度的位置分辨率",{"5":{"90":2}}],["但提示本身不需要被",{"5":{"101":2}}],["但有时在推理时也显式地使用dropout进行多次前向传播",{"5":{"145":2}}],["但有助于逃离局部最小值",{"5":{"134":2}}],["但有效学习率受批量大小影响",{"5":{"134":2}}],["但有不同的任务特定输出头",{"5":{"70":2}}],["但有一个异常样本的误差为10",{"5":{"51":2}}],["但如果批次大小增加到8",{"5":{"157":2}}],["但如果损失函数具有特定的黎曼几何结构",{"5":{"70":2}}],["但如何选择最优的噪声分布是一个开放问题",{"5":{"69":2}}],["但mse作为独立的损失函数",{"5":{"70":2}}],["但强凸性决定了优化的收敛速度",{"5":{"70":2}}],["但是在自注意力中",{"5":{"69":2}}],["但它为许多动态规划和强化学习算法",{"5":{"156":2}}],["但它对损失曲面的几何结构有显著影响",{"5":{"144":2}}],["但它不是凸优化问题",{"5":{"144":2}}],["但它保持了行向量的某些线性相关性质",{"5":{"141":2}}],["但它引入了一个重要的限制性假设",{"5":{"140":2}}],["但它满足所谓的",{"5":{"137":2}}],["但它存在一些已知的局限性",{"5":{"101":2}}],["但它最终会反映在模型的注意力模式中",{"5":{"101":2}}],["但它已经具备了一定的逻辑推理能力",{"5":{"47":2}}],["但它在理解激活函数导数性质方面具有重要的教学价值",{"5":{"42":2}}],["但它在信息论和统计学中具有不可替代的重要性",{"5":{"61":2}}],["但它们正是将矩阵分解的直觉推广到高维空间的桥梁",{"5":{"143":2}}],["但它们不依赖于马尔可夫假设",{"5":{"140":2}}],["但它们在优化梯度上具有相似的简洁形式",{"5":{"70":2}}],["但它们在更深层次上具有内在的联系",{"5":{"70":2}}],["但它们关于模型参数的梯度都具有类似的形式",{"5":{"70":2}}],["但它们的作用机制不同",{"5":{"145":2}}],["但它们的数学定义源自完全不同的理论框架",{"5":{"70":2}}],["但它们的比值是合理的",{"5":{"61":2}}],["但它们都保持着用",{"5":{"42":2}}],["但它并非完美无缺",{"5":{"88":2}}],["但它展示了多层注意力如何通过",{"5":{"92":2,"153":2}}],["但它清晰地展示了多头注意力的并行结构",{"5":{"93":2}}],["但它是通过直接对融合后的损失函数求导得到的",{"5":{"61":2}}],["但它是在训练过程中从数据中学习的",{"5":{"93":2}}],["但其数学分析更加复杂",{"5":{"145":2}}],["但其奇异值快速衰减",{"5":{"143":2}}],["但其softmax结构与信息论框架高度兼容",{"5":{"69":2}}],["但其饱和程度较轻",{"5":{"41":2}}],["但其表达能力受到根本性的限制",{"5":{"71":2}}],["但其分段线性的性质使得它在大规模网络中表现优异",{"5":{"71":2}}],["但其核心洞见对连续激活函数同样适用",{"5":{"46":2}}],["但其梯度的数学结构与上述推导高度相似",{"5":{"61":2}}],["但其列空间和行空间都位于的子空间中",{"5":{"87":1,"152":1}}],["但其列空间和行空间都位于",{"5":{"87":1,"152":1}}],["但其有效自由度不超过​",{"5":{"89":1}}],["但其有效自由度不超过",{"5":{"89":1}}],["但其总参数量与单头注意力完全相同",{"5":{"93":2}}],["但其hessian依赖于预测概率",{"5":{"97":2}}],["但按值裁剪可能改变梯度的方向",{"5":{"41":2,"97":2}}],["但现代硬件和库优化使得其计算开销在可接受范围内",{"5":{"41":2}}],["但范数被限制在以下",{"5":{"41":1}}],["但范数被限制在",{"5":{"41":1}}],["但由于激活两个专家",{"5":{"159":2}}],["但由于",{"5":{"158":2}}],["但由于其输出范围是",{"5":{"41":2}}],["但由于tanh的导数峰值是1而sigmoid只有0",{"5":{"42":2}}],["但由于广播可能在语法上合法而在语义上错误",{"5":{"50":2}}],["但由于softmax是单调变换且保持行和为1",{"5":{"87":2,"152":2}}],["但由于数值精度和softmax的平滑效应",{"5":{"87":2,"152":2}}],["但由于输入嵌入的各维度之间存在相关性",{"5":{"93":1}}],["但由于输入嵌入",{"5":{"93":1}}],["但近年来relu的流行表明",{"5":{"71":2}}],["但softmax激活函数引入了关键的非线性",{"5":{"71":2}}],["但softmax操作本质上是另一种形式的非线性激活",{"5":{"71":2}}],["但深入理解前向传播的数学本质",{"5":{"45":2}}],["但网络在局部区域",{"5":{"45":2}}],["但不能完全替代实际的模型实验",{"5":{"138":2}}],["但不存在上述线性收敛条件中的",{"5":{"136":1}}],["但不参与损失计算",{"5":{"101":2}}],["但不同激活函数的逼近效率是不同的",{"5":{"71":2}}],["但不要求保持原点不变",{"5":{"47":2}}],["但不满足交换律",{"5":{"50":2}}],["但不直接",{"5":{"91":2}}],["但最近的研究表明",{"5":{"48":2}}],["但在moe中",{"5":{"157":2}}],["但在实际执行中",{"5":{"154":2}}],["但在实际应用中仍存在局限性",{"5":{"133":2}}],["但在实践中",{"5":{"146":2}}],["但在推理阶段",{"5":{"146":2}}],["但在语言模型中更常用的是分类分布",{"5":{"96":2}}],["但在处理序列数据的某些场景下",{"5":{"50":2}}],["但在深度学习中的直接应用较少",{"5":{"96":2}}],["但在深度学习中使用较少",{"5":{"50":2}}],["但在序列较短时仍能提供有效的位置区分",{"5":{"88":2}}],["但在非常长的序列",{"5":{"88":2}}],["但在高层可能升高",{"5":{"92":2,"153":2}}],["但在某些边缘情况下",{"5":{"146":2}}],["但在某些场景",{"5":{"45":2}}],["但在某些配置下",{"5":{"93":2}}],["但在测试性能上可能有显著差异",{"5":{"97":2}}],["但矩阵求逆的数学思想",{"5":{"50":2}}],["但又足够高以捕获丰富的语义信息",{"5":{"50":2}}],["但又有所不同",{"5":{"50":2}}],["但奇异值总是非负的",{"5":{"50":2}}],["但这个下界可能非常松散",{"5":{"101":2}}],["但这个拉力被概率",{"5":{"69":1}}],["但这个拉力被概率加权",{"5":{"69":1}}],["但这个估计本身存在方差",{"5":{"51":2}}],["但这是实现高效梯度计算的必要代价",{"5":{"44":2}}],["但这种并行计算的抽象表示对于理解moe的数学结构仍然很有价值",{"5":{"158":2}}],["但这种并行化的代价是位置信息的丢失",{"5":{"91":2}}],["但这种关系在实际的注意力计算中被内容的干扰所淹没",{"5":{"88":2}}],["但这种添加是生硬的",{"5":{"88":2}}],["但这种理解有助于我们分析模型的学习动态和设计改进方案",{"5":{"95":2}}],["但这里因为每行和为1",{"5":{"87":2,"152":2}}],["但这里的区别在于依赖关系的类型",{"5":{"92":2,"153":2}}],["但这并不意味着信息没有传递",{"5":{"92":2,"153":2}}],["但这些参数的设计遵循着精心的配置",{"5":{"93":2}}],["但与其他损失函数之间存在数学上的联系",{"5":{"51":2}}],["但非零",{"5":{"61":2}}],["但非零元素数量减少",{"5":{"87":2,"152":2}}],["但可以从信息论和函数逼近的角度理解",{"5":{"45":2}}],["但可以完全并行化",{"5":{"92":2,"153":2}}],["但可能减慢收敛速度或在某些情况下导致次优收敛",{"5":{"147":2}}],["但可能无法适应所有任务的需求",{"5":{"86":2,"151":2}}],["但可能不是所有任务的最优选择",{"5":{"101":2}}],["但表达能力可能受限",{"5":{"41":2}}],["但表达能力受限",{"5":{"86":2,"151":2}}],["但某些语言现象确实依赖于绝对位置",{"5":{"88":2}}],["但模型自发地获得了相对位置感知能力",{"5":{"88":2}}],["但也意味着推理时需要计算两个专家",{"5":{"159":2}}],["但也可能导致专家分配的不连续性",{"5":{"159":2}}],["但也可能结合其他自监督任务",{"5":{"101":2}}],["但也可以使用其他范数",{"5":{"147":2}}],["但也存在明显的数学和实践局限性",{"5":{"146":2}}],["但也有理论上界",{"5":{"89":2}}],["但也增加了模型的复杂性和参数数量",{"5":{"88":2}}],["但也增加参数量和过拟合风险",{"5":{"89":2}}],["但也为自适应优化算法提供了更多的曲率信息",{"5":{"97":2}}],["但rope将其置于乘法",{"5":{"88":2}}],["但rope的相对位置",{"5":{"88":2}}],["但缺乏严格的理论保证",{"5":{"88":2}}],["但仍需要保持某些计算的精度",{"5":{"48":2}}],["但仍有广阔的改进空间",{"5":{"88":2}}],["但会在处理更长序列时被激活",{"5":{"89":2}}],["但fft的数学框架为理解位置编码的频谱性质提供了重要的理论基础",{"5":{"90":2}}],["但周期非常大",{"5":{"90":2}}],["但高频成分的相位差可能达到或超过",{"5":{"91":2}}],["但我们可以利用三角恒等式来优化",{"5":{"88":2}}],["但我们仍然决定去爬山",{"5":{"92":2,"153":2}}],["但",{"5":{"92":2,"97":1,"132":1,"134":4,"137":2,"153":2,"158":2}}],["但感受野的扩展是渐进的",{"5":{"92":2,"153":2}}],["但注意力的优势在于",{"5":{"92":2,"153":2}}],["但从信息流的角度",{"5":{"92":2,"153":2}}],["但意义截然不同",{"5":{"92":2,"153":2}}],["但当参数量达到",{"5":{"138":2}}],["但当模型规模超过某个临界点时突然出现",{"5":{"138":2}}],["但当模型规模超过某个临界点或训练不充分时",{"5":{"138":2}}],["但当这种极端分布与后续的线性变换结合时",{"5":{"41":2}}],["但当依赖关系跨越整个序列",{"5":{"92":2,"153":2}}],["但实际计算量仅为",{"5":{"159":2}}],["但实际参与计算的专家是概率最高的",{"5":{"158":2}}],["但实际输出质量很低",{"5":{"154":2}}],["但实际的神经网络实现必须在实数域中进行",{"5":{"88":2}}],["但实际效果因任务而异",{"5":{"92":2,"153":2}}],["但实际上图像的信息量远低于这个维度",{"5":{"143":2}}],["但实际上",{"5":{"93":2}}],["但总参数量与单头注意力",{"5":{"93":2}}],["但存在一些函数可以用多头注意力表示",{"5":{"93":2}}],["但并不能保证方差在可控范围内",{"5":{"154":2}}],["但并没有告诉我们如何调整参数以改善输出",{"5":{"44":2}}],["但并行计算的",{"5":{"93":2}}],["但kaiming初始化在实践中也被广泛使用",{"5":{"94":2}}],["但同时也可能改变最终收敛点的性质",{"5":{"149":2}}],["但同时也带来了新的挑战",{"5":{"89":2}}],["但同时也意味着",{"5":{"92":2,"153":2}}],["但同时",{"5":{"93":2}}],["但同样捕获了输入数据的重要结构",{"5":{"94":2}}],["但既不是局部极小值也不是局部极大值",{"5":{"97":1}}],["但正弦余弦编码因其简洁性和良好的外推能力仍被广泛使用",{"5":{"91":2}}],["但正则化效应使得最终解倾向于简单",{"5":{"97":2}}],["正在处理的样本数",{"5":{"157":2}}],["正奖励表示鼓励",{"5":{"156":2}}],["正迁移",{"5":{"133":2}}],["正负迁移的信息论条件",{"5":{"133":2}}],["正负样本对比机制的几何解释",{"2":{"69":1},"5":{"69":1}}],["正确的模型族",{"5":{"140":2}}],["正确选项梯度减去错误选项梯度",{"5":{"101":2}}],["正确答案",{"5":{"96":2,"156":2}}],["正规方程解",{"5":{"144":2}}],["正规方程",{"5":{"70":2}}],["正样本相似度",{"5":{"137":2}}],["正样本损失",{"5":{"137":2}}],["正样本评分",{"5":{"133":2}}],["正样本",{"5":{"69":5,"101":2,"137":2}}],["正样本位于附近",{"5":{"69":1}}],["正样本键",{"5":{"69":2}}],["正样本的编码",{"5":{"69":2}}],["正样本对",{"5":{"69":2,"133":2,"137":4}}],["正则化从不同的角度影响优化",{"5":{"147":2}}],["正则化产生协同效应",{"5":{"147":2}}],["正则化修改目标函数本身",{"5":{"147":2}}],["正则化直接惩罚大的参数范数",{"5":{"147":2}}],["正则化和梯度裁剪经常同时使用",{"5":{"147":2}}],["正则化倾向于产生稀疏解",{"5":{"147":2}}],["正则化是控制模型复杂度",{"5":{"147":2}}],["正则化是深度学习中最核心的技术之一",{"5":{"144":2}}],["正则化强度",{"5":{"145":1}}],["正则化类型",{"5":{"144":1}}],["正则化通过添加二次项",{"5":{"147":2}}],["正则化通过在损失函数中添加惩罚项来修改优化景观",{"5":{"147":2}}],["正则化通过在损失函数中添加惩罚项来改变损失曲面的几何形状",{"5":{"144":2}}],["正则化通过变形等高线",{"5":{"144":2}}],["正则化后的损失函数为",{"5":{"147":2}}],["正则化后",{"5":{"144":2,"147":2}}],["正则化对hessian的影响",{"5":{"144":2}}],["正则化对梯度与损失的影响<",{"5":{"131":1}}],["正则化对梯度与损失的影响",{"0":{"144":1},"4":{"144":1},"5":{"131":4,"144":1}}],["正则化不仅改变极小值的位置",{"5":{"144":2}}],["正则化不仅改变等高线的形状",{"5":{"144":2}}],["正则化的收敛性质",{"5":{"147":2}}],["正则化的数学基础",{"2":{"147":1},"5":{"147":1}}],["正则化的等价性",{"2":{"145":1},"5":{"145":1}}],["正则化的作用远不止于防止过拟合",{"5":{"144":2}}],["正则化的隐式梯度效应",{"2":{"144":1},"5":{"144":1}}],["正则化系数",{"5":{"133":2,"144":2}}],["正则化项在总梯度中的相对贡献变小",{"5":{"144":2}}],["正则化项的相对贡献为",{"5":{"144":2}}],["正则化项占主导",{"5":{"133":2}}],["正则化项",{"5":{"133":2,"144":2}}],["正则化与裁剪的协同效应",{"2":{"147":1},"5":{"147":1}}],["正则化与损失曲面的几何变化",{"2":{"144":1},"5":{"144":1}}],["正则化与高效推理",{"5":{"143":2}}],["正则化与优化算法的相互作用",{"2":{"144":1},"5":{"144":1}}],["正则化与优化",{"5":{"132":2}}],["正则化与信息瓶颈<",{"5":{"131":1}}],["正则化与信息瓶颈",{"0":{"133":1},"4":{"133":1},"5":{"131":4,"133":1}}],["正则化与归一化数学",{"2":{"131":1},"4":{"144":1,"145":1,"146":1},"5":{"131":7}}],["正则化",{"5":{"50":2,"133":2,"144":2,"147":6}}],["正则化等价于在每次更新时按比例缩小参数",{"5":{"147":2}}],["正则化等",{"5":{"61":2}}],["正则化效果较强",{"5":{"145":2}}],["正则化效果较弱",{"5":{"145":2}}],["正则化效果减弱",{"5":{"145":2}}],["正则化效果分析",{"5":{"133":2}}],["正则化效果",{"5":{"61":2}}],["正则化效应减弱",{"5":{"144":2}}],["正则化效应",{"5":{"97":2}}],["正是因为它能够改变传统的缩放关系",{"5":{"159":2}}],["正是因为它对任何矩阵都成立",{"5":{"50":2}}],["正是我们需要定义的",{"5":{"149":1}}],["正是我们需要定义的梯度协方差矩阵",{"5":{"149":1}}],["正是交叉熵损失",{"5":{"101":1}}],["正是这种简单性带来了卓越的计算效率和优异的性能",{"5":{"42":2}}],["正是这些保持的性质限制了线性映射的表达能力",{"5":{"71":2}}],["正是这些代数性质保证了平移等变性的成立",{"5":{"88":2}}],["正是这个函数的傅里叶基函数",{"5":{"90":1}}],["正是非线性激活函数赋予了网络这种强大的表达能力",{"5":{"71":2}}],["正是衡量两个概率分布",{"5":{"71":1}}],["正是为这一目的而设计的度量工具",{"5":{"61":2}}],["正是为了实现这一约束而设计的",{"5":{"95":2}}],["正弦余弦编码的函数空间是固定的",{"5":{"89":4}}],["正弦余弦编码的固定结构提供了一种隐式正则化",{"5":{"89":2}}],["正弦余弦编码的频率分解特性意味着不同频率的成分携带不同尺度的位置信息",{"5":{"71":2}}],["正弦余弦编码的频谱结构",{"2":{"90":1},"5":{"90":1}}],["正弦余弦编码的参数选择是通过实验调优确定的",{"5":{"90":2}}],["正弦余弦编码通常表现更好",{"5":{"89":2}}],["正弦余弦编码与其他编码的比较",{"2":{"91":1},"5":{"91":1}}],["正弦余弦编码是确定性的",{"5":{"89":2,"91":2}}],["正弦余弦编码是一种绝对位置编码",{"5":{"91":2}}],["正弦余弦位置编码的精妙之处在于其背后的频率空间",{"5":{"90":2}}],["正弦余弦位置编码的频率选择",{"5":{"90":2}}],["正弦余弦位置编码的数学定义与推导",{"0":{"91":1},"4":{"91":1},"5":{"91":1,"131":4}}],["正弦余弦位置编码的数学定义与推导<",{"5":{"131":1}}],["正弦余弦位置编码的数学定义",{"2":{"91":1},"5":{"91":1}}],["正弦余弦位置编码的高维空间结构可以通过降维可视化来理解",{"5":{"91":2}}],["正弦余弦位置编码的核心思想是为序列中的每个位置生成一个维的编码向量",{"5":{"91":1}}],["正弦余弦位置编码的核心思想是为序列中的每个位置",{"5":{"91":1}}],["正弦余弦位置编码实际上是在频率空间中构造位置信息的表示",{"5":{"90":2}}],["正弦余弦位置编码可以视为在频域中",{"5":{"90":2}}],["正弦余弦位置编码可以自然地视为不同频率正弦波的叠加",{"5":{"90":2}}],["正弦余弦位置编码采用指数分布的频率",{"5":{"90":2}}],["正弦余弦位置编码",{"5":{"91":2}}],["正弦余弦位置编码中的各个参数都经过精心设计",{"5":{"91":2}}],["正弦余弦位置编码在高维空间中形成特定的几何结构",{"5":{"91":2}}],["正弦余弦位置编码与可学习位置编码",{"5":{"91":2}}],["正弦余弦位置编码以其简洁的数学形式",{"5":{"91":2}}],["正弦位置编码",{"5":{"132":1}}],["正弦位置编码存在根本性的数学缺陷",{"5":{"88":2}}],["正弦位置编码也采用了类似的思想",{"5":{"88":2}}],["正弦位置编码在理论上可以外推到任意长度",{"5":{"88":2}}],["正弦位置编码和rope代表了两种截然不同的位置编码范式",{"5":{"88":2}}],["正弦位置编码和可学习的绝对位置嵌入都属于这一类别",{"5":{"88":2}}],["正弦位置编码虽然简单",{"5":{"88":2}}],["正弦位置编码将位置编码为幅度",{"5":{"88":1}}],["正弦位置编码将位置编码为",{"5":{"88":1}}],["正弦位置编码采用的是加性注入的设计哲学",{"5":{"88":1}}],["正弦位置编码采用的是",{"5":{"88":1}}],["正弦位置编码的信息特性体现了三角函数编码的优势",{"5":{"132":1}}],["正弦位置编码的信息容量",{"5":{"132":2}}],["正弦位置编码的数学性质",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["正弦位置编码的一个重要数学性质是",{"5":{"92":2,"153":2}}],["正弦编码天然支持长度外推",{"5":{"89":2}}],["正弦编码在推理时无需额外计算",{"5":{"89":2}}],["正弦编码提供了",{"5":{"89":2}}],["正弦编码的函数空间具有明确的结构",{"5":{"89":2}}],["正弦编码的优势在于可以处理任意长度的序列",{"5":{"91":2}}],["正弦编码没有可训练参数",{"5":{"91":2}}],["正弦编码隐含了",{"5":{"91":2}}],["正弦编码和可学习编码在不同任务和场景下各有优势",{"5":{"91":2}}],["正弦编码或其变体",{"5":{"91":2}}],["正弦与余弦成对出现",{"5":{"91":2}}],["正弦和余弦函数的取值范围是",{"5":{"71":2}}],["正弦和余弦构成了一组正交基",{"5":{"91":2}}],["正弦和余弦的组合使得编码具有",{"5":{"91":2}}],["正弦和余弦的值必然不同",{"5":{"91":2}}],["正弦和余弦曲线随位置变化剧烈",{"5":{"91":2}}],["正弦和余弦是相位相差的同一频率振动",{"5":{"91":1}}],["正弦和余弦是相位相差",{"5":{"91":1}}],["正弦函数和余弦函数在区间上是单射的",{"5":{"91":1}}],["正弦函数和余弦函数在区间",{"5":{"91":1}}],["正如我们在4",{"5":{"46":2}}],["正交",{"5":{"48":2,"51":1,"90":1}}],["正交性",{"5":{"143":2}}],["正交性与正交矩阵",{"2":{"50":1},"5":{"50":1}}],["正交性是线性代数中一个核心概念",{"5":{"50":2}}],["正交性有着多方面的应用",{"5":{"50":2}}],["正交矩阵有几个重要的性质",{"5":{"50":2}}],["正交矩阵保持向量的范数不变",{"5":{"50":2}}],["正交矩阵的逆矩阵等于其转置矩阵",{"5":{"50":2}}],["正交矩阵的行列式的绝对值为1",{"5":{"50":2}}],["正交矩阵是满足的方阵",{"5":{"50":1}}],["正交矩阵是满足",{"5":{"50":1}}],["正交初始化是一种常用的权重初始化方法",{"5":{"50":2}}],["正交初始化将权重矩阵初始化为正交矩阵",{"5":{"50":2}}],["正交约束在优化中被用于防止权重矩阵的秩退化",{"5":{"50":2}}],["正交变换仍然是有价值的分析工具",{"5":{"50":2}}],["正交投影给出了从目标向量到预测空间的最短距离",{"5":{"51":2}}],["正交投影是唯一的",{"5":{"51":2}}],["正交投影是一个线性算子",{"5":{"51":2}}],["正交投影的几何意义可以从以下几个方面理解",{"5":{"51":2}}],["正交投影的另一个重要几何性质是误差分解",{"5":{"51":2}}],["正交投影与最优预测",{"2":{"51":1},"5":{"51":1}}],["正交投影与投影矩阵的初始化",{"2":{"94":1},"5":{"94":1}}],["正面或反面",{"5":{"96":2}}],["正向鼓励覆盖的所有模式",{"5":{"96":1}}],["正向",{"5":{"96":1}}],["正数很大或负数很负",{"5":{"61":2}}],["正值性",{"5":{"86":2,"151":2}}],["正定性与曲率",{"2":{"135":1},"5":{"135":1}}],["正定性约束限制了可表示的注意力模式",{"5":{"86":2,"151":2}}],["正定hessian",{"5":{"51":2}}],["正定",{"5":{"86":2,"134":2,"135":4,"151":2}}],["正定核只能表示",{"5":{"86":2,"151":2}}],["正定核函数对应于特征函数展开",{"5":{"87":2,"152":2}}],["正定gram矩阵",{"5":{"89":2}}],["正因如此",{"5":{"88":2}}],["正态初始化和零初始化",{"5":{"89":2}}],["正态初始化",{"5":{"89":2}}],["并引入一个路由函数",{"5":{"159":2}}],["并掌握相应的优化策略",{"5":{"157":2}}],["并转移到",{"5":{"156":2}}],["并给智能体一个即时奖励",{"5":{"156":2}}],["并据此更新策略参数",{"5":{"155":2}}],["并据此设计更有效的学习算法",{"5":{"133":2}}],["并没有告诉我们",{"5":{"154":2}}],["并非静态不变",{"5":{"149":2}}],["并非所有权重组合都能产生帕累托最优解",{"5":{"70":2}}],["并推导了其与批量大小的缩放关系",{"5":{"149":2}}],["并不为零",{"5":{"149":1}}],["并不总是成立",{"5":{"148":2}}],["并不能获得线性的性能提升",{"5":{"138":2}}],["并详细推导了批归一化",{"5":{"146":2}}],["并揭示了方差与保留概率之间的函数关系",{"5":{"145":2}}],["并区分了有放缩版本和无放缩版本两种实现方式",{"5":{"145":2}}],["并通过流水线重叠计算和通信",{"5":{"159":2}}],["并通过梯度下降来最小化这个方差",{"5":{"158":2}}],["并通过变分推断来学习",{"5":{"145":2}}],["并通过实证研究描述了奇异值的快速衰减模式",{"5":{"87":2,"152":2}}],["并通过实证研究的发现",{"5":{"93":2}}],["并探讨激活函数",{"5":{"148":2}}],["并探讨这一性质如何与其他正则化机制相互作用",{"5":{"145":2}}],["并探讨其在矩阵运算中的几何意义",{"5":{"95":2}}],["并最终提升模型的泛化能力",{"5":{"144":2}}],["并展示从简单的n",{"5":{"140":2}}],["并证明其满足递推关系",{"5":{"136":1}}],["并利用凸性条件",{"5":{"136":2}}],["并利用贝叶斯公式",{"5":{"133":1}}],["并利用贝叶斯公式进行重组",{"5":{"133":1}}],["并在推理时切换到硬top",{"5":{"159":2}}],["并在推理时对这些子网络的预测进行平均",{"5":{"145":2}}],["并在必要时使用",{"5":{"136":2}}],["并在理论指导下进行针对性的改进",{"5":{"135":2}}],["并在整个训练过程中保持固定",{"5":{"91":2}}],["并指导有效的训练策略设计",{"5":{"135":2}}],["并指导注意力机制的优化",{"5":{"132":2}}],["并收敛到损失函数的临界点集",{"5":{"134":2}}],["并将其乘以该状态",{"5":{"155":2}}],["并将标准化后的输出进行仿射变换",{"5":{"132":2}}],["并将计算速度提高近一倍",{"5":{"50":2}}],["并且对超参数的选择更加鲁棒",{"5":{"150":2}}],["并且具有以下性质",{"5":{"70":2}}],["并且计算成本更低",{"5":{"48":2}}],["并且其结构与输入数据的内在维度密切相关",{"5":{"87":2,"152":2}}],["并且与后续的内积运算具有良好的兼容性",{"5":{"88":2}}],["并归一化后作为加权系数",{"5":{"69":2}}],["并与反向传播计算的梯度进行比较",{"5":{"44":2}}],["并化简",{"5":{"44":1}}],["并按与前向传播相同的顺序计算",{"5":{"45":2}}],["并按与前向传播相反的顺序传播这些梯度",{"5":{"45":2}}],["并为理解网络的表达能力和计算特性提供了理论工具",{"5":{"45":2}}],["并为反向传播的梯度计算提供数据结构支持",{"5":{"45":2}}],["并行策略",{"5":{"157":1}}],["并行策略和显存优化",{"5":{"157":2}}],["并行性的提升",{"5":{"140":2}}],["并行层次",{"5":{"45":1}}],["并行粒度",{"5":{"45":1}}],["并行计算",{"5":{"140":1}}],["并行计算与硬件加速",{"2":{"93":1},"5":{"93":1}}],["并行计算特性",{"5":{"93":2}}],["并各自产生输出​",{"5":{"46":1}}],["并各自产生输出",{"5":{"46":1}}],["并相应地调整学习率",{"5":{"48":2}}],["并有能力实现自己的深度学习框架或深入理解现有框架的底层机制",{"5":{"44":2}}],["并有效缓解梯度消失问题",{"5":{"50":2}}],["并讨论了mse与其他损失函数",{"5":{"51":2}}],["并分析反向传播的计算复杂度",{"5":{"44":2}}],["并分析了偏差",{"5":{"51":2}}],["并分析了谱半径与信息传播收敛性的关系",{"5":{"87":2,"152":2}}],["并分析这一能力的理论极限",{"5":{"92":2,"153":2}}],["并深入探讨其与hessian矩阵的联系以及对优化过程的深刻影响",{"5":{"149":2}}],["并深入探讨裁剪机制本身如何产生隐式正则化效应",{"5":{"147":2}}],["并深入探讨激活函数导数对神经网络训练的影响",{"5":{"42":2}}],["并深入分析其与注意力机制中softmax运算的内在联系",{"5":{"61":2}}],["并介绍业界标准解决方案",{"5":{"61":2}}],["并建立与注意力机制章节的数学联系",{"5":{"70":2}}],["并建立与后续章节的联系",{"5":{"47":2}}],["并建立",{"5":{"61":2}}],["并从概率论的角度解释大模型中涌现能力的本质",{"5":{"138":2}}],["并从几何和概率两个角度分析了神经元的行为",{"5":{"47":2}}],["并从表示学习的理论视角探讨多头机制如何增强模型的表达能力",{"5":{"93":2}}],["非精确学习率等原因引入的其他噪声源",{"5":{"149":2}}],["非负整数时间点",{"5":{"134":2}}],["非负",{"5":{"69":2}}],["非负性",{"5":{"61":4,"87":2,"96":3,"137":2,"152":2}}],["非负性对所有成立",{"5":{"96":1}}],["非负性直接继承自kl散度的非负性和熵的非负性",{"5":{"61":1}}],["非常高效",{"5":{"41":2}}],["非常小",{"5":{"41":1,"146":1,"150":2}}],["非常小时",{"5":{"61":2}}],["非常接近",{"5":{"42":2}}],["非常接近1",{"5":{"88":2}}],["非常数",{"5":{"71":2}}],["非常大时",{"5":{"61":2}}],["非线性层可以提取更高阶的统计特征",{"5":{"133":2}}],["非线性表达能力",{"5":{"41":2}}],["非线性的数学必要性",{"2":{"71":1},"5":{"71":1}}],["非线性变换打破表达瓶颈",{"2":{"71":1},"5":{"71":1}}],["非线性",{"5":{"47":2,"71":5}}],["非线性激活引入了输入特征之间的交互",{"5":{"133":2}}],["非线性激活函数增加了表示的信息处理能力",{"5":{"133":2}}],["非线性激活函数的",{"5":{"71":2}}],["非线性激活的信息论作用",{"5":{"133":2}}],["非线性激活使得网络能够学习越来越复杂的特征组合",{"5":{"45":2}}],["非线性特征映射通过堆叠多层神经网络来学习复杂的特征映射",{"5":{"86":2,"151":2}}],["非线性位置变换破坏了旋转群的代数结构",{"5":{"88":2}}],["非线性依赖的局限是另一个理论考虑",{"5":{"88":1}}],["非线性依赖的局限",{"5":{"88":1}}],["非平凡",{"5":{"71":2}}],["非单调性",{"5":{"71":2}}],["非饱和但有界",{"5":{"71":2}}],["非凸函数的次梯度收敛",{"5":{"136":2}}],["非凸优化是深度学习面临的现实挑战",{"5":{"48":2}}],["非凸性意味着梯度下降可能收敛到局部最小值而非全局最小值",{"5":{"48":2}}],["非凸性意味着存在多个局部极小值",{"5":{"97":2}}],["非凸",{"5":{"97":2}}],["非零奇异值的个数等于矩阵的秩",{"5":{"143":2}}],["非零奇异值的数量等于矩阵的秩​",{"5":{"89":1}}],["非零奇异值的数量等于矩阵的秩",{"5":{"89":1}}],["非零元素比例为",{"5":{"87":2,"152":2}}],["非零元素集中在主对角线附近的条对角线上",{"5":{"87":1,"152":1}}],["非零元素集中在主对角线附近的",{"5":{"87":1,"152":1}}],["非对角线元素是变量之间的协方差",{"5":{"96":2}}],["非对称的成对交互",{"5":{"93":2}}],["可扩展至超大规模",{"5":{"159":1}}],["可扩展性",{"5":{"69":2}}],["可调节σ²",{"5":{"145":1}}],["可变长度",{"5":{"140":2}}],["可见",{"5":{"138":2}}],["可分解为逐变量贡献",{"5":{"137":2}}],["可分为以下几种类型",{"5":{"136":2}}],["可表示为",{"5":{"137":2}}],["可用于检测分布偏移",{"5":{"137":2}}],["可视为一种隐式正则化",{"5":{"136":2}}],["可视为对梯度进行预条件化的更新",{"5":{"134":2}}],["可使系统稳定到目标不动点",{"5":{"136":2}}],["可通过",{"5":{"135":2}}],["可通过梯度下降进行优化",{"5":{"133":2}}],["可借用控制理论和动力系统的成熟工具分析收敛性",{"5":{"134":2}}],["可统一表示为状态更新规则",{"5":{"134":2}}],["可提取更高阶特征",{"5":{"133":2}}],["可预测的输出",{"5":{"140":2}}],["可预测的技术特性",{"5":{"138":2}}],["可预测的",{"5":{"70":2}}],["可以引导门控网络学习更均衡的专家分配策略",{"5":{"159":2}}],["可以实现可学习的稀疏路由",{"5":{"159":2}}],["可以远小于",{"5":{"159":2}}],["可以增长时",{"5":{"159":2}}],["可以同时使用",{"5":{"158":2}}],["可以同时使用而不冲突",{"5":{"145":2}}],["可以分析负载均衡损失的梯度对专家分配的影响",{"5":{"158":2}}],["可以分解为旋转",{"5":{"45":1}}],["可以分解为",{"5":{"45":1,"143":1}}],["可以推导",{"5":{"158":2}}],["可以推导出严格的泛化误差界",{"5":{"133":2}}],["可以推导出sigmoid的三阶导数",{"5":{"42":2}}],["可以推导出mae",{"5":{"51":2}}],["可以定义共享专家和独立专家的组合",{"5":{"158":2}}],["可以定义为各层雅可比矩阵范数的乘积",{"5":{"41":2}}],["可以这样总结",{"5":{"154":1}}],["可以适当减小",{"5":{"154":2}}],["可以适应不同训练阶段的需求",{"5":{"133":1}}],["可以适应特定任务的分布",{"5":{"89":2}}],["可以为细尺度分配更多的通道数以捕捉细节信息",{"5":{"150":2}}],["可以为不同的层设置不同的裁剪策略",{"5":{"147":2}}],["可以立即验证该噪声向量的均值为零",{"5":{"149":2}}],["可以更精细地分析梯度行为",{"5":{"148":2}}],["可以仅基于",{"5":{"146":1}}],["可以稳定地在网络中传播",{"5":{"148":2}}],["可以稳定梯度的流动",{"5":{"146":2}}],["可以稳定计算为",{"5":{"61":2}}],["可以对输入进行标准化处理",{"5":{"146":4}}],["可以控制高斯dropout引入的噪声方差",{"5":{"145":2}}],["可以减少损失曲面的曲率",{"5":{"148":2}}],["可以减少过拟合",{"5":{"145":2}}],["可以减少总参数量同时保持各层所需的信息分辨率",{"5":{"89":2}}],["可以学习",{"5":{"142":1}}],["可以学习从低级局部模式到高级抽象语义的层次化表示",{"5":{"71":2}}],["可以从模型学习的条件概率分布来理解",{"5":{"138":1}}],["可以从样本级",{"5":{"45":2}}],["可以处理批次大小为1的情况",{"5":{"150":2}}],["可以处理高维和复杂分布的互信息估计",{"5":{"137":2}}],["可以处理任意位置的查询",{"5":{"91":2}}],["可以非常大",{"5":{"133":2}}],["可以非零",{"5":{"132":1}}],["可以指导位置编码的设计和注意力结构的优化",{"5":{"132":2}}],["可以采用稀疏softmax",{"5":{"101":2}}],["可以视为两个分布在fisher信息度量下的",{"5":{"70":1}}],["可以视为一种隐式的平滑机制",{"5":{"144":2}}],["可以视为一种",{"5":{"69":2,"87":1,"152":1}}],["可以视为一个傅里叶级数展开",{"5":{"90":2}}],["可以视为绝对位置编码和相对位置编码的桥梁",{"5":{"91":2}}],["可以视为对值向量",{"5":{"92":1,"153":1}}],["可以确保优化过程收敛到帕累托前沿",{"5":{"70":2}}],["可以使用分布熵的近似计算或采样估计",{"5":{"154":2}}],["可以使用较大的阈值或完全不裁剪",{"5":{"147":2}}],["可以使用以下修正公式",{"5":{"146":2}}],["可以使用核密度估计",{"5":{"137":2}}],["可以使用",{"5":{"70":3}}],["可以使用黎曼梯度下降",{"5":{"70":1}}],["可以使用常量内存",{"5":{"89":2}}],["可以借鉴交叉熵优化的数值稳定性处理策略",{"5":{"70":2}}],["可以很容易地修改为",{"5":{"69":2}}],["可以是简单的加权求和",{"5":{"150":2}}],["可以是任意复杂的函数",{"5":{"150":2}}],["可以是任意形式的得分函数",{"5":{"69":2}}],["可以是注意力机制或前馈网络的输出",{"5":{"146":2}}],["可以是温度",{"5":{"69":1}}],["可以是温度或​",{"5":{"69":1}}],["可以证明使方差最小化的最优基线是",{"5":{"155":1}}],["可以证明使方差最小化的最优基线是状态价值函数",{"5":{"155":1}}],["可以证明在合适的假设下",{"5":{"148":2}}],["可以证明的列是正交的",{"5":{"143":1}}],["可以证明最优策略为",{"5":{"101":2}}],["可以证明上述下界不等式",{"5":{"69":2}}],["可以证明",{"5":{"93":2,"101":2,"143":1,"147":2,"149":2,"155":2}}],["可以形成复杂的决策边界",{"5":{"71":2}}],["可以通过反向传播自动学习",{"5":{"41":1}}],["可以通过调整每个曲面的位置",{"5":{"71":2}}],["可以通过采样来估计",{"5":{"71":1}}],["可以通过采样",{"5":{"71":1}}],["可以通过填充零向量或截断来处理",{"5":{"88":2}}],["可以通过最小二乘逼近正弦编码",{"5":{"89":1}}],["可以通过缓存机制减少空间需求",{"5":{"93":2}}],["可以通过温度参数来调节softmax的",{"5":{"96":1}}],["可以通过温度参数",{"5":{"96":1}}],["可以取任意值",{"5":{"71":2,"91":1}}],["可以正交对角化为",{"5":{"45":2}}],["可以精确重构任意复杂的连续函数模式",{"5":{"71":2}}],["可以精确地计算任意复杂函数的导数",{"5":{"45":2}}],["可以充分利用硬件的并行计算能力",{"5":{"45":2}}],["可以完全并行执行",{"5":{"45":2}}],["可以将训练一个超大规模moe模型的资源需求降低到可接受的范围内",{"5":{"157":2}}],["可以将通信开销隐藏在实际计算中",{"5":{"157":2}}],["可以将卷积运算分解为更简单的运算的组合",{"5":{"143":2}}],["可以将有效数据量",{"5":{"138":2}}],["可以将混沌训练轨迹引导到稳定不动点",{"5":{"136":2}}],["可以将多个样本的梯度计算合并为矩阵运算",{"5":{"44":2}}],["可以将权重矩阵按列",{"5":{"45":1}}],["可以将权重矩阵",{"5":{"45":1}}],["可以将批次进一步划分为更小的微批次",{"5":{"50":2}}],["可以将时间复杂度降低到",{"5":{"87":2,"152":2}}],["可以将每个头分配给一个流式多处理器",{"5":{"93":2}}],["可以系统地研究训练过程的收敛性",{"5":{"134":2}}],["可以系统地探索激活函数的参数空间",{"5":{"41":2}}],["可以系统地组织前向传播的计算步骤",{"5":{"45":2}}],["可以理解为value向量在注意力权重分布下的期望",{"5":{"70":1}}],["可以理解为寻找能够最好地逼近目标映射的复合函数形式",{"5":{"45":2}}],["可以理解为",{"5":{"45":2}}],["可以理解为将输入向量",{"5":{"47":1}}],["可以理解为一维数组中的有序数集",{"5":{"50":2}}],["可以理解为在键空间中对查询向量进行的一种",{"5":{"95":2}}],["可以得到原矩阵的最佳",{"5":{"143":1}}],["可以得到原矩阵的最佳秩近似",{"5":{"143":1}}],["可以得到查询矩阵",{"5":{"141":1}}],["可以得到查询矩阵​",{"5":{"141":1}}],["可以得到",{"5":{"69":2,"93":1}}],["可以得到各阶矩",{"5":{"96":2}}],["可以得到p值",{"5":{"96":2}}],["可以得到最优解",{"5":{"51":2}}],["可以得到不同稀疏程度的注意力矩阵",{"5":{"86":2,"151":2}}],["可以得到和",{"5":{"93":1}}],["可以在性能提升和计算效率之间取得平衡",{"5":{"150":2}}],["可以在模型效率和性能之间取得有意义的平衡",{"5":{"143":2}}],["可以在压缩比和近似精度之间取得平衡",{"5":{"143":2}}],["可以在几乎不损失模型性能的前提下大幅减少参数量",{"5":{"143":2}}],["可以在任何情况下使用",{"5":{"41":2}}],["可以在保持模型性能的同时显著减少参数量和计算开销",{"5":{"50":2}}],["可以用来推导上界",{"5":{"148":2}}],["可以用于表示高阶交互关系",{"5":{"50":2}}],["可以用投影矩阵表示",{"5":{"51":1}}],["可以用投影矩阵",{"5":{"51":1}}],["可以用一个矩阵",{"5":{"50":1}}],["可以用一个标量来精确量化",{"5":{"61":2}}],["可以用振幅或方差来度量",{"5":{"90":2}}],["可以显著减少训练时间和推理延迟",{"5":{"146":2}}],["可以显著减少参数量和计算量",{"5":{"50":2}}],["可以显著减少不必要的内存分配和数据拷贝",{"5":{"50":2}}],["可以显著减少模型参数和计算量",{"5":{"87":2,"152":2}}],["可以一次性完成所有样本的计算",{"5":{"50":2}}],["可以看作是优化",{"5":{"144":2}}],["可以看作是查询张量和键张量的收缩",{"5":{"50":1}}],["可以看作一个四阶张量",{"5":{"143":1}}],["可以看作张量收缩操作",{"5":{"143":2}}],["可以看出",{"5":{"41":1,"69":1}}],["可以验证",{"5":{"51":2,"90":2}}],["可以被自然地表示为张量操作",{"5":{"143":2}}],["可以被视为一个",{"5":{"71":2}}],["可以被分解为投影部分和垂直部分的和",{"5":{"51":1}}],["可以发现其蕴含着深刻的统计学内涵",{"5":{"61":2}}],["可以设置较小的阈值",{"5":{"147":2}}],["可以设置较大的阈值",{"5":{"147":2}}],["可以设置",{"5":{"61":2}}],["可以与所有位置交互",{"5":{"86":2,"151":2}}],["可以揭示其作为信息选择",{"5":{"132":2}}],["可以揭示其信息处理能力的本质",{"5":{"71":2}}],["可以揭示其数值特性",{"5":{"87":2,"152":2}}],["可以避免数值溢出",{"5":{"86":2,"151":2}}],["可以避免softmax进入极端区域",{"5":{"87":2,"152":2}}],["可以递归更新",{"5":{"87":2,"152":2}}],["可以近似表示为低秩分解",{"5":{"87":1,"152":1}}],["可以高效地处理大批量的序列",{"5":{"88":2}}],["可以无缝集成到各种模型架构中",{"5":{"88":2}}],["可以提高外推性能",{"5":{"89":2}}],["可以考虑压缩存储",{"5":{"89":2}}],["可以考虑增量计算",{"5":{"89":2}}],["可以统一表示",{"5":{"90":2}}],["可以获得更深刻的洞察",{"5":{"90":2}}],["可以唯一确定任意位置",{"5":{"90":2}}],["可以唯一表示该频率分量的任意相位",{"5":{"91":2}}],["可以唯一恢复位置信息",{"5":{"91":2}}],["可以简化为",{"5":{"91":2}}],["可以清晰地看到预期的u型曲线模式",{"5":{"51":2}}],["可以清晰地看到",{"5":{"91":2}}],["可以并行执行",{"5":{"45":2,"92":2,"153":2}}],["可以并行",{"5":{"92":2,"153":2}}],["可以保证泛化误差的有界性",{"5":{"61":2}}],["可以保存在寄存器或共享内存中",{"5":{"92":2,"153":2}}],["可以直接传递到深层",{"5":{"150":2}}],["可以直接计算梯度而不需要重参数化技巧",{"5":{"145":2}}],["可以直接用信息论工具分析",{"5":{"132":2}}],["可以直接",{"5":{"92":2,"153":2}}],["可以类比为",{"5":{"93":2}}],["可以构造和",{"5":{"93":1}}],["可以构造",{"5":{"93":1}}],["可以表示为确定函数",{"5":{"71":1}}],["可以表示为",{"5":{"94":2}}],["可以帮助我们从另一个角度理解多头注意力的本质",{"5":{"93":2}}],["可以帮助模型探索不同的注意力模式",{"5":{"95":2}}],["可以让模型专注于最相关的位置",{"5":{"95":2}}],["可以灵活地适应各种序列处理场景",{"5":{"95":2}}],["可以根据输入文本的语言",{"5":{"159":2}}],["可以根据输入内容进行调整",{"5":{"150":2}}],["可以根据输入动态调整",{"5":{"101":2}}],["可以根据任务难度或优先级进行调整",{"5":{"133":2}}],["可以根据梯度范数",{"5":{"101":2}}],["可能从一个专家跳转到另一个完全不同的专家",{"5":{"159":2}}],["可能影响主任务的优化",{"5":{"158":2,"159":2}}],["可能表明门控网络陷入了局部最优",{"5":{"158":2}}],["可能成为瓶颈",{"5":{"157":2}}],["可能有",{"5":{"149":1}}],["可能有个为零或接近零的特征值",{"5":{"149":1}}],["可能偏离临界状态",{"5":{"148":2}}],["可能引入不稳定性",{"5":{"147":1}}],["可能引入性能下降",{"5":{"146":2}}],["可能使模型更倾向于找到",{"5":{"147":2}}],["可能严重限制优化速度",{"5":{"147":2}}],["可能破坏这种保证",{"5":{"147":2}}],["可能改变梯度的方向",{"5":{"147":2}}],["可能限制网络的表示能力",{"5":{"146":2}}],["可能限制了位置信息的利用",{"5":{"89":2}}],["可能限制了其捕获复杂模式的能力",{"5":{"93":2}}],["可能变得非常小",{"5":{"145":2}}],["可能存在多重共线性",{"5":{"140":2}}],["可能需要适当增加正则化系数",{"5":{"144":2}}],["可能需要更多训练数据或模型改进",{"5":{"137":2}}],["可能需要使用加权的交叉熵或",{"5":{"70":2}}],["可能发散",{"5":{"136":2}}],["可能发现更好的局部最小值",{"5":{"136":2}}],["可能帮助跳出局部最小值",{"5":{"136":2}}],["可能过于保守",{"5":{"133":2}}],["可能用于特定场景如数据增强",{"5":{"133":2}}],["可能不适用于训练后期",{"5":{"147":1}}],["可能不足以推动模型从错误中学习",{"5":{"70":2}}],["可能不精确",{"5":{"61":2}}],["可能不遵循线性位置关系",{"5":{"88":2}}],["可能不符合正弦余弦编码的假设",{"5":{"89":2}}],["可能还有value",{"5":{"69":2}}],["可能产生问题",{"5":{"41":2}}],["可能产生范数增长",{"5":{"41":2}}],["可能放大或缩小梯度",{"5":{"41":2}}],["可能导致参数跳出合理的数值范围",{"5":{"148":2}}],["可能导致参数跳入参数空间中远离当前区域的点",{"5":{"147":1}}],["可能导致参数",{"5":{"147":1}}],["可能导致参数沿周期轨道运动",{"5":{"136":2}}],["可能导致极限环的产生",{"5":{"136":2}}],["可能导致在极小值附近的震荡",{"5":{"136":2}}],["可能导致",{"5":{"41":2}}],["可能导致训练不稳定",{"5":{"42":2}}],["可能导致训练过程不稳定或收敛较慢",{"5":{"96":2}}],["可能导致优化目标与概率解释不一致",{"5":{"61":2}}],["可能导致优化轨迹偏离连续梯度流",{"5":{"97":2}}],["可能导致数值问题",{"5":{"97":2}}],["可能捕获更复杂的模式",{"5":{"41":2}}],["可能带来性能的提升",{"5":{"41":2}}],["可能直接落在饱和区域",{"5":{"41":1}}],["可能具有不同的范数尺度",{"5":{"41":1}}],["可能是局部",{"5":{"143":2}}],["可能是曲线",{"5":{"71":2}}],["可能是超平面",{"5":{"51":2}}],["可能对权重和偏置维护不同的优化状态",{"5":{"46":2}}],["可能对训练数据中的噪声过度敏感",{"5":{"51":2}}],["可能溢出双精度浮点数的表示范围",{"5":{"61":2}}],["可能下溢为",{"5":{"61":2}}],["可能在某些场景下并非最优",{"5":{"86":2,"151":2}}],["可能很大",{"5":{"86":2,"151":2}}],["可能无法捕捉复杂的数据模式",{"5":{"51":2}}],["可能无法精确表示这类模式",{"5":{"86":2,"151":2}}],["可能无法同时捕获足够多样的特征",{"5":{"93":2}}],["可能略有不同",{"5":{"87":2,"152":2}}],["可能丢失高维信息",{"5":{"87":2,"152":2}}],["可能负责某种",{"5":{"87":4,"152":4}}],["可能保持较大的谱范数",{"5":{"87":2,"152":2}}],["可能学习到最优的组合",{"5":{"89":2}}],["可能出现少数专家承担大部分计算负载",{"5":{"159":2}}],["可能出现",{"5":{"90":2}}],["可能为负或较小",{"5":{"91":2}}],["可能相交",{"5":{"93":1}}],["可能",{"5":{"97":1}}],["可能会让模型更倾向于生成立即能获得高分的",{"5":{"156":2}}],["可能会生成与训练数据分布差异较大的输出",{"5":{"154":2}}],["可能会超出浮点数的精度范围",{"5":{"88":2}}],["可能会带来性能提升",{"5":{"101":2}}],["可加性",{"5":{"71":2}}],["可微top",{"5":{"159":2}}],["可微",{"5":{"71":2}}],["可微函数是凸函数的充要条件是其梯度单调不减",{"5":{"48":2}}],["可微性保证其可以作为神经网络的输出层进行端到端优化",{"5":{"61":1}}],["可微性",{"5":{"61":1}}],["可微位置编码通过可学习的参数化方式来自动发现最优的位置编码函数",{"5":{"88":1}}],["可微位置编码",{"5":{"88":1}}],["可得累积损失下降量与累积梯度范数的关系",{"5":{"136":2}}],["可得宽度与信息容量的对数关系",{"5":{"133":2}}],["可得",{"5":{"44":2,"133":1,"134":2,"135":4,"137":4,"145":2}}],["可验证为正交矩阵",{"5":{"45":2}}],["可达数十亿",{"5":{"48":2}}],["可学习参数",{"5":{"150":2}}],["可学习位置编码的矩阵性质",{"0":{"89":1},"4":{"89":1},"5":{"89":1,"131":4}}],["可学习位置编码的矩阵性质<",{"5":{"131":1}}],["可学习位置编码的定义与参数化",{"2":{"89":1},"5":{"89":1}}],["可学习位置编码的数学分析不仅有助于深入理解其工作原理",{"5":{"89":2}}],["可学习位置编码的表达式能力是指它能够表示的位置函数的范围",{"5":{"89":2}}],["可学习位置编码的表达能力虽然灵活",{"5":{"89":2}}],["可学习位置编码的初始化对训练动态有重要影响",{"5":{"89":2}}],["可学习位置编码的梯度流动是理解训练动态的关键",{"5":{"89":2}}],["可学习位置编码的收敛性分析涉及优化理论",{"5":{"89":2}}],["可学习位置编码的参数效率是实际应用中的重要考量",{"5":{"89":2}}],["可学习位置编码的参数量为​",{"5":{"89":1}}],["可学习位置编码的参数量为",{"5":{"89":1}}],["可学习位置编码的存储和计算优化是实际部署中的重要问题",{"5":{"89":2}}],["可学习位置编码的性能对超参数选择敏感",{"5":{"89":2}}],["可学习位置编码的有效表示可以用个秩",{"5":{"89":1}}],["可学习位置编码的有效表示可以用",{"5":{"89":1}}],["可学习位置编码",{"5":{"89":2}}],["可学习位置编码涉及参数化矩阵的性质分析",{"5":{"89":2}}],["可学习位置编码没有固定的数学公式",{"5":{"89":2}}],["可学习位置编码在transformer架构的早期研究和某些特定任务上取得了良好的性能",{"5":{"89":2}}],["可学习位置编码在嵌入空间中形成特定的几何结构",{"5":{"89":2}}],["可学习位置编码在理论上可以逼近正弦余弦位置编码",{"5":{"89":2}}],["可学习位置编码在外推时面临的挑战是",{"5":{"89":2}}],["可学习位置编码与正弦余弦位置编码在数学性质上有显著差异",{"5":{"89":2}}],["可学习位置编码通常收敛到某种",{"5":{"89":2}}],["可学习位置编码通常与正弦余弦编码表现相当",{"5":{"89":2}}],["可学习位置编码作为一种重要的位置编码范式",{"5":{"89":2}}],["可学习位置编码将位置编码矩阵视为可训练的参数",{"5":{"89":2}}],["可学习位置编码将位置编码视为可学习的参数矩阵",{"5":{"91":2}}],["可学习编码能够表示什么样的位置函数",{"5":{"89":2}}],["可学习编码的矩阵性质",{"5":{"70":2}}],["可学习编码的矩阵具有怎样的秩和奇异值结构",{"5":{"89":2}}],["可学习编码的表达能力更加灵活",{"5":{"89":2}}],["可学习编码的函数空间由训练数据决定",{"5":{"89":2}}],["可学习编码的函数空间更加灵活但也更加模糊",{"5":{"89":2}}],["可学习编码的函数空间是维度不超过的线性子空间上的所有函数",{"5":{"89":1}}],["可学习编码的函数空间是维度不超过",{"5":{"89":1}}],["可学习编码的外推能力取决于学习到的编码结构",{"5":{"89":2}}],["可学习编码的秩最多为​",{"5":{"89":1}}],["可学习编码的秩最多为",{"5":{"89":1}}],["可学习编码学习到的基可能与正弦编码的基完全不同",{"5":{"89":2}}],["可学习编码是数据驱动的",{"5":{"89":2}}],["可学习编码可以通过最小二乘逼近正弦编码",{"5":{"89":1}}],["可学习编码可以自动发现领域特定的位置模式",{"5":{"89":2}}],["可学习编码可以学习任意复杂的位置表示模式",{"5":{"91":2}}],["可学习编码可能更优",{"5":{"89":2}}],["可学习编码可能取得更好的性能",{"5":{"91":2}}],["可学习编码在此基础上学习",{"5":{"89":2}}],["可学习编码",{"5":{"89":1}}],["可学习编码只能处理训练时见过的长度",{"5":{"91":2}}],["可学习编码需要存储参数矩阵",{"5":{"89":2}}],["可学习编码需要的参数量",{"5":{"91":1}}],["可学习编码需要",{"5":{"91":1}}],["可学习编码没有这种先验",{"5":{"91":2}}],["可学习的门控注意力",{"5":{"132":2}}],["可学习的稀疏注意力",{"5":{"132":2}}],["可学习的稀疏模式将稀疏模式参数化",{"5":{"86":2,"151":2}}],["可学习的位置编码在某些场景下能够取得更好的性能",{"5":{"91":2}}],["可外推性",{"5":{"88":2}}],["可区分性",{"5":{"91":2}}],["可作为后续注意力层的输入",{"5":{"91":2}}],["可逆性",{"5":{"91":2}}],["可逆性意味着变换是对应的",{"5":{"94":2}}],["可泛化",{"5":{"92":2,"153":2}}],["可泛化的解",{"5":{"97":2}}],["相等",{"5":{"159":2}}],["相等时",{"5":{"157":2}}],["相反",{"5":{"148":2,"156":2}}],["相反地",{"5":{"61":2}}],["相轨迹平行于",{"5":{"147":2}}],["相轨迹与标准梯度下降相同",{"5":{"147":2}}],["相图特征",{"5":{"147":2}}],["相互独立",{"5":{"145":2}}],["相应地",{"5":{"145":2}}],["相应的奇异向量才会被保留",{"5":{"144":2}}],["相应变换",{"5":{"135":2}}],["相变类比与临界现象",{"5":{"138":1}}],["相变类比与临界现象为涌现能力提供了更深刻的数学洞见",{"5":{"138":1}}],["相变",{"5":{"138":2}}],["相变发生",{"5":{"137":2}}],["相空间中",{"5":{"147":2}}],["相空间",{"5":{"134":2}}],["相空间相关概念",{"5":{"134":2}}],["相似度函数",{"5":{"137":2}}],["相似度很高时",{"5":{"69":1}}],["相似度",{"5":{"69":5,"94":2}}],["相似度越高",{"5":{"95":2}}],["相关的计算和内存访问被完全跳过",{"5":{"159":2}}],["相关的等效正则化系数",{"5":{"144":2}}],["相关理论和技术也将继续发展深化",{"5":{"147":2}}],["相关",{"5":{"41":1,"70":2,"87":2,"89":2,"97":2,"152":2}}],["相关系数",{"5":{"96":2}}],["相关系数的取值范围为",{"5":{"96":2}}],["相关系数消除了量纲的影响",{"5":{"96":2}}],["相关系数可用于分析不同词嵌入维度之间的冗余程度",{"5":{"96":2}}],["相对标准差为",{"5":{"145":2}}],["相对谱误差",{"5":{"142":2}}],["相对frobenius误差",{"5":{"142":2}}],["相对误差小于",{"5":{"44":2}}],["相对于超球面坐标系的方位角",{"5":{"146":2}}],["相对于标准正态分布",{"5":{"96":2}}],["相对于其他",{"5":{"61":2}}],["相对熵",{"5":{"61":2,"70":3}}],["相对位置敏感性",{"5":{"70":2,"97":2}}],["相对位置应该决定注意力权重",{"5":{"88":2}}],["相对位置是什么",{"5":{"88":2}}],["相对位置编码能力",{"5":{"132":2}}],["相对位置编码",{"5":{"88":2,"91":2,"92":2,"153":2}}],["相对位置编码问题转化为",{"5":{"90":2}}],["相对位置编码的数学形式为",{"5":{"91":2}}],["相对位置编码的优点",{"5":{"91":2}}],["相对位置编码的缺点",{"5":{"91":2}}],["相对位置的频率表示",{"2":{"90":1},"5":{"90":3}}],["相对位置的频率表示具有以下特点",{"5":{"90":2}}],["相对位置信息被",{"5":{"90":2}}],["相对位置对应于圆群上的平移",{"5":{"90":2}}],["相对位置对应于复平面上的相对旋转",{"5":{"90":2}}],["相对位置与内容无关的交互",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["相对位置可表达性",{"5":{"91":2}}],["相对位置可学习性",{"5":{"92":2,"153":2}}],["相对位置",{"5":{"101":2,"132":2}}],["相对距离",{"5":{"88":1}}],["相一致",{"5":{"45":2}}],["相乘",{"5":{"50":2,"93":2}}],["相比top",{"5":{"158":2}}],["相比于遵循当前策略",{"5":{"155":2}}],["相比于均方误差",{"5":{"61":2}}],["相比xavier初始化",{"5":{"148":2}}],["相比sigmoid",{"5":{"148":2}}],["相比无动量的",{"5":{"135":2}}],["相比之下",{"5":{"41":2,"51":2,"70":4}}],["相比",{"5":{"90":2}}],["相当于在核空间中引入了可学习的权重",{"5":{"86":2,"151":2}}],["相同超参数在不同运行中产生显著不同的结果",{"5":{"136":2}}],["相同超参数可能产生不同的最终结果",{"5":{"136":2}}],["相同",{"5":{"87":1,"135":2,"152":1}}],["相同的输入总是产生相同的输出",{"5":{"159":2}}],["相同的输入嵌入在不同空间中被赋予了不同的角色",{"5":{"94":2}}],["相同的位置差产生的编码差异较小",{"5":{"90":2}}],["相同的位置差产生的编码差异较大",{"5":{"90":2}}],["相同的内容嵌入",{"5":{"91":2}}],["相同的架构从对称的初始状态出发",{"5":{"93":2}}],["相同位置的位置投影是相同的",{"5":{"91":2}}],["相角",{"5":{"88":2}}],["相位滞后使得系统",{"5":{"136":2}}],["相位旋转",{"5":{"88":2}}],["相位调制",{"5":{"88":1}}],["相位均匀分布",{"5":{"90":2}}],["相位​随位置线性增加",{"5":{"90":1}}],["相位随位置均匀进动",{"5":{"90":2}}],["相位编码对应于圆群",{"5":{"90":2}}],["相位向量的映射是一一对应的",{"5":{"90":2}}],["相位",{"5":{"90":1}}],["相位变化很小",{"5":{"91":2}}],["相邻像素之间存在强相关性",{"5":{"143":2}}],["相邻项差值趋于零",{"5":{"136":2}}],["相邻词之间通常存在更强的语义关联",{"5":{"88":2}}],["相邻或相近的位置",{"5":{"91":2}}],["相邻位置的编码应该相似",{"5":{"89":2}}],["相邻位置的编码向量相似",{"5":{"89":4}}],["相邻位置的编码变化平滑",{"5":{"90":2}}],["相邻位置的点在球面上相邻",{"5":{"91":2}}],["相邻位置在低维度区域的热力图模式相似度较低",{"5":{"91":2}}],["相加",{"5":{"158":2,"159":2}}],["相加保持了输入维度不变",{"5":{"91":2}}],["相加是一种线性融合方式",{"5":{"91":2}}],["相加操作允许梯度同时流回内容编码和位置编码",{"5":{"91":2}}],["相距很远的位置",{"5":{"91":2}}],["相近位置的编码向量在高维空间中相距较近",{"5":{"91":2}}],["本章将从标度律",{"5":{"138":2}}],["本章从信息论的视角审视大模型",{"5":{"137":2}}],["本章的核心目标是揭示一个更深层次的统一性",{"5":{"69":2}}],["本章的内容与第七章",{"5":{"101":2}}],["本节简要介绍batch",{"5":{"148":2}}],["本节探讨xavier初始化和he初始化的数学原理及其对梯度流的影响",{"5":{"148":2}}],["本节分析深度",{"5":{"148":2}}],["本节还介绍了变分dropout和高斯dropout等扩展形式",{"5":{"145":2}}],["本节还讨论了损失函数与注意力机制之间的数学联系",{"5":{"70":2}}],["本节介绍几种常用的估计技术",{"5":{"137":2}}],["本节介绍实用的优化方法",{"5":{"133":2}}],["本节小结与延伸讨论",{"2":{"149":1},"5":{"149":1}}],["本节小结",{"2":{"44":1,"45":1,"47":1,"51":1,"61":1,"70":1,"71":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"95":1,"97":1,"101":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"140":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"152":1,"153":1},"5":{"44":1,"45":1,"47":1,"51":1,"61":1,"70":1,"71":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1,"93":1,"95":1,"97":1,"101":1,"132":1,"133":1,"134":1,"135":1,"136":1,"137":1,"140":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"152":1,"153":1}}],["本节将深入探讨专家网络的数学表示",{"5":{"158":2}}],["本节将深入探讨这些技术细节",{"5":{"157":2}}],["本节将深入分析这些数值稳定性问题",{"5":{"61":2}}],["本节将理论分析转化为实用的训练策略",{"5":{"136":2}}],["本节将把这些理论基础应用于大语言模型",{"5":{"101":2}}],["本节将从显存消耗模型",{"5":{"157":2}}],["本节将从三个关键角度分析这种影响",{"5":{"149":2}}],["本节将从sgd的噪声模型出发",{"5":{"149":2}}],["本节将从张量维度",{"5":{"146":2}}],["本节将从概率论的基本原理出发",{"5":{"140":2}}],["本节将从多个维度对不同损失函数的数学结构进行系统性的对比分析",{"5":{"70":2}}],["本节将从多个角度深入分析梯度饱和的根源",{"5":{"41":2}}],["本节将从更深的层次分析softmax操作",{"5":{"69":2}}],["本节将从信息论和概率论的角度",{"5":{"69":2}}],["本节将从信息论的基石概念出发",{"5":{"61":2}}],["本节将从函数逼近论的角度",{"5":{"71":2}}],["本节将从数学的角度深入分析这两种现象的根源",{"5":{"41":2}}],["本节将从数学的角度分析梯度流的各种机制",{"5":{"41":2}}],["本节将从数学的角度系统性地推导各类激活函数的导数公式",{"5":{"42":2}}],["本节将从数学的角度系统性地分析激活函数的理论基础",{"5":{"71":2}}],["本节将从数学推导的角度",{"5":{"44":2}}],["本节将从数学建模的角度",{"5":{"47":2}}],["本节将从数学定义出发",{"5":{"51":2,"95":2,"136":2}}],["本节将从数学本质出发",{"5":{"88":2}}],["本节将从数学角度严谨地分析这两种现象的成因",{"5":{"148":2}}],["本节将从数学角度严谨地推导三种主流归一化算法的定义",{"5":{"146":2}}],["本节将从数学角度严格分析梯度裁剪的多种形式",{"5":{"147":2}}],["本节将从数学角度严格推导dropout的期望保持性质",{"5":{"145":2}}],["本节将从数学角度深入分析正则化如何影响梯度计算",{"5":{"144":2}}],["本节将从数学角度深入分析为什么深度网络权重具有低秩特性",{"5":{"142":2}}],["本节将从数学角度深入分析注意力矩阵的内在结构",{"5":{"141":2}}],["本节将从数学角度深入分析注意力机制是如何实现这一突破的",{"5":{"92":2,"153":2}}],["本节将从数学角度系统分析主要的注意力变体",{"5":{"86":2,"151":2}}],["本节将从数学角度系统推导和解释正弦余弦位置编码的定义",{"5":{"91":2}}],["本节将从复合函数",{"5":{"45":2}}],["本节将从谱分解",{"5":{"87":2,"152":2}}],["本节将从线性代数和矩阵分析的角度",{"5":{"89":2}}],["本节将从傅里叶分析的基本原理出发",{"5":{"90":2}}],["本节将从矩阵推导的角度系统地阐述多头注意力的计算过程",{"5":{"93":2}}],["本节将从矩阵变换的角度深入探讨query",{"5":{"94":2}}],["本节将从优化理论的角度",{"5":{"97":2}}],["本节将系统阐述moe的核心理念",{"5":{"159":2}}],["本节将系统阐述如何将神经网络表示为矩阵运算的形式",{"5":{"46":2}}],["本节将系统地介绍这些分解技术的数学原理",{"5":{"143":2}}],["本节将系统分析梯度爆炸的数学根源",{"5":{"41":2}}],["本节将系统分析各类归一化技术的数学原理及其对梯度稳定性的影响",{"5":{"41":2}}],["本节将系统分析激活函数与transformer其他组件",{"5":{"71":2}}],["本节将系统性地分析主要激活函数的概率解释",{"5":{"71":2}}],["本节将揭示交叉熵损失函数与统计学中经典的最大似然估计",{"5":{"61":2}}],["本节将详细展示从二分类到多分类任务的完整推导过程",{"5":{"61":2}}],["本节将详细推导",{"5":{"61":2}}],["本节从概率论的基本原理出发",{"5":{"140":1}}],["本节从收敛性",{"5":{"136":2}}],["本节从数学角度分析震荡的产生机制和影响因素",{"5":{"136":2}}],["本节从数学的角度系统性地分析了梯度饱和与梯度爆炸的根源",{"5":{"41":2}}],["本节从",{"5":{"135":2}}],["本节从信息论的基础概念出发",{"5":{"137":2}}],["本节从信息论的基本概念",{"5":{"61":2}}],["本节从信息论的视角系统分析了注意力机制的数学原理和信息流动特性",{"5":{"132":2}}],["本节从多个维度系统性地对比了均方误差和交叉熵两种主要损失函数的数学结构",{"5":{"70":2}}],["本节从多个数学视角系统性地分析了激活函数的角色",{"5":{"71":2}}],["本节从多个数学视角深入分析了前向传播的本质",{"5":{"45":1}}],["本节从谱分析和低秩结构的角度深入探讨了注意力矩阵的数学性质",{"5":{"87":2,"152":2}}],["本节从频率空间的角度系统分析了正弦余弦位置编码的数学原理",{"5":{"90":2}}],["本节从优化理论的角度系统性地分析了损失函数的优化性质",{"5":{"97":2}}],["本节系统阐述了信息瓶颈理论及其在正则化中的应用",{"5":{"133":2}}],["本节系统阐述了反向传播算法的数学理论基础",{"5":{"44":1}}],["本节系统阐述信息瓶颈的数学理论",{"5":{"133":2}}],["本节系统分析注意力机制的信息论基础",{"5":{"132":2}}],["本节系统地分析了深度神经网络中梯度消失与梯度爆炸的数学条件",{"5":{"148":2}}],["本节系统地分析了梯度裁剪与正则化的数学基础及其相互关系",{"5":{"147":2}}],["本节系统地分析了正则化技术对梯度计算和损失曲面的影响",{"5":{"144":2}}],["本节系统地分析了可学习位置编码的矩阵性质",{"5":{"89":2}}],["本节系统地阐述了神经元的数学模型",{"5":{"47":2}}],["本节系统地介绍了梯度协方差矩阵这一核心概念",{"5":{"149":2}}],["本节系统地介绍了深度学习中三种主流归一化方法的数学原理与公式推导",{"5":{"146":1}}],["本节系统地介绍了矩阵与张量分解的数学基础及其在深度学习中的应用",{"5":{"143":2}}],["本节系统地介绍了均方误差",{"5":{"51":2}}],["本节系统地介绍了正弦余弦位置编码的数学定义",{"5":{"91":2}}],["本节系统地介绍了scaled",{"5":{"95":2}}],["本节系统地探讨了多头注意力的矩阵推导与表达能力分析",{"5":{"93":2}}],["本节系统性地分析了大语言模型中使用的各种损失函数",{"5":{"101":2}}],["本节的核心内容可以概括为以下几点",{"5":{"44":2,"45":2}}],["本节的内容可以概括为以下几个核心要点",{"5":{"47":2}}],["本节建立的概率解释框架为",{"5":{"61":2}}],["本节深入探讨了dropout的期望保持性质及其数学基础",{"5":{"145":2}}],["本节深入探讨了旋转位置编码",{"5":{"88":2}}],["本节深入探讨神经网络训练过程中的收敛性",{"5":{"136":2}}],["本节深入分析系统的稳定性性质",{"5":{"135":2}}],["本节深入分析了注意力机制建模长程依赖的数学原理",{"5":{"92":2,"153":2}}],["本节暂时忽略位置编码的影响",{"5":{"94":2}}],["本身可能较大",{"5":{"158":2}}],["本身是一个完整的",{"5":{"149":1}}],["本身是一个完整的对称正定矩阵",{"5":{"149":1}}],["本身是",{"5":{"134":2}}],["本身是线性的",{"5":{"45":1}}],["本身是线性变换",{"5":{"47":1}}],["本身",{"5":{"45":2}}],["本身的数值问题",{"5":{"61":2}}],["本身的分布高度依赖于的量级",{"5":{"94":1}}],["本身的分布高度依赖于",{"5":{"94":1}}],["本质上测量的是这些向量之间的相似性",{"5":{"141":1}}],["本质上是贝叶斯推断的近似",{"5":{"137":2}}],["本质上是对输入",{"5":{"45":1}}],["本质上是在寻找一组参数",{"5":{"51":2}}],["本质上是一个依赖于输入数据的动态矩阵",{"5":{"87":2,"152":2}}],["本质上是各频率成分相位差余弦的叠加",{"5":{"90":2}}],["本质上都是在解决一个相同的数学问题",{"5":{"61":2}}],["计算成本翻倍",{"5":{"159":2}}],["计算成本呈平方级上升",{"5":{"159":2}}],["计算成本较高",{"5":{"41":2}}],["计算策略梯度",{"5":{"155":2}}],["计算td目标",{"5":{"155":2}}],["计算折扣累积回报",{"5":{"155":2}}],["计算回报",{"5":{"155":2}}],["计算整个批次中该通道所有样本的均值和方差",{"5":{"150":2}}],["计算可知",{"5":{"149":2}}],["计算可以重新排列为",{"5":{"86":2,"151":2}}],["计算可以利用tensor",{"5":{"93":1}}],["计算高效",{"5":{"146":1}}],["计算得到",{"5":{"146":1}}],["计算该样本所有特征通道的均值和方差",{"5":{"150":2}}],["计算该样本所有特征的均值和方差",{"5":{"146":2}}],["计算该头的互信息",{"5":{"132":2}}],["计算所有样本在该特征上的均值和方差",{"5":{"146":2}}],["计算所有位置对的某种关联度量",{"5":{"92":1,"153":1}}],["计算各项",{"5":{"145":2}}],["计算各层的信息保留",{"5":{"132":2}}],["计算各层的输出",{"5":{"46":2}}],["计算模3展开",{"5":{"143":2}}],["计算模2展开",{"5":{"143":2}}],["计算模1展开",{"5":{"143":2}}],["计算方法及其在深度学习中的关键应用",{"5":{"143":2}}],["计算方向",{"5":{"44":1}}],["计算简单",{"5":{"140":1}}],["计算需要建模个可能的序列",{"5":{"140":1}}],["计算需要次顺序计算",{"5":{"92":1,"153":1}}],["计算预算通常以",{"5":{"138":2}}],["计算预测误差",{"5":{"51":2}}],["计算最优的moe配置满足",{"5":{"159":2}}],["计算最优的缩放关系为",{"5":{"159":2}}],["计算最优训练的定义与原理",{"5":{"138":1}}],["计算最优训练的定义与原理来自于对计算约束的深入分析",{"5":{"138":1}}],["计算最优策略建议",{"5":{"138":2}}],["计算最优策略的核心是在这个边界附近进行资源配置",{"5":{"138":2}}],["计算最优配置满足参数和数据之间的某种平衡关系",{"5":{"138":2}}],["计算最优",{"5":{"138":2}}],["计算最优与数据最优",{"2":{"138":1},"5":{"138":1}}],["计算分母",{"5":{"137":2}}],["计算分子",{"5":{"137":2}}],["计算分类任务中的正确预测数量等",{"5":{"96":2}}],["计算经验分布",{"5":{"137":2}}],["计算稳态解的方差并利用特征值分解可得上述关系",{"5":{"136":2}}],["计算困难",{"5":{"133":2}}],["计算信息效率",{"5":{"133":2}}],["计算信息压缩率",{"5":{"132":2}}],["计算层输出",{"5":{"133":4}}],["计算压缩比",{"5":{"133":2}}],["计算两个多元高斯分布之间的",{"5":{"133":2}}],["计算门控值",{"5":{"132":2}}],["计算稀疏注意力",{"5":{"132":2}}],["计算稀疏注意力分数矩阵的时间复杂度为",{"5":{"86":2,"151":2}}],["计算注意力多样性损失",{"5":{"132":2}}],["计算不同注意力头之间熵的差异",{"5":{"132":2}}],["计算不同位置对的点积",{"5":{"91":2}}],["计算与其他头的互信息",{"5":{"132":2}}],["计算每个头的熵",{"5":{"132":2}}],["计算每个位置的输出",{"5":{"69":1}}],["计算每次的统计量估计",{"5":{"96":2}}],["计算",{"5":{"44":5,"45":3,"61":8,"86":2,"87":2,"88":1,"92":4,"135":2,"140":1,"145":2,"151":2,"152":2,"153":4,"157":2}}],["计算效率相对于稠密模型提升了",{"5":{"159":2}}],["计算效率更高",{"5":{"145":2}}],["计算效率",{"2":{"41":1},"5":{"41":3,"91":2}}],["计算效率的理论分析",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["计算效率显著提升",{"5":{"87":2,"152":2}}],["计算效率极低",{"5":{"92":2,"153":2}}],["计算矩阵的谱半径通常需要数值方法",{"5":{"41":2}}],["计算梯度",{"5":{"44":2,"48":2,"136":2}}],["计算权重梯度",{"5":{"44":2}}],["计算偏置梯度",{"5":{"44":2}}],["计算偏置梯度约为​",{"5":{"44":1}}],["计算偏置梯度约为",{"5":{"44":1}}],["计算前一层误差",{"5":{"44":2}}],["计算的是query和key之间的相似度矩阵",{"5":{"71":2}}],["计算的flops约为​",{"5":{"44":1}}],["计算类型",{"5":{"44":1}}],["计算图",{"5":{"45":4}}],["计算图是前向传播构建的",{"5":{"44":2}}],["计算图是一个有向无环图",{"5":{"45":2}}],["计算图是有向无环图",{"5":{"45":2}}],["计算图是连接数学理论和工程实现的桥梁",{"5":{"45":2}}],["计算图是表示数学表达式的有向无环图",{"5":{"48":2}}],["计算图的表示与实现",{"2":{"45":1},"5":{"45":1}}],["计算图的前向遍历",{"5":{"45":2}}],["计算​后合并结果",{"5":{"45":1}}],["计算顺序",{"5":{"45":1}}],["计算复杂度的维度分析",{"2":{"140":1},"5":{"140":1}}],["计算复杂度与效率权衡",{"5":{"150":1}}],["计算复杂度与效率权衡是实际应用中必须考虑的问题",{"5":{"150":1}}],["计算复杂度与类别数线性相关等",{"5":{"101":2}}],["计算复杂度与内存效率",{"2":{"50":1},"5":{"50":1}}],["计算复杂度分析",{"2":{"44":1,"95":1,"146":1},"5":{"44":1,"95":1,"146":1}}],["计算复杂度",{"5":{"86":2,"95":2,"132":4,"143":1,"145":1,"151":2,"159":1}}],["计算复杂度从降低到",{"5":{"87":1,"152":1}}],["计算复杂度从",{"5":{"87":1,"152":1}}],["计算复杂度对比等数学工具",{"5":{"92":2,"153":2}}],["计算量将增加约16倍",{"5":{"159":2}}],["计算量较大",{"5":{"146":1}}],["计算量减少",{"5":{"41":2}}],["计算量约为lu分解的一半",{"5":{"50":2}}],["计算量与头数成正比",{"5":{"93":4}}],["计算会产生",{"5":{"61":2}}],["计算也存在数值不稳定的风险",{"5":{"61":2}}],["计算损失",{"5":{"61":2,"101":2,"132":2}}],["计算块级别的注意力分数",{"5":{"86":2,"151":2}}],["计算行最大值",{"5":{"86":2,"151":2}}],["计算指数差值",{"5":{"86":2,"151":2}}],["计算和",{"5":{"87":1,"88":1,"92":1,"152":1,"153":1}}],["计算其协方差",{"5":{"149":2}}],["计算其输出关于输入的梯度",{"5":{"48":2}}],["计算其奇异值分解",{"5":{"87":2,"152":2}}],["计算转换为",{"5":{"87":1,"152":1}}],["计算旋转矩阵的四个元素并执行矩阵乘法",{"5":{"88":2}}],["计算角度数组",{"5":{"88":2}}],["计算query和key向量",{"5":{"88":2}}],["计算attention分数",{"5":{"88":2}}],["计算加速",{"5":{"89":2}}],["计算优化方面",{"5":{"89":2}}],["计算开销很小",{"5":{"89":2}}],["计算开销可能更大",{"5":{"91":2}}],["计算时间为",{"5":{"157":4}}],["计算时",{"5":{"92":2,"153":2}}],["计算一个软分布来表示每个位置的相关性",{"5":{"69":2}}],["计算一次性完成所有位置对的点积相似度",{"5":{"92":2,"153":2}}],["计算它们之间的相对旋转角度",{"5":{"90":2}}],["计算它们表示之间的余弦相似度",{"5":{"93":2}}],["计算第个分量的期望",{"5":{"145":1}}],["计算第组内所有头的注意力输出并拼接",{"5":{"93":1}}],["计算第",{"5":{"93":1,"145":1}}],["计算查询与键之间的相似度矩阵",{"5":{"95":2}}],["直线映射为直线",{"5":{"71":2}}],["直接影响训练的稳定性和学习效率",{"5":{"154":1}}],["直接影响模型对序列结构的建模能力",{"5":{"70":2}}],["直接控制新旧策略之间的分布差异",{"5":{"154":2}}],["直接使用top",{"5":{"159":2}}],["直接使用均方根对输入进行缩放归一化",{"5":{"146":2}}],["直接使用偏好数据优化策略",{"5":{"101":2}}],["直接方法",{"5":{"140":2}}],["直接对",{"5":{"144":1}}],["直接对求偏导数",{"5":{"144":1}}],["直接对梯度向量求",{"5":{"135":2}}],["直接对复合函数求导会导致计算量爆炸",{"5":{"44":2}}],["直接互信息",{"5":{"132":2}}],["直接贡献减去冗余重叠",{"5":{"132":2}}],["直接偏好优化",{"2":{"101":1},"5":{"101":3}}],["直接加权可能导致训练不稳定",{"5":{"101":2}}],["直接加权可能导致某些任务的梯度主导训练",{"5":{"70":2}}],["直接加到净输入上",{"5":{"44":1}}],["直接由正则化项对损失函数的影响分析",{"5":{"133":2}}],["直接由前向传播的计算顺序可得",{"5":{"45":2}}],["直接由定义2",{"5":{"46":2}}],["直接计算行列式可得上述方程",{"5":{"136":2}}],["直接计算无穷级数求和",{"5":{"136":2}}],["直接计算",{"5":{"47":2,"154":2}}],["直接计算交叉熵",{"5":{"61":2}}],["直接计算正弦和余弦函数是昂贵的",{"5":{"88":2}}],["直接求解特征方程通常是不切实际的",{"5":{"50":2}}],["直接优化kl散度",{"5":{"61":2}}],["直接继承自kl散度的非负性和熵的非负性",{"5":{"61":1}}],["直接连接",{"5":{"92":2,"153":2}}],["直接参与了位置",{"5":{"92":1,"153":1}}],["直接在",{"5":{"97":2}}],["直到策略收敛或达到预设的训练步数",{"5":{"155":2}}],["直到与边界相交",{"5":{"147":2}}],["直到梯度减小到阈值以下",{"5":{"147":2}}],["直到计算预算耗尽",{"5":{"138":2}}],["直到达到局部最低点",{"5":{"134":2}}],["直到",{"5":{"45":2}}],["直觉上",{"5":{"61":2,"88":2,"147":2}}],["直观地理解",{"5":{"155":2}}],["直观地说",{"5":{"148":2}}],["直观地讲",{"5":{"88":2}}],["直观解释",{"2":{"145":1},"5":{"145":1}}],["直观理解",{"5":{"134":6,"135":8}}],["直观上解释",{"5":{"149":2}}],["直观上",{"5":{"95":2,"96":2}}],["直积群的元素是个元素的元组",{"5":{"88":1}}],["直积群的元素是",{"5":{"88":1}}],["对对应行",{"5":{"159":2}}],["对输入",{"5":{"158":2}}],["对输出进行归一化",{"5":{"86":2,"151":2}}],["对未来的奖励给予几乎与即时奖励同等的重要性",{"5":{"156":2}}],["对话系统或任何需要模型生成响应的场景",{"5":{"156":2}}],["对策略参数",{"5":{"155":2}}],["对优化动力学的影响",{"2":{"149":1},"5":{"149":1}}],["对每一层的净输入进行标准化",{"5":{"148":2}}],["对每个任务的信息保留项加权求和",{"5":{"133":2}}],["对每个位置",{"5":{"69":1}}],["对每个位置的query和key向量应用rope旋转",{"5":{"88":2}}],["对每个通道内的所有空间位置和batch应用相同的归一化参数",{"5":{"41":2}}],["对每个",{"5":{"61":2}}],["对反向传播也不传递梯度",{"5":{"148":2}}],["对整个参数向量的梯度进行统一裁剪",{"5":{"147":2}}],["对整个小批量数据计算均值和方差",{"5":{"146":2}}],["对标准化后的值进行缩放和平移",{"5":{"146":2}}],["对正则化损失求梯度并设为零",{"5":{"144":2}}],["对进行svd",{"5":{"143":1}}],["对其进行奇异值分解",{"5":{"142":2}}],["对参数扰动不敏感",{"5":{"136":2}}],["对参数扰动更鲁棒",{"5":{"134":2,"135":2}}],["对初始条件的敏感性可用",{"5":{"136":2}}],["对初始条件敏感依赖",{"5":{"136":2}}],["对某个常数",{"5":{"136":2}}],["对从0到求和并利用",{"5":{"136":1}}],["对模型平均和集成学习更友好",{"5":{"135":2}}],["对共享参数的影响程度",{"5":{"133":1}}],["对信息瓶颈目标关于",{"5":{"133":1}}],["对信息瓶颈目标关于求变分导数",{"5":{"133":1}}],["对抗鲁棒性的信息论解释",{"5":{"133":2}}],["对抗信息瓶颈",{"5":{"133":2}}],["对比sgd的离散更新形式",{"5":{"149":2}}],["对比估计",{"5":{"133":2}}],["对比学习的理论局限",{"5":{"101":2}}],["对比学习中的infonce同样基于信息论",{"5":{"69":2}}],["对比学习中的infonce损失以及transformer中的注意力机制",{"5":{"69":2}}],["对比学习和注意力机制",{"5":{"69":2}}],["对比学习和注意力机制的统一数学工具",{"5":{"69":2}}],["对比学习",{"5":{"69":3,"70":2,"101":2}}],["对齐机制",{"5":{"101":2}}],["对齐程度",{"5":{"95":2}}],["对上述最优策略取对数",{"5":{"101":2}}],["对称正定矩阵",{"5":{"149":1}}],["对称矩阵",{"5":{"135":2}}],["对称的",{"5":{"70":1}}],["对称性大大简化了hessian矩阵的分析和计算",{"5":{"48":2}}],["对称性",{"5":{"90":2,"137":2}}],["对称性破缺",{"5":{"93":2}}],["对位置",{"5":{"69":1,"86":1,"92":3,"151":1,"153":3}}],["对位置编码的处理会保留或抑制不同频率的成分",{"5":{"71":2}}],["对梯度的压缩效应",{"5":{"148":1}}],["对梯度的范数进行约束",{"5":{"147":2}}],["对梯度稳定性的影响",{"5":{"148":2}}],["对梯度有双重影响",{"5":{"69":1}}],["对梯度向量逐元素限制在范围内",{"5":{"44":1}}],["对梯度向量逐元素限制在",{"5":{"44":1}}],["对该负样本的关注程度成正比",{"5":{"69":1}}],["对的形式",{"5":{"101":2}}],["对的梯度",{"5":{"69":3}}],["对的总影响是通过所有中间变量传导的",{"5":{"48":1}}],["对于相同规模的模型",{"5":{"159":2}}],["对于相同的信息保留量",{"5":{"143":2}}],["对于相同的逼近精度",{"5":{"71":2}}],["对于设计和训练高效的moe模型至关重要",{"5":{"159":2}}],["对于设计和实现高效的大规模训练系统至关重要",{"5":{"50":2}}],["对于权重低于阈值的专家",{"5":{"159":2}}],["对于软路由",{"5":{"158":2}}],["对于top",{"5":{"158":2,"159":2}}],["对于tanh",{"5":{"41":2,"42":2}}],["对于训练有效的moe模型至关重要",{"5":{"158":2}}],["对于掌握moe的训练动态和优化策略至关重要",{"5":{"158":2}}],["对于掌握语言模型的生成机制和优化原理至关重要",{"5":{"96":2}}],["对于拥有",{"5":{"158":2}}],["对于参数量在数十亿规模的moe模型",{"5":{"157":2}}],["对于参数化的概率模型",{"5":{"149":2}}],["对于all",{"5":{"157":2}}],["对于moe模型",{"5":{"157":2,"159":4}}],["对于使用adam优化器的训练",{"5":{"157":2}}],["对于fp32",{"5":{"157":2}}],["对于fp16",{"5":{"157":2}}],["对于在大规模训练场景中充分发挥moe架构的潜力至关重要",{"5":{"158":2}}],["对于在大规模硬件上训练和部署moe模型至关重要",{"5":{"157":2}}],["对于在大规模集群上高效部署moe模型至关重要",{"5":{"157":2}}],["对于在大规模分布式环境中高效训练和部署moe模型至关重要",{"5":{"157":2}}],["对于专家",{"5":{"157":2}}],["对于轨迹中出现的每一个",{"5":{"155":2}}],["对于轨迹上的每一个时间步",{"5":{"155":2}}],["对于优势为负的动作",{"5":{"154":2}}],["对于优势为正的动作",{"5":{"154":2}}],["对于那些能提高目标函数价值的动作",{"5":{"154":2}}],["对于layer",{"5":{"150":2}}],["对于反向传播过程中的梯度",{"5":{"150":2}}],["对于负对数似然损失",{"5":{"149":2}}],["对于xavier初始化的网络",{"5":{"148":2}}],["对于简单的凸优化问题",{"5":{"147":2}}],["对于简单的输入使用较少的头",{"5":{"93":2}}],["对于凸损失函数",{"5":{"147":2}}],["对于凸函数",{"5":{"147":2}}],["对于凸优化问题",{"5":{"147":2}}],["对于凸的",{"5":{"51":2}}],["对于批次输入的处理需要特别考虑",{"5":{"158":2}}],["对于批归一化层",{"5":{"147":2}}],["对于批量输入",{"5":{"44":2,"45":2}}],["对于靠近输入层的层",{"5":{"147":2}}],["对于靠近输出层的层",{"5":{"147":2}}],["对于自然语言处理中的transformer模型",{"5":{"146":2}}],["对于自回归模型",{"5":{"140":4}}],["对于自回归语言模型",{"5":{"139":2}}],["对于卷积神经网络",{"5":{"146":2,"148":2}}],["对于第层权重​",{"5":{"148":1}}],["对于第",{"5":{"145":2,"148":1}}],["对于无放缩版本的dropout",{"5":{"145":2}}],["对于预测概率接近1的类别",{"5":{"144":2}}],["对于张量",{"5":{"143":2}}],["对于目标权重矩阵",{"5":{"142":2}}],["对于现代大语言模型",{"5":{"140":2}}],["对于估计量",{"5":{"139":2}}],["对于某个小的",{"5":{"138":2}}],["对于某些",{"5":{"150":2}}],["对于某些函数类",{"5":{"71":2}}],["对于某些复杂函数",{"5":{"87":2,"152":2}}],["对于给定的上下文",{"5":{"140":2}}],["对于给定的真实分布",{"5":{"139":2}}],["对于给定的计算预算",{"5":{"138":2}}],["对于给定的参数",{"5":{"51":2}}],["对于合理规划模型训练",{"5":{"138":2}}],["对于取值空间大小为",{"5":{"137":1}}],["对于取值空间大小为的离散随机变量",{"5":{"137":1}}],["对于连续动作空间",{"5":{"154":2}}],["对于连续变量",{"5":{"137":4}}],["对于连续随机变量",{"5":{"96":4}}],["对于周期为",{"5":{"136":1}}],["对于周期为的周期轨道",{"5":{"136":1}}],["对于动量优化器",{"5":{"136":2}}],["对于动态系统",{"5":{"134":2}}],["对于前两层",{"5":{"135":2}}],["对于标量输入",{"5":{"145":2}}],["对于标量函数",{"5":{"135":2}}],["对于标准化的输入",{"5":{"87":2,"152":2}}],["对于从",{"5":{"135":2}}],["对于从理论层面把握注意力机制的工作原理",{"5":{"87":2,"152":2}}],["对于正定",{"5":{"134":2,"135":2}}],["对于正弦位置编码",{"5":{"88":2}}],["对于梯度all",{"5":{"157":2}}],["对于梯度范数",{"5":{"148":2}}],["对于梯度下降方法",{"5":{"135":2}}],["对于梯度下降系统",{"5":{"134":2,"136":4}}],["对于梯度流动系统",{"5":{"134":2}}],["对于充分小的学习率",{"5":{"134":2}}],["对于序列中的每个位置",{"5":{"157":2}}],["对于序列中位置",{"5":{"132":2}}],["对于序列长度和注意力维度",{"5":{"50":1}}],["对于序列长度",{"5":{"50":1,"95":1}}],["对于序列长度为",{"5":{"86":2,"95":2,"151":2}}],["对于序列长度较大的应用场景",{"5":{"95":1}}],["对于离散变量",{"5":{"137":2}}],["对于离散时间系统",{"5":{"135":2,"136":2}}],["对于离散动作空间",{"5":{"101":2,"154":2}}],["对于离散随机变量",{"5":{"96":5,"137":2,"139":2}}],["对于离散随机变量及其概率分布",{"5":{"96":1}}],["对于排名需求",{"5":{"70":2}}],["对于校准需求",{"5":{"70":2}}],["对于0",{"5":{"70":2}}],["对于平方损失",{"5":{"70":2}}],["对于交叉熵与",{"5":{"70":2}}],["对于交叉熵",{"5":{"70":8}}],["对于交叉熵损失",{"5":{"70":6,"97":4}}],["对于均方误差与",{"5":{"70":2}}],["对于均方误差",{"5":{"70":12,"97":6}}],["对于回归任务",{"5":{"70":2}}],["对于其他",{"5":{"69":2}}],["对于固定的",{"5":{"51":2,"61":1,"92":1,"153":1}}],["对于固定的符号模式",{"5":{"45":4}}],["对于固定的个事件",{"5":{"61":1}}],["对于固定的和",{"5":{"92":1,"153":1}}],["对于固定的查询位置",{"5":{"95":2}}],["对于激活函数",{"5":{"41":4,"42":2}}],["对于对称矩阵",{"5":{"41":2,"50":2,"135":2}}],["对于",{"5":{"41":2,"44":1,"45":1,"86":2,"89":1,"90":2,"91":8,"95":1,"96":1,"97":1,"133":1,"135":6,"137":3,"138":2,"142":2,"148":4,"150":2,"151":2,"155":2,"159":2}}],["对于线性网络",{"5":{"135":2}}],["对于线性变换",{"5":{"41":2}}],["对于线性层",{"5":{"41":2}}],["对于线性系统​",{"5":{"41":1}}],["对于线性系统",{"5":{"41":1}}],["对于线性不可分的数据集",{"5":{"47":2}}],["对于线性模型",{"5":{"51":4,"144":2}}],["对于线性回归模型",{"5":{"51":2}}],["对于网络中的一条前向路径",{"5":{"41":2}}],["对于视觉任务",{"5":{"41":2}}],["对于这种复杂的分布",{"5":{"71":2}}],["对于sigmoid函数",{"5":{"41":2}}],["对于sigmoid",{"5":{"41":2,"42":2}}],["对于sigmoid激活函数",{"5":{"41":2}}],["对于sigmoid和tanh激活函数",{"5":{"41":2}}],["对于softmax激活函数",{"5":{"71":2}}],["对于输入层附近的层",{"5":{"148":2}}],["对于输入向量",{"5":{"61":2,"69":2,"71":2}}],["对于输入​",{"5":{"45":1}}],["对于输入",{"5":{"45":1}}],["对于输入序列中的每个位置",{"5":{"93":2}}],["对于输出层附近的层",{"5":{"148":2}}],["对于输出层",{"5":{"44":2}}],["对于输出向量的第个元素",{"5":{"46":1}}],["对于输出向量的第",{"5":{"46":1}}],["对于输出投影",{"5":{"93":2}}],["对于输出投影阶段",{"5":{"93":2}}],["对于隐藏层",{"5":{"44":2}}],["对于常见的分类和回归任务",{"5":{"44":2}}],["对于常见的均方误差和交叉熵损失",{"5":{"44":2}}],["对于常见的的场景",{"5":{"93":1}}],["对于常见的",{"5":{"93":1}}],["对于层神经网络",{"5":{"137":1}}],["对于层网络",{"5":{"133":1}}],["对于层归一化",{"5":{"41":2}}],["对于层全连接网络",{"5":{"44":1,"45":1}}],["对于神经网络",{"5":{"44":2}}],["对于神经网络等非线性模型",{"5":{"51":2}}],["对于神经网络中的mse梯度",{"5":{"51":2}}],["对于神经网络损失函数",{"5":{"97":2}}],["对于按范数裁剪",{"5":{"44":2}}],["对于映射",{"5":{"45":2}}],["对于gpt",{"5":{"45":2}}],["对于异或等线性不可分的模式无能为力",{"5":{"46":2}}],["对于逻辑与运算",{"5":{"47":2}}],["对于逻辑或运算",{"5":{"47":2}}],["对于逻辑非运算",{"5":{"47":2}}],["对于超大规模模型",{"5":{"45":2}}],["对于超平面",{"5":{"47":2}}],["对于超长序列",{"5":{"89":2}}],["对于超长序列的位置编码",{"5":{"89":2}}],["对于超出​的序列",{"5":{"91":1}}],["对于超出",{"5":{"91":1}}],["对于分类任务",{"5":{"41":2,"44":1,"69":2,"70":2}}],["对于分析神经网络的能力边界和训练动态至关重要",{"5":{"47":2}}],["对于性质",{"5":{"47":6}}],["对于理解深度学习框架的工作原理至关重要",{"5":{"47":2}}],["对于多元函数的复合",{"5":{"48":2}}],["对于多分类问题",{"5":{"47":2}}],["对于多分类任务",{"5":{"61":2,"70":2}}],["对于函数",{"5":{"48":4}}],["对于每条轨迹中的每个时间步",{"5":{"155":2}}],["对于每一对人类偏好的回复",{"5":{"154":2}}],["对于每一行成立",{"5":{"87":1,"152":1}}],["对于每一行",{"5":{"87":1,"152":1}}],["对于每层",{"5":{"44":2}}],["对于每个采样得到的转移",{"5":{"155":2}}],["对于每个特征通道",{"5":{"150":2}}],["对于每个特征维度",{"5":{"146":2}}],["对于每个query",{"5":{"69":2,"70":2}}],["对于每个query块​",{"5":{"86":1,"151":1}}],["对于每个query块",{"5":{"86":1,"151":1}}],["对于每个操作节点",{"5":{"48":2}}],["对于每个位置",{"5":{"88":2,"96":2,"101":2}}],["对于每个测试样本",{"5":{"96":2}}],["对于每个样本",{"5":{"51":2,"146":2,"150":2}}],["对于每个key块​和value块​",{"5":{"86":1,"151":1}}],["对于每个key块",{"5":{"86":1,"151":1}}],["对于每个频率​",{"5":{"90":1}}],["对于每个频率",{"5":{"90":1}}],["对于每个",{"5":{"92":2,"140":2,"153":2}}],["对于每个分量乘积",{"5":{"95":2}}],["对于残差连接",{"5":{"41":2}}],["对于残差形式的问题",{"5":{"48":2}}],["对于二元语法模型",{"5":{"140":2}}],["对于二元随机变量",{"5":{"137":2}}],["对于二次函数",{"5":{"144":2}}],["对于二次型问题",{"5":{"134":2,"135":2,"136":2}}],["对于二次型损失函数",{"5":{"134":2}}],["对于二次可微函数",{"5":{"48":2}}],["对于二分类",{"5":{"70":2}}],["对于二分类问题",{"5":{"47":2}}],["对于二分类任务",{"5":{"61":2}}],["对于具有l",{"5":{"48":2}}],["对于不同的随机初始化",{"5":{"48":2}}],["对于不同的输出层配置",{"5":{"44":2}}],["对于不同的输入序列",{"5":{"91":2}}],["对于乘积​",{"5":{"48":1}}],["对于乘积",{"5":{"48":1}}],["对于确定性分布",{"5":{"96":2}}],["对于两个随机变量和",{"5":{"96":1}}],["对于两个随机变量",{"5":{"96":1}}],["对于两个离散概率分布和",{"5":{"96":1}}],["对于两个离散概率分布",{"5":{"96":1}}],["对于两个分布和",{"5":{"96":1}}],["对于两个分布",{"5":{"96":1}}],["对于两个单位向量",{"5":{"95":2}}],["对于深度学习的非凸优化",{"5":{"147":2}}],["对于深度学习模型",{"5":{"97":2}}],["对于深度卷积层",{"5":{"147":2}}],["对于深度神经网络训练轨迹",{"5":{"136":2}}],["对于深度神经网络",{"5":{"135":4}}],["对于深度为",{"5":{"133":1}}],["对于深度为的网络",{"5":{"133":1}}],["对于深层网络",{"5":{"41":2}}],["对于深层网络和长序列",{"5":{"50":2}}],["对于深入理解语言模型的工作原理和改进模型设计都有重要意义",{"5":{"96":2}}],["对于词汇表大小为的语言模型",{"5":{"96":1}}],["对于词汇表大小为",{"5":{"96":1}}],["对于任何满足",{"5":{"155":2}}],["对于任何整数",{"5":{"50":2}}],["对于任意标量",{"5":{"146":2}}],["对于任意实矩阵",{"5":{"143":2}}],["对于任意查询矩阵",{"5":{"141":1}}],["对于任意查询矩阵和键矩阵",{"5":{"141":1}}],["对于任意随机变量序列",{"5":{"140":2}}],["对于任意假设类",{"5":{"133":2}}],["对于任意置换",{"5":{"101":2}}],["对于任意向量",{"5":{"70":2}}],["对于任意",{"5":{"41":2,"91":2,"139":2,"148":2}}],["对于任意连续函数和任意",{"5":{"71":1}}],["对于任意连续函数",{"5":{"71":1}}],["对于任意输入和标量",{"5":{"140":1}}],["对于任意输入",{"5":{"45":2,"140":1,"141":2}}],["对于任意矩阵范数",{"5":{"135":2}}],["对于任意矩阵",{"5":{"50":2}}],["对于任意概率分布",{"5":{"61":2,"137":2}}],["对于任意位置和",{"5":{"87":1,"152":1}}],["对于任意位置",{"5":{"87":1,"91":2,"152":1}}],["对于任意平移量",{"5":{"88":2}}],["对于任意两个概率分布",{"5":{"137":1,"139":1}}],["对于任意两个概率分布和",{"5":{"137":1,"139":1}}],["对于任意两个分布和",{"5":{"61":1}}],["对于任意两个分布",{"5":{"61":1}}],["对于任意两个不同的位置和",{"5":{"91":1}}],["对于任意两个不同的位置",{"5":{"91":1}}],["对于任意距离",{"5":{"92":2,"153":2}}],["对于任务特定的场景",{"5":{"91":2}}],["对于方阵",{"5":{"50":2}}],["对于方阵投影",{"5":{"94":2}}],["对于矩阵参数",{"5":{"144":2}}],["对于矩阵和",{"5":{"50":1}}],["对于矩阵方程",{"5":{"50":2}}],["对于矩阵",{"5":{"50":1,"143":2}}],["对于大语言模型rlhf训练这类样本昂贵的场景尤为重要",{"5":{"154":2}}],["对于大语言模型中的全连接层",{"5":{"50":2}}],["对于大多数语言建模任务",{"5":{"138":2}}],["对于大型矩阵",{"5":{"50":2}}],["对于大型模型",{"5":{"89":2}}],["对于新的测试样本",{"5":{"51":2}}],["对于非凸优化",{"5":{"147":2}}],["对于非凸函数",{"5":{"136":2}}],["对于非凸的mse损失",{"5":{"51":2}}],["对于非线性系统",{"5":{"134":2}}],["对于非对称矩阵",{"5":{"41":2}}],["对于非方阵投影",{"5":{"94":2}}],["对于非单位向量",{"5":{"95":2}}],["对于概率接近1的事件",{"5":{"61":2}}],["对于概率空间中发生概率为的事件",{"5":{"61":1}}],["对于概率空间中发生概率为",{"5":{"61":1}}],["对于包含个样本的数据集",{"5":{"61":1}}],["对于包含",{"5":{"61":1}}],["对于列和也为1的双随机矩阵",{"5":{"87":2,"152":2}}],["对于一个参数化概率模型",{"5":{"139":2}}],["对于一个批次",{"5":{"50":2}}],["对于一维离散系统",{"5":{"136":2}}],["对于一般非凸函数",{"5":{"136":2}}],["对于一般的行随机矩阵",{"5":{"87":2,"152":2}}],["对于所有成立",{"5":{"87":1,"140":1,"152":1}}],["对于所有特征值成立",{"5":{"87":2,"152":2}}],["对于所有",{"5":{"87":3,"90":2,"92":1,"95":1,"140":1,"144":2,"152":3,"153":1,"154":2}}],["对于注意力机制",{"5":{"69":2}}],["对于注意力矩阵",{"5":{"87":4,"152":4}}],["对于注意力矩阵的幂",{"5":{"87":2,"152":2}}],["对于注意力计算阶段",{"5":{"93":2}}],["对于更一般的注意力矩阵",{"5":{"87":2,"152":2}}],["对于秩为的矩阵",{"5":{"87":1,"152":1}}],["对于秩为",{"5":{"87":1,"152":1}}],["对于位置",{"5":{"88":3,"91":2,"92":3,"153":3}}],["对于位置和维度索引",{"5":{"88":1}}],["对于位置信息不那么重要的任务",{"5":{"89":2}}],["对于位置编码的分析",{"5":{"90":2}}],["对于位置依赖",{"5":{"92":2,"153":2}}],["对于位置接近序列开头",{"5":{"92":1,"153":1}}],["对于完整的d维向量",{"5":{"88":2}}],["对于实对称矩阵",{"5":{"50":2}}],["对于实际应用中的任意维度d",{"5":{"88":2}}],["对于把握位置编码设计的本质至关重要",{"5":{"88":2}}],["对于relu激活",{"5":{"148":4}}],["对于relu激活函数",{"5":{"41":2}}],["对于rope编码",{"5":{"88":2}}],["对于向量输入",{"5":{"145":2}}],["对于向量值函数",{"5":{"44":2}}],["对于向量",{"5":{"88":2}}],["对于典型配置",{"5":{"86":2,"151":2}}],["对于典型的位置编码",{"5":{"89":2}}],["对于​的行",{"5":{"89":1}}],["对于维随机向量",{"5":{"96":1}}],["对于维度配对",{"5":{"90":2}}],["对于维度为的矩阵",{"5":{"94":1}}],["对于维度为",{"5":{"94":1}}],["对于小的旋转角度",{"5":{"90":2}}],["对于高频专家",{"5":{"158":2}}],["对于高频成分",{"5":{"90":2}}],["对于高维系统",{"5":{"136":2}}],["对于高维度",{"5":{"90":2,"91":6}}],["对于高斯编码器",{"5":{"133":2}}],["对于低频专家",{"5":{"158":2}}],["对于低频成分",{"5":{"90":2}}],["对于低维度",{"5":{"90":2,"91":6}}],["对于最大的",{"5":{"69":1}}],["对于最大的​",{"5":{"69":1}}],["对于最高频率成分",{"5":{"90":2}}],["对于最低频率成分",{"5":{"90":2}}],["对于互不谐波的多个频率",{"5":{"90":2}}],["对于需要精细位置区分的任务",{"5":{"89":2}}],["对于需要长度外推的任务",{"5":{"91":2}}],["对于需要建模长程依赖的任务",{"5":{"92":2,"153":2}}],["对于语言模型",{"5":{"41":2,"156":4}}],["对于语法依赖",{"5":{"92":2,"153":2}}],["对于语义依赖",{"5":{"92":2,"153":2}}],["对于近距离的位置对",{"5":{"92":2,"153":2}}],["对于远距离的位置对",{"5":{"92":2,"153":2}}],["对于query投影",{"5":{"93":2}}],["对于query",{"5":{"93":2,"94":2}}],["对于长度为",{"5":{"89":1,"92":1,"140":1,"153":1}}],["对于长度为的输入序列",{"5":{"89":1}}],["对于长度为的序列",{"5":{"92":1,"140":1,"153":1}}],["对于长序列场景",{"5":{"93":2}}],["对于头数较多的模型",{"5":{"93":2}}],["对于复合函数",{"5":{"45":2}}],["对于复杂的输入使用更多的头",{"5":{"93":2}}],["对于的序列长度",{"5":{"95":1}}],["对于可微函数",{"5":{"97":2}}],["对于随机初始化的神经网络",{"5":{"97":2}}],["对于过参数化的神经网络",{"5":{"97":2}}],["对于强凸二次型问题",{"5":{"136":4}}],["对于强凸函数",{"5":{"48":2}}],["对于强凸和利普希茨连续的损失函数",{"5":{"97":1}}],["对所有采样轨迹中的每个时间步",{"5":{"155":2}}],["对所有输入一视同仁",{"5":{"132":2}}],["对所有",{"5":{"61":5,"71":1,"96":1,"97":1,"132":1,"134":6,"135":6,"136":1,"148":1,"156":2,"158":8,"159":4}}],["对所有成立",{"5":{"71":1,"148":1}}],["对所有层",{"5":{"44":1}}],["对所有向量",{"5":{"50":1}}],["对所有可能的训练集",{"5":{"51":2}}],["对所有位置都保持不变",{"5":{"88":1}}],["对",{"5":{"44":3,"45":2,"48":1,"61":2,"69":3,"95":1,"135":2,"136":3,"143":5,"155":2,"157":4}}],["对加法和乘法封闭",{"5":{"71":2}}],["对数softmax的计算稳定性",{"5":{"140":2}}],["对数变换是单调递增函数",{"5":{"139":2}}],["对数据分布变化更不敏感",{"5":{"135":2}}],["对数函数",{"5":{"70":1}}],["对数似然与交叉熵损失的等价性为实际训练提供了坚实的理论基础",{"5":{"140":2}}],["对数似然与交叉熵的等价性",{"5":{"140":2}}],["对数似然与损失函数",{"2":{"140":1},"5":{"140":1}}],["对数似然的一阶条件",{"5":{"149":2}}],["对数似然的梯度为",{"5":{"139":2}}],["对数似然的单调性",{"5":{"139":2}}],["对数似然为",{"5":{"61":2,"71":2,"139":2}}],["对数似然可以分解为序列中每个位置的条件对数概率之和",{"5":{"96":2}}],["对数似然函数的hessian矩阵的负期望等于fisher信息矩阵",{"5":{"149":2}}],["对数似然函数的梯度可以分解为各位置梯度的和",{"5":{"140":2}}],["对数似然函数定义为",{"5":{"140":2}}],["对数似然函数为",{"5":{"96":2,"139":2}}],["对数似然函数",{"5":{"61":1,"140":2}}],["对数几率",{"5":{"71":2,"96":2}}],["对数运算的引入使得kl散度具有概率解释上的优势",{"5":{"61":2}}],["对数频率轴上的等间隔对应于几何级数的频率间隔",{"5":{"90":2}}],["对应top",{"5":{"159":2}}],["对应硬top",{"5":{"157":2}}],["对应硬路由和弱混合行为",{"5":{"41":2}}],["对应被选中位置为1",{"5":{"157":2}}],["对应扩散项",{"5":{"149":2}}],["对应更重要的奇异方向",{"5":{"142":1}}],["对应到大语言模型中",{"5":{"138":2}}],["对应谱半径",{"5":{"134":2}}],["对应的最优价值函数记为",{"5":{"156":2}}],["对应的感受野大小为",{"5":{"150":2}}],["对应的噪声特征方向具有正的特征值",{"5":{"149":2}}],["对应的次梯度",{"5":{"147":2}}],["对应的梯度为",{"5":{"147":2}}],["对应的状态更新为",{"5":{"135":2}}],["对应的子词索引集合为",{"5":{"101":2}}],["对应的预测为",{"5":{"51":1}}],["对应的核函数为",{"5":{"86":6,"151":6}}],["对应的旋转矩阵为",{"5":{"88":2}}],["对应的旋转角度",{"5":{"88":2}}],["对应的时域序列会受到影响",{"5":{"90":2}}],["对应的角频率为",{"5":{"90":1}}],["对应的编码向量​和​是不同的",{"5":{"91":1}}],["对应的编码向量",{"5":{"91":1}}],["对应hardmax",{"5":{"71":2}}],["对应输出神经元的索引",{"5":{"46":1}}],["对应输入特征的索引",{"5":{"46":1}}],["对应",{"5":{"48":4,"51":1,"135":4,"159":2}}],["对应于保留信息与压缩程度的权衡",{"5":{"133":1}}],["对应于特征值1",{"5":{"87":2,"152":2}}],["对应于最大的非平凡特征值",{"5":{"87":2,"152":2}}],["对应于的位置",{"5":{"89":1}}],["对应于不同的编码配置",{"5":{"89":2}}],["对应于",{"5":{"89":1}}],["对应不同的频率成分",{"5":{"90":1}}],["对应圆群上的点",{"5":{"90":1}}],["对应唯一的相位向量",{"5":{"90":1}}],["对应周期10000",{"5":{"90":1}}],["对应周期1",{"5":{"90":1}}],["对应长周期",{"5":{"91":2}}],["对应短周期",{"5":{"91":2}}],["对应低频旋转",{"5":{"88":2}}],["对应低频振动",{"5":{"91":2}}],["对应高频旋转",{"5":{"88":2}}],["对应高频振动",{"5":{"91":2}}],["对应维度的分量同时变大",{"5":{"95":2}}],["对注意力输出进行非线性变换",{"5":{"71":2}}],["对常数有",{"5":{"96":1}}],["对常数",{"5":{"96":1}}],["对任意连续函数",{"5":{"136":2}}],["对任意常数",{"5":{"47":2,"96":1}}],["对任意常数和随机变量",{"5":{"96":1}}],["对任意",{"5":{"96":1,"134":2}}],["对异常值的敏感性",{"2":{"51":1},"5":{"51":3}}],["对异常值的敏感性等",{"5":{"51":2}}],["对这个优化问题求导并令导数为零",{"5":{"51":2}}],["对第",{"5":{"61":2}}],["对角矩阵的对角元素称为矩阵的奇异值",{"5":{"143":1}}],["对角矩阵的元素可以学习",{"5":{"142":1}}],["对角矩阵",{"5":{"142":1,"143":1}}],["对角元素为各频率成分的旋转因子​",{"5":{"90":1}}],["对角元素为各频率成分的旋转因子",{"5":{"90":1}}],["对角线上",{"5":{"45":1}}],["对角线元素接近0",{"5":{"95":1}}],["对角线元素保持在较大的值",{"5":{"95":1}}],["对角线元素",{"5":{"95":2,"149":2}}],["对偶性意味着",{"5":{"90":2}}],["对矩阵的每一行进行归一化",{"5":{"95":1}}],["对键位置",{"5":{"95":1}}],["对稀疏特征使用较大的学习率",{"5":{"97":2}}],["feigenbaum",{"5":{"136":4}}],["features",{"5":{"132":4}}],["feature",{"5":{"70":2,"86":2,"151":2}}],["feedback",{"5":{"101":2,"154":2}}],["feed",{"5":{"71":2}}],["focal",{"5":{"70":2}}],["forcing",{"5":{"140":4}}],["form",{"5":{"136":2}}],["forall",{"5":{"42":2,"141":2,"144":2}}],["forward",{"5":{"44":2,"45":4,"46":2,"71":2,"132":2}}],["fourier",{"5":{"90":4}}],["f",{"5":{"42":3,"46":1,"132":2,"133":2,"137":10,"142":5,"144":3}}],["f^",{"5":{"46":6}}],["fim",{"5":{"149":2}}],["fixed",{"5":{"134":2}}],["fill",{"5":{"132":2}}],["fisher",{"5":{"70":22,"97":2,"133":4,"135":4,"137":10,"149":2}}],["fisher信息的等价形式",{"5":{"139":2}}],["fisher信息矩阵具有若干重要性质",{"5":{"149":2}}],["fisher信息矩阵",{"2":{"139":1},"5":{"139":3}}],["fisher信息矩阵定义为score函数的协方差矩阵",{"5":{"139":2}}],["fisher信息矩阵定义为",{"5":{"71":2,"149":2}}],["fisher信息矩阵定义了概率流形上的黎曼度量",{"5":{"71":2}}],["fisher信息度量",{"5":{"70":2,"71":2}}],["fine",{"5":{"50":2,"154":2}}],["field",{"5":{"92":2,"145":2,"153":2}}],["frobenius",{"5":{"144":1}}],["frobenius范数与l2正则化的关系",{"5":{"144":2}}],["frobenius范数定义为",{"5":{"144":2}}],["frobenius范数正则化也是常用的选择",{"5":{"144":2}}],["frobenius范数正则化",{"2":{"144":1},"5":{"144":1}}],["frobenius范数是最直观的矩阵范数",{"5":{"50":2}}],["frobenius范数常用于权重衰减",{"5":{"50":2}}],["frobenius范数因其可导性好而最常用",{"5":{"50":2}}],["frac",{"5":{"42":72,"46":2,"61":48,"69":2,"97":8,"141":12,"142":6}}],["frank",{"5":{"47":2}}],["frequency",{"5":{"90":2,"91":4,"136":2,"137":2}}],["fp16混合精度",{"5":{"157":2}}],["fp16",{"5":{"50":2,"93":2,"97":10,"157":4}}],["fp16或bf16",{"5":{"89":2}}],["fp32",{"5":{"50":2,"97":4,"157":2}}],["func",{"5":{"148":4}}],["function梯度估计的变分梯度",{"5":{"159":2}}],["function",{"5":{"45":4,"47":2,"51":2,"61":2,"93":2,"96":6,"101":2,"155":2,"156":8}}],["fused",{"5":{"61":4}}],["fully",{"5":{"92":2,"153":2}}],["flat",{"5":{"134":2}}],["flash注意力的数学原理",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["flash注意力的复杂度与性能分析",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["flash注意力的时间复杂度与标准注意力相同",{"5":{"86":2,"151":2}}],["flash注意力的内存访问包括",{"5":{"86":2,"151":2}}],["flash注意力的io优化收益更大",{"5":{"86":2,"151":2}}],["flash注意力的后续版本进一步优化了算法实现和硬件利用率",{"5":{"86":2,"151":2}}],["flash注意力的各个版本都遵循相同的核心理论",{"5":{"86":2,"151":2}}],["flash注意力",{"2":{"86":1,"151":1},"5":{"86":5,"151":5}}],["flash注意力将query",{"5":{"86":2,"151":2}}],["flash注意力相比标准实现可以获得2",{"5":{"86":2,"151":2}}],["flash注意力约为字节",{"5":{"86":1,"151":1}}],["flash注意力约为",{"5":{"86":1,"151":1}}],["flash",{"5":{"86":8,"92":2,"93":2,"151":8,"153":2}}],["floquet",{"5":{"136":10}}],["flow",{"5":{"41":2,"50":4,"134":2}}],["floating",{"5":{"45":2}}],["flops",{"5":{"46":4,"138":4}}],["far",{"5":{"156":2}}],["factor",{"5":{"156":2}}],["factored",{"5":{"48":2,"135":2}}],["fac",{"5":{"48":2,"135":4}}],["fac基于层间独立性假设",{"5":{"48":2}}],["fast",{"5":{"90":2}}],["ffn",{"5":{"46":2,"71":2}}],["fft",{"5":{"90":2}}],["也值得深入分析",{"5":{"154":1}}],["也会倾向于找到具有良好泛化性质的解",{"5":{"147":2}}],["也等于",{"5":{"137":1}}],["也等于和共享的信息量",{"5":{"137":1}}],["也就是更平坦的极小值",{"5":{"149":2}}],["也就是最小化mse",{"5":{"51":2}}],["也就没有信息量",{"5":{"137":2}}],["也帮助我们从数据压缩和信息流动的角度理解模型的学习机制",{"5":{"137":2}}],["也称为concrete分布",{"5":{"157":2}}],["也称为策略的价值",{"5":{"155":2}}],["也称为权重衰减",{"5":{"144":2}}],["也称为平衡点或驻点",{"5":{"134":2}}],["也称为逻辑函数",{"5":{"42":2}}],["也称为隐藏表示或隐藏激活",{"5":{"46":2}}],["也称为多项分布",{"5":{"96":2}}],["也称为正态分布",{"5":{"96":2}}],["也称为信息含量",{"5":{"61":2}}],["也称为相对熵",{"5":{"61":2,"137":2}}],["也影响梯度的传播特性",{"5":{"41":2}}],["也为我们理解模型的泛化能力和优化过程提供了理论框架",{"5":{"139":2}}],["也为我们理解模型行为",{"5":{"137":2}}],["也为设计更稳定",{"5":{"136":2}}],["也为设计更稳定的网络架构",{"5":{"135":2}}],["也为设计更高效",{"5":{"132":2}}],["也为设计新的训练策略和分析训练不稳定性提供了理论基础",{"5":{"134":2}}],["也为设计新的正则化方法和训练策略提供了重要的理论指导",{"5":{"133":2}}],["也为设计新的激活函数和理解训练动态提供了理论指导",{"5":{"71":2}}],["也为设计新的位置编码方案提供了理论基础",{"5":{"89":2}}],["也为改进模型训练策略提供了理论基础",{"5":{"101":2}}],["也为深入理解大语言模型的数学基础提供了整体视角",{"5":{"70":2}}],["也为在复杂模型中选择合适的损失函数提供理论指导",{"5":{"70":2}}],["也为在实际机器学习项目中做出明智的损失函数选择提供了理论工具",{"5":{"51":2}}],["也为未来新型激活函数的发展提供了指导",{"5":{"41":2}}],["也为分析和改进现有优化算法提供了理论工具",{"5":{"42":2}}],["也为分析神经网络的信息流和表示学习提供了数学工具",{"5":{"71":2}}],["也为分析位置编码的性质",{"5":{"90":2}}],["也为分析注意力机制如何利用位置信息提供了视觉基础",{"5":{"91":2}}],["也为理解transformer架构中非线性组件的设计提供了数学基础",{"5":{"71":2}}],["也为理解线性回归的闭式解提供了几何直觉",{"5":{"51":2}}],["也为选择和理解损失函数提供了坚实的统计学基础",{"5":{"61":2}}],["也为下一节学习可学习位置编码的矩阵性质",{"5":{"90":2}}],["也非常小",{"5":{"41":1}}],["也使得sigmoid在某些特定场景下仍有应用价值",{"5":{"42":2}}],["也是近年深度学习优化理论的重要发现之一",{"5":{"149":2}}],["也是参数更新的直接输入",{"5":{"44":2}}],["也是",{"5":{"45":1,"92":2,"153":2}}],["也是理解反向传播算法的必要前提",{"5":{"46":2}}],["也是证明神经网络泛化性质的关键工具",{"5":{"50":2}}],["也是为什么许多研究致力于开发高效的注意力变体",{"5":{"50":2}}],["也是设计新注意力变体的关键洞察",{"5":{"87":2,"152":2}}],["也是其主要挑战的来源",{"5":{"89":2}}],["也是最经典的编码方法之一",{"5":{"91":2}}],["也采用了类似的思想",{"5":{"96":2}}],["也指导着我们理解和解释模型训练过程中的各种现象",{"5":{"96":2}}],["也决定了它在网络中的具体作用方式",{"5":{"50":2}}],["也适用于反向传播",{"5":{"61":2}}],["也揭示了rope与其他位置编码方法的本质区别",{"5":{"88":2}}],["也减少了出错的可能性",{"5":{"88":2}}],["也只是落在实数轴的某个位置上",{"5":{"88":1}}],["也编码了内容信息",{"5":{"92":2,"153":2}}],["也可以看作是在每个时间步",{"5":{"156":2}}],["也可以是更复杂的注意力机制或特征重排操作",{"5":{"150":2}}],["也可以尝试结合两种编码",{"5":{"89":2}}],["也可以使用不同的学习率",{"5":{"89":2}}],["也可以与内容投影同时计算",{"5":{"91":2}}],["也可能正交",{"5":{"93":2}}],["也有助于提高分布式训练的效率",{"5":{"157":2}}],["也有消极的一面",{"5":{"136":2}}],["也有明确的数学动机",{"5":{"91":2}}],["也有其",{"5":{"95":2}}],["也不会被压缩",{"5":{"148":2}}],["也不会太大导致饱和",{"5":{"95":2}}],["也不会太大",{"5":{"95":2}}],["也不影响最优参数的位置",{"5":{"51":2}}],["也接近0",{"5":{"95":1}}],["也直接影响最终模型的性能和泛化能力",{"5":{"97":2}}],["求方差",{"5":{"146":1}}],["求均值",{"5":{"146":1}}],["求变分导数",{"5":{"133":1}}],["求导得",{"5":{"135":2}}],["求导",{"5":{"42":2,"70":2}}],["求偏导数",{"5":{"144":1}}],["求偏导",{"5":{"61":2}}],["求和轴",{"5":{"146":1}}],["求和",{"5":{"146":2}}],["求和后仍非负",{"5":{"137":2}}],["求和并利用",{"5":{"136":1}}],["求和过程中相邻项相消",{"5":{"136":2}}],["求和即可",{"5":{"51":2}}],["求和简化为对单个非零项的计算",{"5":{"61":2}}],["实部为零",{"5":{"136":2}}],["实部序列和虚部序列都是振幅为1的正弦波",{"5":{"90":1}}],["实部序列",{"5":{"90":1}}],["实际应用中通常采用更复杂的门控设计",{"5":{"159":2}}],["实际应用中的考虑",{"2":{"139":1},"5":{"139":1}}],["实际应用中的行为控制",{"2":{"136":1},"5":{"136":1}}],["实际实现中通常采用",{"5":{"159":4}}],["实际实现中通常使用straight",{"5":{"157":2}}],["实际分配频率的均衡确保了计算资源的均匀利用",{"5":{"158":2}}],["实际网络中的权重矩阵往往具有病态的条件数",{"5":{"148":2}}],["实际训练中的资源分配策略",{"5":{"138":1}}],["实际训练中的资源分配策略需要综合考虑多个因素",{"5":{"138":1}}],["实际意义",{"5":{"134":2,"135":2}}],["实际中通常使用更稳定的变体",{"5":{"158":2}}],["实际中",{"5":{"132":2}}],["实际参与传播的神经元数量约为​​",{"5":{"41":1}}],["实际参与传播的神经元数量约为",{"5":{"41":1}}],["实际显著",{"5":{"96":2}}],["实际上就是一种温度调节",{"5":{"70":1}}],["实际上是概率信息瓶颈的特例",{"5":{"133":2}}],["实际上是对数",{"5":{"70":2}}],["实际上是一个巨大的查找矩阵",{"5":{"50":2}}],["实际上是在自适应地构造基函数集合",{"5":{"93":2}}],["实际上是在",{"5":{"94":1}}],["实际上在最大化序列不同位置之间的互信息",{"5":{"69":2}}],["实际上都共享同一个数学骨架",{"5":{"69":2}}],["实际上等于",{"5":{"61":2,"89":2}}],["实际上",{"5":{"61":2,"86":2,"89":2,"148":2,"150":2,"151":2}}],["实际上有更强的界",{"5":{"87":2,"152":2}}],["实际上定义了输入空间的一种特定的分块对角结构",{"5":{"93":2}}],["实际性能测试表明",{"5":{"86":2,"151":2}}],["实际观察到的秩通常仍然远小于",{"5":{"87":2,"152":2}}],["实际的神经网络由大量神经元组成",{"5":{"46":2}}],["实际的",{"5":{"87":2,"152":2}}],["实际的上界为​",{"5":{"89":1}}],["实际的上界为",{"5":{"89":1}}],["实际是​",{"5":{"87":1,"152":1}}],["实际是",{"5":{"87":1,"152":1}}],["实践应用与挑战",{"5":{"133":2}}],["实践意义",{"5":{"69":2}}],["实践中的信息瓶颈优化",{"2":{"133":1},"5":{"133":1}}],["实践中温度参数的选择",{"5":{"69":1}}],["实践中常用的策略是从较小的值",{"5":{"41":2}}],["实践中",{"5":{"51":2,"89":6,"90":2,"93":2,"97":2,"154":2,"157":2,"158":2,"159":2}}],["实践中由于",{"5":{"89":2}}],["实践中通常将dropout放在batch",{"5":{"145":2}}],["实践中通常需要仔细的学习率调度",{"5":{"41":2}}],["实践中通常使用",{"5":{"97":2}}],["实践表明深度学习模型通常能够找到性能不错的解",{"5":{"48":2}}],["实践表明",{"5":{"97":2,"150":2}}],["实数",{"5":{"50":2}}],["实数编码向量可以通过取复数编码的实部和虚部得到",{"5":{"90":2}}],["实数编码的点积与复数编码的内积直接相关",{"5":{"90":2}}],["实现高效重叠需要仔细设计通信和计算的执行顺序",{"5":{"157":2}}],["实现方式",{"5":{"145":1}}],["实现降维目的",{"5":{"143":2}}],["实现多任务的信息共享",{"5":{"133":2}}],["实现门控注意力机制",{"5":{"132":2}}],["实现信息压缩",{"5":{"133":2}}],["实现信息流的自适应调节",{"5":{"132":2}}],["实现信息流的动态调节",{"5":{"132":2}}],["实现信息效率最大化",{"5":{"132":2}}],["实现更精细的信息过滤",{"5":{"132":2}}],["实现模型并行",{"5":{"45":2}}],["实现建议",{"5":{"61":2}}],["实现了对每个样本的独立归一化",{"5":{"146":2}}],["实现了对每个特征通道的独立归一化",{"5":{"146":2}}],["实现了更高灵活性的分解",{"5":{"143":2}}],["实现了真正的自回归建模而不依赖马尔可夫假设",{"5":{"140":2}}],["实现了真正的全局信息传递",{"5":{"132":2}}],["实现了真正的并行计算",{"5":{"91":2}}],["实现了从原始分数到信息选择概率的转换",{"5":{"132":2}}],["实现了从被动接受信息到主动选择和聚焦信息的转变",{"5":{"132":2}}],["实现了信息传递效率的质的飞跃",{"5":{"132":2}}],["实现了线性复杂度",{"5":{"86":2,"151":2}}],["实现了有效的正则化",{"5":{"87":2,"152":2}}],["实现了相对位置不变性",{"5":{"91":2}}],["实现了全局信息的并行建模",{"5":{"95":2}}],["实现的计算",{"5":{"87":1,"152":1}}],["实现",{"5":{"87":1,"96":1,"152":1}}],["实现上的简洁性是第二个因素",{"5":{"88":1}}],["实现上的简洁性",{"5":{"88":1}}],["实现长度外推",{"5":{"91":2}}],["实现简单",{"5":{"91":2,"154":2}}],["实现相对复杂",{"5":{"91":2,"154":2}}],["实现复杂的长程依赖建模",{"5":{"92":2,"153":2}}],["实证分析",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["实证分析注意力矩阵的奇异值分布可以揭示其低秩结构的实际表现",{"5":{"87":2,"152":2}}],["实证分析表明",{"5":{"92":2,"153":2}}],["实证研究表明",{"5":{"87":2,"92":2,"93":2,"142":2,"147":4,"149":2,"152":2,"153":2}}],["实验验证",{"5":{"148":2}}],["实验表明",{"5":{"41":2,"145":2,"146":2,"147":2,"148":2,"150":2}}],["实验观察表明",{"5":{"87":2,"89":2,"93":2,"135":2,"152":2}}],["实验结果表明",{"5":{"91":2}}],["实体与其属性的关联",{"5":{"92":2,"153":2}}],["如足够宽的神经网络",{"5":{"159":2}}],["如64或128",{"5":{"159":2}}],["如引入",{"5":{"158":2}}],["如交叉熵损失",{"5":{"158":2,"159":2}}],["如交叉熵损失函数",{"5":{"51":2}}],["如infiniband",{"5":{"157":2}}],["如infonce",{"5":{"71":2}}],["如价值迭代和q",{"5":{"156":2}}],["如回报的平均值",{"5":{"155":2}}],["如学习率足够小",{"5":{"149":2}}],["如第",{"5":{"148":2}}],["如第层",{"5":{"148":2}}],["如第一层编码器",{"5":{"92":2,"153":2}}],["如深度学习",{"5":{"147":2}}],["如深层神经网络",{"5":{"51":2}}],["如深层神经网络在有限数据上训练",{"5":{"51":2}}],["如均值",{"5":{"147":2}}],["如均匀关注所有位置",{"5":{"87":2,"152":2}}],["如下",{"5":{"146":2}}],["如在线学习",{"5":{"146":2}}],["如在可学习编码上叠加正弦编码",{"5":{"89":2}}],["如彩色图像的三个通道",{"5":{"143":2}}],["如vgg",{"5":{"142":2}}],["如导数",{"5":{"138":2}}],["如水结冰",{"5":{"138":2}}],["如特定语言的复杂语法",{"5":{"138":2}}],["如经过严格清洗的网页文本",{"5":{"138":2}}],["如推理",{"5":{"138":4}}],["如熵",{"5":{"137":2}}],["如余弦相似度或内积",{"5":{"137":2}}],["如余弦退火",{"5":{"136":2}}],["如余弦退火或循环学习率",{"5":{"136":2}}],["如重采样",{"5":{"137":2}}],["如1",{"5":{"136":2}}],["如100k+",{"5":{"88":2}}],["如全连接层或卷积层",{"5":{"132":2}}],["如对异常值敏感",{"5":{"101":2}}],["如对底层使用较小学习率",{"5":{"97":2}}],["如对比学习损失",{"5":{"101":2}}],["如去噪自编码",{"5":{"101":2}}],["如几何分布",{"5":{"101":2}}],["如几百到几千",{"5":{"90":2}}],["如whole",{"5":{"101":2}}],["如warmup",{"5":{"41":2}}],["如warmup或更高的学习率",{"5":{"89":2}}],["如正交性",{"5":{"70":2}}],["如正弦余弦编码或rope",{"5":{"101":2}}],["如正弦余弦编码的频率分解",{"5":{"71":2}}],["如正弦余弦编码",{"5":{"71":2}}],["如情感分析",{"5":{"89":2,"101":2}}],["如squad的跨度预测",{"5":{"101":2}}],["如sigmoid和tanh的线性区",{"5":{"148":2}}],["如sigmoid",{"5":{"71":2}}],["如softmax归一化",{"5":{"86":2,"151":2}}],["如",{"5":{"50":2,"61":2,"69":2,"70":8,"86":1,"87":2,"88":4,"89":6,"90":4,"91":3,"92":6,"93":1,"94":2,"95":2,"96":4,"97":12,"133":2,"136":6,"145":4,"146":2,"147":6,"151":1,"152":2,"153":6}}],["如果设置",{"5":{"159":2}}],["如果训练不当",{"5":{"159":2}}],["如果训练数据中的位置分布不均衡",{"5":{"89":2}}],["如果训练数据支持这种平滑性假设",{"5":{"89":2}}],["如果每个专家",{"5":{"159":2}}],["如果每一层都进行低秩近似",{"5":{"142":2}}],["如果每一层的激活都处于饱和状态",{"5":{"41":2,"42":2}}],["如果能够根据输入内容动态地决定应该使用哪些参数",{"5":{"159":2}}],["如果专家分配概率的方差很大",{"5":{"159":2}}],["如果专家",{"5":{"158":2}}],["如果专家的专业得分高但没有被激活",{"5":{"158":2}}],["如果门控网络学习到一组参数使得对于大多数输入",{"5":{"158":2}}],["如果门控网络将大多数输入路由到少数专家",{"5":{"158":2}}],["如果路由决策过于集中",{"5":{"157":2}}],["如果奖励是有界的",{"5":{"156":2}}],["如果优势函数为负",{"5":{"155":2}}],["如果优势函数为正",{"5":{"155":2}}],["如果它带来了负向的回报",{"5":{"155":2}}],["如果它最终带来了正向的回报",{"5":{"155":2}}],["如果kl散度太小",{"5":{"154":2}}],["如果策略比率偏离1太远",{"5":{"154":2}}],["如果目标函数",{"5":{"150":2}}],["如果一个堆叠的网络层能够学习到恒等映射",{"5":{"150":2}}],["如果一组函数满足以下条件",{"5":{"71":2}}],["如果噪声是各向同性的",{"5":{"149":2}}],["如果噪声是拉普拉斯分布的",{"5":{"70":2}}],["如果激活值为正",{"5":{"148":2}}],["如果激活函数的导数接近零",{"5":{"44":2}}],["如果谱半径",{"5":{"148":2}}],["如果学习率过小",{"5":{"155":2}}],["如果学习率过大",{"5":{"97":2,"155":2}}],["如果学习率",{"5":{"147":2}}],["如果忽略仿射变换参数",{"5":{"146":1}}],["如果忽略仿射变换参数和",{"5":{"146":1}}],["如果将模型规模扩大为隐藏维度",{"5":{"159":2}}],["如果将输入向量",{"5":{"146":1}}],["如果将输入向量乘以一个正数",{"5":{"146":1}}],["如果将分类问题错误地使用均方误差作为损失函数",{"5":{"61":2}}],["如果需要无偏估计",{"5":{"146":2}}],["如果没有仿射变换",{"5":{"146":2}}],["如果没有足够的数据或适当的正则化",{"5":{"92":2,"153":2}}],["如果网络中每一层都以概率",{"5":{"145":1}}],["如果网络中每一层都以概率应用dropout",{"5":{"145":1}}],["如果dropout应用在线性变换之后",{"5":{"145":2}}],["如果原始hessian的特征值为",{"5":{"144":2}}],["如果某一层的雅可比矩阵具有很小的最小奇异值",{"5":{"148":2}}],["如果某些雅可比矩阵是奇异的",{"5":{"148":2}}],["如果某些",{"5":{"141":1}}],["如果某些​重复出现",{"5":{"141":1}}],["如果某个复杂模式在训练数据中出现的频率为",{"5":{"138":2}}],["如果某个任务的损失主导",{"5":{"70":2}}],["如果某个头的注意力矩阵秩很低",{"5":{"87":2,"152":2}}],["如果真实数据分布",{"5":{"140":1}}],["如果真实数据分布属于模型族",{"5":{"140":1}}],["如果真实性能与随机性能之间存在显著差距",{"5":{"138":2}}],["如果似然函数是严格凹的",{"5":{"139":2}}],["如果参数空间",{"5":{"139":1}}],["如果参数空间是紧致的",{"5":{"139":1}}],["如果参数标度指数",{"5":{"138":2}}],["如果模型对第",{"5":{"138":2}}],["如果模型认为它们不相关",{"5":{"92":2,"153":2}}],["如果继续训练",{"5":{"137":2}}],["如果数据标度指数",{"5":{"138":2}}],["如果数据增强策略具有周期性",{"5":{"136":2}}],["如果数据中的噪声是高斯分布的",{"5":{"70":2}}],["如果偏好数据存在噪声或不一致性",{"5":{"101":2}}],["如果不使用这两个参数",{"5":{"150":2}}],["如果不存在另一个解",{"5":{"101":1}}],["如果不存在另一个解使得在所有任务上都不差且至少一个任务上更好",{"5":{"101":1}}],["如果不包含位置信息",{"5":{"101":2}}],["如果不是很大",{"5":{"51":1}}],["如果使用batch",{"5":{"150":2}}],["如果使用",{"5":{"70":2}}],["如果使用正弦余弦位置编码",{"5":{"70":2}}],["如果使用张量运算而非循环",{"5":{"50":2}}],["如果类别不平衡",{"5":{"70":2}}],["如果交叉熵损失达到最小",{"5":{"70":2}}],["如果温度过高",{"5":{"69":2}}],["如果已经与很相似",{"5":{"69":1}}],["如果",{"5":{"41":18,"42":1,"48":2,"50":2,"51":1,"61":5,"69":3,"86":2,"87":3,"89":3,"91":3,"92":1,"97":2,"101":2,"136":2,"138":2,"139":2,"141":1,"143":2,"144":4,"145":4,"147":1,"148":20,"151":2,"152":3,"153":1,"154":4,"158":8,"159":6}}],["如果初始权重过大",{"5":{"41":2}}],["如果很大而没有进行适当的缩放",{"5":{"41":1}}],["如果的概率随训练进行而增加",{"5":{"147":1}}],["如果的某些行是线性相关的",{"5":{"141":1}}],["如果的各元素独立同分布且方差为",{"5":{"41":1}}],["如果的非主特征值很小",{"5":{"41":1}}],["如果的秩为",{"5":{"89":1}}],["如果接近均匀混合",{"5":{"41":1}}],["如果输入梯度的某些维度较大",{"5":{"41":2}}],["如果神经元的输入",{"5":{"42":2}}],["如果为正",{"5":{"42":1}}],["如果梯度主要分布在被压缩的方向上",{"5":{"148":2}}],["如果梯度经过一个",{"5":{"148":2}}],["如果梯度矩阵的范数随层数增加而指数级增长",{"5":{"41":2}}],["如果梯度范数超过阈值",{"5":{"44":2}}],["如果梯度范数为",{"5":{"97":2}}],["如果hessian正定",{"5":{"48":2}}],["如果hessian负定",{"5":{"48":2}}],["如果hessian既有正特征值又有负特征值",{"5":{"48":2}}],["如果hessian半正定或半负定",{"5":{"48":2}}],["如果函数的二阶偏导数连续",{"5":{"48":1}}],["如果函数",{"5":{"48":1}}],["如果p值小于预先设定的显著性水平",{"5":{"96":2}}],["如果我们能保证每次更新后",{"5":{"154":2}}],["如果我们将其视为对某个参数化分布族",{"5":{"70":2}}],["如果我们训练一个自编码器",{"5":{"69":2}}],["如果我们随机打乱序列的顺序",{"5":{"69":2}}],["如果我们把每个位置",{"5":{"69":1}}],["如果我们把每个位置视为一个潜在的",{"5":{"69":1}}],["如果我们把",{"5":{"69":1}}],["如果我们把视为",{"5":{"69":1}}],["如果我们定义",{"5":{"69":4}}],["如果我们使用相同的测试集评估两个模型",{"5":{"96":2}}],["如果我们对每个神经元单独进行数学描述和计算",{"5":{"46":2}}],["如果我们对进行qr分解",{"5":{"87":1,"152":1}}],["如果我们对",{"5":{"87":1,"152":1}}],["如果我们对输入序列进行重新排序",{"5":{"92":2,"153":2}}],["如果我们希望​能够感知到​的信息",{"5":{"92":1,"153":1}}],["如果我们希望​",{"5":{"92":1,"153":1}}],["如果我们希望",{"5":{"92":2,"153":2}}],["如果同时检验多个假设",{"5":{"96":2}}],["如果存在常数和某个",{"5":{"148":1}}],["如果存在常数使得对所有都有",{"5":{"148":1}}],["如果存在常数",{"5":{"148":6}}],["如果存在输入区域使得趋近于其渐近值",{"5":{"41":1}}],["如果存在输入区域使得",{"5":{"41":3,"42":2}}],["如果存在非零向量和标量满足",{"5":{"50":1}}],["如果存在非零向量",{"5":{"50":1}}],["如果存在群作用使得对所有",{"5":{"88":1}}],["如果存在群作用",{"5":{"88":1}}],["如果有特征值的实部为正",{"5":{"50":2}}],["如果只在输入变换上应用dropout",{"5":{"145":2}}],["如果只改变这个参数而保持其他参数不变",{"5":{"48":2}}],["如果只有少数几个奇异值很大",{"5":{"50":2}}],["如果奇异值分布比较均匀",{"5":{"50":2}}],["如果主元都不为零",{"5":{"50":2}}],["如果进行行交换",{"5":{"50":2}}],["如果满足",{"5":{"50":1}}],["如果权重归一化为",{"5":{"51":2}}],["如果权重未归一化",{"5":{"51":2}}],["如果假设噪声服从拉普拉斯分布",{"5":{"51":2}}],["如果是偏差问题",{"5":{"51":2}}],["如果是方差问题",{"5":{"51":2}}],["如果是常数",{"5":{"86":1,"151":1}}],["如果是正定的",{"5":{"86":1,"151":1}}],["如果对于所有状态",{"5":{"155":2}}],["如果对于任意​和任意",{"5":{"51":1}}],["如果对于任意",{"5":{"51":1}}],["如果对每一层应用裁剪阈值",{"5":{"147":2}}],["如果对应维度相等",{"5":{"50":2}}],["如果对进行平移",{"5":{"90":1}}],["如果对进行某种操作",{"5":{"90":1}}],["如果对",{"5":{"90":2}}],["如果与越接近",{"5":{"61":1}}],["如果在一次更新后",{"5":{"154":2}}],["如果在之后应用dropout",{"5":{"145":1}}],["如果在某一范围内观察到损失随参数量变化遵循幂律",{"5":{"138":2}}],["如果在",{"5":{"61":2,"145":1}}],["如果在该点的梯度为零",{"5":{"97":2}}],["如果且",{"5":{"87":1,"152":1}}],["如果秩较高",{"5":{"87":2,"152":2}}],["如果核函数是正定的",{"5":{"86":2,"151":2}}],["如果核函数是",{"5":{"87":2,"152":2}}],["如果编码向量的均值为零",{"5":{"89":2}}],["如果编码是平滑的则外推较好",{"5":{"89":2}}],["如果编码矩阵​​的秩较低",{"5":{"89":1}}],["如果编码矩阵",{"5":{"89":1}}],["如果注意力权重表现出局部性",{"5":{"87":2,"152":2}}],["如果注意力权重表现出全局性",{"5":{"87":2,"152":2}}],["如果注意力权重主要集中在邻近位置",{"5":{"92":2,"153":2}}],["如果注意力权重分布均匀",{"5":{"89":2}}],["如果注意力权重分布集中",{"5":{"89":2}}],["如果注意力权重分散在很远的位置",{"5":{"92":2,"153":2}}],["如果​",{"5":{"91":1}}],["如果​较小",{"5":{"92":1,"153":1}}],["如果要建模跨越整个序列的长程依赖",{"5":{"92":2,"153":2}}],["如果中间层能够调整注意力权重",{"5":{"92":2,"153":2}}],["如果两个策略差异很大",{"5":{"154":2}}],["如果两个策略完全相同",{"5":{"154":2}}],["如果两个头学习到相似的特征",{"5":{"93":2}}],["如果两个头学习到正交的特征",{"5":{"93":2}}],["如果两个头学习到完全相同的特征",{"5":{"93":2}}],["如果两个头的输出​和是独立的",{"5":{"93":1}}],["如果两个头的输出",{"5":{"93":1}}],["如果两种方法计算的梯度足够接近",{"5":{"44":2}}],["如果基函数集合足够丰富",{"5":{"93":2}}],["如果高度相关",{"5":{"93":2}}],["如果头数是的约数",{"5":{"93":1}}],["如果头数",{"5":{"93":1}}],["如果所有特征值的实部都为负",{"5":{"50":2}}],["如果所有输入​都加上一个较大的常数",{"5":{"95":1}}],["如果所有输入",{"5":{"95":1}}],["如果查询向量和键向量都是独立同分布的高斯随机向量",{"5":{"141":1}}],["如果查询向量在某个维度上的分量较大",{"5":{"95":1}}],["如果查询向量",{"5":{"95":1,"141":1}}],["如0",{"5":{"41":2,"95":2,"96":2,"148":2,"154":4}}],["如rope",{"5":{"41":2,"91":2}}],["如relu",{"5":{"41":2,"48":2}}],["如relu或gelu",{"5":{"71":2}}],["如relu激活函数就是典型的逐元素运算",{"5":{"50":1}}],["如relu激活函数",{"5":{"50":1}}],["如rbf核",{"5":{"87":2,"152":2}}],["如凸优化问题和解析解的存在性",{"5":{"71":2}}],["如逻辑回归",{"5":{"71":2}}],["如图像中的远程目标关系",{"5":{"150":2}}],["如图像",{"5":{"71":2}}],["如变分自编码器的离散潜在变量",{"5":{"71":2}}],["如fisher信息",{"5":{"71":2}}],["如gelu或relu",{"5":{"159":2}}],["如gelu或swish",{"5":{"158":2}}],["如gelu",{"5":{"71":4}}],["如gpt系列",{"5":{"140":2}}],["如gpt",{"5":{"47":2,"71":2,"93":2,"146":2}}],["如边缘",{"5":{"45":2,"150":2}}],["如物体类别",{"5":{"45":2}}],["如矩阵乘法",{"5":{"45":2}}],["如张量",{"5":{"45":2}}],["如jacobian矩阵计算",{"5":{"45":2}}],["如初始化为零",{"5":{"46":2}}],["如adam",{"5":{"46":2}}],["如异或问题",{"5":{"47":2}}],["如pytorch",{"5":{"47":2,"93":2,"145":2}}],["如pytorch和tensorflow中",{"5":{"50":2}}],["如pytorch的tensorflow的张量计算",{"5":{"93":2}}],["如抛硬币的结果",{"5":{"96":2}}],["如词汇关系",{"5":{"71":2}}],["如词语的嵌入向量",{"5":{"96":2}}],["如判断一个词是否属于某个类别",{"5":{"96":2}}],["如分析神经网络输出的分布特性",{"5":{"96":2}}],["如结合vae的表示学习",{"5":{"96":2}}],["如高斯分布",{"5":{"70":2,"96":2}}],["如基于贝叶斯方法的超参数优化",{"5":{"96":2}}],["如随机初始化",{"5":{"89":2,"96":2}}],["如两个模型或两种策略",{"5":{"96":2}}],["如用户满意度",{"5":{"96":2}}],["如tanh在原点附近",{"5":{"148":2}}],["如transformer为",{"5":{"140":2}}],["如t",{"5":{"50":2}}],["如tensor",{"5":{"86":2,"151":2}}],["如tensorrt",{"5":{"93":2}}],["如bert",{"5":{"142":2}}],["如bpe",{"5":{"101":2}}],["如blue",{"5":{"70":2}}],["如bonferroni校正或false",{"5":{"96":2}}],["如bf16",{"5":{"50":2}}],["如何在与环境",{"5":{"156":2}}],["如何找到一个最优策略",{"5":{"155":2}}],["如何有效训练深层神经网络始终是一个核心挑战",{"5":{"150":2}}],["如何最优地聚合这些梯度以平衡计算效率和优化质量",{"5":{"149":2}}],["如何最优地分配资源以获得最佳性能",{"5":{"138":2}}],["如何与单层性质共同决定梯度命运",{"5":{"148":2}}],["如何随着模型参数规模",{"5":{"138":2}}],["如何高效地计算这个复合函数的输出",{"5":{"45":2}}],["如何将矩阵运算分解为可以在不同设备上独立执行的子运算",{"5":{"50":2}}],["如何自动学习或设计这种稀疏模式",{"5":{"87":2,"152":2}}],["如何设计更好的特征映射以平衡表达能力和计算效率",{"5":{"87":1,"152":1}}],["如何设计更好的特征映射",{"5":{"87":1,"152":1}}],["如何让模型感知序列中元素的位置信息一直是一个核心问题",{"5":{"91":2}}],["如线性注意力",{"5":{"50":2}}],["如线性模型",{"5":{"51":2}}],["如线性模型拟合非线性数据",{"5":{"51":2}}],["如测量噪声",{"5":{"51":2}}],["如神经元排列的不变性",{"5":{"48":2}}],["如神经网络的情况",{"5":{"51":2}}],["如mae",{"5":{"51":2}}],["如cls标记或特殊标记",{"5":{"86":2,"151":2}}],["如滑动窗口",{"5":{"86":2,"151":2}}],["如h100",{"5":{"86":2,"151":2}}],["如或",{"5":{"86":1,"92":2,"151":1,"153":2}}],["如或4096",{"5":{"91":1}}],["如视觉任务中的目标检测和图像分割",{"5":{"150":2}}],["如视觉",{"5":{"86":2,"151":2}}],["如每行都是one",{"5":{"87":2,"152":2}}],["如需要高秩矩阵表示的函数",{"5":{"87":2,"152":2}}],["如低秩近似",{"5":{"87":2,"152":2}}],["如训练早期或特定数据分布",{"5":{"87":2,"152":2}}],["如512或1024",{"5":{"88":2}}],["如主要关注少数几个位置",{"5":{"87":2,"152":2}}],["如主谓一致",{"5":{"88":2}}],["如主语",{"5":{"92":2,"93":2,"153":2}}],["如修饰关系",{"5":{"88":2}}],["如层归一化",{"5":{"87":2,"152":2}}],["如层次化的语法结构",{"5":{"88":2}}],["如l1正则化",{"5":{"142":2}}],["如l2范数",{"5":{"133":2}}],["如longformer",{"5":{"87":2,"152":2}}],["如llama",{"5":{"88":2}}],["如句子边界",{"5":{"88":2}}],["如超过100k",{"5":{"88":2}}],["如循环结构",{"5":{"88":2}}],["如过拟合风险",{"5":{"89":2}}],["如某些位置出现频率很低",{"5":{"89":2}}],["如零初始化",{"5":{"89":2}}],["如使用低秩分解存储和而非完整",{"5":{"89":1}}],["如使用低秩分解存储",{"5":{"89":1}}],["如旋转位置编码",{"5":{"90":2}}],["如截断或微调",{"5":{"91":2}}],["如最大似然",{"5":{"70":2}}],["如最后一层编码器或解码器",{"5":{"92":2,"153":2}}],["如取最大值",{"5":{"92":2,"153":2}}],["如代码分析",{"5":{"89":2}}],["如代码",{"5":{"89":2}}],["如代词与指代对象的关联",{"5":{"92":2,"153":2}}],["如长文档",{"5":{"86":2,"151":2}}],["如长文档处理",{"5":{"95":2}}],["如长文本建模",{"5":{"41":2}}],["如长文本生成",{"5":{"91":2}}],["如长距离的修饰关系",{"5":{"92":2,"153":2}}],["如位置编码",{"5":{"90":2}}],["如位置和位置之间的关联",{"5":{"92":1,"153":1}}],["如位置",{"5":{"92":1,"153":1}}],["如距离越近权重越大",{"5":{"92":2,"153":2}}],["如距离矩阵",{"5":{"92":2,"153":2}}],["如相对位置表达能力",{"5":{"90":2}}],["如相邻词之间的关系",{"5":{"93":2}}],["如同一序列中相邻的位置",{"5":{"69":2}}],["如同一序列中相邻的token",{"5":{"71":2}}],["如同义词或上下文中相关的词",{"5":{"93":2}}],["如短语结构",{"5":{"88":2}}],["如短序列处理",{"5":{"93":2}}],["如与​可比",{"5":{"93":1}}],["如傅里叶基",{"5":{"93":2}}],["如固定​",{"5":{"93":1}}],["如固定",{"5":{"93":1}}],["如xavier初始化或kaiming初始化",{"5":{"94":2}}],["如前文中的相关token",{"5":{"69":2}}],["如前节分析的结论",{"5":{"94":2}}],["如大语言模型生成token",{"5":{"154":2}}],["如大语言模型",{"5":{"97":2}}],["td误差只依赖于局部的状态转移信息",{"5":{"155":2}}],["td",{"5":{"155":2}}],["t|",{"5":{"144":2}}],["tt分解在量子多体物理和深度学习中有重要应用",{"5":{"143":2}}],["tt分解作为tucker的推广",{"5":{"143":2}}],["tt",{"5":{"143":2}}],["t^2",{"5":{"140":1}}],["t+1",{"5":{"69":2,"97":2,"142":2,"144":10}}],["t",{"5":{"69":6,"97":27,"132":46,"133":12,"135":4,"136":12,"137":2,"141":9,"142":12,"144":26}}],["tau",{"5":{"141":2,"142":4}}],["taylor",{"5":{"137":2}}],["target",{"5":{"133":4}}],["target=",{"5":{"21":10,"44":1,"45":2,"47":2,"48":3,"50":2,"61":1,"96":3,"131":51,"143":1,"148":2}}],["task",{"5":{"70":2,"101":2,"132":2,"133":2}}],["tag",{"5":{"42":50,"46":22,"61":18,"69":8,"86":4,"97":8,"151":4}}],["tanh是零中心的",{"5":{"148":2}}],["tanh有两点改进",{"5":{"148":2}}],["tanh有两个显著差异",{"5":{"41":2}}],["tanh的强梯度压缩",{"5":{"148":2}}],["tanh的最大导数为1",{"5":{"148":2}}],["tanh的输出范围为",{"5":{"148":2}}],["tanh的输出以零为中心",{"5":{"41":2}}],["tanh的输出以0为中心",{"5":{"42":2}}],["tanh的导数仍然小于1",{"5":{"148":2}}],["tanh的导数为",{"5":{"41":2}}],["tanh的导数",{"5":{"42":2}}],["tanh的导数峰值是1而非0",{"5":{"41":2}}],["tanh的导数峰值更大",{"5":{"42":2}}],["tanh的零中心性质",{"5":{"42":2}}],["tanh的零中心性质缓解了这个问题",{"5":{"42":2}}],["tanh的零中心输出消除了这个问题",{"5":{"42":2}}],["tanh的梯度信号更强",{"5":{"41":2}}],["tanh的梯度",{"5":{"42":2}}],["tanh的饱和区域",{"5":{"41":2}}],["tanh的饱和问题依然严重",{"5":{"42":2}}],["tanh函数的饱和特性",{"2":{"41":1},"5":{"41":1}}],["tanh函数的饱和特性与sigmoid类似",{"5":{"41":1}}],["tanh函数的梯度饱和区域为",{"5":{"41":2}}],["tanh函数的数学定义与性质",{"2":{"42":1},"5":{"42":1}}],["tanh函数的导数与性质",{"2":{"42":1},"5":{"42":1}}],["tanh函数的导数为",{"5":{"42":1}}],["tanh函数",{"2":{"148":1},"5":{"41":1,"42":7,"148":1}}],["tanh函数是奇函数",{"5":{"42":2}}],["tanh函数可以表示为sigmoid函数的缩放和平移",{"5":{"42":2}}],["tanh",{"5":{"41":1,"42":2,"47":2,"71":4,"148":2}}],["tanh与sigmoid有密切的数学关系",{"5":{"42":2}}],["tanh与sigmoid的饱和比较",{"5":{"41":2}}],["tanh与sigmoid的关系",{"5":{"42":2}}],["tanh可以视为sigmoid的缩放和平移版本",{"5":{"42":2}}],["tanh和sigmoid存在直接的数学关系",{"5":{"42":2}}],["tanh本质上是sigmoid的",{"5":{"42":2}}],["tanh以0为中心",{"5":{"42":2}}],["tanh先对输入进行2倍缩放",{"5":{"42":2}}],["tanh导数",{"5":{"42":4}}],["tanh导数的取值范围",{"5":{"42":2}}],["tanh导数的取值范围是",{"5":{"42":2}}],["tanh导数同样会迅速衰减至",{"5":{"42":2}}],["tanh在激活值接近零的区域",{"5":{"42":2}}],["tanh输出区间",{"5":{"42":1}}],["tanh输出",{"5":{"42":1}}],["tangent",{"5":{"88":2}}],["temporal",{"5":{"155":2}}],["temperature=t",{"5":{"132":2}}],["temperature",{"5":{"69":2,"70":2,"95":2}}],["tex",{"2":{"154":4,"155":5,"156":11}}],["text",{"5":{"42":58,"46":28,"61":4,"69":10,"86":12,"141":20,"142":26,"144":30,"146":6,"151":12}}],["teacher",{"5":{"140":2}}],["telescoping",{"5":{"136":2}}],["test",{"5":{"96":2}}],["tensorflow",{"5":{"47":2,"61":2,"93":2,"145":2}}],["tensor",{"5":{"50":4,"93":4,"132":4,"143":8,"149":2}}],["total",{"5":{"132":2}}],["torch",{"5":{"132":14}}],["token嵌入的集合",{"5":{"101":1}}],["token嵌入的集合在置换下保持不变",{"5":{"101":1}}],["token的注意",{"5":{"101":4}}],["token的交互特征",{"5":{"71":2}}],["token",{"5":{"70":4,"101":4,"138":2,"156":2}}],["tokens",{"5":{"88":4,"140":2}}],["to",{"5":{"42":2,"95":2,"132":4,"133":6,"135":4,"137":6,"141":2,"145":2,"157":11,"159":2}}],["toeplitz矩阵",{"5":{"93":2}}],["topological",{"5":{"45":2}}],["top",{"2":{"159":1},"5":{"96":2,"141":22,"142":10,"143":12,"157":2,"159":18}}],["trial",{"5":{"156":2}}],["trick",{"5":{"145":2}}],["true",{"5":{"132":1,"136":2}}],["tr",{"5":{"46":2,"142":2}}],["tree",{"5":{"50":2}}],["train",{"5":{"143":2}}],["training",{"5":{"97":2,"146":2,"154":4}}],["trajectory",{"5":{"134":2,"155":2}}],["transition",{"5":{"156":2}}],["transcritical",{"5":{"136":2}}],["transformer和需要处理单个样本的场景",{"5":{"146":2}}],["transformer通过掩码注意力确保位置",{"5":{"140":1}}],["transformer通过掩码注意力确保位置只能看到位置到",{"5":{"140":1}}],["transformer通过自注意力机制实现了真正的全上下文依赖",{"5":{"140":2}}],["transformer",{"2":{"132":1},"5":{"41":6,"50":2,"70":8,"88":2,"97":8,"132":12,"138":4,"140":1,"146":1,"150":4}}],["transformer中",{"5":{"41":4}}],["transformer中的自注意力机制也可以从张量角度理解",{"5":{"143":2}}],["transformer中的梯度爆炸机制",{"2":{"41":1},"5":{"41":1}}],["transformer中的激活函数选择不是随意的",{"5":{"71":2}}],["transformer中的前馈网络",{"5":{"71":2}}],["transformer中的注意力机制可以被看作一个三阶张量",{"5":{"50":2}}],["transformer采用了深度网络的架构设计",{"5":{"71":2}}],["transformer在各种任务上的成功部分归功于其注意力矩阵的隐含低秩结构",{"5":{"87":2,"152":2}}],["transformer架构中包含两种layer",{"5":{"150":2}}],["transformer架构虽然解决了rnn的长期依赖问题",{"5":{"41":2}}],["transformer架构的出现从根本上改变了这一局面",{"5":{"91":2}}],["transformer的一个注意力头可以表示为",{"5":{"150":2}}],["transformer的自注意力机制允许每个位置直接访问序列中的所有其他位置",{"5":{"140":2}}],["transformer的自注意力机制",{"5":{"140":2}}],["transformer的自注意力机制本身不包含显式的激活函数",{"5":{"71":2}}],["transformer的自注意力突破",{"2":{"140":1},"5":{"140":1}}],["transformer的不同层捕获不同",{"5":{"92":2,"153":2}}],["transformer的原始论文使用了个头的配置",{"5":{"93":1}}],["transformer的原始论文使用了",{"5":{"93":1}}],["transformer原始论文提出的正弦位置编码",{"5":{"92":2,"153":2}}],["transformation",{"5":{"47":2,"146":2}}],["transform",{"5":{"90":6,"91":2,"150":2}}],["transpose",{"5":{"50":2,"132":16}}],["tradeoff",{"5":{"51":2}}],["tube",{"5":{"143":2}}],["tucker分解则通过引入核张量的概念",{"5":{"143":2}}],["tucker分解通常需要更少的参数",{"5":{"143":2}}],["tucker分解更合适",{"5":{"143":2}}],["tucker分解的计算通常采用交替最小二乘法",{"5":{"143":2}}],["tucker分解的计算方法",{"2":{"143":1},"5":{"143":1}}],["tucker分解与svd密切相关",{"5":{"143":2}}],["tucker分解与svd的关系",{"2":{"143":1},"5":{"143":1}}],["tucker分解退化为cp分解的特定形式",{"5":{"143":2}}],["tucker分解假设基张量是一个紧凑的",{"5":{"143":2}}],["tucker分解",{"5":{"143":3}}],["tucker",{"0":{"143":1},"4":{"143":1},"5":{"50":6,"131":5,"143":2}}],["tuning",{"5":{"50":2,"154":2}}],["tishby",{"5":{"133":2}}],["timesn",{"5":{"143":1}}],["times",{"5":{"46":14,"143":1}}],["tilde",{"5":{"144":4}}],["tiling",{"5":{"86":2,"151":2,"159":2}}],["tiles",{"5":{"86":2,"151":2}}],["through估计器",{"5":{"157":2}}],["through",{"5":{"157":2}}],["thread",{"5":{"93":2}}],["thought",{"5":{"138":2}}],["theta|",{"5":{"144":4}}],["theta",{"5":{"144":72}}],["theory",{"5":{"137":2}}],["theorem",{"5":{"96":2,"155":2}}],["then",{"5":{"136":16}}],["the",{"5":{"47":2}}],["输出相对于输入的梯度几乎处处为零",{"5":{"159":2}}],["输出一个",{"5":{"159":2}}],["输出一个标量分数",{"5":{"154":2}}],["输出一组权重",{"5":{"159":2}}],["输出可能发生剧烈变化",{"5":{"159":2}}],["输出可能包含非常小的值",{"5":{"97":2}}],["输出可以紧凑地写为",{"5":{"158":2}}],["输出维度或卷积核大小上有所区别",{"5":{"150":2}}],["输出方差",{"5":{"145":2}}],["输出方差的数学推导",{"2":{"145":1},"5":{"145":1}}],["输出的定义",{"5":{"154":2}}],["输出的方差也接近零",{"5":{"145":2}}],["输出的方差为",{"5":{"145":2}}],["输出的组合",{"5":{"70":4}}],["输出向量的尺度被约束在合理的范围内",{"5":{"146":2}}],["输出向量的第个分量为​",{"5":{"145":1}}],["输出向量",{"5":{"140":1,"145":1}}],["输出向量是概率分布",{"5":{"140":1}}],["输出向量为​",{"5":{"46":1}}],["输出向量为",{"5":{"46":1}}],["输出信息量",{"5":{"133":2}}],["输出求导",{"5":{"70":2}}],["输出是专家输出的凸组合",{"5":{"158":2}}],["输出是值的加权平均",{"5":{"132":2}}],["输出是加权平均的期望值",{"5":{"69":2}}],["输出是类别的概率",{"5":{"69":2}}],["输出是各头注意力的拼接和投影",{"5":{"93":2}}],["输出",{"5":{"44":2,"50":2,"61":8,"70":2,"90":1,"132":10,"133":10,"135":6,"136":4,"137":8,"143":4}}],["输出层的误差",{"5":{"148":2}}],["输出层的预测分布是一个维的概率向量",{"5":{"96":1}}],["输出层的预测分布是一个",{"5":{"96":1}}],["输出层概率",{"5":{"69":2}}],["输出层",{"5":{"44":2,"61":2}}],["输出层有个神经元",{"5":{"71":1}}],["输出层有",{"5":{"71":1}}],["输出层误差信号",{"5":{"44":2}}],["输出层误差信号与预测误差成正比",{"5":{"44":1}}],["输出层误差信号与预测误差",{"5":{"44":1}}],["输出层误差",{"5":{"44":2}}],["输出层误差为",{"5":{"44":2}}],["输出层误差简化为预测误差的平均值",{"5":{"44":2}}],["输出饱和和梯度饱和",{"5":{"41":2}}],["输出饱和关注的是激活函数输出值进入平坦区域的物理现象",{"5":{"41":2}}],["输出饱和",{"5":{"41":2}}],["输出饱和是激活函数值域边界的行为",{"5":{"41":2}}],["输出均值为0",{"5":{"42":2,"148":2}}],["输出总是正的",{"5":{"42":2}}],["输出映射",{"5":{"71":2}}],["输出分布关于输入的fisher信息矩阵正好是softmax雅可比矩阵与自身转置的乘积",{"5":{"71":1}}],["输出分布",{"5":{"71":1}}],["输出节点表示网络的最终输出",{"5":{"45":2}}],["输出为​",{"5":{"45":1}}],["输出为",{"5":{"45":1,"46":2,"92":2,"144":2,"150":2,"153":2}}],["输出为1",{"5":{"47":6}}],["输出为0",{"5":{"47":2}}],["输出值较低",{"5":{"71":2}}],["输出值均为正且和为1",{"5":{"47":2}}],["输出表示被",{"5":{"87":2,"152":2}}],["输出表示第个频率成分的复振幅",{"5":{"90":1}}],["输出通过递归关系定义",{"5":{"92":2,"153":2}}],["输出投影矩阵会将所有头的信息混合起来",{"5":{"93":1}}],["输出投影矩阵​隐含地学习了一种加权方案",{"5":{"93":1}}],["输出投影矩阵",{"5":{"93":2}}],["输出投影后的结果",{"5":{"93":2}}],["输出投影负责混合信息",{"5":{"93":2}}],["输入与输出之间的差异",{"5":{"150":2}}],["输入激活的范数",{"5":{"148":2}}],["输入通常是单个样本而非批量",{"5":{"146":2}}],["输入张量为",{"5":{"146":2}}],["输入张量通常是四维的",{"5":{"146":2}}],["输入张量的形状通常是",{"5":{"50":2}}],["输入信息量",{"5":{"133":2}}],["输入位置的信息条件熵定义为注意力分布的不确定性度量",{"5":{"132":2}}],["输入到激活函数的线性组合可能直接落在饱和区域",{"5":{"41":1}}],["输入到激活函数的线性组合",{"5":{"41":1}}],["输入梯度和输出梯度满足",{"5":{"41":1}}],["输入梯度",{"5":{"41":1}}],["输入",{"5":{"44":2,"132":12,"133":10,"135":4,"136":4,"137":8,"143":2,"150":2}}],["输入数据通过多层非线性变换",{"5":{"44":2}}],["输入数据依次通过每一层的仿射变换和非线性激活",{"5":{"45":2}}],["输入节点表示网络的输入数据",{"5":{"45":2}}],["输入节点",{"5":{"45":2}}],["输入的无限精度被截断为有限范围的输出",{"5":{"71":2}}],["输入的",{"5":{"61":2}}],["输入为",{"5":{"87":2,"152":2}}],["输入矩阵与堆叠后的投影矩阵相乘",{"5":{"93":1}}],["输入矩阵​与堆叠投影矩阵相乘",{"5":{"93":1}}],["输入矩阵",{"5":{"93":2,"94":1}}],["输入矩阵分别左乘这三个投影矩阵",{"5":{"94":1}}],["输入嵌入矩阵的数学表示",{"2":{"94":1},"5":{"94":1}}],["输入嵌入通常还需要加上位置编码",{"5":{"94":2}}],["输入序列通常需要填充到统一的长度以进行批处理",{"5":{"95":2}}],["输入序列经过多层transformer架构的变换后",{"5":{"96":2}}],["不参与任何非零权重的计算",{"5":{"158":2}}],["不参与梯度更新",{"5":{"91":2}}],["不均衡的专家负载会导致某些设备过载而其他设备空闲",{"5":{"157":2}}],["不接近极端值",{"5":{"150":2}}],["不需调参",{"5":{"147":1}}],["不需要额外的缩放操作",{"5":{"145":2}}],["不需要额外的相对位置偏置",{"5":{"88":2}}],["不需要显式的奖励模型",{"5":{"101":2}}],["不需要大的更新",{"5":{"101":2}}],["不需要经过",{"5":{"61":2}}],["不需要复杂的插值策略",{"5":{"88":2}}],["不需要训练",{"5":{"91":2}}],["不适用于rnn",{"5":{"146":1}}],["不依赖批量大小",{"5":{"146":2}}],["不依赖于动作",{"5":{"155":2}}],["不依赖于批次大小",{"5":{"150":2}}],["不依赖于其他样本",{"5":{"41":2}}],["不依赖于特定的数据分布或训练技巧",{"5":{"88":2}}],["不依赖于",{"5":{"89":1}}],["不计算均值和方差",{"5":{"146":2}}],["不等式",{"5":{"137":2}}],["不等于",{"5":{"96":2}}],["不规则震荡行为",{"5":{"136":2}}],["不变",{"5":{"145":2}}],["不变性",{"5":{"136":2}}],["不变集原理",{"5":{"134":2}}],["不稳定方向",{"5":{"135":2}}],["不稳定",{"5":{"134":2,"135":2}}],["不定",{"5":{"134":2,"135":2}}],["不动点失稳并产生极限环",{"5":{"136":2}}],["不动点失稳",{"5":{"136":4}}],["不动点的谱分类",{"5":{"134":2}}],["不动点的性质",{"5":{"134":2}}],["不动点",{"5":{"134":4}}],["不受序列长度的影响",{"5":{"150":2}}],["不受批量中其他样本的影响",{"5":{"146":2}}],["不受其他特征的影响",{"5":{"146":2}}],["不受压缩约束限制",{"5":{"133":2}}],["不受层归一化的影响",{"5":{"41":2}}],["不对称且加速的",{"5":{"70":1}}],["不对称性是kl散度区别于传统距离的关键特征",{"5":{"61":1}}],["不对称性",{"5":{"61":3}}],["不具有这种与fisher信息的直接联系",{"5":{"70":2}}],["不确定性估计",{"5":{"137":2}}],["不确定性最大的是均匀分布",{"5":{"137":2}}],["不确定性高",{"5":{"96":2}}],["不确定性量化和小样本学习",{"5":{"96":2}}],["不确定性程度",{"5":{"61":2}}],["不确定性度量",{"5":{"61":2}}],["不确定性",{"5":{"89":2}}],["不同类型的输入往往需要模型的不同",{"5":{"159":2}}],["不同专家共享部分参数",{"5":{"158":2}}],["不同轨迹之间可能有很大的差异",{"5":{"155":2}}],["不同尺度的变换输出不仅直接相加",{"5":{"150":2}}],["不同尺度的变换",{"5":{"150":2}}],["不同时间步的输入序列长度可能不同",{"5":{"146":2}}],["不同模态之间的信息流动可用协作信息论描述",{"5":{"137":2}}],["不同特征方向的收敛因子差异由",{"5":{"135":2}}],["不同特征方向的收敛差异",{"5":{"135":2}}],["不同特征维度进行独立的",{"5":{"86":2,"151":2}}],["不同方向收敛速度差异大",{"5":{"135":2}}],["不同优化算法",{"5":{"134":2}}],["不同估计方法在偏差和方差之间有不同的权衡",{"5":{"133":2}}],["不同任务之间的信息共享是一个核心问题",{"5":{"133":2}}],["不同任务共享相同的注意力层",{"5":{"70":2}}],["不同系数的正则化效果如下",{"5":{"133":2}}],["不同归一化方法在信息保持和计算效率之间有不同的权衡",{"5":{"132":2}}],["不同初始化策略的比较",{"5":{"41":2}}],["不同注意力分布对应截然不同的谱结构",{"5":{"41":2}}],["不同设备处理不同样本",{"5":{"41":2}}],["不同的专家被分配到不同的计算设备上",{"5":{"159":2}}],["不同的专家可以专门学习数据分布的不同区域或不同类型的模式",{"5":{"159":2}}],["不同的基线选择会带来不同程度的方差缩减效果",{"5":{"155":2}}],["不同的激活函数具有不同的导数性质",{"5":{"148":2}}],["不同的分解方法对应不同的基张量假设",{"5":{"143":2}}],["不同的超参数配置对应不同的不动点",{"5":{"136":2}}],["不同的任务可能需要模型关注输入的不同部分",{"5":{"101":2}}],["不同的掩码策略对应于不同的",{"5":{"101":2}}],["不同的掩码策略会导致不同的损失函数形式和模型学习行为",{"5":{"101":2}}],["不同的注意力头可能专门负责处理不同类型的输入或任务",{"5":{"70":2}}],["不同的下游任务可能需要不同特性的激活函数",{"5":{"41":2}}],["不同的权重矩阵定义了不同的投影方向",{"5":{"45":2}}],["不同的神经元学习不同的特征表示",{"5":{"47":2}}],["不同的特征映射对应不同的核函数",{"5":{"86":2,"151":2}}],["不同的稀疏模式对应不同的归纳偏置和计算特性",{"5":{"86":2,"151":2}}],["不同的稀疏模式导致不同的谱特性",{"5":{"87":2,"152":2}}],["不同的初始化可能收敛到不同的局部最优",{"5":{"89":2}}],["不同的频率成分携带函数的不同",{"5":{"90":2}}],["不同的频率成分贡献不同的相对位置表示",{"5":{"90":2}}],["不同的位置应当具有不同的编码向量",{"5":{"91":2}}],["不同的位置编码会导致不同的注意力分布",{"5":{"91":2}}],["不同的位置编码",{"5":{"101":2}}],["不同的头可以专门负责建模不同类型的依赖关系",{"5":{"92":2,"153":2}}],["不同的头协同工作",{"5":{"92":2,"153":2}}],["不同的头对应于不同的投影方向",{"5":{"93":2}}],["不同的头从不同的随机起点开始优化",{"5":{"93":2}}],["不同的头学习不同的关联模式",{"5":{"93":2}}],["不同的架构有不同的归纳偏置",{"5":{"92":2,"153":2}}],["不同的",{"5":{"93":2}}],["不同的损失函数定义了不同的",{"5":{"70":1}}],["不同的损失函数定义了不同的几何结构",{"5":{"70":1}}],["不同的损失函数在优化难度上存在显著差异",{"5":{"97":2}}],["不同",{"5":{"45":2,"48":2,"92":2,"93":2,"133":2,"153":2}}],["不同输出神经元",{"5":{"45":2}}],["不同列之间的计算不相互依赖",{"5":{"45":2}}],["不同样本之间的计算是相互独立的",{"5":{"45":2}}],["不同样本的重要性可能有所不同",{"5":{"51":2}}],["不同组的query头使用不同的key和value",{"5":{"86":2,"151":2}}],["不同头表现出不同的衰减模式",{"5":{"87":2,"152":2}}],["不同头的衰减速度不同",{"5":{"87":2,"152":2}}],["不同头的学习任务是相对独立的",{"5":{"93":2}}],["不同头之间仍然存在间接的交互",{"5":{"93":2}}],["不同频率的正交基函数为模型提供了不同尺度的位置信息",{"5":{"70":2}}],["不同频率成分提供不同精度的位置表示",{"5":{"90":2}}],["不同位置的token可能被路由到不同的专家",{"5":{"158":2}}],["不同位置的序列长度不同",{"5":{"150":2}}],["不同位置的编码趋于相似",{"5":{"90":2}}],["不同位置对应不同的数值",{"5":{"88":2}}],["不同位置具有不同的编码",{"5":{"90":2}}],["不同位置有不同的编码",{"5":{"92":2,"153":2}}],["不同维度的标度指数",{"5":{"138":1}}],["不同维度的标度指数反映了各个因素对模型性能的边际贡献",{"5":{"138":1}}],["不同维度使用不同的频率",{"5":{"90":2}}],["不同维度对应不同",{"5":{"91":2}}],["不同层或不同训练阶段可能出现不同的现象",{"5":{"148":2}}],["不同层的压缩时机",{"5":{"133":2}}],["不同层的注意力可能关注不同类型的关系",{"5":{"71":2}}],["不同层和不同头的有效依赖跨度差异很大",{"5":{"92":2,"153":2}}],["不同层和头表现出不同的注意力模式",{"5":{"92":2,"153":2}}],["不同损失函数的优化难度比较",{"2":{"97":1},"5":{"97":1}}],["不同损失函数对学习率的敏感性不同",{"5":{"97":2}}],["不同训练阶段的最优学习率可能变化",{"5":{"97":2}}],["不应被归一化消除",{"5":{"41":2}}],["不仅有助于正确使用这些归一化技术",{"5":{"146":2}}],["不仅有助于正确实现和调优dropout超参数",{"5":{"145":2}}],["不仅有助于正确应用低秩近似技术",{"5":{"142":2}}],["不仅有助于诊断和解决训练中的实际问题",{"5":{"144":2,"147":2}}],["不仅有助于设计更高效的深度学习模型",{"5":{"143":2}}],["不仅有助于设计更高效的模型压缩算法",{"5":{"143":2}}],["不仅有助于深入理解llm的训练机制",{"5":{"101":2}}],["不仅有助于我们把握神经网络的工作原理",{"5":{"45":2}}],["不仅会使得表达式繁琐冗长",{"5":{"46":2}}],["不仅定义了优化的目标函数",{"5":{"51":2}}],["不仅计算高效",{"5":{"96":2}}],["不相关时等号才成立",{"5":{"96":1}}],["不当使用容易引入隐蔽且难以察觉的",{"5":{"50":2}}],["不当的实现可能导致数值溢出",{"5":{"44":2}}],["不当的学习率调度可能导致收敛振荡",{"5":{"97":2}}],["不是必需的",{"5":{"146":1}}],["不是对每个特征维度计算批量统计量",{"5":{"146":2}}],["不是欧几里得距离",{"5":{"70":2,"71":2}}],["不是很大",{"5":{"51":1}}],["不是为每个位置分配一个固定的数值",{"5":{"88":2}}],["不考虑概率分布的归一化约束",{"5":{"61":2}}],["不随增长",{"5":{"86":1,"151":1}}],["不随",{"5":{"86":1,"151":1}}],["不随序列长度增长",{"5":{"87":2,"152":2}}],["不改变其超平面性质",{"5":{"47":2}}],["不改变线性相关性",{"5":{"87":2,"152":2}}],["不区分它们的先后顺序",{"5":{"88":2}}],["不会接收到任何梯度来调整其门控参数",{"5":{"158":2}}],["不会偏离旧的策略",{"5":{"154":2}}],["不会在前向传播或反向传播中压缩信号",{"5":{"148":2}}],["不会在不同特征之间引入相关性",{"5":{"145":2}}],["不会因为裁剪而引入额外的偏差",{"5":{"147":2}}],["不会经过子层的复杂非线性变换",{"5":{"146":1}}],["不会增加",{"5":{"137":2,"147":2}}],["不会过度放大或抑制梯度",{"5":{"41":2}}],["不会出现训练过程中位置编码",{"5":{"88":2}}],["不会出现",{"5":{"91":2}}],["不会感知到原始的顺序信息",{"5":{"92":2,"153":2}}],["不会丢失信息",{"5":{"94":2}}],["不影响优化结果",{"5":{"51":1}}],["不影响flash",{"5":{"88":2}}],["不如让模型自己发现最适合任务的编码方式",{"5":{"89":2}}],["不超过最低频率成分的周期",{"5":{"90":1}}],["不显式计算完整的注意力矩阵",{"5":{"87":1,"152":1}}],["不显式计算完整的",{"5":{"87":1,"152":1}}],["不显式地使用位置信息",{"5":{"88":2}}],["不显著增加模型的整体计算负担",{"5":{"91":2}}],["不包含位置信息",{"5":{"88":2}}],["不包含任何位置信息",{"5":{"92":2,"153":2}}],["不干扰内容信息的表示",{"5":{"92":2,"153":2}}],["不存在明确的特征尺度",{"5":{"135":2}}],["不存在饱和问题",{"5":{"41":2}}],["不存在",{"5":{"92":2,"153":2}}],["不擅长学习哪类函数",{"5":{"92":2,"153":2}}],["不进行显式的加权混合",{"5":{"93":2}}],["不可约损失",{"5":{"138":2}}],["不可减少误差",{"5":{"51":2}}],["不可减少误差是mse的理论下界",{"5":{"51":2}}],["不可逆的变换",{"5":{"94":2}}],["不一定是正交的",{"5":{"94":2}}],["不再是伯努利变量",{"5":{"145":2}}],["不再能够简化为的形式",{"5":{"71":1}}],["不再能够简化为",{"5":{"71":1}}],["不再依赖于维度",{"5":{"95":2}}],["不能使用未来信息",{"5":{"41":2}}],["不能",{"5":{"95":2}}],["层模型",{"5":{"159":2}}],["层权重",{"5":{"148":1}}],["层权重的梯度范数随层数递减的现象",{"5":{"148":1}}],["层权重的梯度为",{"5":{"44":2}}],["层使用sigmoid激活函数",{"5":{"148":1}}],["层反向传播到第",{"5":{"148":1}}],["层净输入的敏感度",{"5":{"148":1}}],["层数指数衰减",{"5":{"148":2}}],["层需要经过以下雅可比矩阵连乘",{"5":{"148":2}}],["层输出为",{"5":{"145":2}}],["层输出关于参数的",{"5":{"135":2}}],["层依赖的学习误差为",{"5":{"138":2}}],["层依赖时",{"5":{"138":2}}],["层依赖关系的任务",{"5":{"138":2}}],["层神经网络",{"5":{"137":1}}],["层残差网络",{"5":{"135":2}}],["层激活的",{"5":{"135":2}}],["层激活函数的导数",{"5":{"44":1}}],["层到第",{"5":{"135":2,"148":1}}],["层间雅可比矩阵乘积的范数",{"5":{"148":1}}],["层间的奇异性会",{"5":{"148":2}}],["层间信息流动遵循数据处理不等式",{"5":{"133":2}}],["层间差异",{"5":{"87":2,"152":2}}],["层",{"5":{"45":2,"61":2,"89":1,"92":2,"93":1,"133":2,"148":2,"153":2}}],["层归一化对输入",{"5":{"146":1}}],["层归一化对输入的梯度不会经过子层的复杂非线性变换",{"5":{"146":1}}],["层归一化首先计算第",{"5":{"146":1}}],["层归一化首先计算第个样本的均值",{"5":{"146":1}}],["层归一化和rmsnorm在特征轴",{"5":{"146":1}}],["层归一化和rmsnorm在特征轴上聚合信息",{"5":{"146":1}}],["层归一化和均方根归一化的具体计算公式",{"5":{"146":2}}],["层归一化通过在特征维度上计算统计量",{"5":{"146":2}}],["层归一化通常部署在注意力层和前馈层之间",{"5":{"41":2}}],["层归一化适用于循环神经网络",{"5":{"146":2}}],["层归一化与rmsnorm的计算操作对比",{"5":{"146":1}}],["层归一化与rmsnorm的计算操作对比从表中可以看出",{"5":{"146":1}}],["层归一化与transformer的配合",{"2":{"41":1},"5":{"41":1}}],["层归一化中的去均值化操作",{"5":{"146":2}}],["层归一化将每个样本的特征向量投影到以原点为球心",{"5":{"146":2}}],["层归一化保持了个体内",{"5":{"146":2}}],["层归一化是",{"5":{"146":1}}],["层归一化是样本条件独立的",{"5":{"146":1}}],["层归一化是标准",{"5":{"41":1}}],["层归一化具有几个重要的数学特性",{"5":{"146":2}}],["层归一化位于残差分支内部",{"5":{"146":2}}],["层归一化位于残差分支的输入端",{"5":{"41":2}}],["层归一化位于残差分支的输出端",{"5":{"41":2}}],["层归一化被放置在每个子层",{"5":{"146":2}}],["层归一化同样引入了可学习的仿射变换参数",{"5":{"146":2}}],["层归一化由ba等人于2016年在论文",{"5":{"146":2}}],["层归一化可以视为一种信息瓶颈操作",{"5":{"132":2}}],["层归一化导致的信息损失反映了归一化操作的信息论影响",{"5":{"132":2}}],["层归一化计算输入的均值和方差",{"5":{"132":2}}],["层归一化",{"2":{"146":1},"5":{"41":7,"146":7,"150":2}}],["层归一化定义为",{"5":{"41":2}}],["层归一化的优点是推理时稳定",{"5":{"146":2}}],["层归一化的样本条件独立性",{"5":{"146":2}}],["层归一化的均值向量",{"5":{"146":2}}],["层归一化的核心思想是",{"5":{"146":2}}],["层归一化的数学特性",{"2":{"146":1},"5":{"146":1}}],["层归一化的数学原理",{"2":{"41":1},"5":{"41":1}}],["层归一化的统计量维度",{"5":{"146":2}}],["层归一化的统计量",{"5":{"132":2}}],["层归一化的关键优势是它不依赖于batch",{"5":{"41":2}}],["层归一化的计算特性",{"5":{"41":2}}],["层归一化在transformer架构中扮演着核心角色",{"5":{"146":1}}],["层归一化在transformer中的应用",{"2":{"146":1},"5":{"146":1}}],["层归一化在特征维度上进行归一化",{"5":{"41":2}}],["层归一化在每个样本内部计算统计量",{"5":{"41":2}}],["层归一化稳定了注意力计算和前馈计算的输入分布",{"5":{"41":2}}],["层后的梯度范数上界为",{"5":{"148":1}}],["层后",{"5":{"41":1,"42":1,"148":1}}],["层的残差网络",{"5":{"150":2}}],["层的残差贡献",{"5":{"92":1,"153":1}}],["层的网络",{"5":{"148":1}}],["层的累积雅可比矩阵为",{"5":{"148":1}}],["层的雅可比矩阵",{"5":{"148":1}}],["层的维度",{"5":{"148":1}}],["层的净输入为",{"5":{"148":1}}],["层的净输入",{"5":{"148":1}}],["层的权重矩阵为",{"5":{"148":1}}],["层的权重矩阵和偏置向量",{"5":{"46":1}}],["层的深度神经网络",{"5":{"148":1}}],["层的裁剪阈值",{"5":{"147":2}}],["层的全连接网络",{"5":{"145":2}}],["层的近似误差为",{"5":{"142":1}}],["层的激活向量为",{"5":{"148":1}}],["层的激活值",{"5":{"135":2}}],["层的激活函数",{"5":{"46":1}}],["层的",{"5":{"135":2}}],["层的信息损失",{"5":{"133":1}}],["层的计算为",{"5":{"41":1}}],["层的计算定义为",{"5":{"44":1}}],["层的误差信号",{"5":{"44":1}}],["层的误差信号矩阵为",{"5":{"44":1}}],["层的前馈网络",{"5":{"41":1,"42":1,"47":1}}],["层的前馈神经网络",{"5":{"45":1,"46":1}}],["层的输入",{"5":{"41":1}}],["层的输入为",{"5":{"45":1,"46":1,"92":1,"150":2,"153":1}}],["层的输出",{"5":{"45":1,"145":2}}],["层的输出为",{"5":{"47":1}}],["层的概念在神经网络架构设计中具有核心地位",{"5":{"46":2}}],["层的感受野大小约为",{"5":{"92":1,"153":1}}],["层的表示向量",{"5":{"71":1}}],["层的表示",{"5":{"92":1,"137":1,"153":1}}],["层的transformer",{"5":{"92":1,"153":1}}],["层前馈网络",{"5":{"44":1}}],["层前馈网络定义的函数​是各层映射的复合",{"5":{"45":1}}],["层前馈网络定义的函数",{"5":{"45":1}}],["层前馈网络的计算可以表示为一系列函数的复合",{"5":{"46":2}}],["层误差信号",{"5":{"44":1}}],["层偏置的梯度为",{"5":{"44":2}}],["层全连接网络",{"5":{"44":1,"45":1}}],["层映射",{"5":{"45":2}}],["层内并行",{"5":{"45":2}}],["层内的输出神经元",{"5":{"45":1}}],["层非线性变换",{"5":{"45":1}}],["层操作",{"5":{"47":2}}],["层网络中",{"5":{"45":1}}],["层网络",{"5":{"47":1,"133":1}}],["层网络的等效仿射变换形式",{"5":{"47":1}}],["层次化注意力实现了从低层到高层的信息抽象",{"5":{"132":2}}],["层次化信息抽象",{"5":{"132":2}}],["层次化特征学习",{"5":{"71":2}}],["层次化位置编码针对文档级别的长上下文设计",{"5":{"88":1}}],["层次化位置编码",{"5":{"88":1}}],["层级化的依赖建模",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["层级特征与依赖跨度",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["层级",{"5":{"92":2,"153":2}}],["层位置",{"5":{"92":1,"153":1}}],["层转换为一个高维向量",{"5":{"94":2}}],["overfitting",{"5":{"145":2}}],["otimes",{"5":{"143":8}}],["ott",{"5":{"136":2}}],["observed",{"5":{"137":4}}],["ogy",{"5":{"136":4}}],["opt",{"5":{"142":10}}],["optimality",{"5":{"156":2}}],["optimal",{"5":{"138":4,"156":2}}],["optimization",{"5":{"101":4,"147":2,"154":2,"155":2}}],["openai的研究人员分析了",{"5":{"138":2}}],["operations",{"5":{"45":2}}],["o",{"5":{"69":4,"86":2,"132":4,"140":1,"151":2}}],["o^",{"5":{"69":8}}],["oord等人于2018年在论文",{"5":{"69":2}}],["odot",{"5":{"42":2,"86":6,"97":4,"143":18,"151":6}}],["odds",{"5":{"61":2,"71":2}}],["order",{"5":{"45":2}}],["or",{"5":{"47":4}}],["orthant",{"5":{"45":2}}],["orthogonality",{"5":{"51":2}}],["oracle",{"5":{"92":2,"153":2}}],["out",{"5":{"101":2,"154":2}}],["outliers",{"5":{"51":2}}],["outer",{"5":{"61":2}}],["output",{"5":{"86":2,"132":8,"151":2}}],["one",{"5":{"61":2,"70":2,"96":4,"137":2,"139":2,"141":4,"143":2}}],["only特性可能限制了对这类信息的捕捉",{"5":{"88":2}}],["通信效率也越高",{"5":{"157":2}}],["通信计算重叠是隐藏通信开销的关键技术",{"5":{"157":2}}],["通信量为",{"5":{"157":4}}],["通信开销",{"5":{"157":1}}],["通信开销也相应减少",{"5":{"157":2}}],["通信开销是分布式moe训练中不可忽视的因素",{"5":{"157":2}}],["通信开销分析和优化策略三个方面进行深入讨论",{"5":{"157":2}}],["通信需要",{"5":{"157":2}}],["通信的数学理论",{"5":{"61":2}}],["通过增加参数量",{"5":{"159":2}}],["通过增加网络的深度",{"5":{"46":2}}],["通过正则化门控网络来促进专家的均匀使用",{"5":{"158":2}}],["通过正确的归一化累积得到最终结果",{"5":{"86":2,"151":2}}],["通过合理设计流水线",{"5":{"157":2}}],["通过使用学习到的价值函数",{"5":{"155":2}}],["通过使用行最大值进行归一化",{"5":{"86":2,"151":2}}],["通过梯度下降更新参数",{"5":{"155":2}}],["通过梯度下降优化",{"5":{"86":1,"151":1}}],["通过融合这些多尺度表示",{"5":{"150":2}}],["通过不同尺度的变换",{"5":{"150":2}}],["通过不同的方式定义",{"5":{"101":2}}],["通过fisher矩阵联系",{"5":{"149":2}}],["通过标量情形的直观分析和矩阵情形的严格证明",{"5":{"148":2}}],["通过奇异值分解",{"5":{"148":2}}],["通过修改损失函数来控制模型复杂度",{"5":{"147":2}}],["通过跳跃连接为梯度提供了",{"5":{"147":2}}],["通过归一化每层的激活值来稳定分布",{"5":{"147":2}}],["通过移除均值中心化步骤",{"5":{"146":2}}],["通过重参数化",{"5":{"146":2}}],["通过随机噪声注入隐式地惩罚大权重",{"5":{"145":2}}],["通过严格的数学证明",{"5":{"145":2}}],["通过严格的数学推导",{"5":{"144":2}}],["通过严格的数学推导和几何直觉",{"5":{"41":2}}],["通过仿射变换恢复表达能力",{"5":{"144":2}}],["通过减去均值和除以标准差",{"5":{"144":2}}],["通过l2正则化",{"5":{"144":2}}],["通过张量收缩保持输入的结构",{"5":{"143":2}}],["通过张量运算",{"5":{"50":2}}],["通过分解权重矩阵并保留主要奇异值",{"5":{"143":2}}],["通过分析其频谱特征",{"5":{"149":2}}],["通过分析每一层激活值的统计特性来自动选择最优的保留概率",{"5":{"145":2}}],["通过分析分岔图",{"5":{"136":2}}],["通过分析信息共享模式来理解多任务学习的优势",{"5":{"133":2}}],["通过分析模型参数与训练数据之间的信息流动",{"5":{"133":2}}],["通过分析",{"5":{"69":1,"135":2}}],["通过分析与真实互信息之间的关系",{"5":{"69":1}}],["通过分析权重矩阵的奇异值分布",{"5":{"50":2}}],["通过分析注意力权重的位置分布",{"5":{"92":2,"153":2}}],["通过分析注意力权重矩阵的分布和头的输出表示",{"5":{"93":2}}],["通过三个线性投影矩阵",{"5":{"141":2}}],["通过神经网络架构和权重来参数化这些条件概率分布",{"5":{"140":1}}],["通过transformer架构克服了n",{"5":{"140":2}}],["通过个条件概率的乘积计算",{"5":{"140":1}}],["通过泰勒展开和期望的线性性质可得",{"5":{"139":2}}],["通过适当选择学习率和正则化参数",{"5":{"136":2}}],["通过适当选择",{"5":{"136":2}}],["通过适当的初始化和正则化实现",{"5":{"89":2}}],["通过构造能量函数",{"5":{"136":1}}],["通过构造能量函数并证明其满足递推关系",{"5":{"136":1}}],["通过构造",{"5":{"136":2}}],["通过构建计算图",{"5":{"45":2}}],["通过建立收敛性理论",{"5":{"136":2}}],["通过谱分析",{"5":{"135":2}}],["通过积分计算和极限行为分析可得",{"5":{"135":2}}],["通过历史梯度信息估计各参数方向的有效曲率",{"5":{"135":2}}],["通过以下关系关联",{"5":{"135":2}}],["通过局部线性近似",{"5":{"134":2}}],["通过连续化可获得对离散算法行为的新洞察",{"5":{"134":2}}],["通过信息论的透镜",{"5":{"137":2}}],["通过信息论的视角",{"5":{"132":2}}],["通过信息瓶颈的视角",{"5":{"133":2}}],["通过比较共享表示与私有表示的信息量可以判断迁移效果",{"5":{"133":2}}],["通过比较可学习编码与正弦编码的矩阵性质",{"5":{"89":2}}],["通过显式控制信息量来正则化模型",{"5":{"133":2}}],["通过直接控制表示中的信息量来防止过拟合",{"5":{"133":2}}],["通过直接计算序列中任意两个位置之间的关联强度",{"5":{"95":2}}],["通过特征值分解优化目标函数",{"5":{"133":2}}],["通过kkt条件得到最优性条件",{"5":{"133":2}}],["通过学习率与",{"5":{"136":2}}],["通过学习门控值自适应控制信息流动",{"5":{"132":2}}],["通过学习来确定每个位置应该关注哪些位置",{"5":{"86":2,"151":2}}],["通过降维实现信息压缩",{"5":{"132":2}}],["通过压缩和保留两个操作实现信息处理",{"5":{"132":2}}],["通过对",{"5":{"142":1}}],["通过对施加稀疏正则化",{"5":{"142":1}}],["通过对这两个矩阵的特征值分析",{"5":{"135":2}}],["通过对比经典的n",{"5":{"140":2}}],["通过对比正负样本来学习信息量度量",{"5":{"133":2}}],["通过对比学习",{"5":{"69":2}}],["通过对随机假设的后验分布进行分析得到上界",{"5":{"133":2}}],["通过对矩母函数求导并在处求值",{"5":{"96":1}}],["通过对矩母函数求导并在",{"5":{"96":1}}],["通过对注意力矩阵进行低秩分解和近似",{"5":{"87":2,"152":2}}],["通过规范化每层的输入分布",{"5":{"41":2}}],["通过矩阵乘法的结合律和分配律",{"5":{"71":2}}],["通过序列顺序隐含",{"5":{"71":2}}],["通过",{"5":{"70":2,"71":2,"132":2,"135":4,"140":1,"158":2}}],["通过定义搜索空间和评估指标",{"5":{"41":2}}],["通过定义误差信号",{"5":{"44":2}}],["通过批量处理",{"5":{"44":2}}],["通过公式逐层传播到输入层",{"5":{"44":1}}],["通过公式",{"5":{"44":1}}],["通过卷积核提取局部特征",{"5":{"46":2}}],["通过注意力机制聚合信息",{"5":{"46":2}}],["通过在训练时使用gumbel",{"5":{"159":2}}],["通过在目标函数中添加kl散度惩罚项",{"5":{"154":2}}],["通过在有效目标函数中加入曲率惩罚",{"5":{"149":2}}],["通过在反向传播时放大损失值来间接放大梯度",{"5":{"44":2}}],["通过在隐藏层引入非线性激活函数",{"5":{"47":2}}],["通过在低精度",{"5":{"97":2}}],["通过一次矩阵运算同时处理多个样本",{"5":{"47":2}}],["通过hessian矩阵可以判断驻点的性质",{"5":{"48":2}}],["通过引入",{"5":{"146":1}}],["通过引入和",{"5":{"146":1}}],["通过引入随机性防止过拟合",{"5":{"132":2}}],["通过引入非线性变换",{"5":{"47":2}}],["通过引入一个校正因子来修复adam在训练早期的方差问题",{"5":{"48":2}}],["通过引入样本权重来反映这种重要性的差异",{"5":{"51":2}}],["通过引入遗忘因子",{"5":{"86":2,"151":2}}],["通过编码函数",{"5":{"69":1}}],["通过编码向量表示",{"5":{"71":2}}],["通过编码器学习将输入映射到该分布的参数",{"5":{"96":2}}],["通过边际似然",{"5":{"96":2}}],["通过逆运算解线性方程组",{"5":{"50":2}}],["通过保留最大的",{"5":{"143":1}}],["通过保留最大的个奇异值及其对应的奇异向量",{"5":{"143":1}}],["通过保留最大的k个特征值对应的特征向量",{"5":{"50":2}}],["通过保持的单调性来保证收敛性",{"5":{"48":1}}],["通过保持",{"5":{"48":1}}],["通过gram",{"5":{"50":2}}],["通过惩罚大权重来防止过拟合",{"5":{"50":2}}],["通过向logits添加gumbel噪声并进行softmax变换",{"5":{"71":2}}],["通过向量化操作可以将其转化为线性方程",{"5":{"50":2}}],["通过向量加法直接叠加到内容embedding上",{"5":{"88":2}}],["通过向输入嵌入添加位置相关的信息",{"5":{"91":2}}],["通过仔细规划张量的内存布局和运算顺序",{"5":{"50":2}}],["通过本节的学习",{"5":{"51":2,"86":2,"87":2,"89":2,"90":4,"92":2,"93":4,"94":2,"95":2,"151":2,"152":2,"153":2}}],["通过理解ppo的数学原理",{"5":{"154":2}}],["通过理解收敛",{"5":{"136":2}}],["通过理解误差信号的传播机制",{"5":{"44":2}}],["通过理解",{"5":{"61":2}}],["通过选择适当的分解方法和秩参数",{"5":{"143":2}}],["通过选择适当的秩",{"5":{"143":2}}],["通过选择适当的系数组合可以实现期望的正则化效果",{"5":{"133":2}}],["通过选择适当的权重和偏置",{"5":{"47":2}}],["通过选择合适的",{"5":{"86":2,"145":2,"151":2}}],["通过同时计算前向和后向的信息流来增强上下文建模能力",{"5":{"86":2,"151":2}}],["通过将大型权重张量分解为若干低秩张量",{"5":{"50":2}}],["通过将大型权重张量分解为张量网络的形式",{"5":{"50":2}}],["通过将点积除以​​",{"5":{"87":1,"152":1}}],["通过将点积除以",{"5":{"87":1,"152":1}}],["通过将每两个维度视为复数平面上的一个点",{"5":{"88":2}}],["通过将q和k分别投影到个子空间",{"5":{"93":1}}],["通过将q和k分别投影到",{"5":{"93":1}}],["通过将输入投影到低维空间",{"5":{"94":2}}],["通过控制点积的量级",{"5":{"87":2,"152":2}}],["通过先计算",{"5":{"87":2,"152":2}}],["通过限制每个位置只关注少数位置来降低计算复杂度",{"5":{"87":2,"152":2}}],["通过训练一个较小的",{"5":{"87":2,"152":2}}],["通过让不同维度编码不同频率的位置信息",{"5":{"88":2}}],["通过旋转操作来标记位置",{"5":{"88":2}}],["通过线性化系统在临界点附近求解振荡解",{"5":{"136":2}}],["通过线性关系连接",{"5":{"51":1}}],["通过线性插值来调整位置索引",{"5":{"88":2}}],["通过反向传播学习最优的稀疏结构",{"5":{"86":2,"151":2}}],["通过反向传播自动从数据中学习最优的位置表示",{"5":{"89":2}}],["通过反向传播优化得到",{"5":{"91":2}}],["通过实证研究",{"5":{"89":2}}],["通过调整这些系数可以控制信息压缩和保留的程度",{"5":{"133":2}}],["通过调整相位",{"5":{"90":2}}],["通过调节",{"5":{"86":2,"151":2,"155":2}}],["通过数学归纳法证明",{"5":{"47":2}}],["通过数学推导",{"5":{"90":2}}],["通过其递归结构隐式地携带了位置信息",{"5":{"91":2}}],["通过具体的数值示例",{"5":{"91":2}}],["通过投影矩阵",{"5":{"91":1}}],["通过位置编码",{"5":{"92":2,"153":2}}],["通过位置编码引入序列结构",{"5":{"101":2}}],["通过嵌入表示",{"5":{"92":2,"153":2}}],["通过可学习的方式自动发现最优的稀疏注意力模式",{"5":{"132":2}}],["通过可学习的参数",{"5":{"41":1}}],["通过可学习的参数化方式来自动发现最优的位置编码函数",{"5":{"88":1}}],["通过可视化注意力权重矩阵和统计分析注意力权重的分布特性",{"5":{"92":2,"153":2}}],["通过链式法则展开",{"5":{"92":2,"153":2}}],["通过多层堆叠的层级化结构",{"5":{"92":2,"153":2}}],["通过门控权重",{"5":{"93":2}}],["通过并行化计算增加了表示的丰富性",{"5":{"93":2}}],["通过并行计算在多个子空间中提取特征",{"5":{"93":2}}],["通过与混合专家模型的类比",{"5":{"93":2}}],["通过残差连接和层归一化",{"5":{"94":2}}],["通过计算",{"5":{"135":2}}],["通过计算每个头的净信息贡献来决定剪枝顺序",{"5":{"132":2}}],["通过计算真实排列的统计量在置换分布中的位置",{"5":{"96":2}}],["通过计算查询与各个键的相似度",{"5":{"95":2}}],["通过简单地让不同位置对应不同的旋转",{"5":{"88":2}}],["通过简单地修改注意力分数矩阵",{"5":{"95":2}}],["通过输出投影矩阵将拼接结果映射回维空间",{"5":{"93":1}}],["通过输出投影矩阵进行最终的线性变换",{"5":{"93":1}}],["通过输出投影矩阵",{"5":{"93":2}}],["通过输入变换适应不同需求",{"5":{"95":2}}],["通过动量或二阶信息加速逃离鞍点的过程",{"5":{"97":2}}],["通过自动调整学习率来缓解这个问题",{"5":{"97":2}}],["通过这些优化",{"5":{"157":2}}],["通过这些分析",{"5":{"97":2}}],["通过这种方式",{"5":{"154":2}}],["通过这个关系可以清晰地看到tanh如何从sigmoid演化而来",{"5":{"42":2}}],["通常的做法是在训练过程中动态调整",{"5":{"159":2}}],["通常还需要显式的辅助损失函数来正则化",{"5":{"159":2}}],["通常优先选择本地专家",{"5":{"157":2}}],["通常没有闭式解",{"5":{"156":2}}],["通常每隔几个训练步计算一次平均kl散度用于自适应调整",{"5":{"154":2}}],["通常初始化为1",{"5":{"150":2}}],["通常通过softmax归一化实现",{"5":{"150":2}}],["通常通过sigmoid激活函数实现",{"5":{"150":2}}],["通常通过点积或余弦相似度度量",{"5":{"91":2}}],["通常较大",{"5":{"146":1}}],["通常加在分母的rms值之后",{"5":{"146":2}}],["通常取0或1",{"5":{"148":2}}],["通常取2",{"5":{"147":2}}],["通常取",{"5":{"146":2}}],["通常添加一个小的正常数",{"5":{"146":2}}],["通常只在全连接层之后应用dropout",{"5":{"145":2}}],["通常采用以下策略",{"5":{"145":2}}],["通常与参数标度指数相近",{"5":{"138":2}}],["通常与位置或内容的重要程度相关",{"5":{"87":2,"152":2}}],["通常在训练初期使用较大的",{"5":{"157":2}}],["通常在",{"5":{"138":2}}],["通常在数千量级",{"5":{"50":1}}],["通常用损失函数或困惑度衡量",{"5":{"138":2}}],["通常用非常大的负数",{"5":{"95":2}}],["通常需要引入损失缩放",{"5":{"157":2}}],["通常需要离散化处理连续变量",{"5":{"133":2}}],["通常需要将不规则的稀疏模式转换为规则的块稀疏格式",{"5":{"86":2,"151":2}}],["通常假设",{"5":{"70":2}}],["通常假设向量分量的初始化方差使得",{"5":{"95":2}}],["通常设置在0",{"5":{"154":2}}],["通常设置在较小的值",{"5":{"69":1}}],["通常设置",{"5":{"150":2}}],["通常设置为可学习的参数或使用退火策略",{"5":{"158":2,"159":2}}],["通常设置为0",{"5":{"154":2}}],["通常设置为",{"5":{"145":1}}],["通常设置为左右",{"5":{"145":1}}],["通常设置为1或5",{"5":{"41":2}}],["通常",{"5":{"69":2,"89":2,"158":2,"159":8}}],["通常为1或2",{"5":{"159":2}}],["通常为15",{"5":{"101":2}}],["通常为困惑度的对数形式",{"5":{"138":2}}],["通常为0",{"5":{"101":2}}],["通常为mb",{"5":{"86":1,"151":1}}],["通常为",{"5":{"86":1,"150":2,"151":1}}],["通常是唯一的预训练任务",{"5":{"101":2}}],["通常是编码向量的余弦相似度或点积",{"5":{"69":2}}],["通常是人工选择的噪声分布",{"5":{"69":2}}],["通常是正定的",{"5":{"48":2}}],["通常保持较高秩",{"5":{"41":2}}],["通常能提供更好的收敛性质",{"5":{"48":2}}],["通常使用输入依赖的归一化噪声",{"5":{"159":2}}],["通常使用gae或简单td误差",{"5":{"155":2}}],["通常使用高斯策略的解析解来近似kl散度",{"5":{"154":2}}],["通常使用自然对数以简化梯度计算",{"5":{"137":2}}],["通常使用特殊的填充token",{"5":{"101":2}}],["通常使用默认值",{"5":{"48":1}}],["通常使用对数似然函数",{"5":{"61":1}}],["通常使用",{"5":{"61":1}}],["通常使用门控机制",{"5":{"93":2}}],["通常越小",{"5":{"87":2,"152":2}}],["通常不会同步增大",{"5":{"158":2}}],["通常不唯一",{"5":{"143":1}}],["通常不等于",{"5":{"50":1}}],["通常不是对称矩阵",{"5":{"87":1,"152":1}}],["通常以半精度",{"5":{"89":1}}],["通常具有较高的秩",{"5":{"89":2}}],["通常具有更好的外推性能",{"5":{"91":2}}],["通常远小于",{"5":{"93":1}}],["通常表现更好",{"5":{"91":2}}],["通常表示序列长度",{"5":{"95":1}}],["通常直接计算log",{"5":{"96":2}}],["通常对不同层使用差异化的学习率",{"5":{"97":2}}],["通道维度",{"5":{"41":1}}],["通用逼近器",{"5":{"71":2}}],["通用逼近定理",{"5":{"71":2}}],["通用逼近定理的数学表述",{"2":{"71":1},"5":{"71":1}}],["通用逼近定理的核心数学洞见在于",{"5":{"71":2}}],["通用逼近定理指出",{"5":{"71":2}}],["通用逼近定理表明",{"5":{"71":2}}],["考虑了重要性采样比率",{"5":{"154":2}}],["考虑了relu激活一半神经元输出为零的特性",{"5":{"41":2}}],["考虑sgd在极小值附近的稳态分布",{"5":{"149":2}}],["考虑softmax函数",{"5":{"69":2}}],["考虑分类问题中常用的交叉熵损失函数",{"5":{"149":2}}],["考虑分布",{"5":{"137":1}}],["考虑分布和均匀分布之间的",{"5":{"137":1}}],["考虑使用batch",{"5":{"148":2}}],["考虑使用sigmoid激活函数的神经元",{"5":{"47":2}}],["考虑前向传播中的激活值方差",{"5":{"148":2}}],["考虑裁剪阈值",{"5":{"147":2}}],["考虑简化的标量情形",{"5":{"147":2}}],["考虑简化情况",{"5":{"41":2}}],["考虑输入张量",{"5":{"146":2}}],["考虑特征向量",{"5":{"146":2}}],["考虑添加了l2正则化的损失函数",{"5":{"144":2}}],["考虑应用batchnorm的层",{"5":{"144":2}}],["考虑线性回归问题",{"5":{"144":2}}],["考虑线性层",{"5":{"41":2}}],["考虑中的单位圆",{"5":{"143":1}}],["考虑归一化常数",{"5":{"141":2}}],["考虑矩阵",{"5":{"141":2,"143":2}}],["考虑直接计算",{"5":{"140":1}}],["考虑直接计算与通过链式法则计算的时间复杂度对比",{"5":{"140":1}}],["考虑连续时间动态系统",{"5":{"136":1}}],["考虑连续时间动态系统或离散时间系统",{"5":{"136":1}}],["考虑参数化系统",{"5":{"136":4}}],["考虑离散时间动态系统",{"5":{"136":2}}],["考虑梯度下降的更新规则",{"5":{"147":2}}],["考虑梯度下降的迭代过程",{"5":{"144":2}}],["考虑梯度下降系统",{"5":{"135":2}}],["考虑梯度矩阵的元素​",{"5":{"44":1}}],["考虑梯度矩阵的元素",{"5":{"44":1}}],["考虑非线性动态系统",{"5":{"135":2}}],["考虑动态系统",{"5":{"134":2}}],["考虑表示的拓扑性质",{"5":{"133":2}}],["考虑下游应用需求",{"5":{"70":2}}],["考虑优化难度",{"5":{"70":2}}],["考虑数据分布特性",{"5":{"70":2}}],["考虑贝叶斯决策理论",{"5":{"70":2}}],["考虑查询向量",{"5":{"69":1}}],["考虑查询向量的梯度",{"5":{"69":1}}],["考虑一个包含",{"5":{"149":1}}],["考虑一个包含个训练样本的数据集",{"5":{"149":1}}],["考虑一个包含异常值的数据集",{"5":{"51":2}}],["考虑一个极度简化的标量模型可以帮助建立直观理解",{"5":{"148":2}}],["考虑一个梯度向量",{"5":{"147":2}}],["考虑一个标量输入",{"5":{"145":2}}],["考虑一个标准的神经网络层",{"5":{"145":2}}],["考虑一个标准的监督学习场景",{"5":{"51":2}}],["考虑一个单层网络",{"5":{"144":2}}],["考虑一个单层的多头注意力模块",{"5":{"93":2}}],["考虑一个需要学习",{"5":{"138":2}}],["考虑一个输入",{"5":{"101":2}}],["考虑一个正样本对",{"5":{"69":1}}],["考虑一个正样本对和个负样本",{"5":{"69":1}}],["考虑一个序列",{"5":{"69":4}}],["考虑一个二维简化情况",{"5":{"69":2}}],["考虑一个二维空间中的分类问题",{"5":{"71":2}}],["考虑一个样本",{"5":{"69":2}}],["考虑一个",{"5":{"41":1,"42":1,"92":1,"153":1}}],["考虑一个没有激活函数的浅层神经网络",{"5":{"71":2}}],["考虑一个简单的线性回归模型",{"5":{"145":2}}],["考虑一个简单的线性模型",{"5":{"70":2,"144":2}}],["考虑一个简单的二次损失函数",{"5":{"149":2}}],["考虑一个简单的二维优化问题",{"5":{"144":2}}],["考虑一个简单的二分类问题",{"5":{"70":2}}],["考虑一个简单的infonce实例",{"5":{"69":2}}],["考虑一个简单的例子",{"5":{"71":2,"92":2,"93":2,"153":2}}],["考虑一个简单的互信息估计问题",{"5":{"71":2}}],["考虑一个简单的两层神经网络",{"5":{"48":2}}],["考虑一个简单的单层线性模型",{"5":{"51":2}}],["考虑一个简单的单样本二分类场景",{"5":{"61":2}}],["考虑一个简单的rnn单元",{"5":{"92":2,"153":2}}],["考虑一个简单的长程依赖建模任务",{"5":{"92":2,"153":2}}],["考虑一个简单的1d卷积操作",{"5":{"93":2}}],["考虑一个简化的几何图像",{"5":{"149":2}}],["考虑一个简化的transformer层",{"5":{"45":2}}],["考虑一个简化的二维示例",{"5":{"51":2}}],["考虑一个简化的例子",{"5":{"87":2,"152":2}}],["考虑一个特定的样本集",{"5":{"51":2}}],["考虑一个多分类问题",{"5":{"61":2}}],["考虑一个参数化的模型分布",{"5":{"61":2}}],["考虑一个两层网络",{"5":{"46":2,"145":2}}],["考虑一个两层的情况",{"5":{"92":2,"153":2}}],["考虑一个层的前馈网络",{"5":{"41":1,"42":1}}],["考虑一个层的transformer",{"5":{"92":1,"153":1}}],["考虑一个典型的gpu配置",{"5":{"93":2}}],["考虑一个具有",{"5":{"148":2,"150":2}}],["考虑一个具有层的网络",{"5":{"148":1}}],["考虑一个具有层的深度神经网络",{"5":{"148":1}}],["考虑一个具有平均稀疏度",{"5":{"86":2,"151":2}}],["考虑一个具体的例子",{"5":{"94":2}}],["考虑残差网络的反向传播",{"5":{"41":2}}],["考虑因素",{"5":{"41":1}}],["考虑权重归一化后",{"5":{"41":2}}],["考虑权重和偏置",{"5":{"71":2}}],["考虑批量中每个样本的权重梯度",{"5":{"44":2}}],["考虑",{"5":{"44":1,"87":1,"94":1,"133":2,"143":1,"152":1}}],["考虑函数的非线性程度和表达能力",{"5":{"133":2}}],["考虑函数​",{"5":{"70":1}}],["考虑函数",{"5":{"45":2,"70":3,"97":4}}],["考虑单样本梯度",{"5":{"149":2}}],["考虑单层全连接网络",{"5":{"45":2}}],["考虑单个输出元素​",{"5":{"45":1}}],["考虑单个输出元素",{"5":{"45":1}}],["考虑全连接层的计算",{"5":{"45":2}}],["考虑超平面",{"5":{"47":2}}],["考虑异或",{"5":{"47":2}}],["考虑比值",{"5":{"47":2}}],["考虑将偏好对",{"5":{"101":1}}],["考虑将偏好对视为一个二分类问题",{"5":{"101":1}}],["考虑将样本",{"5":{"69":1}}],["考虑将样本的编码",{"5":{"69":1}}],["考虑将划分为两个子向量和​",{"5":{"96":1}}],["考虑将",{"5":{"91":1,"96":1}}],["考虑将维的编码向量投影到二维平面进行可视化",{"5":{"91":1}}],["考虑预测空间的概念",{"5":{"51":2}}],["考虑模型复杂度从低到高的变化",{"5":{"51":2}}],["考虑目标向量可以被分解为投影部分和垂直部分的和",{"5":{"51":1}}],["考虑目标向量",{"5":{"51":1}}],["考虑三层网络的简单情况",{"5":{"41":2}}],["考虑三个位置",{"5":{"86":2,"151":2}}],["考虑在策略梯度中添加一个基线",{"5":{"155":2}}],["考虑在鞍点",{"5":{"149":1}}],["考虑在鞍点附近对损失函数进行二阶近似",{"5":{"149":1}}],["考虑在给定输入的条件下",{"5":{"51":1}}],["考虑在给定输入",{"5":{"51":1}}],["考虑在线推理场景",{"5":{"86":2,"151":2}}],["考虑内存访问量",{"5":{"86":2,"151":2}}],["考虑门控注意力的一种特殊形式",{"5":{"86":2,"151":2}}],["考虑的第个分量​和的第个分量​",{"5":{"44":1}}],["考虑的任意行的范数为1",{"5":{"87":1,"152":1}}],["考虑信息在注意力图上的传播",{"5":{"87":2,"152":2}}],["考虑注意力计算的输出",{"5":{"87":2,"152":2}}],["考虑注意力矩阵的几种常用范数",{"5":{"87":2,"152":2}}],["考虑注意力矩阵的特征值分解",{"5":{"87":1,"152":1}}],["考虑注意力矩阵的参数化",{"5":{"87":2,"152":2}}],["考虑注意力矩阵",{"5":{"87":1,"152":1}}],["考虑注意力层的输出关于位置编码的梯度",{"5":{"89":1}}],["考虑注意力层的输出关于位置编码",{"5":{"89":1}}],["考虑注意力输出的损失函数",{"5":{"94":2}}],["考虑两种极端情况",{"5":{"87":2,"152":2}}],["考虑两个雅可比矩阵的乘积",{"5":{"148":2}}],["考虑两个位置和",{"5":{"88":1}}],["考虑两个位置和的编码向量​和​",{"5":{"90":1}}],["考虑两个位置和的编码向量​和",{"5":{"92":1,"153":1}}],["考虑两个位置",{"5":{"88":1,"90":1,"92":1,"153":1}}],["考虑两个复数编码向量​和​",{"5":{"90":1}}],["考虑两个复数编码向量",{"5":{"90":1}}],["考虑两个应用了rope的向量",{"5":{"92":2,"153":2}}],["考虑两个相邻的多头注意力层",{"5":{"93":2}}],["考虑两个不同的头和",{"5":{"93":1}}],["考虑两个不同的头",{"5":{"93":1}}],["考虑两层堆叠的情况",{"5":{"94":2}}],["考虑二分类的线性模型",{"5":{"147":2}}],["考虑二分类场景",{"5":{"70":2}}],["考虑二分类问题",{"5":{"47":2}}],["考虑二维平面上的旋转操作",{"5":{"88":2}}],["考虑正弦余弦编码矩阵",{"5":{"89":2}}],["考虑以下分解",{"5":{"87":2,"152":2}}],["考虑以下几种典型的奇异值分布模式",{"5":{"89":2}}],["考虑位置编码的优化问题",{"5":{"89":2}}],["考虑位置编码向量集合",{"5":{"89":2}}],["考虑位置编码向量的范数",{"5":{"91":2}}],["考虑位置和",{"5":{"90":1}}],["考虑位置和位置的编码向量​和​",{"5":{"91":1}}],["考虑位置的编码向量​",{"5":{"90":1}}],["考虑位置的复数编码​",{"5":{"90":1}}],["考虑位置变换算子",{"5":{"90":2}}],["考虑位置",{"5":{"90":3,"91":2}}],["考虑频率",{"5":{"90":2}}],["考虑编码矩阵的奇异值分解",{"5":{"89":2}}],["考虑编码向量​的每个维度配对",{"5":{"90":1}}],["考虑编码向量序列",{"5":{"90":2}}],["考虑编码向量",{"5":{"90":1}}],["考虑第层权重的梯度范数随层数递减的现象",{"5":{"148":1}}],["考虑第",{"5":{"90":1,"93":1,"148":1}}],["考虑第个频率成分的复指数序列",{"5":{"90":1}}],["考虑第个头的输出携带了多少关于输入的信息",{"5":{"93":1}}],["考虑能够唯一表示长度序列所需的最低频率",{"5":{"90":1}}],["考虑能够唯一表示长度",{"5":{"90":1}}],["考虑维度和的配对",{"5":{"91":1}}],["考虑维度",{"5":{"91":1}}],["考虑计算复杂度的对比",{"5":{"92":2,"153":2}}],["考虑从时刻到时刻的信息传递",{"5":{"92":1,"153":1}}],["考虑从时刻",{"5":{"92":1,"153":1}}],["考虑损失函数对第一层权重的梯度",{"5":{"148":2}}],["考虑损失函数关于​的梯度​",{"5":{"92":1,"153":1}}],["考虑损失函数关于​的梯度",{"5":{"92":1,"153":1}}],["考虑损失函数",{"5":{"92":2,"153":2}}],["考虑多层网络的梯度传播",{"5":{"148":2}}],["考虑多层网络的函数空间大小",{"5":{"133":2}}],["考虑多分类场景",{"5":{"70":2}}],["考虑多分类交叉熵损失",{"5":{"61":2}}],["考虑多头注意力中个头的query投影矩阵​的集合",{"5":{"93":1}}],["考虑多头注意力中",{"5":{"93":1}}],["考虑需要存储的中间结果",{"5":{"93":2}}],["考虑​的第一列",{"5":{"94":1}}],["考虑未缩放的注意力分数",{"5":{"94":2}}],["即某些专家的得分接近边界时",{"5":{"159":2}}],["即根据输入",{"5":{"159":2}}],["即批内分配比例",{"5":{"158":2}}],["即我们正在学习的策略",{"5":{"155":2}}],["即梯度仍然是无偏的",{"5":{"155":2}}],["即梯度在期望意义上不会指数级地消失或爆炸",{"5":{"148":2}}],["即增加选择这个动作的概率",{"5":{"154":2}}],["即这是一个",{"5":{"154":4}}],["即新旧策略差异过大",{"5":{"154":2}}],["即输入等于输出",{"5":{"150":2}}],["即输出是输入向左移动10位",{"5":{"92":2,"153":2}}],["即数据分布属于该模型族",{"5":{"149":2}}],["即由于有限批量大小导致的采样方差",{"5":{"149":2}}],["即先进行层归一化",{"5":{"146":2}}],["即令",{"5":{"146":2}}],["即只在输入到隐藏层的变换和隐藏层到输出层的变换中应用dropout",{"5":{"145":2}}],["即权重范数较小的解",{"5":{"145":2}}],["即不同维度之间没有协方差",{"5":{"145":2}}],["即不存在另一个解在所有任务上都不差且至少一个任务上更好",{"5":{"70":2}}],["即非线性变换之后",{"5":{"145":2}}],["即dropout的随机化操作不会改变输出的期望值",{"5":{"145":2}}],["即仅在对角线上有非零元素",{"5":{"143":2}}],["即所有可能的",{"5":{"140":1}}],["即所有可能的元组构成的集合",{"5":{"140":1}}],["即所有可能的预测向量的集合",{"5":{"51":1}}],["即所有可能的预测向量",{"5":{"51":1}}],["即和在给定的条件下独立",{"5":{"137":1}}],["即学习率减半时振幅约减为四分之一",{"5":{"136":2}}],["即损失曲面的总曲率",{"5":{"149":2}}],["即损失",{"5":{"136":2}}],["即损失函数值在连续时间点上呈现",{"5":{"136":2}}],["即损失函数对该层净输入的梯度",{"5":{"44":2}}],["即特征值实部随参数增大而增大",{"5":{"136":2}}],["即出现纯虚特征值",{"5":{"136":2}}],["即区间在映射",{"5":{"136":1}}],["即区间在映射下映射到自身",{"5":{"136":1}}],["即区分两个相近位置的能力取决于频率成分的选择",{"5":{"90":2}}],["即起点和终点相同的轨道",{"5":{"136":2}}],["即谱半径小于1",{"5":{"136":2}}],["即状态转移具有时间可加性",{"5":{"134":2}}],["即在给定的参数量下",{"5":{"138":2}}],["即在某些规模下",{"5":{"138":2}}],["即在时间",{"5":{"134":2}}],["即在真实类别上赋予的概率",{"5":{"96":1}}],["即在真实类别上赋予",{"5":{"96":1}}],["即各头贡献之和减去冗余重叠",{"5":{"132":2}}],["即相对于参考策略",{"5":{"101":2}}],["即模型无法区分序列的不同排列顺序",{"5":{"101":2}}],["即使模型拥有",{"5":{"159":2}}],["即使策略比率继续增大",{"5":{"154":2}}],["即使我们拥有的样本来自另一个不同的分布",{"5":{"154":2}}],["即使某些部分的即时奖励不是最高的",{"5":{"156":2}}],["即使某些层学到的特征不够理想",{"5":{"150":2}}],["即使某些位置被掩码遮挡",{"5":{"91":2}}],["即使某些位置的权重很小",{"5":{"92":2,"153":2}}],["即使当",{"5":{"150":2}}],["即使当前解远离最优解",{"5":{"70":2}}],["即使初始化正确",{"5":{"148":2}}],["即使是微小的初始化偏差",{"5":{"148":2}}],["即使是微小的差异也可能是统计显著的",{"5":{"96":2}}],["即使权重矩阵",{"5":{"148":2}}],["即使其他层的放大能力很强",{"5":{"148":2}}],["即使其他因子很大",{"5":{"148":2}}],["即使没有立即崩溃",{"5":{"148":2}}],["即使没有发生溢出",{"5":{"61":2}}],["即使在超过1000层的情况下也不会出现严重的梯度消失问题",{"5":{"147":2}}],["即使在概率接近",{"5":{"70":2}}],["即使每个残差块的",{"5":{"135":2}}],["即使误差",{"5":{"70":1}}],["即使误差很大",{"5":{"70":1}}],["即使",{"5":{"41":2,"61":2,"70":2,"148":2,"159":2}}],["即使的梯度完全消失",{"5":{"41":1}}],["即使输入梯度很小",{"5":{"41":2}}],["即使网络包含多个隐藏层",{"5":{"71":2}}],["即使不能保证达到全局最优",{"5":{"48":2}}],["即使这些最小值在参数空间中相距甚远",{"5":{"48":2}}],["即使很小",{"5":{"41":1}}],["即使很大的差异也可能不显著",{"5":{"96":2}}],["即使所有原假设都为真",{"5":{"96":2}}],["即使位置索引超出了训练范围",{"5":{"88":2}}],["即使最终只有",{"5":{"157":2}}],["即使最终能够表示相同的编码向量",{"5":{"89":2}}],["即使最低频率成分出现折叠",{"5":{"90":2}}],["即使采用lstm或gru等门控机制缓解这一问题",{"5":{"92":2,"153":2}}],["即使中间层的梯度很小",{"5":{"92":2,"153":2}}],["即进入显著饱和区域",{"5":{"41":2}}],["即",{"5":{"41":2,"42":2,"44":2,"45":5,"46":2,"47":4,"48":5,"50":10,"71":4,"86":5,"87":2,"90":3,"91":2,"95":1,"96":3,"134":4,"135":4,"136":2,"137":3,"138":2,"141":2,"145":4,"146":2,"147":4,"148":3,"149":4,"151":5,"152":2,"155":8,"156":4,"157":2,"158":2}}],["即具有实质贡献的维度数",{"5":{"41":2}}],["即当时​",{"5":{"41":1}}],["即当",{"5":{"41":1}}],["即交叉熵损失的曲率正好由softmax分布的fisher信息度量给出",{"5":{"71":2}}],["即为恒等映射",{"5":{"148":1}}],["即为",{"5":{"44":2}}],["即​",{"5":{"45":1,"48":1,"95":1}}],["即存在常数使得",{"5":{"41":1}}],["即存在常数",{"5":{"41":1}}],["即存在使得对所有成立",{"5":{"136":1}}],["即存在使得",{"5":{"48":1}}],["即存在",{"5":{"48":1,"136":1}}],["即存在一对位置使得且它们之间存在依赖",{"5":{"92":1,"153":1}}],["即存在一对位置",{"5":{"92":1,"153":1}}],["即存在方向使得在时增加",{"5":{"97":1}}],["即存在方向",{"5":{"97":1}}],["即误差信号矩阵在批量维度上的平均值",{"5":{"44":2}}],["即误差按几何级数衰减",{"5":{"48":2}}],["即动量",{"5":{"48":2}}],["即未中心化的方差",{"5":{"48":2}}],["即经过次迭代后",{"5":{"48":1}}],["即经过",{"5":{"48":1}}],["即一般情况下",{"5":{"96":2}}],["即的支持集必须包含的支持集",{"5":{"96":1}}],["即通常不等于",{"5":{"50":1}}],["即对输入进行均匀缩放不改变归一化的结果",{"5":{"146":2}}],["即对所有",{"5":{"136":1}}],["即对所有满足",{"5":{"136":1}}],["即对所有向量成立",{"5":{"50":1}}],["即对于每个输入",{"5":{"51":2}}],["即对于任意秩不超过",{"5":{"143":1}}],["即对于任意秩不超过的矩阵",{"5":{"143":1}}],["即对于任意",{"5":{"51":2}}],["即对于所有成立",{"5":{"90":1}}],["即对于所有维度",{"5":{"91":2}}],["即最大奇异值与最小奇异值的比值很大",{"5":{"148":2}}],["即最大的不确定性",{"5":{"96":2}}],["即最小化mse",{"5":{"51":2}}],["即给定前文的条件下预测下一个词的对数概率之和",{"5":{"96":2}}],["即给定样本",{"5":{"61":2}}],["即预测概率集中在真实类别上",{"5":{"61":2}}],["即是随机采样的子集",{"5":{"86":1,"151":1}}],["即每个专家获得相等的平均路由概率",{"5":{"158":2}}],["即每个专家处理相同比例的输入",{"5":{"158":2}}],["即每个特征",{"5":{"146":1}}],["即每个特征维度有一个均值和一个方差值",{"5":{"146":2}}],["即每个特征的归一化只依赖于该特征在批量中的统计量",{"5":{"146":1}}],["即每个样本的归一化只依赖于该样本本身的特征",{"5":{"146":2}}],["即每个样本有一个均值和一个方差值",{"5":{"146":2}}],["即每个样本只有一个正确的类别",{"5":{"61":2}}],["即每个位置可以访问所有其他位置的信息",{"5":{"132":2}}],["即每个位置平均关注个位置",{"5":{"86":1,"151":1}}],["即每个位置平均关注",{"5":{"86":1,"151":1}}],["即每个位置旋转固定的角度",{"5":{"88":2}}],["即指数核",{"5":{"86":2,"151":2}}],["即自注意力中query和key来自相同输入",{"5":{"87":2,"152":2}}],["即两个位置的相对距离",{"5":{"88":1}}],["即两个位置的",{"5":{"88":1}}],["即点沿圆周旋转角度",{"5":{"90":2}}],["即改变输入序列的顺序不会改变注意力输出的顺序",{"5":{"91":2}}],["即注意力分数显式依赖于相对位置",{"5":{"91":2}}],["即位置0的编码",{"5":{"91":2}}],["即位置的注意力主要集中在哪些位置",{"5":{"92":1,"153":1}}],["即位置",{"5":{"92":1,"153":1}}],["即三个空间的维度与输入嵌入维度相同",{"5":{"94":2}}],["即该方向是",{"5":{"97":2}}],["以改善内存访问的局部性",{"5":{"159":2}}],["以浮点运算次数计",{"5":{"159":2}}],["以学习将不同类型的输入路由到最适合的专家",{"5":{"158":2}}],["以逼近硬选择",{"5":{"157":2}}],["以最大化累积奖励",{"5":{"156":2}}],["以最小化测试损失",{"5":{"138":2}}],["以防止除零错误",{"5":{"150":2}}],["以上",{"5":{"138":2}}],["以获得最大信息增益",{"5":{"137":2}}],["以获得更好的泛化能力",{"5":{"101":2}}],["以指数速度收敛到平衡点",{"5":{"134":2}}],["以维持目标压缩比",{"5":{"133":2}}],["以适应更复杂的应用场景和提供更精细的控制",{"5":{"133":2}}],["以引入位置信息",{"5":{"69":2}}],["以概率收敛到真实参数",{"5":{"140":1}}],["以概率",{"5":{"61":2,"71":2}}],["以下从统计学习理论的角度进行分析",{"5":{"142":2}}],["以下",{"5":{"41":1}}],["以下是一些有前景的未来方向",{"5":{"41":2}}],["以下是深度学习中常用的激活函数及其数学定义",{"5":{"47":2}}],["以类别为基准",{"5":{"71":1}}],["以类别",{"5":{"71":1}}],["以二分类为例",{"5":{"44":2}}],["以处理批量输入",{"5":{"44":2}}],["以relu激活函数为例",{"5":{"45":2}}],["以使",{"5":{"45":1}}],["以及专家激活的熵",{"5":{"158":2}}],["以及辅助专家",{"5":{"158":2}}],["以及合理划分通信消息的大小",{"5":{"157":2}}],["以及中间激活",{"5":{"157":2}}],["以及隐式正则化效应",{"5":{"149":2}}],["以及网络的深度",{"5":{"148":2}}],["以及层间雅可比矩阵乘积的范数",{"5":{"148":1}}],["以及基于验证集性能的超参数搜索",{"5":{"147":2}}],["以及它们如何共同影响深度学习的优化景观和泛化性能",{"5":{"147":2}}],["以及它们在注意力机制中的数学角色",{"5":{"95":2}}],["以及期望保持性质在实践中的意义",{"5":{"145":2}}],["以及梯度裁剪与l2正则化的协同与竞争关系",{"5":{"144":2}}],["以及eckart",{"5":{"143":2}}],["以及对角矩阵",{"5":{"143":2}}],["以及深度神经网络权重矩阵中的低秩结构",{"5":{"143":2}}],["以及预判模型能力边界都具有重要的指导意义",{"5":{"138":2}}],["以及训练过程中的信息动态分析",{"5":{"137":2}}],["以及平坦最小值的性质",{"5":{"135":2}}],["以及为什么残差连接有助于训练深层网络",{"5":{"133":2}}],["以及如何设计更好的优化策略来利用或抑制梯度噪声的特性",{"5":{"149":2}}],["以及如何通过最大似然估计来学习模型参数",{"5":{"140":2}}],["以及如何利用信息论工具理解和改进大模型的训练与泛化性能",{"5":{"133":2}}],["以及如何从复数旋转的角度理解位置变换",{"5":{"90":2}}],["以及多种负载均衡策略的数学原理",{"5":{"158":2}}],["以及多头注意力如何通过信息分解和互补性分析实现信息融合",{"5":{"132":2}}],["以及多强地存在依赖关系",{"5":{"92":2,"153":2}}],["以及位置编码作为特征工程与损失函数优化的相互作用",{"5":{"70":2}}],["以及",{"5":{"69":1,"70":2,"93":2,"148":1}}],["以及个负样本​",{"5":{"69":1}}],["以及实际实现中的数值稳定性问题",{"5":{"44":2}}],["以及计算效率",{"5":{"47":2}}],["以及整个神经网络的矩阵表示形式",{"5":{"47":2}}],["以及局部最小值通常具有相似的损失值等",{"5":{"48":2}}],["以及归一性",{"5":{"96":2}}],["以及某些正则化技术的理论基础",{"5":{"96":2}}],["以及模型输出的不确定性估计",{"5":{"96":2}}],["以及在大模型场景下的应用",{"5":{"133":2}}],["以及在概率图模型中作为共轭先验使用",{"5":{"96":2}}],["以及在变分自编码器中定义潜在空间的先验分布",{"5":{"96":2}}],["以及在神经网络中的梯度计算特性",{"5":{"51":2}}],["以及在不同场景下的适用性",{"5":{"86":2,"151":2}}],["以及在实际应用中的优化策略",{"5":{"89":2}}],["以及评估模型对不同类型输入的响应一致性",{"5":{"96":2}}],["以及评估词嵌入中捕获的语义信息量",{"5":{"96":2}}],["以及总体mse与样本mse的区别",{"5":{"51":2}}],["以及注意力矩阵的谱性质与低秩结构",{"5":{"86":2,"151":2}}],["以及更好的线程分配策略",{"5":{"86":2,"151":2}}],["以及设计更高效的变体都具有重要意义",{"5":{"87":2,"152":2}}],["以及与gram矩阵和核矩阵的关系",{"5":{"87":2,"152":2}}],["以及缩放因子在控制条件数中的作用",{"5":{"87":2,"152":2}}],["以及其在计算优化和泛化能力方面的意义",{"5":{"87":2,"152":2}}],["以及开发新算法奠定了坚实的基础",{"5":{"87":2,"152":2}}],["以及纯位置项",{"5":{"88":2}}],["以及表达能力与泛化能力之间的权衡",{"5":{"89":2}}],["以及参数效率和实践考量",{"5":{"89":2}}],["以及复数形式的统一分析",{"5":{"90":2}}],["以及时域与频域之间的对偶性",{"5":{"90":2}}],["以及频率选择的最优性和正则化效应",{"5":{"90":2}}],["以及后续学习旋转位置编码",{"5":{"90":2}}],["以及内容与位置的交互",{"5":{"91":2}}],["以及编码向量的点积与相对位置的数学关系",{"5":{"91":2}}],["以及编码空间的几何结构",{"5":{"91":2}}],["以及优化大语言模型的性能都具有重要意义",{"5":{"91":2}}],["以及不同头如何通过",{"5":{"92":2,"153":2}}],["以及不同头的输出之间有多少冗余或互补信息",{"5":{"93":2}}],["以及各种变体设计",{"5":{"93":2}}],["以及这一效应在不同优化器",{"5":{"144":2}}],["以及这些数学性质如何为模型压缩和高效微调提供理论依据",{"5":{"142":2}}],["以及这些性质如何影响大模型的能力和训练动态",{"5":{"132":2}}],["以及这种能力的来源和边界",{"5":{"92":2,"153":2}}],["以及这三个投影矩阵在数学上的性质和作用",{"5":{"94":2}}],["以及什么样的内容应该作为",{"5":{"94":2}}],["以此类推直到输入层",{"5":{"48":2}}],["以计算换内存",{"5":{"48":2}}],["以确保激活专家的权重和为1",{"5":{"159":2}}],["以确保每个专家都能得到充分的学习",{"5":{"159":2}}],["以确保数值稳定性",{"5":{"146":2}}],["以确保梯度可以稳定地反向传播",{"5":{"41":2}}],["以确保信号在各层之间平稳传播",{"5":{"96":2}}],["以捕获更复杂的多模态特性",{"5":{"96":2}}],["以transformer模型为例",{"5":{"50":2}}],["以匹配较大的张量",{"5":{"50":2}}],["以保持数值稳定性",{"5":{"50":2}}],["以2为底时",{"5":{"61":2}}],["以平衡表达能力和计算效率",{"5":{"87":1,"152":1}}],["以兼取两者的优势",{"5":{"89":2}}],["以减少内存占用",{"5":{"89":2}}],["以便利用gpu的高效并行计算能力",{"5":{"88":2}}],["以便于按位置查询",{"5":{"89":2}}],["以提高计算效率",{"5":{"69":2}}],["以提高数值稳定性和计算效率",{"5":{"61":2}}],["以提供一定的余量",{"5":{"89":2}}],["以卷积核大小为的卷积为例",{"5":{"92":1,"153":1}}],["以卷积核大小为",{"5":{"92":1,"153":1}}],["以控制计算成本",{"5":{"93":2}}],["以注入序列顺序信息",{"5":{"94":2}}],["以​的梯度为例",{"5":{"94":1}}],["以",{"5":{"94":1}}],["以机器翻译任务为例",{"5":{"95":2}}],["以避免数值计算问题",{"5":{"95":2}}],["以实现更好的优化效果",{"5":{"97":2}}],["以加速收敛",{"5":{"97":2}}],["类真实标签",{"5":{"139":2}}],["类模型的信息分析",{"5":{"137":2}}],["类型",{"5":{"134":3}}],["类别概率",{"5":{"69":1}}],["类别为1的条件概率",{"5":{"47":2}}],["类别分布",{"5":{"96":2}}],["类似地可证",{"5":{"137":2}}],["类似地",{"5":{"41":2,"42":2,"87":2,"93":6,"94":4,"138":2,"152":2}}],["类似的分析要求权重方差的缩放与",{"5":{"148":1}}],["类似的分析要求权重方差的缩放与有关",{"5":{"148":1}}],["类似的稳定性分析有助于理解和解决梯度消失或梯度爆炸问题",{"5":{"50":2}}],["类似的计算对key和value重复进行",{"5":{"93":2}}],["类似于残差块中的",{"5":{"150":2}}],["类似于对每个神经元进行独立的标准化",{"5":{"146":2}}],["类似于cp分解",{"5":{"143":2}}],["类似于物理学中的相变现象",{"5":{"138":2}}],["类似于数据的压缩编码",{"5":{"137":2}}],["类似于",{"5":{"136":2}}],["类似于人类认知过程中的抽象和概括",{"5":{"133":2}}],["类似于带状矩阵",{"5":{"87":2,"152":2}}],["类似于moe模型",{"5":{"93":2}}],["类似argmax操作",{"5":{"96":2}}],["类",{"5":{"61":4}}],["类的概率估计",{"5":{"71":1}}],["类的",{"5":{"61":2,"69":1}}],["类比于人类认知过程中会从多个角度",{"5":{"93":2}}],["样本数量的随机性会导致梯度方差的波动",{"5":{"157":2}}],["样本效率低",{"5":{"155":2}}],["样本间的梯度差异也较小",{"5":{"149":2}}],["样本条件独立",{"5":{"146":1}}],["样本方差衡量了第",{"5":{"146":1}}],["样本方差衡量了第个特征在当前批量中的离散程度",{"5":{"146":1}}],["样本方差的方差与",{"5":{"146":2}}],["样本均值的方差与",{"5":{"146":1}}],["样本均值的方差与成正比",{"5":{"146":1}}],["样本均值的估计误差满足",{"5":{"146":2}}],["样本均值趋向于随机变量的期望",{"5":{"96":2}}],["样本的行为是未定义的",{"5":{"101":2}}],["样本",{"5":{"61":2,"71":1,"137":4}}],["样本可以表示为确定函数",{"5":{"71":1}}],["样本级并行",{"5":{"45":3}}],["样本量很大时",{"5":{"96":2}}],["样本量很小时",{"5":{"96":2}}],["样本mse",{"5":{"51":2}}],["样本mse是总体mse的无偏估计",{"5":{"51":2}}],["样本mse收敛于总体mse",{"5":{"51":2}}],["样本mse与总体mse之间的关系可以用以下方差分解来表示",{"5":{"51":2}}],["最主要的问题是稀疏访问模式与现代硬件的优化方向不完全匹配",{"5":{"159":2}}],["最简单的负载均衡损失是基于专家分配频率的变分损失",{"5":{"158":2}}],["最低效率",{"5":{"157":2}}],["最低频率成分的周期小于序列长度",{"5":{"90":2}}],["最低频率对应周期10000",{"5":{"90":1}}],["最低频率",{"5":{"90":1}}],["最高效率",{"5":{"157":2}}],["最高频率对应周期1",{"5":{"90":1}}],["最高频率",{"5":{"90":1,"91":1}}],["最高频率​对应的周期仍然大于序列长度",{"5":{"91":1}}],["最",{"5":{"149":2}}],["最根本的优化挑战之一",{"5":{"148":2}}],["最强",{"5":{"145":1}}],["最重要的工具",{"5":{"143":2}}],["最近的研究尝试建立更精细的模型来预测涌现规模",{"5":{"138":2}}],["最慢方向的衰减因子接近",{"5":{"135":2}}],["最慢收敛方向",{"5":{"135":2}}],["最快收敛方向",{"5":{"135":2}}],["最快收敛",{"5":{"134":4}}],["最佳适用场景",{"5":{"146":1}}],["最佳线性无偏估计",{"5":{"70":2}}],["最佳均值预测",{"5":{"51":2}}],["最相似的键向量",{"5":{"141":1}}],["最相似的键获得接近1的概率",{"5":{"69":1}}],["最相关",{"5":{"69":2}}],["最后添加残差连接",{"5":{"146":2}}],["最后是仿射变换",{"5":{"146":2}}],["最后补充正交基完成",{"5":{"143":1}}],["最后补充正交基完成和的构造",{"5":{"143":1}}],["最后揭示了infonce与注意力机制中softmax操作的数学统一性",{"5":{"101":2}}],["最后",{"5":{"41":2,"44":2,"51":4,"71":2,"86":2,"87":2,"89":2,"90":2,"91":2,"92":2,"93":4,"97":2,"143":2,"144":2,"146":8,"149":4,"151":2,"152":2,"153":2}}],["最后用这个分布对value进行加权平均",{"5":{"71":2}}],["最后将梯度用于参数更新",{"5":{"44":2}}],["最后旋转回原坐标系",{"5":{"45":2}}],["最后通过对比损失",{"5":{"71":2}}],["最后通过解码器重建原始输入",{"5":{"96":2}}],["最后通过与真实标签计算交叉熵得到损失值",{"5":{"61":2}}],["最后对所有样本的平方误差求平均",{"5":{"51":2}}],["最后一层的误差信号为",{"5":{"51":2}}],["最后建立了交叉熵与最大似然估计之间的深刻联系",{"5":{"61":2}}],["最后在输出空间中进行另一次旋转和反射",{"5":{"94":2}}],["最陡点",{"5":{"42":2}}],["最大分配频率与最小分配频率的比值",{"5":{"158":2}}],["最大容量为",{"5":{"157":2}}],["最大1",{"5":{"148":1}}],["最大值可达1",{"5":{"148":2}}],["最大值仅为",{"5":{"148":2}}],["最大值在处取得",{"5":{"42":2}}],["最大值在",{"5":{"42":2}}],["最大奇异值",{"5":{"144":1}}],["最大项",{"5":{"141":1}}],["最大项将主导这个和",{"5":{"141":1}}],["最大范数",{"5":{"136":2}}],["最大",{"5":{"136":2}}],["最大特征值对应的特征值",{"5":{"136":2}}],["最大特征值对应的特征方向出现零特征值",{"5":{"136":2}}],["最大似然与交叉熵的等价性",{"2":{"139":1},"5":{"139":1}}],["最大似然与交叉熵的详细推导<",{"5":{"131":1}}],["最大似然与交叉熵的详细推导",{"0":{"139":1},"4":{"139":1},"5":{"131":4,"139":1}}],["最大似然估计与交叉熵是深度学习的理论基础",{"5":{"139":2}}],["最大似然估计具有以下性质",{"5":{"139":2}}],["最大似然估计具有一些重要的渐近性质",{"5":{"96":2}}],["最大似然估计",{"5":{"51":2,"61":1,"96":2,"139":2,"140":2}}],["最大似然估计就是找到使对数似然最大的参数值",{"5":{"96":2}}],["最大似然估计等价于最小化mse",{"5":{"51":2}}],["最大似然估计是统计学中参数估计的核心方法",{"5":{"61":2}}],["最大似然估计的理论性质",{"2":{"140":1},"5":{"140":1}}],["最大似然估计的存在性条件",{"2":{"139":1},"5":{"139":1}}],["最大似然估计的数学原理",{"2":{"139":1},"5":{"139":1}}],["最大似然估计的基本原理",{"2":{"61":1},"5":{"61":1}}],["最大似然估计的原理是选择使对数似然最大的参数值",{"5":{"61":1}}],["最大化对数似然",{"5":{"139":2}}],["最大化对数似然等价于最小化交叉熵损失",{"5":{"140":2}}],["最大化对数似然等价于最小化交叉熵",{"5":{"139":2}}],["最大化对数似然等价于最小化",{"5":{"51":2}}],["最大化目标信息量或最小化输入信息量",{"5":{"133":2}}],["最大化",{"5":{"133":2}}],["最大化总信息量同时最小化冗余",{"5":{"132":2}}],["最大化期望奖励同时保持与初始策略的接近",{"5":{"101":2}}],["最大化专家行为的似然",{"5":{"101":2}}],["最大化互信息的下界",{"5":{"69":2}}],["最大化计算效率",{"5":{"88":2}}],["最大序列长度",{"5":{"89":2}}],["最终输出为",{"5":{"158":2}}],["最终输出是所有专家贡献的线性组合",{"5":{"158":2}}],["最终输出是各专家输出的加权组合",{"5":{"93":2}}],["最终揭示dropout在深度学习优化中的深层数学机制",{"5":{"145":2}}],["最终系统可能进入混沌状态",{"5":{"136":2}}],["最终估计",{"5":{"133":2,"137":2}}],["最终产生网络输出",{"5":{"44":2}}],["最终产生网络输出的过程",{"5":{"45":2}}],["最终映射到输出空间",{"5":{"45":4}}],["最终达到较好的收敛效果",{"5":{"48":2}}],["最终得到上述三部分之和",{"5":{"51":2}}],["最终收敛到特定的分布",{"5":{"89":2}}],["最终我们在山顶欣赏到了美丽的日出",{"5":{"92":2,"153":2}}],["最终通过拼接和投影进行融合",{"5":{"93":2}}],["最终学习到不对称的解",{"5":{"93":2}}],["最终的矩阵乘法同样可以高效地批量处理",{"5":{"95":2}}],["最终性能的决定因素",{"5":{"97":2}}],["最广泛使用的损失函数之一",{"5":{"51":2}}],["最优策略",{"5":{"156":1}}],["最优基线",{"5":{"155":4}}],["最优裁剪阈值的正则化解释",{"5":{"147":2}}],["最优的早停时间点需要通过验证集性能来确定",{"5":{"144":2}}],["最优的损失函数是什么呢",{"5":{"70":2}}],["最优性条件",{"5":{"139":2}}],["最优动量系数为",{"5":{"136":2}}],["最优参数满足",{"5":{"134":2}}],["最优学习率与最慢方向的最优学习率存在冲突",{"5":{"135":2}}],["最优学习率与最慢方向的冲突加剧",{"5":{"134":2}}],["最优学习率",{"5":{"134":2}}],["最优学习率可以通过hessian的特征值精确计算",{"5":{"97":2}}],["最优",{"5":{"133":1}}],["最优由广义特征值问题确定",{"5":{"133":1}}],["最优稀疏模式应最小化信息损失与稀疏性之间的权衡",{"5":{"132":1}}],["最优稀疏模式",{"5":{"132":3}}],["最优头配置",{"5":{"132":2}}],["最优预测为",{"5":{"70":1}}],["最优预测为条件众数",{"5":{"70":1}}],["最优预测为条件均值",{"5":{"70":2}}],["最优预测向量是目标向量在预测空间上的正交投影",{"5":{"51":1}}],["最优预测向量",{"5":{"51":1}}],["最优解在压缩与保留之间取得平衡",{"5":{"133":2}}],["最优解",{"5":{"51":1}}],["最优解满足一个关键的性质",{"5":{"51":1}}],["最优解满足梯度为零条件",{"5":{"89":2}}],["最优解处的hessian矩阵应该是半正定的",{"5":{"89":2}}],["最优低秩逼近",{"5":{"89":2}}],["最优编码倾向于保留那些对预测",{"5":{"133":1}}],["最优编码倾向于保留那些对预测有价值的特征",{"5":{"133":1}}],["最优编码的条件",{"5":{"133":2}}],["最优编码的性质还取决于任务和数据分布",{"5":{"89":2}}],["最优编码可能更",{"5":{"89":4}}],["最优逼近可以通过保留前个奇异值和对应的奇异向量来实现",{"5":{"89":1}}],["最优逼近可以通过保留前",{"5":{"89":1}}],["最小二乘意义下",{"5":{"143":2}}],["最小二乘估计对应的预测为",{"5":{"51":1}}],["最小二乘估计",{"5":{"51":1}}],["最小描述长度原理",{"2":{"139":1},"5":{"139":1}}],["最小描述长度",{"5":{"137":2}}],["最小梯度范数会以",{"5":{"136":1}}],["最小梯度范数会以的速率趋于零",{"5":{"136":1}}],["最小特征值",{"5":{"135":2}}],["最小特征值可能很小",{"5":{"97":2}}],["最小温度",{"5":{"132":2}}],["最小化这个迹等价于寻找曲率较小的极小值",{"5":{"149":2}}],["最小化期望损失等价于最小化原始损失加上一个与权重范数成正比的正则化项",{"5":{"145":2}}],["最小化ce等价于最大化对数似然",{"5":{"140":2}}],["最小化鼓励表示对对抗扰动不敏感",{"5":{"133":1}}],["最小化的是",{"5":{"70":2}}],["最小化kl散度",{"5":{"69":2}}],["最小化nce损失等价于最小化数据分布与模型分布之间的kl散度",{"5":{"69":2}}],["最小化infonce损失等价于最大化互信息的下界",{"5":{"69":2,"71":2}}],["最小化负对数似然等价于最大化对数似然",{"5":{"61":2}}],["最小化",{"5":{"61":2,"133":1}}],["最小化交叉熵损失等价于最小化真实标签分布",{"5":{"137":1}}],["最小化交叉熵损失等价于最小化真实标签分布和模型预测分布之间的",{"5":{"137":1}}],["最小化交叉熵损失等价于最小化模型分布与数据分布之间的",{"5":{"61":2}}],["最小化交叉熵等价于最大化数据的对数似然",{"5":{"96":2}}],["最小化交叉熵等价于最小化kl散度",{"5":{"61":2}}],["最小化上式等价于最小化",{"5":{"61":2}}],["最小奇异值为0",{"5":{"87":4,"152":4}}],["最远",{"5":{"92":2,"153":2}}],["最常见的变分dropout使用对数正态先验",{"5":{"145":2}}],["最常见的配置是令",{"5":{"94":2}}],["最常用的方法是加权求和",{"5":{"70":2}}],["个设备负责专家",{"5":{"159":2}}],["个设备",{"5":{"159":2}}],["个设备上",{"5":{"157":4}}],["个不同的函数",{"5":{"159":2}}],["个不同位置的信息标签",{"5":{"132":1}}],["个子块",{"5":{"157":4}}],["个子空间",{"5":{"93":1}}],["个被使用",{"5":{"157":2}}],["个专家参与计算",{"5":{"159":2}}],["个专家网络",{"5":{"159":2}}],["个专家会被实际计算",{"5":{"158":2}}],["个专家都可以同时对输入进行计算",{"5":{"158":2}}],["个专家输出的集合",{"5":{"158":2}}],["个专家的输出",{"5":{"159":2}}],["个专家的moe模型可以表示为函数",{"5":{"159":2}}],["个专家的moe层",{"5":{"158":2}}],["个专家的索引集合",{"5":{"159":4}}],["个专家的前馈网络",{"5":{"159":2}}],["个专家的计算量为",{"5":{"159":2}}],["个专家的计算量",{"5":{"159":2}}],["个专家的累积概率",{"5":{"158":2}}],["个专家的中间激活",{"5":{"157":2}}],["个专家",{"5":{"157":4,"159":14}}],["个专家分配到",{"5":{"157":2}}],["个计算设备",{"5":{"157":2}}],["个尺度的变换",{"5":{"150":2}}],["个尺度的变换函数",{"5":{"150":2}}],["个为零或接近零的特征值",{"5":{"149":1}}],["个特征的样本方差",{"5":{"146":1}}],["个特征在当前批量中的离散程度",{"5":{"146":1}}],["个特征在当前批量中的中心位置",{"5":{"146":1}}],["个特征在当前批量上的样本均值",{"5":{"146":1}}],["个特征",{"5":{"146":1}}],["个小卷积的和",{"5":{"143":1}}],["个右奇异向量",{"5":{"143":1}}],["个左奇异向量更新",{"5":{"143":3}}],["个左奇异向量",{"5":{"143":1}}],["个最大的奇异值",{"5":{"143":1}}],["个最重要位置",{"5":{"132":1}}],["个非零奇异值对应的特征向量构成",{"5":{"143":1}}],["个点",{"5":{"141":1}}],["个条件概率的乘积计算",{"5":{"140":1}}],["个可能的序列",{"5":{"140":1}}],["个词元所需的所有相关信息",{"5":{"140":1}}],["个词元",{"5":{"140":2}}],["个词元的原始置信度评分",{"5":{"96":1}}],["个训练样本的数据集",{"5":{"149":1}}],["个训练序列的真实词元",{"5":{"140":1}}],["个训练序列",{"5":{"140":1}}],["个训练",{"5":{"138":2}}],["个显著的非零特征值",{"5":{"135":2}}],["个正特征值",{"5":{"135":2}}],["个正交基向量",{"5":{"89":1}}],["个负特征值",{"5":{"135":2}}],["个负样本键",{"5":{"69":1}}],["个负样本",{"5":{"69":6}}],["个独立频率分量",{"5":{"132":2}}],["个独立的子计算",{"5":{"45":1}}],["个独立的二维旋转组成",{"5":{"88":2}}],["个任务",{"5":{"101":1}}],["个任务的权重",{"5":{"70":1}}],["个token的信息",{"5":{"101":2}}],["个token的分布仅依赖于其前面的token",{"5":{"101":1}}],["个序列的训练集",{"5":{"101":1}}],["个注意力头的输出为",{"5":{"143":1}}],["个注意力头",{"5":{"69":1,"93":1}}],["个候选样本",{"5":{"69":1}}],["个样本对应的损失函数",{"5":{"149":1}}],["个样本特征向量的",{"5":{"146":1}}],["个样本",{"5":{"45":2,"47":2,"69":1,"157":2}}],["个样本的均方根统计量",{"5":{"146":1}}],["个样本的均值",{"5":{"146":1}}],["个样本的特征向量各分量相对于均值的离散程度",{"5":{"146":1}}],["个样本的特征向量的中心位置",{"5":{"146":1}}],["个样本的方差",{"5":{"146":1}}],["个样本的第",{"5":{"45":1,"139":2,"146":1}}],["个样本的计算可以分解为个独立的子计算",{"5":{"45":1}}],["个样本的计算可以分解为",{"5":{"45":1}}],["个样本的预测值",{"5":{"51":1}}],["个样本的权重",{"5":{"51":1,"138":2}}],["个样本的误差约为1",{"5":{"51":1}}],["个样本的数据集",{"5":{"61":1}}],["个样本的真实类别标签为",{"5":{"61":2}}],["个样本的真实类别",{"5":{"61":1}}],["个样本中",{"5":{"61":2}}],["个样本属于第",{"5":{"61":2}}],["个输出分量",{"5":{"158":2}}],["个输出",{"5":{"45":1}}],["个输出神经元之间的连接强度",{"5":{"46":1}}],["个输出位置时",{"5":{"61":2}}],["个输入独立",{"5":{"148":1}}],["个输入特征与第",{"5":{"46":1}}],["个输入位置的",{"5":{"61":2}}],["个权重加",{"5":{"71":2}}],["个偏置",{"5":{"71":2}}],["个神经元",{"5":{"71":2}}],["个神经元实际上定义了",{"5":{"46":1}}],["个神经元的输出被置零",{"5":{"145":1}}],["个神经元的输出被保留",{"5":{"145":1}}],["个神经元的输出归一化为概率分布",{"5":{"47":1}}],["个神经元的层",{"5":{"47":1}}],["个分量的期望",{"5":{"145":1}}],["个分量的乘积",{"5":{"95":1}}],["个分量为",{"5":{"145":1}}],["个分量",{"5":{"44":2,"92":1,"149":1,"153":1,"159":2}}],["个坐标轴分割成",{"5":{"45":1}}],["个象限",{"5":{"45":1}}],["个超平面",{"5":{"46":1}}],["个元素",{"5":{"46":1}}],["个元素为",{"5":{"87":1,"89":1,"92":1,"96":1,"149":2,"152":1,"153":1}}],["个元素就是向量",{"5":{"87":1,"152":1}}],["个元素是向量",{"5":{"87":1,"152":1}}],["个元素表示第",{"5":{"95":1}}],["个类别之一",{"5":{"47":1}}],["个类别上的概率分布",{"5":{"61":1}}],["个类别上服从类别分布",{"5":{"96":1}}],["个类别的logit",{"5":{"96":1}}],["个索引",{"5":{"50":1,"86":1,"151":1}}],["个参数被完全利用来完成前向传播",{"5":{"159":2}}],["个参数方向上的梯度方差",{"5":{"149":1}}],["个参数来有效描述",{"5":{"143":1}}],["个参数而不是",{"5":{"50":1}}],["个参数",{"5":{"50":1,"89":1}}],["个参数完全描述",{"5":{"87":1,"152":1}}],["个事件",{"5":{"61":1}}],["个位置来实现",{"5":{"159":2}}],["个位置中选择",{"5":{"132":1}}],["个位置",{"5":{"86":2,"89":1,"132":1,"151":2}}],["个位置编码约",{"5":{"90":1}}],["个位置的相对关系",{"5":{"132":1}}],["个位置的输入embedding",{"5":{"88":1}}],["个位置的最小频率间隔",{"5":{"90":1}}],["个位置的嵌入表示",{"5":{"94":1}}],["个自由度",{"5":{"87":2,"93":1,"152":2}}],["个奇异向量",{"5":{"87":1,"152":1}}],["个奇异值及其对应的奇异向量",{"5":{"143":1}}],["个奇异值的截断svd近似",{"5":{"142":1}}],["个奇异值的累积能量比为",{"5":{"142":1}}],["个奇异值通常能够捕获权重矩阵的绝大部分",{"5":{"142":1}}],["个奇异值加上个奇异向量",{"5":{"87":1,"152":1}}],["个奇异值加上",{"5":{"87":1,"152":1}}],["个奇异值",{"5":{"87":1,"152":1}}],["个奇异值和对应的奇异向量来实现",{"5":{"89":1}}],["个复数",{"5":{"88":2}}],["个",{"5":{"88":1,"89":1,"91":2,"135":2,"158":2}}],["个秩一张量的和",{"5":{"50":1}}],["个秩",{"5":{"89":1}}],["个维度携带主要信息",{"5":{"89":1}}],["个维度几乎不携带信息",{"5":{"89":1}}],["个基向量的线性组合",{"5":{"89":1}}],["个频率成分的复振幅",{"5":{"90":1}}],["个频率成分的复指数序列",{"5":{"90":1}}],["个频率成分的频率",{"5":{"90":1}}],["个频率成分在位置",{"5":{"90":1}}],["个频率成分对应的旋转因子",{"5":{"90":1}}],["个频率成分能够编码约​比特的信息",{"5":{"90":1}}],["个频率成分能够编码约",{"5":{"90":1}}],["个频率的周期参数",{"5":{"90":1}}],["个query头为一组",{"5":{"93":2}}],["个头的总参数量为",{"5":{"86":2,"151":2}}],["个头的总复杂度为",{"5":{"93":2}}],["个头的总空间为",{"5":{"93":2}}],["个头的query投影",{"5":{"86":2,"151":2}}],["个头的query投影矩阵",{"5":{"93":1}}],["个头的query",{"5":{"93":1}}],["个头的投影矩阵总参数量为",{"5":{"93":2}}],["个头的投影矩阵按行堆叠",{"5":{"93":1}}],["个头的配置",{"5":{"93":1}}],["个头的输出拼接为",{"5":{"41":1}}],["个头的输出",{"5":{"93":1}}],["个头",{"5":{"93":2,"94":1}}],["个头参与计算",{"5":{"93":1}}],["个头定义的",{"5":{"93":1}}],["个流式多处理器",{"5":{"93":1}}],["个线程束",{"5":{"93":1}}],["个查询向量与第",{"5":{"95":1}}],["个查询向量",{"5":{"95":1}}],["个键向量的点积相似度",{"5":{"95":1}}],["个键向量",{"5":{"95":1}}],["个浮点数",{"5":{"95":1}}],["均方根统计量",{"5":{"146":1}}],["均方根统计量是第个样本特征向量的范数除以",{"5":{"146":1}}],["均方根统计量的几何意义",{"5":{"146":2}}],["均方根归一化具有",{"5":{"146":1}}],["均方根归一化具有缩放不变性",{"5":{"146":1}}],["均方根归一化通过移除去均值化步骤",{"5":{"146":2}}],["均方根归一化适用于大规模语言模型和计算资源受限的场景",{"5":{"146":2}}],["均方根归一化由zhang等人于2019年在论文",{"5":{"146":2}}],["均方根归一化",{"2":{"146":1},"5":{"146":5}}],["均方损失等",{"5":{"145":2}}],["均方渐近稳定",{"5":{"134":2}}],["均方误差可分解为",{"5":{"139":2}}],["均方误差可以使用较大学习率而不会导致不稳定",{"5":{"97":2}}],["均方误差源于统计学中的",{"5":{"70":1}}],["均方误差源于统计学中的二次损失",{"5":{"70":1}}],["均方误差认为0",{"5":{"70":2}}],["均方误差从0增加到",{"5":{"70":2}}],["均方误差对应于",{"5":{"70":1}}],["均方误差对应于欧几里得距离的平方",{"5":{"70":1}}],["均方误差对极端错误的惩罚是有限的",{"5":{"70":2}}],["均方误差对学习率的敏感性相对较低",{"5":{"97":2}}],["均方误差是",{"5":{"70":1}}],["均方误差是与回归任务自然匹配的损失函数",{"5":{"70":2}}],["均方误差是强凸的",{"5":{"70":1}}],["均方误差是凸函数",{"5":{"70":2,"97":2}}],["均方误差梯度",{"5":{"70":2}}],["均方误差的恒定曲率意味着优化空间在所有方向上具有相同的",{"5":{"70":2}}],["均方误差的",{"5":{"70":2}}],["均方误差的数学结构具有几个显著特征",{"5":{"70":2}}],["均方误差的强凸性系数为",{"5":{"97":2}}],["均方误差的梯度与误差项和输入特征的乘积成正比",{"5":{"70":2}}],["均方误差的梯度饱和问题",{"5":{"97":2}}],["均方误差和交叉熵虽然都是衡量模型预测与真实值差异的函数",{"5":{"70":2}}],["均方误差与交叉熵的数学结构对比",{"2":{"70":1},"5":{"70":1}}],["均方误差",{"0":{"51":1},"4":{"51":1},"5":{"51":3,"70":4,"97":1,"131":5}}],["均方误差损失",{"5":{"44":2}}],["均方误差作为回归任务中最常用的损失函数",{"5":{"51":2}}],["均方误差定义为预测值与真实值之差的平方的平均值",{"5":{"51":2}}],["均方误差具有恒定的hessian和良好的凸性",{"5":{"97":2}}],["均值维度",{"5":{"146":2}}],["均值",{"5":{"146":4}}],["均值场近似的期望传播",{"5":{"145":2}}],["均值场近似的期望分析",{"2":{"145":1},"5":{"145":1}}],["均值场近似的核心思想是",{"5":{"145":2}}],["均值场近似",{"5":{"145":2}}],["均值为",{"5":{"135":2}}],["均值为0",{"5":{"41":2,"95":2}}],["均值信息可能包含重要的信号",{"5":{"41":2}}],["均值和方差",{"5":{"132":2,"146":4}}],["均值和协方差相应地分块为",{"5":{"96":2}}],["均值和对数方差",{"5":{"96":2}}],["均为标量",{"5":{"146":2}}],["均为可微函数",{"5":{"45":1}}],["均为",{"5":{"86":2,"93":2,"151":2}}],["均匀假设",{"5":{"133":2}}],["均匀分布熵最大",{"5":{"137":2}}],["均匀分布",{"5":{"70":2,"89":2,"96":2}}],["均匀分布时取得最大值",{"5":{"61":1}}],["均匀分布对应于",{"5":{"89":2}}],["均匀分布的期望为",{"5":{"96":2}}],["均匀分布可用于权重初始化",{"5":{"96":2}}],["均匀初始化",{"5":{"89":2}}],["均匀性",{"5":{"91":2}}],["均匀程度",{"5":{"92":2,"153":2}}],["避免因过大的策略更新导致的训练不稳定问题",{"5":{"154":2}}],["避免过拟合训练数据",{"5":{"144":2}}],["避免过度裁剪导致信息丢失",{"5":{"97":2}}],["避免固定",{"5":{"133":1}}],["避免固定导致的欠压缩或过压缩问题",{"5":{"133":1}}],["避免固定参数导致的训练问题",{"5":{"133":2}}],["避免某些专家过载而其他专家空闲",{"5":{"157":2}}],["避免某些头主导信息处理",{"5":{"132":2}}],["避免某些路径成为",{"5":{"41":2}}],["避免梯度消失问题",{"5":{"132":2,"135":2}}],["避免梯度饱和区域",{"2":{"41":1},"5":{"41":1}}],["避免奖励模型的过拟合导致的策略崩溃",{"5":{"101":2}}],["避免饱和",{"5":{"41":2}}],["避免饱和现象的发生",{"5":{"95":2}}],["避免存在大范围的饱和区域",{"5":{"41":2}}],["避免数值溢出或下溢",{"5":{"47":2}}],["避免了噪声在某些输入上主导决策而在其他输入上可以忽略的问题",{"5":{"159":2}}],["避免了纯策略梯度方法中常见的训练不稳定问题",{"5":{"155":2}}],["避免了对环境动态模型的依赖",{"5":{"155":2}}],["避免了因更新过大而导致的性能下降",{"5":{"154":2}}],["避免了激活值在训练过程中的爆炸或消失",{"5":{"150":2}}],["避免了参数爆炸的问题",{"5":{"150":2}}],["避免了批归一化对批量大小的依赖",{"5":{"146":2}}],["避免了除零错误",{"5":{"146":2}}],["避免了某些任务因梯度范数过大而主导训练",{"5":{"101":2}}],["避免了某些位置主导信息流的问题",{"5":{"41":2}}],["避免了数值溢出",{"5":{"61":2}}],["避免了数值下溢",{"5":{"61":2}}],["避免了无限增长的累积量",{"5":{"86":2,"151":2}}],["避免了分布外的位置编码",{"5":{"88":2}}],["避免了训练过程中的梯度消失或梯度爆炸问题",{"5":{"94":2}}],["避免了梯度饱和问题",{"5":{"97":2}}],["避免重复从全局内存读取",{"5":{"92":2,"153":2}}],["避免重复计算",{"5":{"93":2}}],["避免极端的奇异性",{"5":{"94":2}}],["避免训练不稳定性",{"5":{"95":2}}],["避免softmax函数进入饱和区域",{"5":{"95":2}}],["避免在随机初始化阶段受到过大梯度的干扰",{"5":{"48":2}}],["避免在曲率变化剧烈的区域过早跳跃",{"5":{"97":2}}],["进行排序并取前",{"5":{"159":2}}],["进行更新",{"5":{"154":2}}],["进行归一化",{"5":{"146":2}}],["进行反向传播",{"5":{"146":2}}],["进行线性组合",{"5":{"143":1}}],["进行线性投影",{"5":{"47":1}}],["进行cp分解",{"5":{"143":1}}],["进行svd",{"5":{"143":5}}],["进行重组",{"5":{"133":1}}],["进行掩码",{"5":{"101":2}}],["进行",{"5":{"41":1}}],["进行高度并行化的矩阵运算",{"5":{"47":2}}],["进行非线性变换",{"5":{"47":1}}],["进行大部分计算",{"5":{"50":2}}],["进行了专门优化",{"5":{"86":2,"151":2}}],["进行qr分解",{"5":{"87":1,"152":1}}],["进行平移",{"5":{"90":1}}],["进行某种操作",{"5":{"90":1}}],["进行最终的线性变换",{"5":{"93":1}}],["进行一系列线性变换和非线性变换的组合",{"5":{"93":1}}],["进行梯度估计时",{"5":{"96":2}}],["进行参数更新",{"5":{"97":2}}],["进而影响梯度的命运",{"5":{"148":2}}],["进而影响泛化性能",{"5":{"137":2}}],["进而影响语言建模损失的收敛速度和最终性能",{"5":{"101":2}}],["进而影响模型的收敛速度",{"5":{"42":2}}],["进而",{"5":{"51":2,"134":2}}],["进而使",{"5":{"61":2}}],["进入某个局部极小值的吸引域",{"5":{"149":2}}],["进入消失或爆炸的区域",{"5":{"148":2}}],["进入局部区域",{"5":{"134":2}}],["进入精细搜索",{"5":{"134":2}}],["进入深度饱和区域",{"5":{"41":2}}],["进入函数值更高的区域",{"5":{"48":2}}],["进一步说明了残差连接的另一个重要作用",{"5":{"150":1}}],["进一步增强了模型的表达能力",{"5":{"150":2}}],["进一步增加模型复杂度会增加测试mse",{"5":{"51":2}}],["进一步增加了损失景观的复杂性",{"5":{"97":2}}],["进一步地",{"5":{"149":2}}],["进一步简化了计算",{"5":{"146":2}}],["进一步",{"5":{"145":2}}],["进一步将dropout扩展到循环连接",{"5":{"145":2}}],["进一步将dropout解释为贝叶斯推断",{"5":{"96":2}}],["进一步降低内存占用",{"5":{"86":2,"151":2}}],["进一步观察",{"5":{"91":2}}],["进一步优化了注意力计算的内存访问模式",{"5":{"92":2,"153":2}}],["训练拥有数倍甚至数十倍参数量的大模型成为可能",{"5":{"159":2}}],["训练不稳定性",{"5":{"154":2}}],["训练不稳定甚至发散",{"5":{"41":2}}],["训练更稳定但学习速度可能较慢",{"5":{"154":2}}],["训练稳定性分析",{"5":{"150":1}}],["训练稳定性分析揭示了layer",{"5":{"150":1}}],["训练深层网络的实证效果",{"5":{"150":1}}],["训练深层网络的实证效果验证了highway",{"5":{"150":1}}],["训练初期探索",{"5":{"147":1}}],["训练推理一致",{"5":{"145":1}}],["训练阶段",{"5":{"145":1}}],["训练阶段的数学推导",{"2":{"146":1},"5":{"146":1}}],["训练阶段的数学表达式为",{"5":{"145":4}}],["训练阶段的期望输出等于无dropout时的输出",{"5":{"145":2}}],["训练目标是最小化期望损失",{"5":{"145":2}}],["训练成本高",{"5":{"140":1}}],["训练时的条件分布",{"5":{"140":1}}],["训练时的条件分布与推理时的条件分布完全相同",{"5":{"140":1}}],["训练与推理的一致性",{"2":{"140":1,"145":1},"5":{"140":1,"145":1}}],["训练与推理优化<",{"5":{"131":1}}],["训练与推理优化",{"0":{"157":1},"4":{"108":1,"157":1},"5":{"131":4,"157":1}}],["训练策略等",{"5":{"138":2}}],["训练策略模型",{"5":{"101":1}}],["训练混沌的可控性",{"5":{"136":2}}],["训练轨迹对初始参数敏感",{"5":{"136":2}}],["训练轨迹的",{"5":{"136":2}}],["训练轨迹可能进入混沌状态",{"5":{"136":2}}],["训练轨迹从收敛行为转变为周期振荡行为",{"5":{"136":2}}],["训练动力学中的",{"5":{"136":2}}],["训练动态和部署策略等多个维度",{"5":{"159":2}}],["训练动态和泛化性能之间的深层联系",{"5":{"137":2}}],["训练动态和泛化性能奠定理论基础",{"5":{"45":2}}],["训练动态的尺度不变性",{"2":{"135":1},"5":{"135":1}}],["训练动态系统",{"5":{"134":2}}],["训练震荡的数学分析",{"2":{"136":1},"5":{"136":1}}],["训练曲线不单调",{"5":{"136":2}}],["训练曲线的典型阶段",{"5":{"134":2}}],["训练曲线的动力学解释",{"2":{"134":1},"5":{"134":1}}],["训练曲线定义为函数",{"5":{"134":2}}],["训练曲线",{"5":{"134":2}}],["训练曲线剧烈振荡无法收敛",{"5":{"41":2}}],["训练好的模型",{"5":{"133":2}}],["训练数据或计算资源",{"5":{"159":2}}],["训练数据质量与数量的权衡",{"5":{"138":1}}],["训练数据质量与数量的权衡是数据最优策略必须考虑的因素",{"5":{"138":1}}],["训练数据分布",{"5":{"138":2}}],["训练数据量",{"5":{"159":2}}],["训练数据量和计算量跨越某个临界配置时",{"5":{"138":2}}],["训练数据量和计算量之间的相互依赖关系",{"5":{"138":2}}],["训练数据量和计算资源之间存在稳定的幂律",{"5":{"138":2}}],["训练数据量或计算资源的增加而改善",{"5":{"138":2}}],["训练数据中标签的分布熵",{"5":{"137":1}}],["训练数据中标签的分布熵反映了类别不平衡程度",{"5":{"137":1}}],["训练数据",{"5":{"133":2}}],["训练数据被视为从某个未知的数据生成分布",{"5":{"61":2}}],["训练后期的适度震荡可能表明",{"5":{"136":2}}],["训练后期",{"5":{"133":2}}],["训练奖励模型",{"5":{"101":1}}],["训练奖励模型来预测偏好概率",{"5":{"101":1}}],["训练越不稳定",{"5":{"41":2}}],["训练迭代的总复杂度",{"5":{"44":2}}],["训练停滞",{"5":{"44":1}}],["训练集上的mse是总体mse的估计",{"5":{"51":2}}],["训练得到的模型为",{"5":{"51":2}}],["训练mse也高",{"5":{"51":2}}],["训练mse低但测试mse高",{"5":{"51":2}}],["训练和测试序列长度相近",{"5":{"89":2}}],["训练良好的可学习位置编码通常表现出快速衰减或阶梯状的奇异值分布",{"5":{"89":2}}],["训练良好的编码通常表现出快速衰减或阶梯状的奇异值分布",{"5":{"89":2}}],["训练良好的transformer中的不同注意力头确实会发展出不同的",{"5":{"93":2}}],["训练良好的transformer中不同头表现出不同的熵分布模式",{"5":{"92":2,"153":2}}],["训练良好的transformer中不同头的输出表示往往具有较低的相关性",{"5":{"93":2}}],["训练过程通常涉及复杂的正则化和近似算法",{"5":{"140":1}}],["训练过程通常涉及复杂的正则化和近似算法为了理解现代自回归模型的演进历程",{"5":{"140":1}}],["训练过程持续进行",{"5":{"138":2}}],["训练过程中熵的变化规律",{"5":{"137":2}}],["训练过程中的信息平面轨迹",{"5":{"133":2}}],["训练过程中",{"5":{"92":2,"153":2}}],["训练过程会自然地推动各头学习互补的特征",{"5":{"93":2}}],["训练过程实际上是在调整这些投影矩阵的奇异值结构",{"5":{"94":2}}],["训练过程实际上是在调整和",{"5":{"94":1}}],["训练过程实际上是在调整",{"5":{"94":1}}],["训练过程就是使用随机梯度下降等优化算法来最大化这个目标函数",{"5":{"96":2}}],["训练中不动点稳定性的交换",{"5":{"136":2}}],["训练中",{"5":{"97":2}}],["训练通常需要多个阶段",{"5":{"97":2}}],["使专家分配更加多样化",{"5":{"159":2}}],["使网络能够同时捕捉不同抽象层次和不同感受野的特征",{"5":{"150":2}}],["使信息几乎无损地通过",{"5":{"150":2}}],["使梯度能够直接流向较浅的层",{"5":{"150":2}}],["使梯度能够绕过复杂的非线性变换直接传播到较浅层",{"5":{"50":2}}],["使梯度可以直接从深层传播到浅层",{"5":{"147":2}}],["使训练过程完全崩溃",{"5":{"148":2}}],["使训练阶段的期望输出与原始输出一致",{"5":{"145":2}}],["使训练和推理的期望保持一致",{"5":{"145":2}}],["使它们不参与前向传播和反向传播",{"5":{"145":2}}],["使优化更加稳定",{"5":{"144":2,"148":2}}],["使编码不会过度拟合训练分布",{"5":{"133":2}}],["使编码器输出分布接近先验分布",{"5":{"133":2}}],["使分布更加稳定",{"5":{"132":2}}],["使我们能够理解不同损失函数的数学本质",{"5":{"69":2}}],["使得专家分配变得近乎随机",{"5":{"159":2}}],["使得专家能够更均匀地分配负载",{"5":{"159":2}}],["使得稀疏激活退化为确定性选择",{"5":{"159":2}}],["使得该项贡献为零",{"5":{"158":2}}],["使得moe能够处理序列内的异质性",{"5":{"158":2}}],["使得mse主要由异常样本决定",{"5":{"51":2}}],["使得有效计算时间接近纯计算时间",{"5":{"157":2}}],["使得门控网络能够学习改进路由决策",{"5":{"157":2}}],["使得它在所有状态下的价值都最大化",{"5":{"156":2}}],["使得它在实践中通常比sigmoid表现更好",{"5":{"42":2}}],["使得从长远来看",{"5":{"156":2}}],["使得估计量的期望保持不变",{"5":{"155":2}}],["使得累积期望奖励最大化",{"5":{"155":2}}],["使得新的策略",{"5":{"154":2}}],["使得之前采样的经验数据不再适用于新的策略分布",{"5":{"154":2}}],["使得基于旧策略采样得到的经验数据失效",{"5":{"154":2}}],["使得基于梯度优化的训练过程具有良好的数值性质",{"5":{"61":1}}],["使得layer",{"5":{"150":1}}],["使得linformer的近似误差满足",{"5":{"141":2}}],["使得裁剪操作与某种形式的正则化产生相似的参数收缩效应",{"5":{"147":2}}],["使得参数有可能越过鞍点的",{"5":{"149":2}}],["使得参数更新并非在所有方向上均匀探索",{"5":{"149":2}}],["使得参数既不会增长过快",{"5":{"147":2}}],["使得参数估计可以在无界的实数空间中进行",{"5":{"42":2}}],["使得下一步计算的梯度更加不稳定",{"5":{"147":2}}],["使得掩码值可以在",{"5":{"145":2}}],["使得上述分解成立",{"5":{"143":2}}],["使得所有",{"5":{"138":2}}],["使得所有从",{"5":{"136":1}}],["使得所有从邻域内出发的轨道都渐近趋近于",{"5":{"136":1}}],["使得损失函数值交替变化",{"5":{"136":1}}],["使得损失函数达到最小值",{"5":{"51":2}}],["使得初始距离小于",{"5":{"136":1}}],["使得初始距离小于的轨道在足够长时间后分离至少",{"5":{"136":1}}],["使得初始编码向量具有适当的范数",{"5":{"89":1}}],["使得如果",{"5":{"134":2}}],["使得表示能够捕获更复杂的信息模式",{"5":{"133":2}}],["使得非常深的网络可以稳定训练",{"5":{"132":2}}],["使得网络能够更灵活地适应不同的数据分布和任务需求",{"5":{"150":2}}],["使得网络能够根据具体任务自适应地分配计算资源",{"5":{"132":2}}],["使得网络在训练初期达到或接近临界状态",{"5":{"148":2}}],["使得网络具有稀疏表示的能力",{"5":{"71":2}}],["使得能够更好地捕获位置与其上下文之间的相对关系",{"5":{"101":1}}],["使得真正依赖于位置的具体身份",{"5":{"101":1}}],["使得正样本对的相似度高于负样本对",{"5":{"70":2}}],["使得交叉熵梯度不受饱和影响",{"5":{"70":2}}],["使得输出具有非零方差",{"5":{"145":2}}],["使得输出具有概率解释",{"5":{"70":2}}],["使得输出梯度更加平衡",{"5":{"41":2}}],["使得分布更加",{"5":{"69":2}}],["使得我们可以使用信息论的工具",{"5":{"69":2}}],["使得我们可以使用线性代数的标准工具进行分析",{"5":{"47":2}}],["使得模型性能最大化",{"5":{"138":2}}],["使得模型能够建立任意位置之间的依赖关系",{"5":{"132":2}}],["使得模型能够为不同任务学习不同的",{"5":{"101":2}}],["使得模型能够区分",{"5":{"101":2}}],["使得模型能够区分不同位置的token",{"5":{"101":2}}],["使得模型能够区分不同位置的",{"5":{"70":2}}],["使得模型能够唯一标识每个位置",{"5":{"91":2}}],["使得模型能够学习内容相关的依赖",{"5":{"91":2}}],["使得模型能够学习位置与内容之间的复杂交互关系",{"5":{"91":2}}],["使得模型能够学习多种类型的依赖关系",{"5":{"91":2}}],["使得模型能够稳定地训练",{"5":{"92":2,"153":2}}],["使得模型能够生成多样化的回复",{"5":{"96":2}}],["使得模型在训练数据上的损失函数值最小化",{"5":{"48":2}}],["使得模型分布",{"5":{"61":2}}],["使得模型不会过度拟合训练数据",{"5":{"61":2}}],["使得模型难以对位置进行过拟合",{"5":{"90":2}}],["使得模型难以学习到远距离位置之间的关联",{"5":{"92":2,"153":2}}],["使得",{"5":{"41":1,"45":2,"48":1,"50":3,"69":1,"71":3,"88":2,"92":1,"97":2,"101":4,"134":2,"136":4,"137":1,"140":1,"147":2,"148":9,"153":1}}],["使得和能够捕获输入之间的复杂依赖关系",{"5":{"71":1}}],["使得不同激活单元的传播相互独立",{"5":{"135":2}}],["使得不同层之间的信息传递更加可控",{"5":{"71":2}}],["使得不同特征方向捕捉不同的信息",{"5":{"50":2}}],["使得不同位置具有可区分的编码",{"5":{"71":2}}],["使得不同位置的相位在大多数维度上不会过快收敛",{"5":{"88":2}}],["使得不同维度之间的频率跨度合理",{"5":{"91":2}}],["使得不同的参数化方式可以产生相同的函数",{"5":{"48":2}}],["使得不同的头收敛到不同的解",{"5":{"93":2}}],["使得不同变量对之间的相关性可以直接比较",{"5":{"96":2}}],["使得函数族能够在输入空间中形成",{"5":{"71":1}}],["使得函数族",{"5":{"71":1}}],["使得神经元能够从数据中自动学习最优的连接权重",{"5":{"47":2}}],["使得神经网络能够表示任意复杂的非线性映射关系",{"5":{"71":2}}],["使得神经网络能够拟合任意复杂的非线性函数",{"5":{"47":2}}],["使得整体损失函数朝着下降的方向移动",{"5":{"48":2}}],["使得预测具有不确定性估计",{"5":{"48":2}}],["使得更新方向更加平滑",{"5":{"48":2}}],["使得算法能够适应非平稳的目标函数",{"5":{"48":2}}],["使得等效的变换矩阵具有接近1的特征值",{"5":{"50":2}}],["使得信息的",{"5":{"61":2}}],["使得训练能够收敛到更均衡的解",{"5":{"158":2}}],["使得训练深层网络成为可能",{"5":{"44":2}}],["使得训练更加稳定高效",{"5":{"61":2}}],["使得看似低秩的运算实际上能够表示复杂函数",{"5":{"87":2,"152":2}}],["使得处理超长序列成为可能",{"5":{"87":2,"152":2}}],["使得在相同计算预算下获得更强的模型成为可能",{"5":{"159":2}}],["使得在有限计算预算下训练超大规模模型成为可能",{"5":{"159":2}}],["使得在所有任务上都不差且至少一个任务上更好",{"5":{"101":1}}],["使得在给定的位置索引范围内",{"5":{"88":2}}],["使得在语义相关或语法相关的位置上",{"5":{"95":2}}],["使得高频细节不再那么敏感于绝对位置值",{"5":{"88":2}}],["使得每个设备只需要存储和计算一部分专家",{"5":{"157":2}}],["使得每个位置编码可以表示为",{"5":{"89":2}}],["使得每行成为概率分布",{"5":{"87":2,"152":2}}],["使得编码向量能够同时表达多个尺度的位置信息",{"5":{"90":2}}],["使得对任意",{"5":{"136":2}}],["使得对所有都有",{"5":{"136":2}}],["使得对所有",{"5":{"88":1,"136":2,"148":1}}],["使得对于所有",{"5":{"141":2}}],["使得对于任意输入序列",{"5":{"140":2}}],["使得对于合理的序列长度",{"5":{"91":2}}],["使得对于每个位置",{"5":{"92":2,"153":2}}],["使得sigmoid和tanh激活后的输出方差保持稳定",{"5":{"41":2}}],["使得softmax后的注意力权重为0",{"5":{"91":2}}],["使得位置信息能够被有效学习和泛化",{"5":{"91":2}}],["使得位置只能",{"5":{"91":1}}],["使得位置",{"5":{"91":1,"92":1,"153":1}}],["使得位置和位置成为",{"5":{"92":1,"153":1}}],["使得相位差",{"5":{"91":1}}],["使得长序列注意力计算在实践中更加高效",{"5":{"92":2,"153":2}}],["使得后续的自注意力运算无法干净地分离位置信息和内容信息",{"5":{"88":2}}],["使得后续层中变大",{"5":{"92":1,"153":1}}],["使得后续层中",{"5":{"92":1,"153":1}}],["使得当时",{"5":{"92":1,"153":1}}],["使得当​是",{"5":{"92":1,"153":1}}],["使得当",{"5":{"92":2,"134":2,"153":2}}],["使得注意力模式可以逐层组合",{"5":{"71":2}}],["使得注意力机制具有很高的通用性",{"5":{"92":2,"153":2}}],["使得总的信息量最大化",{"5":{"93":2}}],["使得语义上与位置相关的依赖能够被建模",{"5":{"91":2}}],["使得语义相关的query和key在变换后的空间中具有较高的相似度",{"5":{"94":2}}],["使得梯度既不会太小",{"5":{"95":2}}],["使得生成的文本既流畅又符合语义逻辑",{"5":{"96":2}}],["使得优化过程在这个几何框架下具有更清晰的理论性质",{"5":{"61":2}}],["使得优化过程更加稳定",{"5":{"97":2,"146":2}}],["使得特定的损失函数能够更好地发挥模型的能力",{"5":{"101":2}}],["使激活函数能够适应不同的数据分布和任务需求",{"5":{"41":2}}],["使用通信计算重叠隐藏通信延迟",{"5":{"157":2}}],["使用混合精度训练降低显存和通信开销",{"5":{"157":2}}],["使用梯度检查点优化激活显存",{"5":{"157":2}}],["使用专家并行将专家权重分片到多个设备",{"5":{"157":2}}],["使用ste的近似梯度计算可以表示为",{"5":{"157":2}}],["使用sigmoid函数作为激活函数",{"5":{"71":2}}],["使用优势函数估计",{"5":{"155":2}}],["使用时序差分目标更新价值函数网络",{"5":{"155":2}}],["使用以下公式更新策略参数",{"5":{"155":2}}],["使用当前策略",{"5":{"155":4}}],["使用蒙特卡洛采样来估计轨迹的总回报",{"5":{"155":2}}],["使用ppo进行训练需要大量的工程技巧来确保稳定性",{"5":{"154":2}}],["使用重要性采样",{"5":{"154":2}}],["使用relu或其变体以避免sigmoid",{"5":{"148":2}}],["使用he初始化的网络可以训练到100层甚至更深",{"5":{"148":2}}],["使用矩阵权重和非线性激活函数",{"5":{"148":2}}],["使用裁剪梯度进行梯度下降",{"5":{"147":2}}],["使用有偏估计可以简化计算且性能差异可忽略",{"5":{"146":2}}],["使用有限差分近似导数",{"5":{"48":2}}],["使用高质量的人工标注数据对预训练的语言模型进行微调",{"5":{"154":2}}],["使用高斯分布来生成掩码",{"5":{"145":2}}],["使用高精度",{"5":{"97":2}}],["使用均值场近似",{"5":{"145":2}}],["使用连乘符号",{"5":{"140":2}}],["使用数学归纳法进行严格证明",{"5":{"140":2}}],["使用jensen不等式",{"5":{"139":1}}],["使用jensen不等式定义",{"5":{"139":1}}],["使用批量大小为",{"5":{"139":2}}],["使用神经网络来估计互信息",{"5":{"137":2}}],["使用学习率",{"5":{"136":2}}],["使用自动微分计算梯度",{"5":{"133":2}}],["使用自身表示导数",{"5":{"42":1}}],["使用变分近似优化信息瓶颈目标",{"5":{"133":2}}],["使用变分推断",{"5":{"101":2}}],["使用奖励模型指导策略优化",{"5":{"101":2}}],["使用人类编写的示范数据",{"5":{"101":1}}],["使用人类编写的示范数据训练策略模型",{"5":{"101":1}}],["使用人类对模型输出的偏好判断来训练一个奖励模型",{"5":{"101":2}}],["使用基于人类反馈的强化学习损失",{"5":{"101":2}}],["使用标准的分类损失或序列生成损失",{"5":{"101":2}}],["使用注意力作为编码器",{"5":{"69":2}}],["使用类似的推导",{"5":{"69":2}}],["使用链式法则求导",{"5":{"42":2}}],["使用乘积法则对求导",{"5":{"42":1}}],["使用乘积法则对",{"5":{"42":1}}],["使用商的求导法则对求导",{"5":{"42":1}}],["使用商的求导法则对",{"5":{"42":1}}],["使用",{"5":{"42":1,"48":1,"70":2,"71":2,"93":1,"137":2,"147":4}}],["使用交叉熵或均方误差损失时",{"5":{"44":2}}],["使用推论2",{"5":{"44":2}}],["使用莱布尼茨记号表示为",{"5":{"45":2}}],["使用两层神经网络可以解决异或问题",{"5":{"46":2}}],["使用齐次坐标",{"5":{"46":2}}],["使用代数规则显式推导导数公式",{"5":{"48":2}}],["使用和",{"5":{"48":1}}],["使用了新的异步指令",{"5":{"86":2,"151":2}}],["使用随机矩阵近似",{"5":{"87":2,"152":2}}],["使用核函数近似softmax变换",{"5":{"87":2,"152":2}}],["使用不确定性加权等",{"5":{"70":2}}],["使用不同频率的正弦和余弦函数来编码位置",{"5":{"88":2}}],["使用不同分辨率的位置编码在不同层共享",{"5":{"89":2}}],["使用不同尺度的基函数同时捕获全局和局部特征",{"5":{"90":2}}],["使用这些向量与query",{"5":{"88":2}}],["使用较低的频率",{"5":{"90":2}}],["使用较高的频率",{"5":{"90":2}}],["使用欧拉公式",{"5":{"90":2}}],["使用三角恒等式",{"5":{"90":2}}],["使用复数形式可以更优雅地表示正弦余弦位置编码",{"5":{"90":2}}],["使用复数编码",{"5":{"90":2}}],["使用非对称的频率或相位",{"5":{"91":2}}],["使用​的qkv投影",{"5":{"93":1}}],["使用的是gelu",{"5":{"94":2}}],["使用余弦函数平滑地降低学习率",{"5":{"97":2}}],["使用指数移动平均来累积梯度平方",{"5":{"48":2}}],["使用指数移动平均代替历史梯度平方和",{"5":{"97":2}}],["使用低精度",{"5":{"97":2}}],["使attention的内积天然等变于相对位移",{"5":{"88":2}}],["使注意力机制能够感知序列中各元素的绝对或相对位置",{"5":{"91":2}}],["使注意力分数依赖于query和key之间的相对位置",{"5":{"91":2}}],["使其生成的输出能够获得更高的奖励模型评分",{"5":{"154":2}}],["使其具备基本的指令遵循能力",{"5":{"154":2}}],["使其变得更加",{"5":{"147":2}}],["使其更适合表示学习任务",{"5":{"69":2}}],["使其能够区分不同位置的词元",{"5":{"92":2,"153":2}}],["使其均值接近0",{"5":{"96":2}}],["使其在分类任务中比均方误差更容易优化",{"5":{"97":2}}],["使模型收敛到不同的局部极小值",{"5":{"147":2}}],["使模型学习到跨模态的对齐表示",{"5":{"137":2}}],["使模型完全崩溃",{"5":{"41":2}}],["使模型能够区分不同位置的词元",{"5":{"92":2,"153":2}}],["使学生模型能够学习教师模型的知识",{"5":{"96":2}}],["无偏差",{"5":{"155":2}}],["无偏估计",{"5":{"134":2,"149":1}}],["无压缩",{"5":{"148":1}}],["无放缩",{"5":{"145":1}}],["无放缩版本",{"5":{"145":2}}],["无放缩版本的证明类似",{"5":{"145":2}}],["无放缩版本在训练时直接应用dropout掩码",{"5":{"145":1}}],["无过拟合",{"5":{"140":2}}],["无震荡但收敛速度较慢",{"5":{"136":2}}],["无关",{"5":{"70":1,"87":2,"88":2,"90":1,"91":1,"93":1,"97":1,"152":2}}],["无关的常数",{"5":{"61":2}}],["无限负样本",{"5":{"69":2}}],["无限多",{"5":{"51":2}}],["无激活函数",{"5":{"41":2}}],["无输入",{"5":{"41":2}}],["无衰减地传播",{"5":{"41":2}}],["无论输入",{"5":{"159":2}}],["无论输入张量的维度如何",{"5":{"146":2}}],["无论其参数如何调整都不会影响当前输入的损失",{"5":{"157":2}}],["无论理论解释如何",{"5":{"146":2}}],["无论训练多长时间",{"5":{"71":2}}],["无论训练数据有多少",{"5":{"71":2}}],["无论权重如何调整",{"5":{"46":2}}],["无论堆叠多少层神经元",{"5":{"71":2}}],["无论堆叠多少层",{"5":{"47":2}}],["无论是输入样本的具体内容如何",{"5":{"159":2}}],["无论是分类任务还是注意力聚合",{"5":{"69":2}}],["无论是方阵还是矩形阵",{"5":{"50":2}}],["无论是否满秩",{"5":{"50":2}}],["无论是计算类别概率还是计算注意力权重",{"5":{"61":2}}],["无论预测偏高还是偏低",{"5":{"51":2}}],["无论使用何种模型都无法减少",{"5":{"51":2}}],["无论序列多长",{"5":{"87":2,"152":2}}],["无论与的距离有多远",{"5":{"92":1,"153":1}}],["无论依赖关系的跨度如何",{"5":{"92":2,"153":2}}],["无论依赖关系出现在序列的哪个位置",{"5":{"92":2,"153":2}}],["无论",{"5":{"92":1,"153":1,"154":2}}],["无论设置多少个头",{"5":{"93":2}}],["无论原始随机变量的分布是什么",{"5":{"96":2}}],["无法学习到有意义的专业化分工",{"5":{"159":2}}],["无法学习改进路由决策",{"5":{"158":2}}],["无法根据任务需要调整依赖范围",{"5":{"140":2}}],["无法满足",{"5":{"140":2}}],["无法执行任务",{"5":{"138":2}}],["无法超越输入信息量",{"5":{"133":2}}],["无法超越输入的信息量",{"5":{"133":2}}],["无法通过数据自动学习",{"5":{"47":2}}],["无法将类别0的点放在一侧而将类别1的点放在另一侧",{"5":{"47":2}}],["无法拟合复杂的非线性函数",{"5":{"47":2}}],["无法表示某些负定或非定模式",{"5":{"86":2,"151":2}}],["无法直接用于优化",{"5":{"70":2}}],["无法直接知道一个token在序列中的绝对位置",{"5":{"88":2}}],["无法捕捉复杂的非线性决策边界",{"5":{"159":2}}],["无法捕捉更复杂的位置依赖模式",{"5":{"88":2}}],["无法捕获相距较远的词元之间的语法和语义关系",{"5":{"140":2}}],["无法捕获token之间的复杂依赖关系",{"5":{"71":2}}],["无法唯一区分所有位置",{"5":{"89":2}}],["无法并行化",{"5":{"92":2,"153":2}}],["无法关注位置",{"5":{"92":2,"153":2}}],["无法用单头注意力表示",{"5":{"93":2}}],["无法从输出端有效传播回输入端",{"5":{"95":2}}],["无界的",{"5":{"88":2}}],["无理数比率",{"5":{"90":2}}],["无需显式建模环境",{"5":{"155":2}}],["无需调节额外的超参数",{"5":{"154":2}}],["无需额外的投影层来调整维度",{"5":{"91":2}}],["无需经过中间节点的传递",{"5":{"92":2,"153":2}}],["无需经过中间节点",{"5":{"92":2,"153":2}}],["无需经过中间层的变换",{"5":{"92":2,"153":2}}],["无需改变算法框架",{"5":{"95":2}}],["无损传递",{"5":{"132":2}}],["无损",{"5":{"92":2,"150":2,"153":2}}],["无",{"5":{"97":2,"146":5,"159":1}}],["q=1",{"5":{"143":2}}],["qlora",{"5":{"142":1}}],["q",{"5":{"69":1,"86":15,"132":20,"133":6,"141":20,"143":2,"151":15}}],["q^",{"5":{"69":4}}],["query",{"0":{"94":1},"4":{"94":1},"5":{"61":2,"69":2,"70":4,"71":2,"86":6,"88":6,"91":2,"93":8,"94":7,"95":6,"97":4,"131":5,"132":10,"151":6}}],["query块",{"5":{"86":2,"151":2}}],["query头属于组",{"5":{"86":1,"151":1}}],["query头",{"5":{"86":1,"151":1}}],["query和key向量的点积期望方差为",{"5":{"41":2}}],["query和key矩阵和是输入嵌入经过线性变换得到的",{"5":{"87":1,"152":1}}],["query和key矩阵的维度为",{"5":{"87":2,"152":2}}],["query和key矩阵",{"5":{"87":1,"152":1}}],["query和key不必在原始嵌入空间中相似",{"5":{"94":2}}],["query表达了信息需求的抽象表示",{"5":{"94":2}}],["query空间和key空间的设计体现了",{"5":{"94":2}}],["quadratic",{"5":{"70":2,"136":2}}],["quad",{"5":{"42":6,"46":4,"86":8,"97":6,"141":2,"142":2,"144":2,"151":8}}],["qr分解",{"5":{"50":2}}],["qr分解有多种计算方法",{"5":{"50":2}}],["qr分解在线性最小二乘问题中有重要应用",{"5":{"50":2}}],["qr分解将矩阵分解为正交矩阵和上三角矩阵的乘积",{"5":{"50":1}}],["qr分解将矩阵分解为正交矩阵",{"5":{"50":1}}],["qkv",{"5":{"132":12}}],["qkv投影",{"5":{"93":2}}],["qkv投影的总参数量为​",{"5":{"93":1}}],["qkv投影的总参数量为",{"5":{"93":1}}],["qk点积的尺度问题",{"5":{"41":2}}],["每条轨迹由状态",{"5":{"155":2}}],["每一次参数更新都遍历整个数据集在计算上是不可行的",{"5":{"149":2}}],["每一次非线性变换都扩展了网络能够表示的函数空间",{"5":{"71":2}}],["每一层都执行从输入空间到输出空间的变换",{"5":{"150":2}}],["每一层通过权重矩阵的转置将误差",{"5":{"44":2}}],["每一层网络定义了一个从其输入空间到输出空间的映射",{"5":{"45":2}}],["每一层网络执行了一次从输入空间到输出空间的变换",{"5":{"45":2}}],["每一层的输入分布更加稳定",{"5":{"41":2}}],["每一层的输入和输出都需要被保存用于反向传播",{"5":{"50":2}}],["每一层的输出不仅包含当前层学到的特征变换",{"5":{"150":2}}],["每一层的输出不再强烈依赖于前一层的具体分布",{"5":{"148":2}}],["每一层的输出可以视为输入数据在该层定义的",{"5":{"45":2}}],["每一层的输出作为下一层的输入",{"5":{"94":2}}],["每一层的每个神经元与前一层的所有神经元相连",{"5":{"46":2}}],["每一层的query",{"5":{"94":2}}],["每一层仅进行仿射变换而不应用激活函数",{"5":{"47":2}}],["每一行都是一个有效的概率分布",{"5":{"87":4,"152":4}}],["每一列对应一个频率维度",{"5":{"88":2}}],["每一步计算都需要读取前一步的隐藏状态",{"5":{"92":2,"153":2}}],["每个都可以是一个复杂的神经网络",{"5":{"159":2}}],["每个专家的参数量为",{"5":{"159":4}}],["每个专家的前馈网络维度为",{"5":{"159":2}}],["每个专家可以视为一个专门处理特定类型输入的",{"5":{"159":2}}],["每个专家可以视为一个从输入空间到输出空间的参数化函数映射",{"5":{"158":2}}],["每个专家对输入的处理结果",{"5":{"158":2}}],["每个设备只存储和计算分配给自己的专家",{"5":{"159":2}}],["每个设备只存储一部分专家的参数",{"5":{"157":2}}],["每个设备拥有部分专家的权重",{"5":{"157":2}}],["每个设备处理不同的数据子批次",{"5":{"157":2}}],["每个设备负责",{"5":{"157":2}}],["每个尺度的变换可以具有不同的感受野和参数配置",{"5":{"150":2}}],["每个方向上的标准差减少为原来的",{"5":{"149":2}}],["每个小卷积使用秩一核",{"5":{"143":2}}],["每个标准基向量",{"5":{"141":2}}],["每个条件概率",{"5":{"140":1}}],["每个条件概率的计算复杂度取决于模型架构",{"5":{"140":1}}],["每个子网络对应一种掩码配置",{"5":{"145":2}}],["每个子分布",{"5":{"138":2}}],["每个子矩阵​可以分配给不同的设备",{"5":{"45":1}}],["每个子矩阵",{"5":{"45":1}}],["每个参数需要存储",{"5":{"157":2}}],["每个参数应该对应约",{"5":{"138":2}}],["每个参数分量根据其在梯度中的值进行调整",{"5":{"48":2}}],["每个元素的偏导数按链式规则展开",{"5":{"135":2}}],["每个点代表网络某一层的表示",{"5":{"133":1}}],["每个点代表网络某一层的表示在该平面上的位置",{"5":{"133":1}}],["每个抽象层次的瓶颈约束平衡了信息压缩和信息传递",{"5":{"132":2}}],["每个槽位存储不同类型的信息",{"5":{"132":2}}],["每个头",{"5":{"69":1,"93":2,"132":1}}],["每个头学习到一种特定的",{"5":{"92":2,"153":2}}],["每个头的输出为",{"5":{"132":1}}],["每个头的梯度可能具有不同的范数尺度",{"5":{"41":1}}],["每个头的梯度",{"5":{"41":1}}],["每个头的维度为",{"5":{"93":7,"94":2}}],["每个头的维度为​",{"5":{"93":1}}],["每个头的query向量​就位于张量的第层",{"5":{"93":1}}],["每个头的query",{"5":{"93":1}}],["每个头的注意力分数矩阵维度为",{"5":{"93":2}}],["每个头的key和value可以缓存历史位置的结果",{"5":{"93":2}}],["每个头处理输入的不同",{"5":{"93":2}}],["每个头有其独立的query",{"5":{"69":1}}],["每个头有自己的参数",{"5":{"93":2}}],["每个头有更强的计算能力",{"5":{"93":2}}],["每个头有独立的key和value投影",{"5":{"93":2}}],["每个头独立地对自己的query",{"5":{"93":2}}],["每个头独立进行的注意力计算",{"5":{"93":1}}],["每个头独立进行",{"5":{"93":1}}],["每个头定义了一种特定的关联模式检测器",{"5":{"93":2}}],["每个头只使用自己的一组投影参数",{"5":{"93":2}}],["每个头内部的计算可以利用tensor",{"5":{"93":1}}],["每个头内部的",{"5":{"93":1}}],["每个头可以专注于捕获输入数据的不同特征或不同类型的关联模式",{"5":{"93":2}}],["每个头可以专注于更细粒度的特征",{"5":{"93":2}}],["每个头可以学习不同的投影方向",{"5":{"94":2}}],["每个负样本被推离",{"5":{"69":2}}],["每个位置平均被路由到",{"5":{"157":2}}],["每个位置以固定概率",{"5":{"101":1}}],["每个位置以固定概率被独立地选为掩码位置",{"5":{"101":1}}],["每个位置",{"5":{"69":1,"90":1}}],["每个位置通过编码函数映射到query和key",{"5":{"69":1}}],["每个位置通过softmax聚合其他位置的信息",{"5":{"69":2}}],["每个位置的激活大小为",{"5":{"157":2}}],["每个位置的注意力权重为正",{"5":{"132":2}}],["每个位置的",{"5":{"86":2,"151":2}}],["每个位置的旋转角度较小",{"5":{"88":2}}],["每个位置主要关注邻近位置",{"5":{"87":2,"152":2}}],["每个位置关注所有位置",{"5":{"87":2,"152":2}}],["每个位置关注若干随机选择的位置",{"5":{"86":2,"151":2}}],["每个位置关注若干随机位置或选定的",{"5":{"87":2,"152":2}}],["每个位置是一个独特的编码向量",{"5":{"90":2}}],["每个位置对应唯一的相位向量",{"5":{"90":1}}],["每个位置对应球面上的一个点",{"5":{"91":2}}],["每个位置有一个独立的位置向量",{"5":{"88":2}}],["每个位置有唯一的编码",{"5":{"91":2}}],["每个位置只关注其左右各个位置",{"5":{"86":1,"151":1}}],["每个位置只关注其左右各",{"5":{"86":1,"151":1}}],["每个位置只关注邻近位置",{"5":{"87":2,"152":2}}],["每个位置只能与一个固定大小窗口内的位置交互",{"5":{"92":2,"153":2}}],["每个位置可以与序列中的所有其他位置直接交互",{"5":{"92":2,"153":2}}],["每个位置可以直接访问序列中的所有其他位置",{"5":{"92":2,"153":2}}],["每个位置可以直接与所有其他位置交互",{"5":{"93":2}}],["每个神经元",{"5":{"71":2}}],["每个神经元本质上是在输入空间中划分一个",{"5":{"71":2}}],["每个激活函数在输入空间中定义了一个",{"5":{"71":1}}],["每个激活函数",{"5":{"71":1}}],["每个sigmoid神经元在输入空间中形成一个",{"5":{"71":1}}],["每个sigmoid神经元",{"5":{"71":1}}],["每个样本为维向量",{"5":{"47":1}}],["每个样本为",{"5":{"47":1}}],["每个中间变量的贡献是它对的偏导数与它对的偏导数的乘积之和",{"5":{"48":1}}],["每个中间变量的贡献是它对",{"5":{"48":1}}],["每个索引对应张量的一个维度",{"5":{"50":2}}],["每个秩一矩阵对应一个主成分方向",{"5":{"50":2}}],["每个节点是一个张量",{"5":{"50":2}}],["每个边代表张量的一个维度",{"5":{"50":2}}],["每个微批次独立计算",{"5":{"50":2}}],["每个d维的embedding向量被成对地组织为",{"5":{"88":2}}],["每个块要么完全参与注意力计算",{"5":{"86":2,"151":2}}],["每个块是一个的旋转矩阵",{"5":{"88":1}}],["每个块",{"5":{"88":1}}],["每个块对应一个头的投影",{"5":{"93":2}}],["每个块处理序列的一个子集",{"5":{"93":2}}],["每个的旋转块独立运作",{"5":{"88":1}}],["每个",{"5":{"88":1,"89":1,"90":1}}],["每个维度使用位精度",{"5":{"89":1}}],["每个维度使用",{"5":{"89":1}}],["每个配对对应一个特定的频率成分",{"5":{"90":2}}],["每个是位置的编码向量",{"5":{"89":1}}],["每个是一个复指数序列",{"5":{"90":1}}],["每个频率成分对应一个在单位圆上匀速运动的点",{"5":{"90":2}}],["每个频率成分定义了一个独立的圆群",{"5":{"90":2}}],["每个频率成分定义了一个独立的复平面",{"5":{"90":2}}],["每个频率成分具有相同的能量",{"5":{"90":2}}],["每个注意力层学习token之间的交互模式",{"5":{"71":2}}],["每个注意力头都有自己独立的query",{"5":{"93":2}}],["每个注意力头的计算与单头注意力相同",{"5":{"93":1}}],["每个注意力头的计算可以视为一个独立的",{"5":{"93":2}}],["每个注意力头",{"5":{"93":1}}],["每个核函数关注输入数据的不同方面",{"5":{"93":2}}],["每个输入只激活",{"5":{"159":2}}],["每个输入只激活一个专家",{"5":{"159":2}}],["每个输入只使用其中",{"5":{"159":2}}],["每个输入维度只贡献给特定的头",{"5":{"93":2}}],["每个输出元素需要次乘法和​次加法",{"5":{"45":1}}],["每个输出元素需要",{"5":{"45":1}}],["每个输出神经元与所有输入神经元相连",{"5":{"46":2}}],["每个输出位置只依赖于输入的一个局部窗口",{"5":{"92":2,"153":2}}],["每个输出位置可以依赖于整个输入序列",{"5":{"92":2,"153":2}}],["每个滤波器负责检测输入中的某种局部模式",{"5":{"93":2}}],["每个可以并行执行个线程束",{"5":{"93":1}}],["每个可以并行执行",{"5":{"93":1}}],["每个基函数​将输入矩阵映射到一个的注意力输出",{"5":{"93":1}}],["每个基函数",{"5":{"93":1}}],["每个词被映射为一个高维实数向量",{"5":{"50":2}}],["每个词元首先通过词嵌入",{"5":{"94":2}}],["每个投影矩阵定义了从输入空间到各自子空间的一个线性映射",{"5":{"94":2}}],["每个投影矩阵定义了一个从输入空间到输出空间的线性变换",{"5":{"94":2}}],["每个片段既有其",{"5":{"95":2}}],["每头维度",{"5":{"45":2}}],["每经过若干个epoch将学习率降低一个因子",{"5":{"48":2}}],["每次参数更新后",{"5":{"155":2}}],["每次更新都使得目标函数单调不增",{"5":{"143":2}}],["每次传播的复杂度由模型架构决定",{"5":{"140":2}}],["每次迭代放大主特征值方向的分量",{"5":{"135":2}}],["每次训练迭代",{"5":{"44":2}}],["每次计算一行都需要遍历整个序列",{"5":{"92":2,"153":2}}],["每次只考虑一个正样本和",{"5":{"69":1}}],["每次只考虑一个正样本和个负样本",{"5":{"69":1}}],["每次只使用一小批样本来估计梯度",{"5":{"96":2}}],["每种扩展都在计算效率",{"5":{"132":2}}],["每种操作都有预定义的梯度计算规则",{"5":{"48":2}}],["每种分解都有其特定的应用场景和数值性质",{"5":{"50":2}}],["每步",{"5":{"86":1,"151":1}}],["每步进行矩阵乘法",{"5":{"92":2,"153":2}}],["每步重新加载",{"5":{"92":2,"153":2}}],["每行向量模长为1",{"5":{"87":2,"152":2}}],["每行的非零模式相同",{"5":{"87":2,"152":2}}],["每行的注意力模式可能完全不同",{"5":{"87":2,"152":2}}],["每行只有一个非零元素",{"5":{"87":2,"152":2}}],["每行是有效的概率分布",{"5":{"87":2,"152":2}}],["每行是一个位置的编码",{"5":{"89":2}}],["每行是一个概率分布",{"5":{"93":2}}],["每层至少",{"5":{"148":1}}],["每层都以概率",{"5":{"145":2}}],["每层都进行自注意力计算和前馈网络变换",{"5":{"92":2,"153":2}}],["每层的激活输出为",{"5":{"41":2,"42":2}}],["每组有个头",{"5":{"93":1}}],["每组有",{"5":{"93":1}}],["部分原因就在于减弱了这种隐式正则化效应",{"5":{"149":2}}],["部分神经元",{"5":{"145":2}}],["部分",{"5":{"41":2,"45":1,"134":2}}],["按需激活",{"5":{"159":2}}],["按行划分为",{"5":{"157":2}}],["按效率排序并剪枝",{"5":{"133":2}}],["按净信息贡献排序",{"5":{"132":2}}],["按范数裁剪与按值裁剪",{"5":{"41":2}}],["按范数裁剪保持梯度的方向",{"5":{"41":2,"97":2}}],["按范数裁剪保持了梯度的方向信息",{"5":{"41":2,"97":2}}],["按范数裁剪更为常用",{"5":{"41":2,"97":2}}],["按范数裁剪",{"5":{"44":2}}],["按降序排列",{"5":{"41":2}}],["按值裁剪将每个梯度元素裁剪到区间",{"5":{"41":1}}],["按值裁剪将每个梯度元素裁剪到",{"5":{"41":1,"97":2}}],["按值裁剪",{"5":{"44":2}}],["按元素应用",{"5":{"45":2}}],["按奇异值拉伸或压缩各方向",{"5":{"45":2}}],["按从输入到输出的顺序依次计算各节点的值",{"5":{"45":2}}],["按拓扑排序遍历可以保证计算每个节点时",{"5":{"45":2}}],["按后向顺序计算可以保证每次计算时所需的依赖值已经就绪",{"5":{"45":2}}],["按列",{"5":{"45":1}}],["按列分割",{"5":{"45":1}}],["按列优先的顺序排列矩阵元素",{"5":{"50":2}}],["按frobenius范数",{"5":{"89":2}}],["按头进行拆分",{"5":{"93":1}}],["按32位浮点数计算",{"5":{"95":2}}],["较短的序列可能无法获得足够的样本来估计可靠的统计量",{"5":{"150":2}}],["较浅层的低级特征",{"5":{"150":2}}],["较平坦",{"5":{"136":2}}],["较平坦的分布",{"5":{"95":2}}],["较高的温度产生更有创意",{"5":{"140":2}}],["较高的熵使模型输出分布更加平滑",{"5":{"137":2}}],["较高的掩码率",{"5":{"101":2}}],["较高层",{"5":{"45":2}}],["较低的温度产生更确定",{"5":{"140":2}}],["较低的掩码率",{"5":{"101":2}}],["较低的",{"5":{"69":2}}],["较低层",{"5":{"45":2}}],["较小或为零",{"5":{"158":2}}],["较小时接近ste的梯度",{"5":{"157":2}}],["较小时",{"5":{"41":3,"91":1,"95":1,"133":1,"138":2,"144":4,"145":1}}],["较小的学习率对应较低的温度",{"5":{"144":2}}],["较小的学习率使模型收敛到更稳定的解",{"5":{"144":2}}],["较小的情况下",{"5":{"142":1}}],["较小的使得分布更加",{"5":{"69":1}}],["较小的产生较大的梯度",{"5":{"69":1}}],["较小的确保稳定性但可能限制学习速度",{"5":{"41":1}}],["较小的值表示激活函数具有更好的抗饱和性能",{"5":{"41":1}}],["较小的值表示激活函数的导数接近1",{"5":{"41":1}}],["较小的",{"5":{"41":3,"69":2,"134":2,"144":2,"154":2}}],["较小的奇异值对应于",{"5":{"71":2}}],["较小的奇异值对应的方向会被压缩",{"5":{"94":2}}],["较小的特征值表示该方向上损失变化平缓",{"5":{"97":2}}],["较小",{"5":{"88":2,"91":2,"92":2,"138":4,"145":2,"153":2}}],["较大时更加平滑",{"5":{"157":2}}],["较大时趋近于零",{"5":{"148":1}}],["较大时趋向于无穷大",{"5":{"147":2}}],["较大时分数差异过大导致的熵过低",{"5":{"132":1}}],["较大时",{"5":{"41":1,"86":1,"90":1,"91":1,"95":2,"101":1,"132":1,"133":1,"137":1,"144":6,"145":3,"148":1,"151":1}}],["较大的学习率对应较高的温度",{"5":{"144":2}}],["较大的学习率允许模型探索参数空间的不同区域",{"5":{"144":2}}],["较大的对应更重要的奇异方向",{"5":{"142":1}}],["较大的使得分布更加",{"5":{"69":1}}],["较大的允许快速学习但可能容忍不稳定",{"5":{"41":1}}],["较大的",{"5":{"41":1,"69":3,"89":1,"142":1,"154":2}}],["较大的​允许处理更长的序列",{"5":{"89":1}}],["较大的奇异值对应于",{"5":{"71":2}}],["较大的奇异值对应的方向会被放大",{"5":{"94":2}}],["较大的应用场景",{"5":{"95":1}}],["较大的特征值表示该方向上损失变化剧烈",{"5":{"97":2}}],["较大",{"5":{"88":4,"92":2,"141":2,"145":2,"153":2,"158":2}}],["较多的头数意味着更多的子空间并行计算",{"5":{"93":2}}],["较少的头数意味着更大的头维度",{"5":{"93":2}}],["较尖锐的分布",{"5":{"95":2}}],["glorot初始化",{"5":{"148":2}}],["global",{"5":{"86":2,"136":2,"147":2,"151":2}}],["gd",{"5":{"134":4}}],["gibbs",{"5":{"132":4}}],["gibbs不等式",{"5":{"61":2}}],["gt",{"5":{"42":5,"132":1,"133":1,"135":1,"136":3,"144":3}}],["generalized",{"5":{"155":2}}],["generalization",{"5":{"154":2}}],["generating",{"5":{"96":2}}],["gelfand",{"5":{"134":2}}],["gelu通过引入高斯分布相关的非线性",{"5":{"41":2}}],["gelu涉及误差函数的计算",{"5":{"41":2}}],["gelu的设计考虑了这一点",{"5":{"41":2}}],["gelu的平滑性和概率解释使其成为自然的选择",{"5":{"41":2}}],["gelu的平滑特性有助于保持相邻位置信息的连续性",{"5":{"71":4}}],["gelu的导数",{"5":{"71":2}}],["gelu的导数不会过于接近零",{"5":{"71":2}}],["gelu的非线性变换将这种位置感知的嵌入转换为新的表示",{"5":{"71":2}}],["gelu的定义表明它是输入与其通过标准正态累积分布函数权重的乘积",{"5":{"71":1}}],["gelu的定义",{"5":{"71":1}}],["gelu被选择作为transformer的激活函数",{"5":{"41":2}}],["gelu可以平滑地处理这些概率信息",{"5":{"41":2}}],["gelu等变体通过不同的策略处理负区域",{"5":{"41":2}}],["gelu等",{"5":{"42":2}}],["gelu等激活函数在理论上都具有同等的表达能力",{"5":{"71":2}}],["gelu",{"5":{"42":6,"132":2}}],["gelu提供token内部的特征非线性",{"5":{"71":2}}],["gelu激活函数",{"5":{"47":2}}],["geq",{"5":{"42":4,"141":4,"142":2,"144":2}}],["geoffrey",{"5":{"44":2}}],["geometry",{"5":{"61":4,"70":2}}],["gemm",{"5":{"95":2}}],["g^t",{"5":{"42":2}}],["gumbel",{"2":{"71":1},"5":{"71":15,"132":4,"157":2,"159":6}}],["gumbel分布是极值分布的一种",{"5":{"71":2}}],["gpu线程块",{"5":{"45":1}}],["gpu线程",{"5":{"45":1}}],["gpu共享内存",{"5":{"45":1}}],["gpu的并行计算能力得到充分利用",{"5":{"47":2}}],["gpu可以同时计算批次中所有样本的注意力分数",{"5":{"50":2}}],["gpu对密集矩阵乘法有高度优化的实现",{"5":{"86":2,"151":2}}],["gpt规模模型的flops估算",{"5":{"45":2}}],["gpt",{"5":{"88":4,"140":1,"142":2}}],["gae退化为蒙特卡洛回报",{"5":{"155":2}}],["gae退化为单步td误差",{"5":{"155":2}}],["gae通过引入两个参数",{"5":{"155":2}}],["gae",{"5":{"155":2}}],["gammaγ",{"5":{"146":1}}],["gan",{"5":{"50":2,"133":2}}],["gather通信收集完整的权重或使用分片权重逐块计算",{"5":{"157":2}}],["gate",{"5":{"132":12,"150":4}}],["gatedattention",{"5":{"132":2}}],["gated",{"5":{"86":4,"151":4}}],["gating",{"5":{"93":2,"159":4}}],["gaussian",{"5":{"96":2,"145":3}}],["gauss",{"5":{"96":4}}],["grebogi",{"5":{"136":2}}],["grobman",{"5":{"134":2,"135":4}}],["grouped",{"5":{"86":2,"93":4,"151":2}}],["group",{"5":{"88":2,"90":2}}],["graph",{"5":{"44":3,"45":12,"47":8,"48":9,"50":6,"61":3,"92":2,"96":9,"143":7,"148":6,"153":2}}],["gradient",{"5":{"41":2,"44":8,"48":2,"50":2,"70":2,"133":2,"134":2,"147":2,"155":2,"157":2}}],["grad",{"5":{"48":4,"136":16,"148":4}}],["gram到gpt的演进体现了以下几个重要趋势",{"5":{"140":2}}],["gram组合的数量呈指数增长",{"5":{"140":2}}],["gram马尔可夫假设",{"5":{"140":2}}],["gram模型假设当前词元只依赖于前",{"5":{"140":1}}],["gram模型假设当前词元只依赖于前个词元",{"5":{"140":1}}],["gram模型虽然简单",{"5":{"140":2}}],["gram模型和现代的transformer架构",{"5":{"140":2}}],["gram模型的局限性",{"5":{"140":2}}],["gram模型的马尔可夫假设",{"2":{"140":1},"5":{"140":1}}],["gram模型是最早将链式法则应用于语言建模的尝试",{"5":{"140":2}}],["gram模型",{"5":{"140":2}}],["gram模型到复杂的gpt系列模型是如何在概率框架下自然演进的",{"5":{"140":2}}],["gram",{"5":{"50":2,"87":2,"140":1,"152":2}}],["gram矩阵",{"5":{"86":1,"87":1,"151":1,"152":1}}],["gram矩阵是向量集合中两两点积的矩阵",{"5":{"87":2,"152":2}}],["gram矩阵是度量学习",{"5":{"87":1,"152":1}}],["gram矩阵是线性的",{"5":{"87":2,"152":2}}],["gram矩阵是半正定的",{"5":{"86":1,"151":1}}],["gram矩阵是半正定矩阵",{"5":{"87":2,"152":2}}],["gram矩阵只是点积的直接计算",{"5":{"87":2,"152":2}}],["gram矩阵的秩等于输入矩阵的秩",{"5":{"87":2,"152":2}}],["gram矩阵的第个元素就是向量和之间的余弦相似度",{"5":{"87":1,"152":1}}],["gram矩阵的第个元素是向量和的内积",{"5":{"87":1,"152":1}}],["gram矩阵的第",{"5":{"87":2,"152":2}}],["gram矩阵编码了所有位置对之间的相似性信息",{"5":{"89":2}}],["gqa",{"5":{"86":2,"151":2}}],["gq",{"5":{"86":3,"151":3}}],["gk",{"5":{"86":3,"151":3}}],["gv",{"5":{"86":3,"151":3}}],["g",{"5":{"86":9,"88":3,"97":6,"132":2,"133":6,"136":38,"142":2,"143":4,"144":4,"151":9}}],["google的研究团队发现",{"5":{"88":2}}],["8",{"0":{"154":1,"155":1,"156":1},"2":{"44":1,"50":1,"51":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"96":1,"132":1,"133":1,"134":1,"135":1,"136":1,"139":1,"145":1,"148":1,"151":1,"152":1,"153":1,"154":5,"155":5,"156":5},"4":{"109":1,"110":1,"111":1,"154":1,"155":1,"156":1},"5":{"41":2,"44":3,"45":4,"47":2,"50":1,"51":1,"71":2,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"96":1,"101":2,"131":12,"132":9,"133":7,"134":5,"135":5,"136":7,"137":6,"139":1,"140":4,"143":2,"145":5,"148":1,"151":1,"152":1,"153":1,"154":6,"155":6,"156":6}}],["98",{"5":{"134":2}}],["99",{"5":{"133":2}}],["9更",{"5":{"70":2}}],["9的预测误差在",{"5":{"70":2}}],["9",{"0":{"147":1,"148":1,"149":1},"2":{"44":1,"50":1,"51":1,"86":1,"87":1,"88":1,"92":1,"93":1,"94":1,"95":1,"132":1,"134":1,"147":5,"148":10,"149":6,"151":1,"152":1,"153":1},"4":{"112":1,"113":1,"114":1,"147":1,"148":1,"149":1},"5":{"41":2,"44":3,"45":4,"47":2,"50":1,"51":1,"71":2,"86":1,"87":1,"88":1,"91":6,"92":1,"93":1,"94":1,"95":1,"101":2,"131":12,"132":7,"133":6,"134":5,"135":4,"136":8,"137":8,"140":4,"141":2,"143":2,"145":5,"147":16,"148":21,"149":17,"151":1,"152":1,"153":1}}],["96",{"5":{"46":2}}],["9之间",{"5":{"95":2}}],["所需的信息量",{"5":{"132":1}}],["所需的额外比特数",{"5":{"61":2}}],["所揭示的深刻数学原理",{"5":{"71":2}}],["所有专家变得相似",{"5":{"158":2}}],["所有专家被均匀使用",{"5":{"158":2}}],["所有设备相同",{"5":{"157":2}}],["所有层使用相同的权重初始化",{"5":{"148":2}}],["所有参数全部激活",{"5":{"159":1}}],["所有参数均匀",{"5":{"144":1}}],["所有参数的梯度",{"5":{"44":2}}],["所有样本的条件概率的乘积",{"5":{"140":2}}],["所有样本的表示趋向于聚集在一起",{"5":{"69":2}}],["所有不动点稳定",{"5":{"136":2}}],["所有这些损失函数都共享softmax",{"5":{"101":2}}],["所有这些正交条件意味着对于所有成立",{"5":{"90":1}}],["所有这些正交条件意味着",{"5":{"90":1}}],["所有键的概率相近",{"5":{"69":1}}],["所有键的概率相近实践中温度参数的选择",{"5":{"69":1}}],["所有位置都参与注意力计算",{"5":{"69":2}}],["所有位置",{"5":{"69":2,"92":2,"153":2}}],["所有位置的表示变得相似",{"5":{"87":2,"152":2}}],["所有位置编码向量的范数都相等",{"5":{"91":2}}],["所有位置对之间的依赖",{"5":{"92":2,"153":2}}],["所有指数项趋向于1",{"5":{"71":2}}],["所有特征值满足",{"5":{"136":2}}],["所有特征值位于单位圆内",{"5":{"41":2}}],["所有特征值大于零",{"5":{"48":2}}],["所有特征值的模不超过1",{"5":{"87":2,"152":2}}],["所有可能的预测向量形成空间中的一个子流形",{"5":{"51":1}}],["所有可能的预测向量形成空间中的一条直线",{"5":{"51":1}}],["所有可能的预测向量形成",{"5":{"51":1}}],["所有可能的预测向量",{"5":{"51":1}}],["所有",{"5":{"87":2,"134":2,"152":2,"159":2}}],["所有元素大于零",{"5":{"87":2,"152":2}}],["所有元素都可能非零",{"5":{"87":2,"152":2}}],["所有奇异值约为1",{"5":{"87":2,"152":2}}],["所有奇异值为1",{"5":{"87":2,"152":2}}],["所有奇异值相近",{"5":{"89":2}}],["所有旋转操作的集合关于矩阵乘法构成一个群",{"5":{"88":2}}],["所有权重约",{"5":{"89":2}}],["所有频率成分的相位差都较小",{"5":{"91":2}}],["所有头共享同一个key和value",{"5":{"93":2}}],["所有头的输出张量​沿头维度拼接",{"5":{"93":1}}],["所有头的输出张量",{"5":{"93":1}}],["所有的query",{"5":{"94":2}}],["所界定",{"5":{"47":1}}],["所以1是特征值",{"5":{"87":2,"152":2}}],["所以全1向量是关于特征值1的特征向量",{"5":{"87":1,"152":1}}],["所以全1向量",{"5":{"87":1,"152":1}}],["所谓",{"5":{"87":2,"152":2}}],["所谓长程依赖",{"5":{"92":2,"153":2}}],["所示",{"5":{"93":2,"95":2}}],["更精细的负载均衡损失同时考虑专家分配频率",{"5":{"158":2}}],["更精细的块大小选择",{"5":{"86":2,"151":2}}],["更精确的通信模型需要考虑设备拓扑和通信模式",{"5":{"157":2}}],["更精确的梯度近似方法包括gumbel",{"5":{"157":2}}],["更稳定的训练",{"5":{"155":2}}],["更稳定但可能收敛慢",{"5":{"69":2}}],["更负于",{"5":{"154":2}}],["更通俗地表达",{"5":{"149":2}}],["更具有直接的实践指导意义",{"5":{"149":2}}],["更具有直接的实践指导价值",{"5":{"148":2}}],["更具体地说",{"5":{"149":4}}],["更取决于多层累积的整体效应",{"5":{"148":2}}],["更集中",{"5":{"147":2}}],["更先进的策略是自适应调整裁剪阈值",{"5":{"147":2}}],["更接近原始损失的极小值",{"5":{"144":2}}],["更接近置换或选择矩阵",{"5":{"41":2}}],["更能深化我们对高维数据表示学习的理论认识",{"5":{"143":2}}],["更能深化我们对深度学习表示学习机制的理论认识",{"5":{"143":2}}],["更能为设计新的高效学习算法提供理论指导",{"5":{"142":2}}],["更能为分析网络的表达能力",{"5":{"45":2}}],["更为权重初始化策略",{"5":{"148":2}}],["更为设计新的优化技术提供了理论基础",{"5":{"147":2}}],["更为设计新的正则化技术提供了理论基础",{"5":{"144":2,"145":2}}],["更为设计和分析新的归一化方法提供了理论基础",{"5":{"146":2}}],["更为线性注意力",{"5":{"141":2}}],["更为后续深入探讨注意力机制",{"5":{"140":2}}],["更多样化的输出",{"5":{"140":2}}],["更多的频率成分",{"5":{"90":2}}],["更多的信息传递任务",{"5":{"92":2,"153":2}}],["更揭示了自注意力机制和transformer架构的理论基础",{"5":{"140":2}}],["更好的泛化",{"5":{"136":2}}],["更大的吸引域",{"5":{"136":2}}],["更有效的初始化策略和更优的优化算法提供了坚实的理论基础",{"5":{"135":2}}],["更有价值",{"5":{"71":2}}],["更高惯性",{"5":{"134":2}}],["更高效的训练算法提供了坚实的理论基础",{"5":{"136":2}}],["更高效的方向发展",{"5":{"132":2}}],["更高效的实现利用了以下观察",{"5":{"88":2}}],["更准确地描述深度网络的训练动态",{"5":{"133":2}}],["更可解释的注意力变体提供了理论基础",{"5":{"132":2}}],["更可靠的机器学习系统",{"5":{"137":2}}],["更可靠",{"5":{"51":2}}],["更复杂的掩码策略",{"5":{"101":2}}],["更复杂的网络结构如树状张量网络",{"5":{"50":2}}],["更容易训练",{"5":{"70":2}}],["更合适",{"5":{"70":2}}],["更快的学习",{"5":{"69":2}}],["更是深度学习模型压缩的核心工具",{"5":{"143":2}}],["更是在隐式地最大化输入不同部分之间的互信息",{"5":{"69":2}}],["更是一个几何变换过程",{"5":{"45":2}}],["更是对位置信息与注意力机制关系的根本性重新思考",{"5":{"88":2}}],["更强的鲁棒性",{"5":{"136":2}}],["更强",{"5":{"42":2}}],["更语义化的特征",{"5":{"71":2}}],["更抽象",{"5":{"71":1}}],["更会错失利用矩阵运算简化计算的机会",{"5":{"46":2}}],["更重要的是可能导致参数进入损失函数曲率极大的区域",{"5":{"147":2}}],["更重要的是可以避免在中间步骤中存储",{"5":{"61":2}}],["更重要的是它反映了人类语言生成的本质过程",{"5":{"140":2}}],["更重要的是",{"5":{"47":2,"70":2,"88":2,"91":2,"145":2,"154":2,"155":2,"158":2,"159":2}}],["更新actor",{"5":{"155":2}}],["更新critic",{"5":{"155":2}}],["更新步长难以确定",{"5":{"155":2}}],["更新策略",{"5":{"155":2}}],["更新停止",{"5":{"149":2}}],["更新也不会跳跃过大",{"5":{"147":2}}],["更新核张量",{"5":{"143":2}}],["更新",{"5":{"134":4}}],["更新为",{"5":{"134":2}}],["更新掩码参数",{"5":{"132":2}}],["更新规则为",{"5":{"48":2}}],["更新后的输出为",{"5":{"86":2,"151":2}}],["更新行和",{"5":{"86":2,"151":2}}],["更新输出",{"5":{"86":2,"151":2}}],["更深入到模型容量",{"5":{"159":2}}],["更深刻地影响了现代深度学习中的模型设计",{"5":{"143":2}}],["更深刻地反映了机器学习问题的数学本质",{"5":{"51":2}}],["更深刻揭示了大语言模型设计的数学根基",{"5":{"69":2}}],["更深层次的问题在于",{"5":{"88":2}}],["更低的最低频率",{"5":{"90":2}}],["更简单地",{"5":{"87":2,"152":2}}],["更简单的不等式分析",{"5":{"91":2}}],["更一般地",{"5":{"92":2,"143":2,"153":2}}],["更在理论上展示了如何通过参数共享和并行计算来实现丰富的表示学习",{"5":{"93":2}}],["研究的是梯度在深度网络中传播时的行为",{"5":{"41":2}}],["研究表明",{"5":{"41":2,"48":2,"71":2,"97":2,"138":4,"143":2,"150":2,"159":2}}],["研究发现",{"5":{"48":2,"138":2}}],["研究其频谱特性与语言规律的关系",{"5":{"88":2}}],["研究者提出了多种负载均衡辅助损失函数",{"5":{"158":2}}],["研究者提出了多种变体以适应不同场景",{"5":{"142":2}}],["研究者们引入了基线",{"5":{"155":2}}],["研究者们提出了使用人类偏好数据训练一个",{"5":{"154":1}}],["研究者们提出了使用人类偏好数据训练一个奖励模型",{"5":{"154":1}}],["研究者们提出了大量的注意力机制变体",{"5":{"86":2,"151":2}}],["研究者发现训练良好的transformer确实存在这种",{"5":{"92":2,"153":2}}],["研究者发现某些头专注于学习局部上下文信息",{"5":{"93":2}}],["来换取计算效率的提升",{"5":{"159":2}}],["来处理",{"5":{"159":2}}],["来保护专家专业化",{"5":{"158":2}}],["来保持激活的稳定性",{"5":{"41":1}}],["来分担负载",{"5":{"158":2}}],["来防止梯度下溢",{"5":{"157":2}}],["来防止位置关注位置的信息",{"5":{"91":1}}],["来防止位置",{"5":{"91":1}}],["来计算梯度",{"5":{"157":2}}],["来计算优势函数",{"5":{"155":2}}],["来绕过这个不可微的部分",{"5":{"157":2}}],["来探索环境",{"5":{"156":2}}],["来作为基线",{"5":{"155":1}}],["来预测人类对模型输出的偏好",{"5":{"155":2}}],["来预测偏好概率",{"5":{"101":1}}],["来控制估计的偏差",{"5":{"155":2}}],["来控制总体错误率",{"5":{"96":2}}],["来替代原始的回报",{"5":{"155":2}}],["来平滑损失函数",{"5":{"147":2}}],["来决定哪些神经元被保留",{"5":{"145":2}}],["来决定各专家的权重",{"5":{"93":2}}],["来数学建模",{"5":{"145":2}}],["来补偿裁剪带来的正则化削弱效应",{"5":{"144":2}}],["来减少参数量",{"5":{"143":1}}],["来减少内存带宽消耗",{"5":{"86":2,"151":2}}],["来动态调整不同奇异值的贡献",{"5":{"142":1}}],["来实现",{"5":{"140":1}}],["来衡量",{"5":{"138":2}}],["来限制注意力计算的范围",{"5":{"132":1}}],["来理解任务",{"5":{"101":2}}],["来度量预测与真实分布的差异",{"5":{"69":2}}],["来自奖励模型的分数",{"5":{"154":2}}],["来自于对计算约束的深入分析",{"5":{"138":1}}],["来自二阶导数项",{"5":{"135":2}}],["来自对",{"5":{"135":2}}],["来自不同数据点的表示对",{"5":{"133":2}}],["来自同一数据点的表示对",{"5":{"133":2}}],["来自右边第",{"5":{"101":1}}],["来自右边第个token的信息",{"5":{"101":1}}],["来自左边第",{"5":{"101":1}}],["来自左边第个token的信息",{"5":{"101":1}}],["来自网络的直接输出",{"5":{"70":2}}],["来自数据分布",{"5":{"69":2}}],["来自矩阵乘法",{"5":{"45":1}}],["来稳定训练",{"5":{"41":2,"87":2,"152":2}}],["来估计它们",{"5":{"155":2}}],["来估计状态价值",{"5":{"154":2}}],["来估计真实的梯度向量",{"5":{"149":2}}],["来估计",{"5":{"71":1}}],["来组织偏导数",{"5":{"48":2}}],["来获取每个样本的损失值",{"5":{"61":2}}],["来降低计算复杂度",{"5":{"86":2,"151":2}}],["来隐式地实现类似的目标",{"5":{"69":2}}],["来隐式地编码位置",{"5":{"88":2}}],["来建模超长距离的依赖关系",{"5":{"92":2,"153":2}}],["来建模不同类型的依赖关系",{"5":{"92":2,"153":2}}],["来匹配",{"5":{"94":2}}],["来传递",{"5":{"94":2}}],["来确保在训练初期具有较好的条件数",{"5":{"94":2}}],["来代替",{"5":{"95":2}}],["来近似它",{"5":{"69":2}}],["来近似",{"5":{"96":2}}],["来近似后验分布",{"5":{"96":2}}],["来编码来自分布",{"5":{"96":1}}],["来逼近复杂的后验分布",{"5":{"96":1}}],["来调节softmax的",{"5":{"96":1}}],["来解决这个问题",{"5":{"97":1}}],["来引导模型学习",{"5":{"101":2}}],["理解moe的独特价值",{"5":{"159":2}}],["理解moe训练的梯度计算",{"5":{"157":2}}],["理解专家坍缩的数学机制和防止策略",{"5":{"158":2}}],["理解专家并行的数学原理和通信开销",{"5":{"157":2}}],["理解路由概率的计算",{"5":{"158":2}}],["理解其原理对于掌握大模型对齐技术至关重要",{"5":{"154":2}}],["理解其数学本质对于设计稳定的训练策略至关重要",{"5":{"136":2}}],["理解其数学原理对于深入掌握位置编码的本质",{"5":{"91":2}}],["理解其数学原理对于深入掌握transformer架构",{"5":{"95":2}}],["理解dropout的期望保持性质",{"5":{"145":2}}],["理解正则化对梯度与损失的影响",{"5":{"144":2}}],["理解为什么适当的正则化能够加速收敛",{"5":{"144":2}}],["理解为什么mse的最优解对应于目标向量在预测空间上的正交投影",{"5":{"51":2}}],["理解收敛",{"5":{"136":2}}],["理解收敛性不仅关系到优化算法能否找到最优解",{"5":{"136":2}}],["理解了ppo的数学原理后",{"5":{"154":2}}],["理解了损失",{"5":{"135":2}}],["理解了仿射变换与激活函数的组合如何实现从输入到输出的非线性映射",{"5":{"46":2}}],["理解深度网络训练中常见的梯度消失",{"5":{"135":2}}],["理解梯度计算的数学原理对于诊断训练问题和优化模型性能具有重要意义",{"5":{"157":2}}],["理解梯度协方差矩阵不仅具有理论价值",{"5":{"149":2}}],["理解梯度裁剪与正则化的数学本质",{"5":{"147":2}}],["理解梯度饱和的数学机制对于设计有效的激活函数和训练策略至关重要",{"5":{"41":2}}],["理解梯度流对于设计深层网络架构和训练策略至关重要",{"5":{"41":2}}],["理解这种演化对于设计更好的优化算法至关重要",{"5":{"149":2}}],["理解这种等价性有助于揭示dropout的正则化本质",{"5":{"145":2}}],["理解这种低秩特性的数学本质",{"5":{"142":2}}],["理解这种类比有助于我们预测和控制涌现行为",{"5":{"138":2}}],["理解这种几何结构有助于理解位置编码如何影响注意力机制的行为",{"5":{"91":2}}],["理解这一几何本质对于分析神经网络的表达能力和特征学习机制至关重要",{"5":{"45":2}}],["理解这一几何本质对于分析神经网络的能力和局限性至关重要",{"5":{"47":2}}],["理解这一关系有助于我们从更高的视角理解梯度的计算原理",{"5":{"44":2}}],["理解这一关系对于正确使用交叉熵损失函数至关重要",{"5":{"61":2}}],["理解这些挑战的数学本质",{"5":{"157":2}}],["理解这些数学条件不仅具有理论意义",{"5":{"148":2}}],["理解这些数学条件不仅有助于诊断训练问题",{"5":{"148":2}}],["理解这些数学本质",{"5":{"146":2}}],["理解这些分解技术的数学本质",{"5":{"143":3}}],["理解这些基础原理",{"5":{"142":2}}],["理解这些科学基础",{"5":{"138":2}}],["理解这些损失函数的数学结构",{"5":{"101":2}}],["理解这些差异是选择合适损失函数的基础",{"5":{"70":2}}],["理解这些原因对于理解现代大语言模型的架构设计至关重要",{"5":{"71":2}}],["理解这些并行策略的数学基础",{"5":{"50":2}}],["理解这些参数的选择动机对于深入掌握编码原理至关重要",{"5":{"91":2}}],["理解这些优化性质对于设计和改进大语言模型的训练策略具有重要的理论和实践意义",{"5":{"97":2}}],["理解这个几何结构对于设计有效的优化算法至关重要",{"5":{"71":2}}],["理解这个矩阵的谱性质",{"5":{"87":2,"152":2}}],["理解这两种情况的数学差异对于理解注意力的表达能力至关重要",{"5":{"94":2}}],["理解这三个投影矩阵如何从输入嵌入中生成",{"5":{"95":2}}],["理解反向传播的计算复杂度对于评估训练效率和设计高效算法至关重要",{"5":{"44":2}}],["理解前向传播的数学本质",{"5":{"45":4}}],["理解前向传播的计算复杂度对于评估网络效率",{"5":{"45":1}}],["理解仿射变换的几何本质",{"5":{"47":2}}],["理解矩阵运算与逐样本计算的一致性",{"5":{"47":2}}],["理解它们的数学性质和计算特性对于优化模型性能和调试模型行为至关重要",{"5":{"50":2}}],["理解张量运算的数学本质有助于编写高效的代码和诊断性能瓶颈",{"5":{"50":2}}],["理解不同运算的资源需求对于优化模型设计和硬件利用至关重要",{"5":{"50":2}}],["理解偏导数",{"5":{"48":2}}],["理解偏差",{"5":{"51":2}}],["理解交叉熵的物理意义",{"5":{"61":2}}],["理解rope的局限性对于正确使用它至关重要",{"5":{"88":2}}],["理解rope的数学本质",{"5":{"88":2}}],["理解多层堆叠如何影响长程依赖建模",{"5":{"92":2,"153":2}}],["理解词序承载的信息",{"5":{"92":2,"153":2}}],["理解query",{"5":{"94":4}}],["理解softmax的梯度特性对于训练深度神经网络至关重要",{"5":{"95":2}}],["理解注意力权重矩阵的分布特性是分析注意力如何建模长程依赖的关键",{"5":{"92":2,"153":2}}],["理解注意力机制的计算复杂度对于分析模型的可扩展性至关重要",{"5":{"95":2}}],["理解随机变量和概率分布的概念",{"5":{"96":2}}],["理解最大似然估计",{"5":{"96":2}}],["理解logits的本质",{"5":{"96":2}}],["理论目标",{"5":{"148":1}}],["理论解释复杂",{"5":{"146":1}}],["理论推导",{"2":{"145":1},"5":{"145":1}}],["理论界限",{"2":{"141":1},"5":{"141":1}}],["理论无限",{"5":{"140":1}}],["理论保证",{"5":{"139":2}}],["理论性质分析",{"2":{"139":1},"5":{"139":1}}],["理论与实践的桥梁",{"5":{"136":2}}],["理论和中心流形",{"5":{"136":2}}],["理论是信息论与深度学习交叉领域的核心成果",{"5":{"133":2}}],["理论简洁",{"5":{"41":2}}],["理论上所有",{"5":{"158":2}}],["理论上的秩上界",{"2":{"141":1},"5":{"141":1}}],["理论上可以拟合任意连续函数",{"5":{"47":2}}],["理论上模型可以学习到条件期望的真实形式",{"5":{"51":2}}],["理论上",{"5":{"86":2,"87":2,"90":2,"92":2,"93":2,"145":2,"151":2,"152":2,"153":2,"155":2,"159":2}}],["理论上注意力矩阵的秩最多为​",{"5":{"87":1,"152":1}}],["理论上注意力矩阵",{"5":{"87":1,"152":1}}],["理论上比正弦编码更灵活",{"5":{"91":2}}],["理论上单层注意力能够建模任意",{"5":{"92":2,"153":2}}],["理论证明",{"5":{"88":2}}],["理论分析表明",{"5":{"97":4,"147":2}}],["理想的负载均衡要求",{"5":{"159":2}}],["理想的训练动态是",{"5":{"158":2}}],["理想的路由应该使得专家分配尽可能均匀",{"5":{"158":2}}],["理想的路由策略应该使得每个专家处理的输入量大致相等",{"5":{"157":2}}],["理想的重叠可以将有效时间降低到",{"5":{"157":2}}],["理想的激活函数应该在整个输入范围内都有非零导数",{"5":{"41":2}}],["理想的层次化学习应该满足",{"5":{"71":2}}],["理想的稀疏模式应该保留注意力矩阵的",{"5":{"87":2,"152":2}}],["理想的位置编码应当能够自然地外推到任意长度",{"5":{"88":2}}],["理想的情况是各个头学习到互补的信息",{"5":{"93":2}}],["理想情况下",{"5":{"89":2,"158":4}}],["激活",{"5":{"159":2}}],["激活的专家数为",{"5":{"159":4}}],["激活的峰值显存约为批次大小乘以层数乘以每层激活大小",{"5":{"157":2}}],["激活显存会相应增加",{"5":{"157":2}}],["激活显存可能达到",{"5":{"157":2}}],["激活专家数为",{"5":{"157":2}}],["激活模式是对输入的编码表示",{"5":{"137":2}}],["激活函数导数的最大范数",{"5":{"148":2}}],["激活函数导数的范数应该接近1",{"5":{"41":2}}],["激活函数对梯度稳定性的影响总结",{"5":{"148":1}}],["激活函数对梯度稳定性的影响总结梯度的稳定性不仅取决于单层的雅可比矩阵性质",{"5":{"148":1}}],["激活函数对这种频率分解信息的处理决定了位置信息如何在网络中传递",{"5":{"71":2}}],["激活函数为",{"5":{"148":2}}],["激活函数为阈值函数",{"5":{"46":6}}],["激活函数之前",{"5":{"145":2}}],["激活函数设计的数学原则",{"2":{"41":1},"5":{"41":1}}],["激活函数的几何性质",{"2":{"148":1},"5":{"148":1}}],["激活函数的几何作用",{"5":{"45":2}}],["激活函数的数学角色",{"0":{"71":1},"2":{"47":1},"4":{"71":1},"5":{"21":4,"47":1,"71":1,"131":4}}],["激活函数的数学角色<",{"5":{"21":1,"131":1}}],["激活函数的饱和可以从两个层面理解",{"5":{"41":2}}],["激活函数的计算成本直接影响训练和推理速度",{"5":{"41":2}}],["激活函数的研究仍在继续",{"5":{"41":2}}],["激活函数的导数对角矩阵调制权重矩阵的输出",{"5":{"135":2}}],["激活函数的导数不应该过大",{"5":{"41":2}}],["激活函数的导数性质是理解神经网络训练动力学的核心数学基础",{"5":{"42":2}}],["激活函数的导数结构直接决定了梯度如何在不同层之间传播",{"5":{"42":2}}],["激活函数的信息压缩作用",{"2":{"71":1},"5":{"71":1}}],["激活函数的信息几何视角",{"2":{"71":1},"5":{"71":1}}],["激活函数的引入打破了这种线性局限",{"5":{"71":2}}],["激活函数的引入彻底改变了这一局面",{"5":{"71":2}}],["激活函数的输出值较高",{"5":{"71":2}}],["激活函数的概率解释与损失函数的信息论基础共同构成了分类任务训练的数学框架",{"5":{"71":2}}],["激活函数的非线性变换正是实现这种信息筛选的机制",{"5":{"71":2}}],["激活函数的非线性变换增加了编码的丰富性",{"5":{"71":2}}],["激活函数的选择对梯度的命运有着决定性影响",{"5":{"148":2}}],["激活函数的选择对梯度的稳定性有着决定性影响",{"5":{"148":2}}],["激活函数的选择与注意力机制的设计紧密相关",{"5":{"41":2}}],["激活函数的选择影响这个下界的紧致程度",{"5":{"71":2}}],["激活函数的设计需要满足以下几个条件",{"5":{"47":2}}],["激活函数需要能够表示复杂的非线性映射",{"5":{"41":2}}],["激活函数",{"5":{"41":1,"44":1,"45":3,"47":3,"71":5,"135":4,"148":2}}],["激活函数作为函数逼近的基石",{"2":{"71":1},"5":{"71":1}}],["激活函数与非线性数学",{"2":{"21":1,"131":1},"4":{"41":1,"42":1,"71":1},"5":{"21":7,"131":7}}],["激活函数与互信息最大化",{"2":{"71":1},"5":{"71":1}}],["激活函数与transformer架构的数学协同",{"2":{"71":1},"5":{"71":1}}],["激活函数与位置编码的配合",{"2":{"71":1},"5":{"71":1}}],["激活函数是神经网络能够拟合复杂非线性函数的核心数学组件",{"5":{"71":2}}],["激活函数构成的函数族在适当条件下可以满足这些要求",{"5":{"71":1}}],["激活函数类型与逼近效率",{"5":{"71":2}}],["激活函数可以被理解为对输入信息的某种非线性编码和变换",{"5":{"71":2}}],["激活函数可以视为一种信息瓶颈",{"5":{"71":2}}],["激活函数在这一逼近能力中扮演着核心角色",{"5":{"71":2}}],["激活函数在这个过程中扮演关键角色",{"5":{"71":2}}],["激活函数在这个几何框架中扮演着重要角色",{"5":{"71":2}}],["激活函数在transformer架构中的应用体现了深度学习组件之间数学协同的设计哲学",{"5":{"71":2}}],["激活函数在几何上对仿射变换的结果执行了逐元素的非线性",{"5":{"45":1}}],["激活函数负责非线性切割",{"5":{"45":2}}],["激活函数通过影响概率分布的形状",{"5":{"71":2}}],["激活函数通过引入非线性打破了这一限制",{"5":{"47":2}}],["激活函数通过引入非线性",{"5":{"47":2}}],["激活函数将线性变换的输出通过一个非线性函数进行变换",{"5":{"71":2}}],["激活函数将神经元的净输入映射到输出",{"5":{"47":1}}],["激活分布的稳定化",{"5":{"41":2}}],["激活区",{"5":{"71":2}}],["激活节点",{"5":{"45":2}}],["激活值溢出",{"5":{"44":1}}],["激活值的内存消耗可能超过参数本身",{"5":{"50":2}}],["原点",{"5":{"136":1}}],["原点变为不稳定",{"5":{"136":2}}],["原点是稳定结点",{"5":{"136":1}}],["原点保持不动",{"5":{"71":2}}],["原理的应用",{"5":{"137":2}}],["原理将模型选择与数据压缩联系起来",{"5":{"137":2}}],["原理",{"5":{"135":2,"137":2}}],["原则四",{"5":{"70":2}}],["原则三",{"5":{"70":2}}],["原则二",{"5":{"70":2}}],["原则一",{"5":{"70":2}}],["原则3",{"5":{"41":10}}],["原始参数",{"5":{"157":2}}],["原始梯度不为零",{"5":{"147":2}}],["原始梯度信息仍然可以部分保留",{"5":{"41":2}}],["原始hessian的条件数为",{"5":{"144":2}}],["原始椭圆等高线占主导",{"5":{"144":2}}],["原始损失函数",{"5":{"144":2}}],["原始全参数微调的参数量为",{"5":{"142":2}}],["原始序列和打乱序列之间的kl散度为",{"5":{"69":2}}],["原始注意力的计算复杂度为",{"5":{"141":2}}],["原始注意力计算的时间复杂度为",{"5":{"87":2,"152":2}}],["原始注意力分数矩阵的秩最多为",{"5":{"87":1,"152":1}}],["原始注意力分数矩阵",{"5":{"87":1,"152":1}}],["原始的硬标签损失对正确类别的梯度为",{"5":{"144":2}}],["原始的矩阵有个自由度",{"5":{"87":1,"152":1}}],["原始的",{"5":{"87":1,"152":1}}],["原始输入的梯度贡献仍然保持完整",{"5":{"92":2,"153":2}}],["原始输入中的一部分信息必然被丢弃",{"5":{"94":2}}],["原因有二",{"5":{"147":2}}],["原因在于",{"5":{"46":2}}],["原因在于io效率的提升",{"5":{"86":2,"151":2}}],["原因包括",{"5":{"51":2,"135":2}}],["原假设通常是我们想要拒绝的假设",{"5":{"96":2}}],["然而",{"5":{"41":6,"42":4,"44":2,"45":2,"46":2,"47":2,"48":2,"51":2,"61":2,"69":4,"70":4,"71":6,"86":12,"87":8,"88":14,"89":8,"90":4,"91":2,"92":4,"93":6,"94":2,"96":4,"97":8,"101":4,"136":2,"138":2,"141":2,"143":2,"144":2,"145":2,"146":6,"147":8,"148":16,"149":10,"150":2,"151":12,"152":8,"153":4,"154":4,"155":6,"157":2,"158":8,"159":4}}],["然而在实际实现中",{"5":{"46":2}}],["然后应用于预测更大规模模型的配置",{"5":{"159":2}}],["然后应用链式法则累加梯度",{"5":{"48":2}}],["然后由门控机制选择性地组合它们的输出",{"5":{"158":2}}],["然后由人类标注者选择哪个回复更好",{"5":{"154":2}}],["然后继续遵循策略",{"5":{"156":2}}],["然后最小化均方误差损失",{"5":{"155":2}}],["然后进行归一化处理",{"5":{"146":2}}],["然后进行信息压缩",{"5":{"133":2}}],["然后对输入进行标准化处理",{"5":{"146":2}}],["然后对结果取平均",{"5":{"145":2}}],["然后利用概率链式法则",{"5":{"140":2}}],["然后利用向量化操作同时处理所有维度",{"5":{"88":2}}],["然后",{"5":{"41":2,"51":2,"93":2,"143":2,"144":2}}],["然后经过gelu激活",{"5":{"71":2}}],["然后通过梯度上升更新策略参数",{"5":{"155":2}}],["然后通过仿射变换进行缩放和平移",{"5":{"146":2}}],["然后通过第二层",{"5":{"145":2}}],["然后通过",{"5":{"143":1}}],["然后通过进行线性组合",{"5":{"143":1}}],["然后通过softmax将logits转换为概率分布",{"5":{"101":2}}],["然后通过平移",{"5":{"71":2}}],["然后通过激活函数的导数进行缩放",{"5":{"44":2}}],["然后通过激活函数进行非线性变换",{"5":{"47":1}}],["然后通过激活函数",{"5":{"47":1}}],["然后通过转置将维度排列为​",{"5":{"93":1}}],["然后通过转置将维度排列为",{"5":{"93":1}}],["然后通过最小化近似分布与真实后验分布之间的kl散度来找到最好的近似",{"5":{"96":2}}],["然后从输出层开始",{"5":{"44":2}}],["然后在下一层再次进行线性变换",{"5":{"71":2}}],["然后在各主轴方向上进行缩放和平移",{"5":{"45":2}}],["然后在attention分数上添加一个与相对位置相关的偏置",{"5":{"88":2}}],["然后根据梯度方向更新参数",{"5":{"48":2}}],["然后逐层向后传递",{"5":{"48":2}}],["然后使用这个奖励模型来指导策略优化",{"5":{"101":2}}],["然后使用这些估计的分布来表征原始统计量的不确定性",{"5":{"96":2}}],["然后使用激活函数",{"5":{"71":2}}],["然后使用链式法则组合这些梯度",{"5":{"48":2}}],["然后使用累加的梯度进行一次参数更新",{"5":{"50":2}}],["然后再衰减",{"5":{"48":2}}],["然后再进行衰减",{"5":{"48":2}}],["然后采取相应的策略",{"5":{"51":2}}],["然后与输入特征相乘",{"5":{"51":2}}],["然后计算核张量",{"5":{"143":2}}],["然后计算",{"5":{"61":2}}],["然后详细推导了正弦余弦编码的数学公式",{"5":{"91":2}}],["然后将它们的输出进行融合",{"5":{"150":2}}],["然后将输出平移和缩移到区间",{"5":{"42":1}}],["然后将输出平移和缩移到",{"5":{"42":1}}],["然后将这些关联信息进行融合",{"5":{"93":2}}],["然后将这些权重作为系数对信息内容进行加权平均",{"5":{"95":2}}],["然后将对应的值进行加权聚合",{"5":{"95":2}}],["然后拼接结果",{"5":{"93":2}}],["然后沿奇异值方向进行缩放",{"5":{"94":2}}],["然后检验这些差异的均值是否显著不为零",{"5":{"96":2}}],["特点",{"5":{"143":1}}],["特定领域的专业知识",{"5":{"138":2}}],["特定任务激活函数",{"5":{"41":2}}],["特性使其具有良好的可扩展性",{"5":{"69":2}}],["特性",{"5":{"41":1,"44":1,"45":1,"51":2,"143":1,"159":1}}],["特性是注意力机制建模长程依赖的数学基础",{"5":{"92":2,"153":2}}],["特别广泛应用于transformer架构",{"5":{"150":2}}],["特别重要",{"5":{"150":2}}],["特别当",{"5":{"136":1}}],["特别当接近临界值时",{"5":{"136":1}}],["特别是当批量大小为1时",{"5":{"146":2}}],["特别是当批量大小较大",{"5":{"146":2}}],["特别是当目标分布是高度非高斯的时候",{"5":{"96":2}}],["特别是卷积神经网络",{"5":{"143":2}}],["特别是以lora",{"5":{"142":2}}],["特别是其低秩特性",{"5":{"141":2}}],["特别是",{"5":{"70":2}}],["特别是第五章注意力机制",{"5":{"71":2}}],["特别是gpu",{"5":{"47":2}}],["特别是要保持与矩阵乘法的良好兼容性",{"5":{"88":2}}],["特别是在原始问题条件数较差的情况下",{"5":{"144":2}}],["特别是在分析表示压缩",{"5":{"137":2}}],["特别是在分析优化过程的信息动态和设计新的优化方法方面",{"5":{"137":2}}],["特别是在描述数据分布",{"5":{"137":2}}],["特别是在循环神经网络和transformer中",{"5":{"97":2}}],["特别是在与sigmoid或softmax结合时",{"5":{"97":2}}],["特别地",{"5":{"61":2,"70":2,"136":2}}],["特别值得注意的是",{"5":{"61":2}}],["特别适用于样本量不大或分布未知的情况",{"5":{"96":2}}],["特别适合处理稀疏梯度问题",{"5":{"97":2}}],["特征重用的统计解释",{"5":{"150":1}}],["特征重用的统计解释进一步说明了残差连接的另一个重要作用",{"5":{"150":1}}],["特征累加",{"5":{"150":2}}],["特征三",{"5":{"149":2}}],["特征二",{"5":{"149":2}}],["特征一",{"5":{"149":2}}],["特征轴d",{"5":{"146":2}}],["特征方向上的收敛行为",{"5":{"135":2}}],["特征",{"2":{"135":1},"5":{"135":3}}],["特征工程",{"5":{"70":1}}],["特征维度",{"5":{"41":1}}],["特征空间",{"5":{"45":2}}],["特征空间的维度由神经元的数量决定",{"5":{"47":1}}],["特征空间的维度",{"5":{"47":1}}],["特征空间的几何直觉",{"2":{"50":1},"5":{"50":1}}],["特征空间是机器学习和深度学习中描述数据表示的核心概念",{"5":{"50":2}}],["特征层次假说",{"5":{"45":2}}],["特征值较小的方向收缩较多",{"5":{"144":2}}],["特征值较小的方向对应",{"5":{"142":1}}],["特征值较大的方向",{"5":{"144":2}}],["特征值较大的方向对应",{"5":{"142":2}}],["特征值",{"5":{"136":1}}],["特征值使其回到单位圆内",{"5":{"136":2}}],["特征值称为",{"5":{"136":1}}],["特征值穿过单位圆而非虚轴时",{"5":{"136":2}}],["特征值穿过虚轴",{"5":{"136":2}}],["特征值为",{"5":{"135":2,"136":4}}],["特征值为1",{"5":{"87":2,"152":2}}],["特征值模的稳定性判据",{"5":{"135":2}}],["特征值对应主曲率方向",{"5":{"135":2}}],["特征值估计",{"2":{"135":1},"5":{"135":1}}],["特征值条件",{"5":{"134":1}}],["特征值导致更慢的发散速度",{"5":{"134":2}}],["特征值的分布决定了噪声在不同方向上的强度",{"5":{"149":2}}],["特征值的平方根",{"5":{"45":1}}],["特征值的计算需要求解特征方程",{"5":{"50":2}}],["特征值的乘积等于行列式",{"5":{"87":2,"152":2}}],["特征值的模是判断局部稳定性的关键指标",{"5":{"135":2}}],["特征值的模不超过1",{"5":{"87":2,"152":2}}],["特征值的模为1",{"5":{"87":2,"152":2}}],["特征值和特征向量是线性代数中最深刻也最有应用价值的概念之一",{"5":{"50":2}}],["特征值和特征向量有着多方面的应用",{"5":{"50":2}}],["特征值与特征向量",{"2":{"50":1},"5":{"50":1}}],["特征值与网络稳定性密切相关",{"5":{"50":2}}],["特征值可以是负数",{"5":{"50":2}}],["特征值是实数",{"5":{"50":2}}],["特征值分布偏向小特征值",{"5":{"136":2}}],["特征值分布近似幂律",{"5":{"135":2}}],["特征值分布的尺度律",{"2":{"135":1},"5":{"135":1}}],["特征值分解用于提取数据中方差最大的方向",{"5":{"50":2}}],["特征值分解只适用于方阵且要求矩阵可对角化",{"5":{"50":2}}],["特征值分解和cholesky分解等多种方法",{"5":{"50":2}}],["特征值分解将方阵分解为特征向量矩阵和特征值对角矩阵的乘积",{"5":{"50":2}}],["特征值分解总是可行的",{"5":{"50":2}}],["特征值分解在动力系统分析",{"5":{"50":2}}],["特征值分析是理解矩阵动力学行为的关键",{"5":{"87":2,"152":2}}],["特征值非负",{"5":{"87":2,"152":2}}],["特征向量",{"5":{"50":2}}],["特征向量的数学结构",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["特征向量的结构取决于注意力权重的分布模式",{"5":{"87":2,"152":2}}],["特征向量的正交性也是重要考虑",{"5":{"87":2,"152":2}}],["特征向量可能类似于平滑函数",{"5":{"87":2,"152":2}}],["特征向量可能更分散",{"5":{"87":2,"152":2}}],["特征向量表示这些曲率对应的方向",{"5":{"48":2}}],["特征向量表示分散的主方向",{"5":{"89":2}}],["特征映射函数的设计",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["特征的现象",{"5":{"93":2}}],["特征函数是矩母函数的复数形式",{"5":{"96":2}}],["特征函数提供了有用的分析工具",{"5":{"96":2}}],["随训练步数",{"5":{"158":2}}],["随层数",{"5":{"148":2}}],["随迭代次数急剧增长的现象",{"5":{"147":1}}],["随",{"5":{"41":2,"95":1}}],["随着网络层数的增加",{"5":{"150":2}}],["随着深度学习模型规模的持续增长",{"5":{"143":2,"147":2}}],["随着训练进行逐渐减小噪声以收敛到稳定的分配",{"5":{"159":2}}],["随着训练进行逐渐减小",{"5":{"157":2}}],["随着训练进行",{"5":{"141":2,"158":2}}],["随着训练的进行",{"5":{"89":2,"148":2,"150":2}}],["随着序列长度的增长",{"5":{"141":2}}],["随着n增加",{"5":{"140":2}}],["随着规模增大",{"5":{"138":2}}],["随着参数量的增加",{"5":{"138":2}}],["随着模型规模的增加",{"5":{"138":2}}],["随着学习率或动量参数的变化",{"5":{"136":2}}],["随着迭代次数增加",{"5":{"136":2}}],["随着层深增加信息逐渐变得抽象",{"5":{"132":2}}],["随着层数增加",{"5":{"45":2}}],["随着",{"5":{"70":1}}],["随着接近",{"5":{"70":1}}],["随着阶数增加",{"5":{"42":2}}],["随着上下文长度需求的不断增长",{"5":{"88":2}}],["随着位置的增加",{"5":{"91":1}}],["随着位置增加",{"5":{"91":2}}],["随着位置",{"5":{"91":1}}],["随着批量大小的增加",{"5":{"96":2}}],["随机初始化导致",{"5":{"158":2}}],["随机掩码的引入",{"2":{"145":1},"5":{"145":1}}],["随机向量的注意力分布",{"5":{"141":2}}],["随机向量之间的距离分布变得非常",{"5":{"141":2}}],["随机种子显著影响最终性能",{"5":{"136":2}}],["随机系统",{"5":{"134":2}}],["随机选择一部分位置",{"5":{"101":2}}],["随机性程度",{"5":{"61":2}}],["随机注意力",{"5":{"86":2,"151":2}}],["随机注意力打破了局部性的限制",{"5":{"86":2,"151":2}}],["随机投影",{"5":{"87":2,"152":2}}],["随机梯度",{"5":{"149":1}}],["随机梯度在该方向上的估计方差也相应较大",{"5":{"149":2}}],["随机梯度的协方差矩阵满足以下缩放关系",{"5":{"149":2}}],["随机梯度的微小差异会逐渐放大",{"5":{"93":2}}],["随机梯度是真实梯度",{"5":{"149":1}}],["随机梯度下降",{"5":{"149":2}}],["随机梯度下降的鞍点逃逸",{"5":{"135":2}}],["随机梯度下降的随机动力学",{"2":{"134":1},"5":{"134":1}}],["随机梯度下降的隐式正则化效应",{"5":{"48":2}}],["随机梯度估计的方差",{"5":{"139":2}}],["随机变量",{"5":{"139":2}}],["随机变量与概率分布",{"2":{"96":1},"5":{"96":1}}],["随机变量是样本空间到实数集的映射",{"5":{"96":2}}],["随机变量分为离散随机变量和连续随机变量",{"5":{"96":2}}],["随机采样策略的设计",{"5":{"96":2}}],["随样本量增加而减小",{"5":{"51":2}}],["随后",{"5":{"51":2,"97":2}}],["随后的query",{"5":{"91":2}}],["随维度索引变化",{"5":{"88":1}}],["随维度索引",{"5":{"88":1}}],["随增长",{"5":{"95":1}}],["随时间指数增长",{"5":{"147":2}}],["随时间衰减",{"5":{"144":2}}],["随时间变化",{"5":{"136":2}}],["随时间",{"5":{"97":1}}],["包括",{"5":{"154":1}}],["包括逃离鞍点",{"5":{"149":2}}],["包括单个样本",{"5":{"146":2}}],["包括单词",{"5":{"140":2}}],["包括线性模型",{"5":{"145":2}}],["包括学习率调度",{"5":{"154":2}}],["包括学习率调度的隐式正则化效应",{"5":{"144":2}}],["包括学习率调度和自适应优化算法的收敛性质",{"5":{"97":2}}],["包括dropout与l2正则化的等价性",{"5":{"144":2}}],["包括鞍结分岔",{"5":{"136":2}}],["包括病态条件数对收敛速度的影响",{"5":{"135":2}}],["包括知识蒸馏和模型剪枝",{"5":{"133":2}}],["包括变分信息瓶颈",{"5":{"133":2}}],["包括梯度结构",{"5":{"70":2}}],["包括梯度如何衰减",{"5":{"41":2}}],["包括梯度裁剪和混合精度训练",{"5":{"97":2}}],["包括正弦余弦编码的频率分解",{"5":{"70":2}}],["包括正样本和负样本",{"5":{"69":2}}],["包括在线学习和自回归模型",{"5":{"41":2}}],["包括rnn的时间反向传播问题和transformer中的梯度挑战",{"5":{"41":2}}],["包括有效秩",{"5":{"41":2}}],["包括leaky",{"5":{"42":2}}],["包括逻辑与",{"5":{"47":2}}],["包括损失函数的",{"5":{"48":2}}],["包括gram",{"5":{"50":2}}],["包括结合律",{"5":{"50":2}}],["包括矩阵乘法",{"5":{"50":2}}],["包括其凸性",{"5":{"51":2}}],["包括其凸性与优化特性",{"5":{"51":2}}],["包括基本形式",{"5":{"51":2}}],["包括scaled",{"5":{"86":2,"151":2}}],["包括稀疏注意力",{"5":{"86":2,"151":2}}],["包括奇数情况",{"5":{"88":2}}],["包括周期函数的傅里叶级数展开",{"5":{"90":2}}],["包括长程依赖",{"5":{"92":2,"153":2}}],["包括任意跨度的长程依赖",{"5":{"92":2,"153":2}}],["包括参数估计和假设检验两大类方法",{"5":{"96":2}}],["包括优化景观的几何结构",{"5":{"97":2}}],["包括凸性",{"5":{"97":2}}],["包含以下几个重要方面",{"5":{"155":1}}],["包含前",{"5":{"143":2}}],["包含前个右奇异向量",{"5":{"143":1}}],["包含前向传播",{"5":{"44":2}}],["包含主成分",{"5":{"143":2}}],["包含大量冗余信息",{"5":{"142":1}}],["包含大量的鞍点和局部极小值",{"5":{"97":2}}],["包含模型能够处理的所有可能符号",{"5":{"140":2}}],["包含更多的有效信息",{"5":{"138":2}}],["包含更多类别间相似性的信息",{"5":{"137":2}}],["包含多个局部最小值和鞍点",{"5":{"136":2}}],["包含多少关于序列其他部分的信息",{"5":{"69":1}}],["包含所有奇异值",{"5":{"41":1,"144":2}}],["包含所有常数函数",{"5":{"71":2}}],["包含所有可以通过原点的k维平面上的点",{"5":{"50":2}}],["包含注意力机制和前馈网络",{"5":{"45":2}}],["包含仿射变换和非线性激活两个组成部分",{"5":{"47":2}}],["包含",{"5":{"47":1,"135":2}}],["包含了生成第",{"5":{"140":1}}],["包含了函数关于所有自变量的偏导数信息",{"5":{"48":2}}],["包含了关于所有自变量的二阶偏导数信息",{"5":{"48":2}}],["包含了位置",{"5":{"92":1,"153":1}}],["包含了所有头的query向量",{"5":{"93":1}}],["包含该位置被允许关注的位置索引",{"5":{"86":2,"151":2}}],["包含完全相同的词汇",{"5":{"88":2}}],["包含若干个信息片段",{"5":{"95":2}}],["同transformer",{"5":{"140":1}}],["同样在特征维度d上求和",{"5":{"146":2}}],["同样",{"5":{"141":2}}],["同样具有最大似然解释",{"5":{"69":2}}],["同样将未归一化的相似度分数转换为归一化的注意力权重",{"5":{"61":2}}],["同样使用rope",{"5":{"88":1}}],["同样可以进行批量读取",{"5":{"92":2,"153":2}}],["同样通过在注意力分数上添加掩码来实现",{"5":{"95":2}}],["同时参数量扩展到",{"5":{"159":2}}],["同时为后续的梯度传播和参数优化提供了清晰的数学框架",{"5":{"158":2}}],["同时处理稀疏路由带来的负载均衡问题",{"5":{"157":2}}],["同时包含了所有",{"5":{"150":2}}],["同时避免改变rms的定义",{"5":{"146":2}}],["同时的维度尽可能低",{"5":{"137":1}}],["同时具有良好的可微性",{"5":{"159":2}}],["同时具有更低的计算成本",{"5":{"146":2}}],["同时具有更好的外推性能",{"5":{"88":2}}],["同时具有温度参数控制分布的",{"5":{"132":2}}],["同时进行分类和回归",{"5":{"70":2}}],["同时确保输出满足概率分布的约束",{"5":{"69":2}}],["同时将负样本对的表示推远",{"5":{"69":2}}],["同时将所有负样本键的加权平均推离查询向量",{"5":{"69":2}}],["同时被拉向正样本",{"5":{"69":2}}],["同时",{"5":{"47":2,"61":2,"71":2,"96":2,"137":3,"146":2,"157":2}}],["同时实现了通道间的归一化",{"5":{"41":2}}],["同时归一化稳定了分支路径的激活分布",{"5":{"41":2}}],["同时引入有益的非线性",{"5":{"41":2}}],["同时使得注意力机制能够学习非线性的token交互模式",{"5":{"71":2}}],["同时最小化冗余性",{"5":{"137":2}}],["同时最小化",{"5":{"71":2,"133":1}}],["同时相邻位置之间具有平滑的插值性质",{"5":{"71":2}}],["同时对每个维度施加了非线性变换",{"5":{"71":2}}],["同时对较大的误差给予更大的惩罚",{"5":{"51":2}}],["同时保留了专家专业化学习独特模式的能力",{"5":{"158":2}}],["同时保留了核心的门控思想",{"5":{"150":2}}],["同时保留了输入的相对顺序信息",{"5":{"42":2}}],["同时保留预测所需的信息",{"5":{"137":2}}],["同时保持原始得分的相对顺序",{"5":{"158":2,"159":2}}],["同时保持策略不会偏离原始sft模型太远",{"5":{"154":2}}],["同时保持了良好的归一化效果",{"5":{"146":2}}],["同时保持了对所有头的访问能力",{"5":{"93":2}}],["同时保持大部分计算节省",{"5":{"86":2,"151":2}}],["同时保持或接近原始模型的表达能力",{"5":{"87":2,"152":2}}],["同时保持数值稳定性",{"5":{"97":2}}],["同时通过非线性组合创造了位置",{"5":{"71":2}}],["同时通过累积因子确保数学上的等价性",{"5":{"86":1,"151":1}}],["同时通过累积因子",{"5":{"86":1,"151":1}}],["同时去除",{"5":{"87":2,"152":2}}],["同时又足够小",{"5":{"88":2}}],["同时改变了向量的方向",{"5":{"88":2}}],["同时成立",{"5":{"91":1}}],["同时也使得位置信息能够在后续的线性变换中被有效利用",{"5":{"91":2}}],["同时也指出了其复杂度的局限性及相应的优化方向",{"5":{"92":1,"153":1}}],["同时也指出了其",{"5":{"92":1,"153":1}}],["同时满足因果约束和填充处理的要求",{"5":{"95":2}}],["同时理解评估结果的统计显著性",{"5":{"96":2}}],["同时存在方向使得在时减少",{"5":{"97":1}}],["同时存在方向",{"5":{"97":1}}],["同上",{"5":{"41":1}}],["同一batch内",{"5":{"45":2}}],["同一组内的query头共享同一个key和value",{"5":{"86":2,"151":2}}],["同一套计算范式可以通过不同的掩码模式适应不同的任务需求",{"5":{"95":2}}],["同质化",{"5":{"87":2,"152":2}}],["几个值得注意的延伸方向包括",{"5":{"149":2}}],["几个值得关注的方向包括",{"5":{"88":2}}],["几何变换分解",{"5":{"143":2}}],["几何直观解释",{"2":{"141":1},"5":{"141":1}}],["几何结构",{"5":{"70":1}}],["几何差异的可视化",{"5":{"70":2}}],["几何视角下的损失函数分析",{"2":{"70":1},"5":{"70":1}}],["几何本质",{"5":{"69":2}}],["几何可视化",{"5":{"69":2}}],["几何上",{"5":{"48":2}}],["几何图示与直观理解",{"2":{"51":1},"5":{"51":1}}],["几何解释图示",{"5":{"143":2}}],["几何解释等",{"5":{"70":2}}],["几何解释与梯度特性",{"2":{"42":1},"5":{"42":1}}],["几何解释",{"5":{"61":2,"70":2,"139":2}}],["几乎只关心即时奖励",{"5":{"156":2}}],["几乎全部丢弃",{"5":{"145":2}}],["几乎为零",{"5":{"138":2}}],["几乎处处为零",{"5":{"157":2}}],["几乎处处",{"5":{"137":2}}],["几乎处处相等",{"5":{"61":2}}],["几乎",{"5":{"136":2}}],["几乎所有的计算都在张量之上进行",{"5":{"50":2}}],["几乎所有权重集中在一个位置",{"5":{"87":2,"152":2}}],["几乎所有主流的大语言模型都采用了rope或其变体",{"5":{"88":2}}],["几乎所有临界点",{"5":{"97":2}}],["几乎所有局部极小值都是",{"5":{"97":2}}],["几乎秩1",{"5":{"87":2,"152":2}}],["值为0",{"5":{"158":4,"159":2}}],["值为1",{"5":{"87":2,"152":2}}],["值裁剪",{"5":{"147":2}}],["值裁剪可以提供更细粒度的控制",{"5":{"147":2}}],["值裁剪在某些特定场景下也有其价值",{"5":{"147":2}}],["值裁剪主要惩罚大分量",{"5":{"147":2}}],["值裁剪则独立地处理每个分量",{"5":{"147":2}}],["值裁剪与逐元素裁剪",{"2":{"147":1},"5":{"147":1}}],["值的矩阵乘法",{"5":{"143":2}}],["值的交互",{"5":{"50":2}}],["值矩阵​",{"5":{"141":1}}],["值矩阵",{"5":{"140":2,"141":1}}],["值近似第",{"5":{"135":2}}],["值维度需足够大以编码输出",{"5":{"132":1}}],["值维度需足够大以编码输出所需的信息量",{"5":{"132":1}}],["值接近",{"5":{"132":2}}],["值接近最大",{"5":{"132":2}}],["值空间",{"5":{"132":2}}],["值空间的设计同样具有深意",{"5":{"94":2}}],["值表示激活函数具有更好的抗饱和性能",{"5":{"41":1}}],["值表示激活函数的导数接近1",{"5":{"41":1}}],["值域为",{"5":{"42":4}}],["值即可",{"5":{"42":1}}],["值投影都是通过矩阵乘法实现的",{"5":{"50":2}}],["值向量定义了",{"5":{"132":2}}],["值向量​是​的线性变换",{"5":{"92":1,"153":1}}],["值向量",{"5":{"92":1,"95":2,"132":2,"153":1}}],["值",{"5":{"94":4}}],["值得注意的是",{"5":{"47":2,"51":2,"61":2,"88":4,"91":2,"92":2,"93":4,"94":2,"96":2,"149":2,"153":2}}],["值得在实际应用中仔细考量",{"5":{"94":2}}],["值越大表示方向越接近",{"5":{"95":2}}],["值聚合",{"5":{"95":2}}],["值聚合操作则根据注意力权重进行信息加权求和",{"5":{"95":2}}],["将每个专家的参数分成多个小块",{"5":{"159":2}}],["将每个输入嵌入向量投影到一个​维的查询空间",{"5":{"94":1}}],["将每个输入嵌入向量",{"5":{"94":1}}],["将数据从主存移动到计算单元需要消耗大量时间和能量",{"5":{"159":2}}],["将数据投影到低维子空间",{"5":{"143":2}}],["将当前状态的价值与其后继状态的价值联系起来",{"5":{"156":2}}],["将比率",{"5":{"154":2}}],["将循环结构展开为前馈结构",{"5":{"150":2}}],["将参数限制在有界区域内",{"5":{"147":2}}],["将参数化神经网络",{"5":{"134":2}}],["将dropout建模为连续的概率分布",{"5":{"145":2}}],["将其与",{"5":{"145":2}}],["将其按维度配对组织",{"5":{"90":1}}],["将其按维度配对",{"5":{"90":1}}],["将张量分解应用于卷积核",{"5":{"143":2}}],["将张量表示为有限个秩一张量",{"5":{"143":2}}],["将高阶张量分解为一阶张量",{"5":{"143":2}}],["将高维输入压缩到低维空间",{"5":{"94":2}}],["将结果旋转到最终方向",{"5":{"143":2}}],["将结果旋转到输出空间",{"5":{"45":2}}],["将旋转到某个标准方向",{"5":{"143":1}}],["将主导这个和",{"5":{"141":1}}],["将继续指导我们在更高效的模型架构和训练策略方面的探索",{"5":{"140":2}}],["将自回归模型的概念转化为实际可计算的算法需要通过神经网络来参数化条件概率分布",{"5":{"140":2}}],["将计算预算平均分配给增加参数量和训练数据量是最优策略",{"5":{"138":2}}],["将计算复杂度从二次降低到线性",{"5":{"132":2}}],["将真实标签分布从",{"5":{"137":2}}],["将真实分布和模型分布代入交叉熵定义",{"5":{"61":2}}],["将递推关系",{"5":{"136":1}}],["将递推关系反复展开",{"5":{"136":1}}],["将动量系统的更新矩阵代入特征多项式定义",{"5":{"136":2}}],["将动量方法的状态空间从",{"5":{"135":2}}],["将初始向量分解为特征向量的线性组合",{"5":{"135":2}}],["将不等号方向反转",{"5":{"135":2}}],["将不可直接计算的互信息转化为可优化的变分下界",{"5":{"133":2}}],["将神经网络训练视为离散时间动态系统",{"5":{"134":2}}],["将神经元的净输入",{"5":{"47":1}}],["将优化问题转化为动态系统问题带来以下优势",{"5":{"134":2}}],["将梯度下降",{"5":{"135":2}}],["将梯度下降的更新规则",{"5":{"134":2}}],["将梯度计算的时间复杂度从指数级降低到线性级",{"5":{"44":2}}],["将信息瓶颈理论应用于实践需要处理互信息估计",{"5":{"133":2}}],["将信息从源头高效地导向目的地",{"5":{"132":2}}],["将抽象的互信息量化为可计算的目标函数",{"5":{"133":2}}],["将原始向量映射到高维空间",{"5":{"132":2}}],["将注意力机制建模为信息选择过程",{"5":{"132":2}}],["将注意力建模为编码",{"5":{"132":2}}],["将注意力计算的复杂度从降低到",{"5":{"86":1,"151":1}}],["将注意力计算的复杂度从",{"5":{"86":1,"151":1}}],["将文本切分为token序列",{"5":{"101":2}}],["将文档分解为段落",{"5":{"88":2}}],["将各种问题统一为",{"5":{"69":2}}],["将各层求和",{"5":{"44":2}}],["将各头的输出投影到一个新的空间",{"5":{"93":2}}],["将任意实数向量",{"5":{"69":2}}],["将任意实数向量映射到概率单纯形上",{"5":{"61":2}}],["将正样本对的表示拉近",{"5":{"69":2}}],["将所有序列填充到统一长度",{"5":{"101":2}}],["将所有元素写为矩阵形式",{"5":{"44":2}}],["将所有编码向量初始化为零",{"5":{"89":2}}],["将sigmoid函数重写为",{"5":{"42":2}}],["将空间分为",{"5":{"71":2}}],["将代入对数几率的表达式",{"5":{"71":1}}],["将前一层的表示转换为新的表示",{"5":{"71":1}}],["将实数域映射到",{"5":{"71":1}}],["将实数轴压缩到有限的区间或",{"5":{"71":1}}],["将实数轴压缩到有限的区间",{"5":{"71":1}}],["将",{"5":{"45":1,"46":1,"61":2,"70":4,"71":1,"87":1,"88":1,"90":1,"135":2,"143":1,"152":1,"157":2,"159":2}}],["将query",{"5":{"71":1}}],["将query和key的生成过程代入注意力分数的计算中",{"5":{"94":2}}],["将输出层的损失信息传递到输入层",{"5":{"44":2}}],["将输入分布标准化为单位方差",{"5":{"144":2,"146":2}}],["将输入",{"5":{"133":1}}],["将输入压缩到最小表示中",{"5":{"133":1}}],["将输入信息压缩到紧凑的表示中",{"5":{"132":2}}],["将输入扩展为齐次坐标",{"5":{"46":2}}],["将输入向量映射为",{"5":{"158":2}}],["将输入向量旋转到奇异向量方向",{"5":{"45":2}}],["将输入向量扩展为齐次坐标",{"5":{"47":1}}],["将输入向量",{"5":{"47":1}}],["将输入空间按超平面划分为两个区域",{"5":{"45":1}}],["将输入空间按超平面",{"5":{"45":1}}],["将输入空间划分为两个区域",{"5":{"47":2}}],["将输入的尺度归一化",{"5":{"42":2}}],["将输入的高维信息压缩到低维流形上进行计算",{"5":{"87":2,"152":2}}],["将求和写为矩阵形式",{"5":{"44":2}}],["将按列分割",{"5":{"45":1}}],["将矩阵乘法展开为元素形式",{"5":{"46":2}}],["将权重矩阵和偏置合并为扩展权重矩阵",{"5":{"46":2,"47":2}}],["将到的等式依次代入",{"5":{"46":1}}],["将离散的符号表示转化为连续的数值表示",{"5":{"50":2}}],["将它们累加起来",{"5":{"50":2}}],["将形状较小的张量在某些维度上视为被重复",{"5":{"50":1}}],["将形状较小的张量在某些维度上",{"5":{"50":1}}],["将这一分析应用于雅可比矩阵乘积",{"5":{"148":2}}],["将这一分解应用于投影矩阵​",{"5":{"94":1}}],["将这一分解应用于投影矩阵",{"5":{"94":1}}],["将这个看似不可解的高维问题转化为一系列可计算的条件概率建模问题",{"5":{"140":2}}],["将这个分解代入mse并展开",{"5":{"51":2}}],["将这些概念应用于激活函数分析",{"5":{"71":2}}],["将这两个向量复制或交错排列",{"5":{"88":2}}],["将模型分布",{"5":{"61":2}}],["将交叉熵从信息论的概念转化为可计算的损失函数",{"5":{"61":2}}],["将在第三章进行详细的数学分析",{"5":{"47":2}}],["将在本节最后部分详细讨论",{"5":{"61":2}}],["将logits转化为满足概率公理的分布",{"5":{"61":1}}],["将完整的注意力计算分解为块级别的计算",{"5":{"86":2,"151":2}}],["将或投影到低维空间",{"5":{"87":1,"152":1}}],["将投影矩阵​",{"5":{"87":1,"152":1}}],["将投影矩阵",{"5":{"87":1,"152":1}}],["将序列的顺序信息注入到模型之中",{"5":{"88":2}}],["将位置信息嵌入到token嵌入中",{"5":{"41":2}}],["将位置视为时间信号",{"5":{"88":2}}],["将位置编码问题置于复数框架下后",{"5":{"88":2}}],["将位置编码视为可学习的参数",{"5":{"89":2}}],["将位置索引映射到编码向量",{"5":{"89":2}}],["将变为",{"5":{"88":1}}],["将帮助我们更好地改进和发展下一代位置编码技术",{"5":{"88":2}}],["将点积公式重写为",{"5":{"90":2}}],["将​按频率成分展开",{"5":{"90":1}}],["将复杂度从降低到",{"5":{"90":1}}],["将复杂度从",{"5":{"90":1}}],["将公式",{"5":{"91":2}}],["将编码矩阵参数化为低秩分解",{"5":{"89":1}}],["将编码矩阵",{"5":{"89":1}}],["将编码向量投影到前两个主成分",{"5":{"91":2}}],["将拼接后的多头输出映射回原始模型维度",{"5":{"93":2}}],["将拼接结果映射回",{"5":{"93":1}}],["将被编码为query向量",{"5":{"94":2}}],["将被编码为key向量",{"5":{"94":2}}],["将建立起对注意力机制数学本质的深刻理解",{"5":{"95":2}}],["将相似度矩阵转换为概率分布",{"5":{"95":2}}],["将对应维度的分量相乘后累加",{"5":{"95":2}}],["微调后的前向传播为",{"5":{"142":2}}],["微调时不需要调整整个矩阵",{"5":{"142":2}}],["微分关系导致了导数的简洁表示",{"5":{"42":2}}],["微积分与优化基础<",{"5":{"21":1,"131":1}}],["微积分与优化基础",{"0":{"48":1},"4":{"48":1},"5":{"21":4,"48":1,"131":4}}],["微积分是研究变化率和累积效应的数学分支",{"5":{"48":2}}],["微批处理",{"5":{"50":2}}],["二是基于推理生成答案的分布",{"5":{"138":2}}],["二是保留了信息的冗余性",{"5":{"92":2,"153":2}}],["二元语法的概率计算",{"5":{"140":2}}],["二元熵函数在",{"5":{"137":1}}],["二元熵函数在处取得最大值",{"5":{"137":1}}],["二元熵函数",{"5":{"137":2}}],["二元交叉熵损失为",{"5":{"61":2}}],["二元交叉熵损失",{"5":{"61":1}}],["二字的含义所在",{"5":{"46":2}}],["二阶矩",{"5":{"145":1}}],["二阶矩估计",{"5":{"48":2,"157":2}}],["二阶项给出",{"5":{"137":2}}],["二阶导数",{"5":{"145":2}}],["二阶导数的几何意义表示sigmoid曲线的曲率变化",{"5":{"42":2}}],["二阶导数为正",{"5":{"42":2}}],["二阶导数为负",{"5":{"42":2}}],["二阶张量是矩阵",{"5":{"50":2}}],["二阶张量",{"5":{"50":2}}],["二阶中心矩是方差",{"5":{"96":2}}],["二次收敛是最快的收敛类型之一",{"5":{"136":2}}],["二次收敛",{"5":{"136":2}}],["二次损失",{"5":{"70":1}}],["二次增长",{"5":{"70":2}}],["二次函数",{"5":{"51":2,"70":1}}],["二次型问题的最优动量系数",{"5":{"136":2}}],["二次型问题中学习率与振幅的关系",{"5":{"136":2}}],["二次型",{"5":{"96":2}}],["二分类任务",{"5":{"44":2}}],["二分类任务的交叉熵损失",{"2":{"61":1},"5":{"61":1}}],["二分类问题是分类任务中最简单但最具代表性的情形",{"5":{"61":2}}],["二项分布",{"5":{"96":2}}],["二项分布在语言模型中的应用包括描述一批样本中被mask的词的数量",{"5":{"96":2}}],["二项分布的期望为",{"5":{"96":2}}],["二项分布的共轭先验是beta分布",{"5":{"96":2}}],["加入噪声后的期望损失为",{"5":{"159":2}}],["加速了优化收敛",{"5":{"144":2}}],["加速了训练收敛",{"5":{"42":2}}],["加速收敛同时有效抑制震荡",{"5":{"136":2}}],["加速效果好",{"5":{"134":2}}],["加法",{"5":{"45":2,"48":2,"50":2}}],["加法节点",{"5":{"45":2}}],["加法操作破坏了内容向量的原有结构",{"5":{"88":2}}],["加权后累加",{"5":{"158":2}}],["加权组合输出可以表示为更紧凑的矩阵运算形式",{"5":{"158":2}}],["加权组合输出的梯度计算是反向传播的关键",{"5":{"158":2}}],["加权组合输出的计算方式",{"5":{"158":2}}],["加权组合输出的数学表达",{"2":{"158":1},"5":{"158":1}}],["加权组合退化为仅对被激活的专家求和",{"5":{"158":2}}],["加权组合的数学形式简洁优雅",{"5":{"158":2}}],["加权组合各评估指标",{"5":{"132":2}}],["加权信息瓶颈",{"5":{"133":2}}],["加权",{"5":{"69":1}}],["加权平均中的权重正是softmax概率",{"5":{"69":2}}],["加权求和法",{"5":{"70":2}}],["加权求和结果",{"5":{"42":2}}],["加权求和信息整合与阈值触发机制",{"5":{"47":2}}],["加权和为",{"5":{"47":6}}],["加权和至少为1",{"5":{"47":2}}],["加权均方误差的推广",{"2":{"51":1},"5":{"51":1}}],["加权均方误差",{"5":{"51":2}}],["加权mse的矩阵形式可以通过对角权重矩阵来表示",{"5":{"51":2}}],["加权mse在实际应用中有广泛的用途",{"5":{"51":2}}],["加权形式",{"5":{"51":2}}],["加权聚合",{"5":{"95":2,"132":2}}],["加载矩阵",{"5":{"86":1,"151":1}}],["加载和存储中间状态",{"5":{"86":2,"151":2}}],["加载",{"5":{"86":1,"151":1}}],["加性位置编码无法产生真正具有相对位置感知的注意力模式",{"5":{"88":2}}],["加性注入",{"5":{"88":1}}],["加上正样本共",{"5":{"69":1}}],["加上正样本共个样本",{"5":{"69":1}}],["加上残差连接和层归一化后",{"5":{"92":2,"153":2}}],["vertical",{"2":{"154":1,"155":1,"156":2}}],["vectorization",{"5":{"50":2}}],["vdots",{"5":{"143":6}}],["vocabulary",{"5":{"140":2}}],["vc",{"5":{"133":2}}],["vc维控制",{"5":{"61":2}}],["vc维",{"5":{"87":2,"152":2}}],["vib",{"5":{"133":12}}],["view",{"5":{"132":10}}],["vs",{"5":{"42":2,"146":2}}],["v",{"2":{"155":1},"5":{"42":12,"86":15,"97":10,"132":18,"135":18,"141":18,"144":4,"151":15,"155":1}}],["v^",{"5":{"69":4}}],["v^t",{"5":{"42":4}}],["v^2",{"5":{"42":2}}],["vae",{"5":{"133":6,"137":2}}],["varadhan",{"5":{"137":6}}],["var",{"5":{"42":2,"46":13,"132":2}}],["variance",{"5":{"51":8,"96":2,"142":2,"145":2,"146":2,"155":2}}],["variational",{"5":{"96":2,"145":4}}],["values",{"5":{"137":2}}],["value投影",{"5":{"69":2}}],["value投影会同时考虑内容信息和位置信息",{"5":{"91":2}}],["value投影矩阵",{"5":{"93":4}}],["value投影矩阵的梯度对于分析模型的训练动态至关重要",{"5":{"94":2}}],["value投影定义了从输入空间到某个​维子空间的映射",{"5":{"93":1}}],["value投影定义了从输入空间到某个",{"5":{"93":1}}],["value投影都是独立的可学习参数",{"5":{"94":2}}],["value",{"5":{"45":2,"50":2,"94":2,"95":4,"97":2,"132":8,"136":2,"137":6,"143":2,"147":2,"156":4}}],["value的矩阵表示与变换",{"0":{"94":1},"4":{"94":1},"5":{"94":1,"131":4}}],["value的矩阵表示与变换<",{"5":{"131":1}}],["value的矩阵变换",{"5":{"86":2,"93":2,"151":2}}],["value的矩阵变换是掌握注意力机制的关键一步",{"5":{"94":2}}],["value的矩阵乘法可以在半精度下进行",{"5":{"93":2}}],["value的矩阵推导",{"5":{"95":2}}],["value的计算分别为",{"5":{"91":2}}],["value的投影计算是线性的",{"5":{"71":2}}],["value的投影阶段",{"5":{"93":2}}],["value的投影结果",{"5":{"93":2}}],["value的投影矩阵",{"5":{"94":2}}],["value的维度设计是一个重要的设计决策",{"5":{"94":2}}],["value矩阵划分为小块",{"5":{"86":2,"151":2}}],["value块",{"5":{"86":2,"151":2}}],["value进行注意力计算",{"5":{"93":2}}],["value张量切片独立地进行scaled",{"5":{"93":1}}],["value张量切片",{"5":{"93":1}}],["value是如何从输入嵌入表示中生成的",{"5":{"94":2}}],["value都将从这个基础表示中通过线性变换生成",{"5":{"94":2}}],["value通过三个独立的线性变换从输入嵌入中生成",{"5":{"94":2}}],["value通过三个独立的投影矩阵被映射到不同的空间",{"5":{"94":2}}],["value向量的期望",{"5":{"69":2}}],["value向量承载的是信息的内容本身",{"5":{"94":2}}],["value不需要参与匹配计算",{"5":{"94":2}}],["value空间可以与query",{"5":{"94":2}}],["vaswani等人提出的正弦位置编码",{"5":{"88":2}}],["v0",{"5":{"47":8,"48":4}}],["vllm",{"5":{"93":2}}],["v变换的数学本质",{"5":{"94":2}}],["x^t",{"5":{"135":2}}],["x^2",{"5":{"46":2}}],["xz",{"5":{"133":8}}],["xm",{"5":{"69":1}}],["xavier",{"5":{"135":6,"148":3}}],["xavier初始化可能导致梯度消失",{"5":{"148":2}}],["xavier初始化选择了",{"5":{"148":2}}],["xavier初始化",{"2":{"148":1},"5":{"41":2,"148":3}}],["xavier初始化将权重初始化为",{"5":{"41":2}}],["xavier初始化将权重方差设置为​",{"5":{"41":1}}],["xavier初始化将权重方差设置为",{"5":{"41":1}}],["xavier初始化取两者的几何平均​",{"5":{"41":1}}],["xavier初始化取两者的几何平均",{"5":{"41":1}}],["xavier初始化的方差推导",{"5":{"41":2}}],["xavier初始化的方差为",{"5":{"94":2}}],["xavier初始化和he初始化正是为了避免这个问题而设计的",{"5":{"41":2}}],["xavier初始化和he初始化都假设权重服从某种高斯分布",{"5":{"96":2}}],["xavier方差",{"5":{"41":1}}],["x",{"5":{"42":72,"46":14,"69":7,"132":12,"133":26,"135":10,"137":36,"142":6,"143":20,"144":4}}],["xi",{"5":{"46":4}}],["xor",{"5":{"47":2}}],["xorscene",{"5":{"47":4}}],["xw",{"5":{"86":6,"151":6}}],["xl的相对位置编码",{"5":{"88":2}}],["yorke",{"5":{"136":2}}],["young定理所保证的最佳低秩近似性质",{"5":{"143":2}}],["young",{"5":{"50":2,"143":2}}],["you",{"5":{"91":2,"146":2}}],["y",{"5":{"42":24,"46":4,"61":20,"132":12,"133":12,"135":6,"137":14,"142":2,"143":2,"144":22}}],["yk",{"5":{"42":1}}],["yield",{"5":{"132":2,"133":2}}],["yi",{"5":{"42":1}}],["yarn优化",{"5":{"88":2}}],["yarn",{"5":{"88":2}}],["yet",{"5":{"88":2}}],["|y|",{"5":{"144":8}}],["|0|",{"5":{"144":2}}],["|ℒ",{"5":{"136":2}}],["|λ|",{"5":{"135":2}}],["||g||₂",{"5":{"136":2}}],["||g||",{"5":{"136":4}}],["||y||",{"5":{"135":2}}],["||x",{"5":{"135":2}}],["||w||",{"5":{"135":2}}],["||v",{"5":{"135":2}}],["|",{"5":{"42":4,"46":3,"133":2,"142":3,"143":2,"144":16}}],["|w|",{"5":{"42":2}}],["|f",{"5":{"46":1,"142":1}}],["处最小",{"5":{"137":1}}],["处对",{"5":{"137":1}}],["处于激活区域",{"5":{"41":2}}],["处于深度饱和状态",{"5":{"41":2}}],["处于充分激活状态",{"5":{"41":2}}],["处",{"5":{"42":3,"134":2,"135":2,"139":2,"149":1}}],["处取得最小值",{"5":{"137":1}}],["处取得最大值",{"5":{"137":1,"145":2}}],["处取得",{"5":{"42":2}}],["处有全局最小值",{"5":{"134":2}}],["处有拐点",{"5":{"42":1}}],["处有",{"5":{"42":1}}],["处的有效扩散张量为",{"5":{"149":1}}],["处的随机梯度",{"5":{"149":1}}],["处的二阶近似为",{"5":{"137":1}}],["处的导数为",{"5":{"136":1}}],["处的",{"5":{"134":2,"136":2}}],["处的线性化",{"5":{"134":2}}],["处的偏导数定义为",{"5":{"48":1}}],["处的梯度定义为",{"5":{"48":1}}],["处的相位",{"5":{"90":1}}],["处理创意写作的输入可能主要需要模型的生成表达能力",{"5":{"159":2}}],["处理数学问题的输入可能主要需要模型的数学推理能力",{"5":{"159":2}}],["处理轴和统计量计算三个角度进行系统对比",{"5":{"146":2}}],["处理编码向量",{"5":{"71":2}}],["处理",{"5":{"88":2}}],["处词元的嵌入向量",{"5":{"94":1}}],["处求值",{"5":{"96":1}}],["深网络可以表示更复杂的函数类",{"5":{"133":2}}],["深度累积效应与梯度流",{"2":{"148":1},"5":{"148":1}}],["深度神经网络很难自然地学习到恒等映射",{"5":{"150":2}}],["深度神经网络中的权重矩阵同样表现出类似的低秩特性",{"5":{"143":2}}],["深度神经网络在经过训练后",{"5":{"142":2}}],["深度神经网络的训练稳定性与损失曲面的曲率分布密切相关",{"5":{"144":2}}],["深度神经网络的训练涉及大量的梯度计算和参数更新",{"5":{"44":2}}],["深度神经网络的损失函数通常是非凸的",{"5":{"48":2,"136":2}}],["深度表示学习的目标是学习特征",{"5":{"137":1}}],["深度表示学习的目标是学习特征使得和尽可能大",{"5":{"137":1}}],["深度信息瓶颈",{"5":{"137":2}}],["深度与宽度的数学权衡",{"2":{"71":1},"5":{"71":1}}],["深度优先",{"5":{"71":2}}],["深度网络中的",{"5":{"149":1}}],["深度网络中的往往具有很大的条件数",{"5":{"149":1}}],["深度网络中损失曲面的平坦方向与权重矩阵的低奇异值方向高度相关",{"5":{"142":2}}],["深度网络对雅可比矩阵范数的敏感性是指数级的",{"5":{"148":2}}],["深度网络权重的低秩特性",{"2":{"142":1},"5":{"142":1}}],["深度网络可以看作数据的压缩器",{"5":{"137":2}}],["深度网络",{"5":{"135":2}}],["深度网络训练通常经历以下阶段",{"5":{"134":2}}],["深度网络训练时",{"5":{"133":2}}],["深度网络能表示更复杂的函数类",{"5":{"133":2}}],["深度网络能够表示的函数空间远大于同等参数量的浅层网络",{"5":{"71":2}}],["深度网络经历从",{"5":{"133":2}}],["深度网络通过逐层的非线性变换",{"5":{"71":2}}],["深度网络用指数级更少的参数可以达到与浅层网络相同的表达能力",{"5":{"71":2}}],["深度网络的梯度噪声并非均匀分布在所有参数方向上",{"5":{"149":2}}],["深度网络的",{"5":{"137":2}}],["深度网络的每一层都会损失部分信息",{"5":{"133":2}}],["深度网络的信息论相变",{"5":{"133":2}}],["深度网络的训练稳定性",{"5":{"132":2}}],["深度网络的特征空间层次",{"5":{"45":2}}],["深度网络的表示空间具有层次化的结构",{"5":{"45":2}}],["深度网络往往收敛到具有相似损失值的局部最小值",{"5":{"48":2}}],["深度指数优势",{"5":{"71":2}}],["深度为的网络可以等效于宽度为指数级别的浅层网络",{"5":{"71":1}}],["深度为",{"5":{"71":1}}],["深度",{"5":{"46":2}}],["深度学习泛化能力的一个长期谜题是",{"5":{"149":2}}],["深度学习中的一个深刻现象是",{"5":{"147":2}}],["深度学习中的张量分解应用",{"2":{"143":1},"5":{"143":1}}],["深度学习中的mse损失通常仍然可以优化到较好的解",{"5":{"51":2}}],["深度学习系统的某些关键属性表现出可预测的幂律缩放行为",{"5":{"138":2}}],["深度学习的本质是一个有损压缩过程",{"5":{"133":2}}],["深度学习应用",{"5":{"44":1,"45":1}}],["深度学习框架只需要实现softmax的梯度",{"5":{"69":2}}],["深度学习框架",{"5":{"47":2}}],["深度学习框架会在不显式复制数据的情况下",{"5":{"50":2}}],["深度学习框架也提供了高效的函数来计算矩阵的特征值分解和奇异值分解",{"5":{"50":2}}],["深入理解稀疏激活的效率优势",{"5":{"159":2}}],["深入理解缩放定律的数学本质",{"5":{"159":2}}],["深入理解优势函数对于掌握现代强化学习算法至关重要",{"5":{"155":2}}],["深入理解这种差异",{"5":{"88":2}}],["深入分析了",{"5":{"132":2}}],["深入分析激活函数的数学角色",{"5":{"71":2}}],["深入分析单层神经元的数学结构",{"5":{"47":2}}],["深入分析其参数结构和计算复杂度",{"5":{"93":2}}],["深入剖析前向传播的内在机制",{"5":{"45":2}}],["深入剖析缩放因子的统计学原理",{"5":{"95":2}}],["深层出现梯度爆炸而浅层出现梯度消失",{"5":{"148":2}}],["深层网络中dropout的累积效应",{"5":{"145":2}}],["深层网络因此陷入",{"5":{"41":2}}],["深层后进入压缩阶段",{"5":{"133":2}}],["深层注意力矩阵通常比浅层注意力矩阵有更快的奇异值衰减",{"5":{"87":2,"152":2}}],["深层",{"5":{"87":2,"92":4,"152":2,"153":4}}],["网络能够同时利用全局和局部信息",{"5":{"150":2}}],["网络具有多个神经元",{"5":{"148":2}}],["网络具有足够的容量记忆训练数据",{"5":{"133":2}}],["网络可能选择让",{"5":{"150":4}}],["网络可能同时存在梯度消失和梯度爆炸的区域",{"5":{"148":2}}],["网络可以学习恢复任意的均值和方差",{"5":{"146":2}}],["网络无法有效学习",{"5":{"145":2}}],["网络权重编码了数据的结构信息",{"5":{"137":2}}],["网络深度与容量的关系",{"5":{"133":2}}],["网络开始压缩表示",{"5":{"133":2}}],["网络优先学习预测输出",{"5":{"133":2}}],["网络架构设计之间的深层联系",{"5":{"41":2}}],["网络包含一个隐藏层",{"5":{"71":2}}],["网络的lipschitz常数上界",{"5":{"144":2}}],["网络的表达能力始终局限于输入的线性变换",{"5":{"71":2}}],["网络的原始输出",{"5":{"71":2}}],["网络的学习目标",{"5":{"45":2}}],["网络现在是分段线性或非线性的",{"5":{"71":1}}],["网络现在是",{"5":{"71":1}}],["网络参数​",{"5":{"44":1}}],["网络参数",{"5":{"44":1,"137":2}}],["网络通过前向传播计算输出",{"5":{"45":2}}],["网络输出是输入的线性函数",{"5":{"45":1}}],["网络输出",{"5":{"45":1}}],["网络处理输入的不同部分",{"5":{"93":2}}],["逐元素非线性变换",{"5":{"71":2}}],["逐元素应用于向量",{"5":{"71":1}}],["逐元素乘法约为​",{"5":{"44":1}}],["逐元素乘法约为",{"5":{"44":1}}],["逐元素乘法",{"5":{"44":1}}],["逐元素运算对张量中的每个独立元素应用相同的函数",{"5":{"50":2}}],["逐步揭示非线性变换的必要性和数学价值",{"5":{"71":2}}],["逐步推导出各层参数的梯度计算公式",{"5":{"44":2}}],["逐步推导出完整的梯度计算公式",{"5":{"44":2}}],["逐步推广到现代深度学习中常用的连续激活神经元",{"5":{"47":2}}],["逐步扩展",{"5":{"92":2,"153":2}}],["逐层计算误差信号和参数梯度",{"5":{"44":2}}],["逐层传播到输入层",{"5":{"44":1}}],["逐块计算注意力输出",{"5":{"86":2,"151":2}}],["逐块加载",{"5":{"86":2,"151":2}}],["逐位置计算角度的正弦和余弦",{"5":{"88":1}}],["逐渐增大并经过临界值",{"5":{"136":1}}],["逐渐增加直到找到稳定的最大允许值",{"5":{"41":2}}],["逐渐收敛到不同的局部最优解",{"5":{"93":2}}],["上一节介绍的贝尔曼最优方程主要适用于状态空间和动作空间较小",{"5":{"155":2}}],["上聚合信息",{"5":{"146":2}}],["上求和",{"5":{"146":2}}],["上单调递增",{"5":{"139":1}}],["上升",{"5":{"136":4}}],["上添加互信息正则化项",{"5":{"133":1}}],["上界估计",{"5":{"133":2}}],["上界",{"5":{"132":2}}],["上下文相关的信息",{"5":{"150":2}}],["上下文长度",{"5":{"140":1}}],["上下文长度的增长",{"5":{"140":2}}],["上下文长度固定",{"5":{"140":2}}],["上下文",{"5":{"101":2}}],["上定义一个注意力分布",{"5":{"69":2}}],["上",{"5":{"69":1,"70":1,"71":1,"88":2,"96":1,"149":2}}],["上表现为线性函数",{"5":{"45":2}}],["上述关系在最小值点附近最为准确",{"5":{"149":2}}],["上述定理的直观含义是",{"5":{"145":2}}],["上述梯度公式揭示了infonce优化过程中的核心机制",{"5":{"69":2}}],["上述表达式可以简化为",{"5":{"71":2}}],["上述矩阵运算等价于对每个样本单独进行神经元计算后的结果堆叠",{"5":{"47":2}}],["上取得最大值",{"5":{"48":2}}],["上取得最小值",{"5":{"48":2}}],["上为零",{"5":{"48":2}}],["上的梯度下降等价于直接优化",{"5":{"142":1}}],["上的性能可以用一个统计估计量来衡量",{"5":{"138":2}}],["上的一个严格凹函数",{"5":{"61":2}}],["上的效率不如密集矩阵运算",{"5":{"86":2,"151":2}}],["上的平移操作",{"5":{"90":2}}],["上的任务",{"5":{"93":2}}],["上是可接受的",{"5":{"157":2}}],["上是相近的",{"5":{"70":2}}],["上是单射的",{"5":{"91":1}}],["平方损失会急剧增大",{"5":{"158":2}}],["平方损失为",{"5":{"158":2}}],["平方操作确保了误差的正定性",{"5":{"51":2}}],["平方操作使得这个异常样本的误差平方为100",{"5":{"51":2}}],["平缓",{"5":{"147":2}}],["平衡",{"5":{"147":2}}],["平衡点处的",{"5":{"136":1}}],["平衡点渐近稳定",{"5":{"134":2}}],["平衡点不稳定",{"5":{"134":2}}],["平衡点局部渐近稳定",{"5":{"134":2}}],["平衡点",{"5":{"134":2,"136":1}}],["平衡信息选择的多样性和重要性",{"5":{"132":2}}],["平均路由概率",{"5":{"158":2}}],["平均水平",{"5":{"155":2}}],["平均而言",{"5":{"144":2}}],["平均不确定性",{"5":{"137":2}}],["平均信息量",{"5":{"137":2}}],["平均",{"5":{"41":2,"157":2}}],["平均绝对误差",{"5":{"51":2,"70":2}}],["平均所需的信息量",{"5":{"61":2}}],["平行线保持平行",{"5":{"71":2}}],["平坦极小值的隐式偏好",{"2":{"149":1},"5":{"149":1}}],["平坦最小值具有以下优点",{"5":{"136":2}}],["平坦最小值通常对应更好的泛化性能",{"5":{"135":2}}],["平坦最小值",{"5":{"135":2}}],["平坦最小值的泛化优势",{"5":{"135":2}}],["平坦最小值的",{"2":{"135":1},"5":{"135":1}}],["平坦最小值的动力学特征",{"5":{"134":2}}],["平坦区域的",{"5":{"134":2}}],["平坦",{"5":{"48":2,"69":2,"71":2,"87":2,"89":2,"95":2,"96":2,"142":2,"144":2,"147":2,"149":2,"152":2}}],["平移变换则将整个超平面平移",{"5":{"47":1}}],["平移变换",{"5":{"47":1}}],["平移不变性",{"5":{"47":2,"90":2}}],["平移不变",{"5":{"87":2,"152":2}}],["平移等变性",{"5":{"88":3}}],["平移等变性的群论定义如下",{"5":{"88":2}}],["平滑化",{"5":{"147":2}}],["平滑窗口大小",{"5":{"136":2}}],["平滑过渡到",{"5":{"132":2}}],["平滑的",{"5":{"88":2}}],["平滑",{"5":{"89":2,"91":2,"150":2}}],["都位于",{"5":{"141":1}}],["都有",{"5":{"136":2,"148":1}}],["都有指向所有其他节点的边",{"5":{"92":2,"153":2}}],["都很小",{"5":{"133":1}}],["都用于防止优化过程中的过大更新",{"5":{"101":2}}],["都是未知的",{"5":{"155":2}}],["都是概率分布",{"5":{"154":2}}],["都是开放的研究问题",{"5":{"149":2}}],["都是独立同分布的高斯随机向量",{"5":{"141":1}}],["都是将一个向量",{"5":{"70":2}}],["都是研究特定数学结构的几何性质",{"5":{"70":2}}],["都是在softmax框架下比较查询与候选集的匹配程度",{"5":{"69":2}}],["都是简单的查表",{"5":{"89":2}}],["都是振幅为1的正弦波",{"5":{"90":1}}],["都是",{"5":{"93":2}}],["都是鞍点或局部极小值",{"5":{"97":2}}],["都与",{"5":{"70":2}}],["都使用相同形式的梯度计算",{"5":{"69":2}}],["都无法达到良好的分类效果",{"5":{"71":2}}],["都只能将平面划分为两个半平面",{"5":{"47":1}}],["都可以作为激活函数",{"5":{"71":2}}],["都可以理解为学习一个能够逼近真实条件期望函数的映射",{"5":{"51":2}}],["都可以分解为",{"5":{"94":1,"143":1}}],["都可能很大",{"5":{"61":2}}],["都利用了注意力矩阵的低秩或稀疏结构",{"5":{"87":2,"152":2}}],["都接近1",{"5":{"91":2}}],["都存在一定比例的注意力权重分配给距离为的位置对",{"5":{"92":1,"153":1}}],["都存在一定比例的注意力权重分配给距离为",{"5":{"92":1,"153":1}}],["都对应",{"5":{"70":1}}],["都对应或",{"5":{"70":1}}],["都对这种张量运算进行了高度优化",{"5":{"93":2}}],["都对多头注意力的并行实现进行了高度优化",{"5":{"93":2}}],["都需要",{"5":{"95":1}}],["都离不开这些基本概念",{"5":{"96":2}}],["决策树等二元分类问题中有重要应用",{"5":{"137":2}}],["决策树的深度",{"5":{"51":2}}],["决策边界",{"5":{"47":4,"71":2}}],["决定每个专家对当前输入的贡献程度",{"5":{"159":2}}],["决定为每个输入选择哪个函数",{"5":{"159":2}}],["决定哪些信息通过",{"5":{"133":2}}],["决定哪些头可以被剪枝",{"5":{"132":2}}],["决定信息流向哪",{"5":{"132":1}}],["决定了标度律在预测方面的可靠性",{"5":{"138":1}}],["决定了知识传输的上限",{"5":{"133":2}}],["决定了哪些位置对可以被注意到",{"5":{"132":1}}],["决定了每个输入位置的信息贡献比例",{"5":{"132":1}}],["决定了各任务在优化过程中的相对重要性",{"5":{"101":2}}],["决定了系统的稳定性",{"5":{"41":1}}],["决定了梯度在对应方向上的缩放因子",{"5":{"41":1}}],["决定了不同特征之间的线性组合方式",{"5":{"47":1}}],["决定了它所代表的线性变换的类型",{"5":{"50":2}}],["决定了信息的计量单位",{"5":{"61":1}}],["决定了什么样的输入模式会产生较高的注意力分数",{"5":{"94":1}}],["决定",{"5":{"61":2,"88":3,"97":2,"135":2,"136":1,"150":2}}],["方法来估计熵",{"5":{"137":2}}],["方法",{"5":{"137":2,"140":1}}],["方法通过在不动点附近施加小扰动来控制混沌系统",{"5":{"136":2}}],["方法通过对比学习最大化的下界",{"5":{"137":1}}],["方法通过对比学习最大化",{"5":{"133":1,"137":1}}],["方法通过对比学习最大化同时最小化",{"5":{"133":1}}],["方法在",{"5":{"135":2}}],["方法的收敛性",{"5":{"135":2}}],["方法估计极端特征值",{"5":{"135":2}}],["方法利用",{"5":{"135":2}}],["方法3",{"5":{"132":2}}],["方法2",{"5":{"132":2}}],["方法1",{"5":{"132":2}}],["方向上的信息",{"5":{"146":1}}],["方向上的投影",{"5":{"146":1}}],["方向振荡",{"5":{"144":2}}],["方向归一化",{"5":{"142":1}}],["方向和幅度来构造任意复杂的曲面",{"5":{"71":2}}],["方向导数",{"5":{"48":2}}],["方向导数在梯度方向",{"5":{"48":2}}],["方向",{"5":{"48":4,"134":2}}],["方差很大",{"5":{"159":2}}],["方差越小",{"5":{"158":2}}],["方差显著降低",{"5":{"155":2}}],["方差设置",{"5":{"148":1}}],["方差维度",{"5":{"146":2}}],["方差表示第个样本的特征向量各分量相对于均值的离散程度",{"5":{"146":1}}],["方差控制",{"5":{"145":1}}],["方差最大化条件",{"5":{"145":2}}],["方差与保留概率的关系",{"2":{"145":1},"5":{"145":1}}],["方差与协方差",{"2":{"96":1},"5":{"96":1}}],["方差减小",{"5":{"138":2}}],["方差代表梯度噪声",{"5":{"134":2}}],["方差",{"5":{"51":4,"96":3,"146":1,"147":2,"158":2}}],["方差分解",{"5":{"51":6,"139":2}}],["方差分解中的三个项各有明确的数学意义",{"5":{"51":2}}],["方差分解对于实际模型选择和调优至关重要",{"5":{"51":2}}],["方差分解是理解机器学习泛化误差的核心理论工具",{"5":{"51":2}}],["方差反映了模型对训练数据中随机变化的响应程度",{"5":{"51":2}}],["方差始终非负",{"5":{"51":2}}],["方差权衡",{"2":{"139":1},"5":{"51":2,"133":2,"139":1,"144":2,"155":2}}],["方差权衡可以通过学习曲线来可视化",{"5":{"51":2}}],["方差权衡的可视化",{"2":{"51":1},"5":{"51":1}}],["方差权衡的数学表达可以通过模拟实验来验证",{"5":{"51":2}}],["方差权衡的数学本质和实践意义",{"5":{"51":2}}],["方差主导了mse",{"5":{"51":2}}],["方差为零",{"5":{"159":2}}],["方差为一附近",{"5":{"148":2}}],["方差为1",{"5":{"41":2}}],["方差为1的标准正态分布附近",{"5":{"41":2}}],["方差为1的特殊高斯分布",{"5":{"96":2}}],["方差为",{"5":{"95":2,"96":9,"135":2,"148":4}}],["方差为​",{"5":{"96":2}}],["方差为的高斯分布",{"5":{"96":1}}],["方差和不可减少误差三个部分",{"5":{"51":2}}],["方差和协方差是概率论中最重要的一阶和二阶统计量",{"5":{"96":2}}],["方差可以展开为",{"5":{"96":2}}],["方差的取值范围为",{"5":{"158":2}}],["方差的平方根称为标准差",{"5":{"96":2}}],["方差的性质包括",{"5":{"96":2}}],["方差用于分析梯度的波动",{"5":{"96":2}}],["方差达到克拉美",{"5":{"96":2}}],["方差有限",{"5":{"96":1}}],["方差接近1",{"5":{"96":2}}],["任意",{"5":{"148":1}}],["任意维度可运算",{"5":{"143":1}}],["任意实矩阵",{"5":{"143":1}}],["任意实矩阵都可以分解为",{"5":{"143":1}}],["任意非常数有界连续函数",{"5":{"71":2}}],["任意矩阵可以分解为",{"5":{"45":1}}],["任意矩阵",{"5":{"45":1,"94":1}}],["任意矩阵都可以分解为",{"5":{"94":1}}],["任意位置都可以被唯一标识",{"5":{"89":2}}],["任意位置之间可以存在依赖关系",{"5":{"92":2,"153":2}}],["任意两个选项的相对选择概率仅取决于它们各自的",{"5":{"69":2}}],["任意两个位置之间可以直接建立联系",{"5":{"92":2,"153":2}}],["任意连续函数都可以用这些基函数的线性组合来逼近",{"5":{"93":2}}],["任何微小的参数变化都可能使",{"5":{"148":1}}],["任何微小的参数变化都可能使偏离1",{"5":{"148":1}}],["任何微小的扰动都会使系统偏离临界点",{"5":{"148":2}}],["任何超平面都只能将平面划分为两个半平面",{"5":{"47":1}}],["任何超平面",{"5":{"47":1}}],["任何回归模型的最终目标",{"5":{"51":2}}],["任何其他投影点都会产生更大的误差",{"5":{"51":2}}],["任何局部最优解都是全局最优解",{"5":{"48":2}}],["任何局部最小值都是全局最小值",{"5":{"51":2,"97":2}}],["任何高斯随机变量都可以通过标准化变换转换为标准高斯随机变量",{"5":{"96":1}}],["任何高斯随机变量都可以通过标准化变换",{"5":{"96":1}}],["任务有",{"5":{"156":2}}],["任务内在复杂度",{"5":{"138":2}}],["任务性能才会",{"5":{"138":2}}],["任务性能才会显著提升",{"5":{"138":2}}],["任务的梯度与共享参数的相关性可用互信息度量",{"5":{"133":1}}],["任务的损失函数为",{"5":{"101":1}}],["任务梯度的信息论解释",{"5":{"133":2}}],["任务间存在冲突",{"5":{"133":2}}],["任务共享表示的信息论分析",{"5":{"133":2}}],["任务损失加稀疏性惩罚",{"5":{"132":2}}],["任务权重",{"5":{"101":1}}],["任务权重的选择直接影响优化轨迹和最终性能",{"5":{"101":1}}],["任务权重的影响",{"5":{"101":2}}],["任务",{"5":{"101":1,"133":1}}],["任务适配性与损失函数选择",{"2":{"70":1},"5":{"70":1}}],["任务契合",{"5":{"41":2}}],["任务类型等因素有关",{"5":{"87":2,"152":2}}],["任务相关的相似度函数",{"5":{"94":2}}],["任务中",{"5":{"95":2}}],["任务完成率等指标",{"5":{"96":2}}],["语音",{"5":{"71":2}}],["语言建模方法的对比",{"5":{"140":1}}],["语言建模方法的对比注记",{"5":{"140":1}}],["语言建模目标",{"5":{"140":2}}],["语言建模的目标是找到参数",{"5":{"140":1}}],["语言建模的目标是找到参数使得",{"5":{"140":1}}],["语言建模的第一步是建立严格的数学框架来描述自然语言的结构",{"5":{"140":2}}],["语言建模的核心是对自然语言序列的概率分布进行建模",{"5":{"140":2}}],["语言建模的数学定义",{"2":{"140":1},"5":{"140":1}}],["语言建模的负对数似然损失为",{"5":{"101":2}}],["语言建模损失本质上是交叉熵损失在序列预测任务中的应用",{"5":{"101":2}}],["语言建模损失",{"2":{"101":1},"5":{"101":3}}],["语言建模损失使用下一个token作为正样本",{"5":{"101":2}}],["语言",{"5":{"71":2,"94":2}}],["语言模型可能会发现一些",{"5":{"154":2}}],["语言模型在不同参数量",{"5":{"138":2}}],["语言模型",{"5":{"86":2,"151":2,"154":2}}],["语言模型的核心数学目标是学习一个概率分布",{"5":{"140":2}}],["语言模型的测试损失与模型规模",{"5":{"138":2}}],["语言模型的输出层正是类别分布的一个典型应用",{"5":{"96":2}}],["语言模型的训练目标就是学习这个条件概率分布",{"5":{"96":2}}],["语言模型的训练目标是最大化训练语料库的对数似然",{"5":{"96":2}}],["语言模型预测下一个词的概率分布为",{"5":{"96":2}}],["语言中的某些结构",{"5":{"88":2}}],["语言中存在许多非线性位置依赖",{"5":{"88":2}}],["语言具有强烈的顺序性",{"5":{"92":2,"153":2}}],["语句含义",{"5":{"45":2}}],["语义相关",{"5":{"69":1}}],["语义相近的词在空间中距离较近",{"5":{"50":2}}],["语义关系",{"5":{"71":2}}],["语义不同的词则距离较远",{"5":{"50":2}}],["语义依赖",{"5":{"92":2,"153":2}}],["语义头往往表现出全局注意力模式",{"5":{"92":2,"153":2}}],["语义角色关系",{"5":{"93":2}}],["语义空间",{"5":{"94":2}}],["语法依赖",{"5":{"88":2,"92":2,"153":2}}],["语法关系",{"5":{"93":2}}],["形状不变",{"5":{"143":2}}],["形状相同的三阶张量",{"5":{"48":1}}],["形式",{"5":{"133":2,"137":2}}],["形式在数学上非常优美",{"5":{"42":2}}],["形成所谓的",{"5":{"159":2}}],["形成恶性循环",{"5":{"147":2,"159":2}}],["形成中性稳定的周期轨道",{"5":{"136":2}}],["形成费根鲍姆",{"5":{"136":2}}],["形成完整的信息处理流水线",{"5":{"132":2}}],["形成更复杂的token交互模式",{"5":{"71":2}}],["形成多层网络结构",{"5":{"46":2}}],["形成",{"5":{"51":1,"88":1}}],["形成维的",{"5":{"88":1}}],["形成转折关系",{"5":{"92":2,"153":2}}],["形成对比",{"5":{"92":2,"153":2}}],["形成一个完全连通图",{"5":{"92":2,"153":2}}],["形成一个维度为的矩阵",{"5":{"93":1}}],["形成一个维度为",{"5":{"93":1}}],["形成了鲜明对比",{"5":{"149":2}}],["形成了",{"5":{"138":2}}],["形成了明显的分工模式",{"5":{"93":2}}],["形成了紧密的呼应",{"5":{"101":2}}],["形容词",{"5":{"92":2,"153":2}}],["用这批样本的平均梯度来近似全量梯度",{"5":{"149":2}}],["用基向量线性表示",{"5":{"143":1}}],["用更紧凑的参数化形式来表示原始数据或模型",{"5":{"143":2}}],["用模型分布",{"5":{"137":1}}],["用模型分布编码真实分布的平均代价",{"5":{"137":1}}],["用途",{"5":{"69":1}}],["用logits表示",{"5":{"71":2}}],["用于平衡即时奖励和未来奖励的重要性",{"5":{"156":2}}],["用于计算优势函数",{"5":{"154":2}}],["用于约束近似后验与先验分布之间的距离",{"5":{"145":2}}],["用于对异常值鲁棒的场景",{"5":{"70":2}}],["用于模拟独立随机变量序列的最大值分布",{"5":{"71":2}}],["用于将模型的logits输出转换为概率分布",{"5":{"47":2}}],["用于处理序列数据",{"5":{"46":2}}],["用于处理超大序列或有限gpu内存的情况",{"5":{"50":2}}],["用于描述卷积操作的线性变换",{"5":{"87":2,"152":2}}],["用于建模连续的随机过程",{"5":{"96":2}}],["用于根据样本数据判断关于总体的假设是否成立",{"5":{"96":2}}],["用于比较两个版本",{"5":{"96":2}}],["用秩不超过k的矩阵对的最佳近似",{"5":{"50":1}}],["用秩不超过k的矩阵对",{"5":{"50":1}}],["用不同复杂度的模型进行训练",{"5":{"51":2}}],["用矩阵形式表示为",{"5":{"88":2}}],["用矩阵语言表达就是",{"5":{"88":2}}],["用户在使用时应注意",{"5":{"61":2}}],["用户可能需要模型处理比训练时更长的上下文",{"5":{"88":2}}],["用群作用把",{"5":{"88":2}}],["用​替代",{"5":{"89":1}}],["用",{"5":{"89":1}}],["完美契合了自回归的数学框架",{"5":{"140":2}}],["完美条件",{"5":{"134":2}}],["完形填空任务的数学分析",{"2":{"101":1},"5":{"101":1}}],["完全不可行",{"5":{"140":2}}],["完全不确定时熵最大",{"5":{"61":2}}],["完全集中的注意力",{"5":{"132":4}}],["完全分散的注意力",{"5":{"132":4}}],["完全一致",{"5":{"70":1}}],["完全抑制负信息",{"5":{"71":2}}],["完全无法捕获和之间的乘积交互",{"5":{"71":1}}],["完全无法捕获",{"5":{"71":1}}],["完全确定时熵为零",{"5":{"61":2}}],["完全忽略绝对位置会限制模型对这类信息的捕捉能力",{"5":{"88":2}}],["完全相同",{"5":{"46":1,"93":2}}],["完全相关",{"5":{"92":2,"153":2}}],["完全对称",{"5":{"96":2}}],["完成了从线性预测到概率预测的关键一步",{"5":{"71":2}}],["完成了从输入空间到输出空间的非线性变换",{"5":{"47":2}}],["完整的梯度链式法则为",{"5":{"157":2}}],["完整的rlhf",{"5":{"154":2}}],["完整的位置角度矩阵为",{"5":{"88":2}}],["完整",{"5":{"135":2}}],["完整信息访问",{"5":{"132":2}}],["完整地描述了多头注意力的计算过程",{"5":{"93":2}}],["到一个局部区域内",{"5":{"147":2}}],["到2016年层归一化",{"5":{"146":2}}],["到2021年前后rope",{"5":{"88":2}}],["到其他类别上",{"5":{"144":2}}],["到高阶张量分解",{"5":{"143":2}}],["到第",{"5":{"135":2}}],["到是生成目标",{"5":{"101":1}}],["到损失函数结构的系统性对比",{"5":{"101":2}}],["到离散类别标签",{"5":{"70":1}}],["到连续输出",{"5":{"70":1}}],["到概率单纯形的投影",{"5":{"69":2}}],["到",{"5":{"46":1,"47":1,"50":1,"69":2,"70":4,"71":2,"87":1,"88":6,"92":4,"95":2,"101":4,"132":1,"133":2,"135":2,"138":2,"140":1,"152":1,"153":4,"159":2}}],["到对应的输入维度",{"5":{"44":2}}],["到几何解释",{"5":{"51":2}}],["到真实分布所在的低维流形上",{"5":{"61":2}}],["到低维空间",{"5":{"87":2,"152":2}}],["到位置",{"5":{"87":1,"92":4,"132":1,"152":1,"153":4}}],["到位置差1时的约3",{"5":{"91":2}}],["到内容嵌入中",{"5":{"91":2}}],["到时刻",{"5":{"92":1,"153":1}}],["到查询空间的新坐标系中",{"5":{"94":2}}],["到监督微调阶段的多任务损失",{"5":{"101":2}}],["两部分相等",{"5":{"154":4}}],["两边左乘",{"5":{"144":2}}],["两个离散随机变量",{"5":{"137":1}}],["两个离散随机变量和的联合熵定义为它们联合分布的不确定性",{"5":{"137":1}}],["两个概率分布和的交叉熵定义为",{"5":{"139":1}}],["两个概率分布和之间的kl散度定义为",{"5":{"139":1}}],["两个概率分布和之间的",{"5":{"137":1}}],["两个概率分布",{"5":{"137":1,"139":2}}],["两个概率分布之间的",{"5":{"70":2,"71":2}}],["两个不动点合并为一个半稳定不动点",{"5":{"136":2}}],["两个不动点在分岔点处",{"5":{"136":2}}],["两个随机变量和之间的互信息定义为联合分布与边缘分布乘积之间的",{"5":{"137":1}}],["两个随机变量和之间的互信息定义为",{"5":{"69":1,"71":1}}],["两个随机变量",{"5":{"69":1,"71":1,"137":1}}],["两个神经元的组合可以解决异或问题",{"5":{"46":2}}],["两个维度相同的向量可以逐分量相加",{"5":{"50":2}}],["两个二阶张量",{"5":{"50":2}}],["两个矩阵和的乘积",{"5":{"50":1}}],["两个矩阵",{"5":{"50":1}}],["两个三阶张量和在第二和第三维度上的收缩产生一个的矩阵",{"5":{"50":1}}],["两个三阶张量",{"5":{"50":1}}],["两个向量和如果满足",{"5":{"50":1}}],["两个向量",{"5":{"50":1}}],["两个旋转的复合仍是一个旋转",{"5":{"88":2}}],["两个位置的编码之间的关系可以通过相对旋转来描述",{"5":{"90":2}}],["两个位置的相位差为​",{"5":{"90":1}}],["两个位置的相位差为",{"5":{"90":1}}],["两个独立同分布随机变量乘积的期望和方差可以计算如下",{"5":{"95":2}}],["两个模型没有差异",{"5":{"96":2}}],["两种损失的优势互补",{"5":{"158":2}}],["两种损失函数的梯度结构反映了它们在优化动态上的本质差异",{"5":{"70":2}}],["两种裁剪方式有本质的区别",{"5":{"147":2}}],["两种dropout实现方式的对比",{"5":{"145":1}}],["两种dropout实现方式的对比有放缩版本",{"5":{"145":1}}],["两种不同的训练策略",{"5":{"138":2}}],["两种应用可以共享许多数学分析工具",{"5":{"70":2}}],["两种应用在数学形式上完全相同",{"5":{"70":2}}],["两种最常用的损失函数",{"5":{"70":2}}],["两种",{"5":{"45":2}}],["两种编码各有优劣",{"5":{"89":2}}],["两者之间的关系是",{"5":{"156":2}}],["两者之间也存在重要的区别",{"5":{"93":2}}],["两者在实践中往往协同作用",{"5":{"147":2}}],["两者在优化意义上是等价的",{"5":{"61":2}}],["两者共同作用",{"5":{"147":2}}],["两者共同决定了大语言模型的性能和能力边界",{"5":{"101":2}}],["两者是正交的",{"5":{"145":2}}],["两者都改善了优化的数值稳定性",{"5":{"147":2}}],["两者都是",{"5":{"101":2}}],["两者都是函数值与其",{"5":{"42":2}}],["两者都是对称矩阵",{"5":{"87":2,"152":2}}],["两者都存在梯度饱和问题",{"5":{"42":2}}],["两者都涉及概率加权的信号传递",{"5":{"71":2}}],["两者都描述了数据点之间的成对关系",{"5":{"87":2,"152":2}}],["两者都可以视为某种相似度矩阵",{"5":{"87":2,"152":2}}],["两者都能得到有效的更新",{"5":{"91":2}}],["两者的差即为梯度噪声",{"5":{"149":2}}],["两者的正则化机制相互补充",{"5":{"145":2}}],["两者的组合使得网络能够学习复杂的非线性决策边界",{"5":{"45":2}}],["两者的输出再通过某种方式组合",{"5":{"46":2}}],["两者本质上都是在将一个未归一化的得分向量转换为有效的概率分布",{"5":{"61":2}}],["两者完全一致",{"5":{"88":2}}],["两者相乘得到",{"5":{"70":2}}],["两者相加得到最终表示",{"5":{"88":2}}],["两者成对出现",{"5":{"91":2}}],["两者通过逐元素相加",{"5":{"91":2}}],["两层网络的异或解",{"5":{"46":2}}],["操作类型",{"5":{"146":1}}],["操作在两种场景下的统一性",{"5":{"70":2}}],["操作作为统一的主题贯穿两者",{"5":{"70":2}}],["操作的数学等价性",{"5":{"70":2}}],["操作贯穿了整个",{"5":{"70":2}}],["操作",{"5":{"45":2,"69":2,"71":2,"95":2,"143":2,"146":4}}],["操作来将相似度分数转换为概率分布",{"5":{"61":2}}],["操作与第五章注意力机制中的",{"5":{"61":2}}],["操作是一种典型的长程依赖",{"5":{"92":2,"153":2}}],["操作对输入尺度敏感",{"5":{"97":2}}],["高偏差",{"5":{"155":2}}],["高方差",{"5":{"155":2}}],["高方差意味着算法收敛速度慢",{"5":{"155":2}}],["高方差问题",{"5":{"155":2}}],["高阶项和偏差项可能变得显著",{"5":{"149":2}}],["高阶分解",{"5":{"143":1}}],["高阶svd",{"5":{"143":2}}],["高阶导数与泰勒展开",{"2":{"42":1},"5":{"42":1}}],["高阶导数的复杂结构反映了sigmoid非线性变换的深层特性",{"5":{"42":2}}],["高质量的数据",{"5":{"138":2}}],["高度非均匀的特征值分布",{"5":{"149":2}}],["高度不平衡分布熵小",{"5":{"137":2}}],["高度表达的位置编码可能对训练位置过拟合",{"5":{"89":2}}],["高周期区",{"5":{"136":2}}],["高周期轨道与混沌",{"5":{"136":2}}],["高曲率方向",{"5":{"135":2}}],["高效的专家并行实现需要最小化设备间数据传输量",{"5":{"159":2}}],["高效的矩阵并行计算",{"5":{"95":2}}],["高效利用计算资源",{"5":{"138":2}}],["高效transformer",{"5":{"132":1}}],["高互补性",{"5":{"132":2}}],["高冗余度表示存在信息冗余",{"5":{"132":2}}],["高熵输出表示模型对输入",{"5":{"137":1}}],["高熵输出表示模型对输入的理解不充分",{"5":{"137":1}}],["高熵表示模型对预测结果不确定",{"5":{"137":2}}],["高熵",{"5":{"132":2}}],["高频",{"5":{"71":2}}],["高频成分编码细粒度的位置关系",{"5":{"71":2}}],["高频成分编码局部细节",{"5":{"88":2}}],["高频成分对应短程位置依赖",{"5":{"88":2}}],["高频成分",{"5":{"88":2,"90":6,"91":2}}],["高频成分使用可学习编码",{"5":{"89":2}}],["高频成分描述函数的细节变化",{"5":{"90":2}}],["高频成分覆盖短距离的位置关系",{"5":{"90":2}}],["高频成分的间隔较小",{"5":{"90":2}}],["高频成分不携带信息",{"5":{"90":2}}],["高频成分捕获位置的精细变化和短距离差异",{"5":{"90":2}}],["高频成分捕获短距离的位置变化",{"5":{"90":2}}],["高频端可表示的最高频率受奈奎斯特采样定理限制",{"5":{"90":2}}],["高级语义特征",{"5":{"45":2}}],["高斯dropout不需要显式采样掩码",{"5":{"145":2}}],["高斯dropout与伯努利dropout在二阶矩上匹配",{"5":{"145":2}}],["高斯dropout的计算优势",{"5":{"145":2}}],["高斯dropout的期望与方差",{"5":{"145":2}}],["高斯dropout的数学性质",{"2":{"145":1},"5":{"145":1}}],["高斯dropout",{"5":{"145":3}}],["高斯情况下的闭式解",{"5":{"133":2}}],["高斯",{"5":{"48":6}}],["高斯噪声假设与最大似然估计",{"2":{"51":1},"5":{"51":1}}],["高斯假设下的mle问题简化为最小化平方误差之和",{"5":{"51":2}}],["高斯分布与多元高斯",{"2":{"96":1},"5":{"96":1}}],["高斯分布",{"5":{"96":2}}],["高斯分布之所以如此重要",{"5":{"96":2}}],["高斯分布在卷积",{"5":{"96":2}}],["高斯分布的期望为",{"5":{"96":2}}],["高斯分布的众数",{"5":{"96":2}}],["高斯分布的偏度为0",{"5":{"96":2}}],["高斯分布的熵",{"5":{"96":2}}],["高斯分布的共轭先验还是高斯分布",{"5":{"96":2}}],["高斯分布具有最大的熵",{"5":{"96":2}}],["高斯分布扮演着多种重要角色",{"5":{"96":2}}],["高斯分布可用于建模响应的不确定性",{"5":{"96":2}}],["高斯分布通常作为潜在空间的先验分布",{"5":{"96":2}}],["高斯输出分布用于回归任务",{"5":{"96":2}}],["高斯过程是函数的先验分布",{"5":{"96":2}}],["高斯过程回归在超参数优化",{"5":{"96":2}}],["高斯过程的一个优势是它提供了预测的不确定性估计",{"5":{"96":2}}],["高带宽内存",{"5":{"86":2,"151":2}}],["高维数据往往具有冗余性",{"5":{"143":2}}],["高维空间的稀疏性",{"5":{"141":2}}],["高维情况下为抛物柱面",{"5":{"70":2}}],["高维度",{"5":{"88":2,"90":2,"91":2}}],["高维度对位置变化不敏感",{"5":{"90":2}}],["高维度使用高频的设计能够提供多尺度的位置表示",{"5":{"90":2}}],["高维度能够编码位置的细粒度信息",{"5":{"91":2}}],["高维度捕获位置的精细变化",{"5":{"91":2}}],["高维度区域呈现低频波动",{"5":{"91":2}}],["高速缓存",{"5":{"86":2,"151":2}}],["高速公路",{"5":{"92":2,"147":2,"153":2}}],["高层使用粗粒度编码",{"5":{"89":2}}],["高层在局部特征的基础上进行全局整合",{"5":{"92":2,"153":2}}],["高于rnn的",{"5":{"92":1,"153":1}}],["高",{"5":{"97":1,"132":6,"145":1}}],["陡峭程度",{"5":{"70":2}}],["陡峭",{"5":{"48":2,"71":2}}],["缩放定律可以重写为仅关于计算量的函数",{"5":{"159":2}}],["缩放定律",{"5":{"159":2}}],["缩放定律的数学解释",{"2":{"159":1},"5":{"159":1}}],["缩放不变性",{"5":{"146":1}}],["缩放后",{"5":{"143":2}}],["缩放后点积的方差保持为常数",{"5":{"95":2}}],["缩放因子防止分数差异过大",{"5":{"132":1}}],["缩放因子防止softmax进入极端区域",{"5":{"87":2,"152":2}}],["缩放因子实际上就是一种温度调节",{"5":{"70":1}}],["缩放因子",{"5":{"69":1,"70":1,"87":2,"95":2,"132":3,"152":2}}],["缩放因子有效地",{"5":{"69":1}}],["缩放因子的这一作用与批归一化",{"5":{"94":2}}],["缩放因子的统计学原理",{"2":{"95":1},"5":{"95":1}}],["缩放因子的数学推导与作用",{"2":{"95":1},"5":{"95":1}}],["缩放因子的引入还可以被视为一种",{"5":{"95":2}}],["缩放因子的引入极大地改善了梯度流的质量",{"5":{"95":2}}],["缩放因子​​的引入部分原因就是为了控制注意力矩阵的条件数",{"5":{"87":1,"152":1}}],["缩放因子​​的引入部分解决了这个问题",{"5":{"87":1,"152":1}}],["缩放因子​​的设计正是为了抵消维度增长对点积量级的影响",{"5":{"95":1}}],["缩放因子​​可以被理解为一个温度参数",{"5":{"95":1}}],["缩放点积注意力",{"5":{"71":2,"95":2}}],["缩放点积注意力的数学定义",{"2":{"95":1},"5":{"95":1}}],["缩放和反射",{"5":{"71":2}}],["缩放和反射等基本操作的组合",{"5":{"94":2}}],["缩放和剪切等基本变换的组合",{"5":{"45":2}}],["缩放和平移",{"5":{"45":2}}],["缩放等仿射变换",{"5":{"47":2}}],["缩放",{"5":{"50":2,"88":2,"143":4}}],["缩放改变向量的模长",{"5":{"50":2}}],["缩放的倍数就是特征值",{"5":{"50":2}}],["缩放频率参数等价于对位置信号进行了某种",{"5":{"88":2}}],["缩放对梯度稳定性的影响",{"2":{"94":1},"5":{"94":1}}],["该定义表明",{"5":{"149":2}}],["该定理表明",{"5":{"90":2}}],["该假设认为",{"5":{"146":2}}],["该估计量的方差随模型规模",{"5":{"138":2}}],["该损失函数最大化图像和文本表示之间的互信息",{"5":{"137":2}}],["该公式量化了模态间的信息冗余和互补性",{"5":{"137":2}}],["该公式表明振幅随学习率增大而迅速增加",{"5":{"136":2}}],["该目标鼓励每一层表示都进行有效的信息压缩",{"5":{"137":2}}],["该上界表明",{"5":{"137":2}}],["该函数在逻辑回归",{"5":{"137":2}}],["该最优值使增强系统的谱半径最小化",{"5":{"136":2}}],["该区域通常呈三角形或梯形",{"5":{"136":2}}],["该正规形的相图分析如下",{"5":{"136":2}}],["该结果表明梯度下降以",{"5":{"136":1}}],["该结果表明梯度下降以的速率收敛到最优解",{"5":{"136":1}}],["该结论结合了强凸性和",{"5":{"136":2}}],["该方程的根对应于扩展系统的特征值",{"5":{"136":2}}],["该方程表明",{"5":{"133":2}}],["该方程刻画了最优编码分布的形式",{"5":{"133":2}}],["该头对输出的直接贡献",{"5":{"132":2}}],["该分布度量当前位置",{"5":{"69":1}}],["该分布度量当前位置对每个位置的",{"5":{"69":1}}],["该分布度量查询与每个候选的匹配程度",{"5":{"69":2}}],["该隐藏层有个神经元",{"5":{"71":1}}],["该隐藏层有",{"5":{"71":1}}],["该层定义的映射​为",{"5":{"45":1}}],["该层定义的映射",{"5":{"45":1}}],["该表示保留了位置信息",{"5":{"71":2}}],["该表达式可以直接从定义",{"5":{"61":2}}],["该点在圆周上旋转的角度为",{"5":{"90":2}}],["该先验倾向于低频的位置变化",{"5":{"90":2}}],["该向量的各维度由不同频率的正弦和余弦函数值组成",{"5":{"91":2}}],["该位置的交叉熵损失为",{"5":{"96":2}}],["该批量内样本的均值和方差是整体数据均值和方差的良好估计",{"5":{"96":2}}],["该指标是总体性能的一个估计",{"5":{"96":2}}],["描述从当前状态到下一状态的映射",{"5":{"134":2}}],["描述",{"5":{"45":2,"71":2,"96":2}}],["描述的是在数据分布层面上的期望损失",{"5":{"51":2}}],["描述单次二元试验的成功概率",{"5":{"96":2}}],["描述独立事件发生的时间间隔",{"5":{"96":2}}],["描述了最优价值函数满足的性质",{"5":{"156":2}}],["描述了系统在状态空间中沿闭合轨道无限期运动的现象",{"5":{"136":2}}],["描述了系统的迭代更新规则",{"5":{"134":2}}],["描述了数据点之间的相似性结构",{"5":{"87":2,"152":2}}],["描述了随机变量的",{"5":{"96":2}}],["描述词嵌入向量的分布特性",{"5":{"96":2}}],["描述分布的对称性",{"5":{"96":2}}],["描述分布的尾部厚度",{"5":{"96":2}}],["引导优化过程朝向更好的极小值",{"5":{"144":2}}],["引发了关于其本质的深入研究",{"5":{"138":2}}],["引理是理解高维数据低维嵌入的基础工具",{"5":{"141":2}}],["引理",{"5":{"132":52,"133":40,"134":26,"135":44,"136":40,"137":36,"139":9,"140":25,"141":6,"142":10,"143":14,"144":20,"145":18,"146":14}}],["引言",{"2":{"44":1,"45":1,"46":1},"5":{"44":1,"45":1,"46":1}}],["引入",{"5":{"150":2}}],["引入随机掩码来形式化描述dropout操作",{"5":{"145":2}}],["引入的绝对噪声也较小",{"5":{"145":2}}],["引入低频长周期震荡",{"5":{"136":2}}],["引入任务相关性的加权信息瓶颈允许多任务场景下的差异化优化",{"5":{"133":2}}],["引入拉格朗日乘子确保概率归一化",{"5":{"133":2}}],["引入温度参数",{"5":{"132":1}}],["引入温度参数的",{"5":{"132":1}}],["引入注意力熵正则化来控制注意力分布的集中程度",{"5":{"132":2}}],["引入熵正则化的注意力目标",{"5":{"132":2}}],["引入了",{"5":{"69":2}}],["引入了可学习的参数和连续可微的激活函数",{"5":{"47":2}}],["引入了一定的噪声",{"5":{"41":2}}],["引入了一个额外的非线性变换层",{"5":{"93":2}}],["引入门控机制来调节信息流动",{"5":{"86":2,"151":2}}],["引入复指数函数",{"5":{"90":2}}],["引入缩放因子后",{"5":{"94":2}}],["引入可学习的门控机制来调整softmax的行为",{"5":{"101":2}}],["批归一化与层归一化的统计量维度对比",{"5":{"146":1}}],["批归一化与层归一化的统计量维度对比层归一化在transformer架构中扮演着核心角色",{"5":{"146":1}}],["批归一化引入了两个可学习的仿射变换参数",{"5":{"146":1}}],["批归一化引入了两个可学习的仿射变换参数和",{"5":{"146":1}}],["批归一化首先计算第",{"5":{"146":1}}],["批归一化首先计算第个特征在当前批量上的样本均值",{"5":{"146":1}}],["批归一化通过在批量维度上计算统计量",{"5":{"146":2}}],["批归一化通常应用于卷积层之后",{"5":{"146":2}}],["批归一化通常应用于通道维度",{"5":{"41":2}}],["批归一化适用于卷积神经网络",{"5":{"146":2}}],["批归一化难以处理这种异构数据",{"5":{"146":2}}],["批归一化虽然取得了巨大成功",{"5":{"146":2}}],["批归一化将原本的线性变换",{"5":{"146":2}}],["批归一化将激活值的分布规范化到均值为0",{"5":{"41":2}}],["批归一化是最早提出的归一化技术",{"5":{"146":2}}],["批归一化",{"2":{"146":1},"5":{"41":7,"44":1,"146":6,"148":2}}],["批归一化的均值向量",{"5":{"146":1}}],["批归一化的均值向量和方差向量",{"5":{"146":1}}],["批归一化的优点是能够利用批量内的样本多样性进行归一化",{"5":{"146":2}}],["批归一化的性能会显著下降",{"5":{"146":2}}],["批归一化的统计量计算依赖于批量大小",{"5":{"146":2}}],["批归一化的统计量计算",{"5":{"146":2}}],["批归一化的核心思想是",{"5":{"146":2}}],["批归一化的核心思想是规范化每层的输入分布",{"5":{"41":2}}],["批归一化的局限性",{"2":{"146":1},"5":{"146":1}}],["批归一化的数学原理",{"2":{"41":1},"5":{"41":1}}],["批归一化的梯度计算",{"5":{"41":2}}],["批归一化的反向传播需要计算四个梯度",{"5":{"41":2}}],["批归一化定义为",{"5":{"41":2}}],["批归一化对每个通道独立计算统计量",{"5":{"41":2}}],["批归一化在批量轴",{"5":{"146":1}}],["批归一化在批量轴上聚合信息",{"5":{"146":1}}],["批归一化在循环神经网络",{"5":{"146":2}}],["批归一化在训练和推理阶段使用不同的统计量计算方式",{"5":{"146":2}}],["批归一化在训练过程中维护了全局统计量的移动平均",{"5":{"146":2}}],["批归一化在训练阶段使用当前批量的统计量",{"5":{"146":2}}],["批归一化在特征维度上是独立的",{"5":{"146":2}}],["批归一化在卷积网络中的应用",{"5":{"41":2}}],["批归一化在这些场景中会遇到统计量估计不稳定的问题",{"5":{"41":2}}],["批量内噪声",{"5":{"149":1}}],["批量内样本的统计量可能不够稳定或不够代表",{"5":{"146":2}}],["批量越大",{"5":{"149":2}}],["批量越小",{"5":{"149":2}}],["批量轴n",{"5":{"146":1}}],["批量统计量",{"5":{"146":2}}],["批量大小对协方差矩阵的影响具有直接的缩放关系",{"5":{"149":2}}],["批量大小对归一化稳定性的影响",{"5":{"146":2}}],["批量大小对梯度估计的影响",{"2":{"139":1},"5":{"139":1}}],["批量大小与学习率的等价性",{"5":{"134":2}}],["批量前向传播",{"5":{"44":2}}],["批量误差信号",{"5":{"44":2}}],["批量损失通常定义为各样本损失的平均值",{"5":{"44":2}}],["批量损失对净输入的梯度为",{"5":{"44":2}}],["批量权重梯度",{"5":{"44":2}}],["批量情况下",{"5":{"44":4}}],["批量偏置梯度",{"5":{"44":2}}],["批量标签​",{"5":{"44":1}}],["批量标签",{"5":{"44":1}}],["批量处理的梯度计算",{"2":{"44":1},"5":{"44":1}}],["批量处理",{"5":{"47":2}}],["批量处理还提供了隐式的正则化效果",{"5":{"47":2}}],["批量计算",{"5":{"92":2,"153":2}}],["批量归一化",{"5":{"96":2}}],["批量归一化对每一层的输入进行标准化",{"5":{"96":2}}],["批处理与并行计算的张量视图",{"2":{"50":1},"5":{"50":1}}],["批处理",{"5":{"50":2,"51":2}}],["批处理是在现有维度之外增加一个新的批处理维度",{"5":{"50":2}}],["批处理的数学优势来自于矩阵运算的并行性",{"5":{"50":2}}],["批处理相当于将多个独立的矩阵运算堆叠成一个大的张量运算",{"5":{"50":2}}],["标注不同参数区域的行为类型",{"5":{"136":2}}],["标度律提供了有价值的预测框架",{"5":{"138":2}}],["标度律主要针对损失函数这一指标",{"5":{"138":2}}],["标度律可能出现偏差",{"5":{"138":2}}],["标度律的适用范围与局限性",{"5":{"138":1}}],["标度律的适用范围与局限性是实际应用中必须考虑的问题",{"5":{"138":1}}],["标度律的核心发现是",{"5":{"138":2}}],["标度律的研究表明",{"5":{"138":2}}],["标度律揭示了模型的性能",{"5":{"138":2}}],["标度律是研究系统性质如何随系统规模变化而变化的数学规律",{"5":{"138":2}}],["标度律<",{"5":{"131":1}}],["标度律",{"0":{"138":1},"4":{"138":1},"5":{"131":4,"138":1}}],["标量情形的直观分析",{"2":{"148":1},"5":{"148":1}}],["标量损失",{"5":{"132":2}}],["标量损失函数的梯度",{"5":{"44":1}}],["标量链式法则",{"5":{"44":2}}],["标量对矩阵的链式法则",{"5":{"44":2}}],["标量",{"5":{"45":2}}],["标量乘法",{"5":{"50":2}}],["标量乘法是将一个向量与一个标量",{"5":{"50":2}}],["标签分布和数据特性",{"5":{"137":2}}],["标签分布的熵",{"5":{"137":2}}],["标签为1",{"5":{"101":2}}],["标签服从伯努利分布",{"5":{"47":1}}],["标签",{"5":{"47":1,"132":2,"156":2}}],["标签应该使用类别索引",{"5":{"61":2}}],["标签平滑改变了分类损失的梯度分布",{"5":{"144":2}}],["标签平滑会引入一个惩罚项",{"5":{"144":2}}],["标签平滑鼓励预测分布远离极端值",{"5":{"144":2}}],["标签平滑对预测分布的影响",{"5":{"144":2}}],["标签平滑等价于在原始损失",{"5":{"144":2}}],["标签平滑后的分布为",{"5":{"144":2}}],["标签平滑的kl散度解释",{"5":{"144":2}}],["标签平滑的正则化效应",{"2":{"144":1},"5":{"144":1}}],["标签平滑的熵增效应",{"5":{"137":2}}],["标签平滑",{"5":{"96":2,"137":2,"144":2}}],["标签平滑可以防止模型对训练数据过度自信",{"5":{"96":2}}],["标签平滑相当于在训练时使用真实分布与均匀分布的混合作为目标分布",{"5":{"96":2}}],["标准transformer层的计算量主要来自两个矩阵乘法",{"5":{"159":2}}],["标准sgd并没有显式的正则化项来惩罚尖锐的极小值",{"5":{"149":2}}],["标准化后的值通过可学习的缩放",{"5":{"148":2}}],["标准化后的",{"5":{"146":1}}],["标准化后的满足零均值和单位方差",{"5":{"146":1}}],["标准化均值差异",{"5":{"96":2}}],["标准归一化操作将输入分布转换为标准正态分布",{"5":{"146":2}}],["标准dropout",{"5":{"145":1}}],["标准dropout使用离散的伯努利掩码",{"5":{"145":2}}],["标准基向量",{"5":{"141":2}}],["标准自注意力的数学定义",{"2":{"141":1},"5":{"141":1}}],["标准型理论证明",{"5":{"135":2}}],["标准信息瓶颈理论存在以下主要局限",{"5":{"133":2}}],["标准信息瓶颈理论有多种扩展",{"5":{"133":2}}],["标准高斯先验分布",{"5":{"133":2}}],["标准高斯分布",{"5":{"96":2}}],["标准的rnn",{"5":{"145":2}}],["标准的自注意力计算需要",{"5":{"141":2}}],["标准的bert使用随机均匀掩码",{"5":{"101":2}}],["标准的多头注意力在所有头上都进行完整的计算",{"5":{"93":2}}],["标准",{"5":{"70":2,"132":2}}],["标准nce存在几个局限性",{"5":{"69":2}}],["标准优化算法的内存开销巨大",{"5":{"48":2}}],["标准注意力机制存在一个根本性的计算瓶颈",{"5":{"86":2,"151":2}}],["标准注意力机制的一些设计选择",{"5":{"86":2,"151":2}}],["标准注意力的计算瓶颈在于softmax操作",{"5":{"86":2,"151":2}}],["标准注意力的递归更新需要维护完整的注意力矩阵",{"5":{"86":2,"151":2}}],["标准注意力的内存访问包括",{"5":{"86":2,"151":2}}],["标准注意力的hbm访问量约为",{"5":{"86":2,"151":2}}],["标准注意力计算需要将完整的注意力矩阵存储在sram",{"5":{"86":1,"151":1}}],["标准注意力计算需要将完整的注意力矩阵",{"5":{"86":1,"151":1}}],["标准实现需要将数据从hbm",{"5":{"86":2,"151":2}}],["标准rope的一个潜在问题是",{"5":{"88":2}}],["标准rope的频率参数为",{"5":{"88":2}}],["标准差为1的范围内",{"5":{"150":2}}],["标准差为1的分布",{"5":{"150":2}}],["标准差为",{"5":{"95":4}}],["标准差也为1",{"5":{"95":2}}],["标准差将是时的2倍",{"5":{"95":1}}],["标准差将是",{"5":{"95":1}}],["标准vae假设潜在变量服从标准高斯分布",{"5":{"96":2}}],["标准梯度下降的学习率上界为",{"5":{"147":2}}],["标准梯度下降的动态系统形式",{"5":{"134":2}}],["标准梯度下降在适当的学习率下保证收敛",{"5":{"147":2}}],["标准梯度下降在逃离鞍点时可能很慢",{"5":{"97":2}}],["标准梯度下降算法可表示为",{"5":{"134":2}}],["标准梯度下降以的速度收敛",{"5":{"48":1}}],["标准梯度下降以",{"5":{"48":1}}],["标准梯度下降使用全局统一的学习率",{"5":{"97":2}}],["标记",{"5":{"88":2}}],["标志着计算神经科学和人工智能的诞生",{"5":{"47":2}}],["标志着位置编码研究的成熟",{"5":{"88":2}}],["标识",{"5":{"95":4}}],["故为极大值点",{"5":{"145":2}}],["故收敛到某极限",{"5":{"136":2}}],["故对所有成立",{"5":{"132":1}}],["故",{"5":{"44":7,"45":5,"47":2,"132":1,"137":6,"145":2}}],["故的维度为​",{"5":{"44":1}}],["故也是的线性函数",{"5":{"45":1}}],["称为矩阵",{"5":{"143":1}}],["称为score函数",{"5":{"139":2}}],["称为在区间",{"5":{"136":1}}],["称为feigenbaum常数",{"5":{"136":2}}],["称为均方渐近稳定",{"5":{"134":2}}],["称为系统的状态更新函数",{"5":{"134":2}}],["称为logit",{"5":{"71":2}}],["称为logits",{"5":{"96":1}}],["称为",{"5":{"44":2,"96":1,"134":4,"136":3}}],["称为奇异值",{"5":{"45":2,"50":2}}],["称为净输入",{"5":{"47":2}}],["称为欠拟合",{"5":{"50":2}}],["称为特殊正交群",{"5":{"88":2}}],["称为第",{"5":{"90":1}}],["称为精度矩阵",{"5":{"96":2}}],["称为马氏距离",{"5":{"96":2}}],["常使用log",{"5":{"139":2}}],["常识问答",{"5":{"138":2}}],["常需使用有偏估计量",{"5":{"133":2}}],["常见做法是保持裁剪阈值不变",{"5":{"147":2}}],["常见归一化方法的数学表述<",{"5":{"131":1}}],["常见归一化方法的数学表述",{"0":{"146":1},"4":{"146":1},"5":{"131":4,"146":1}}],["常见输出层误差信号",{"5":{"44":2}}],["常见的正则化形式包括",{"5":{"147":2}}],["常见的任务包括",{"5":{"101":2}}],["常见的裁剪方式包括",{"5":{"44":2}}],["常见的数值稳定性问题与解决方案本节系统阐述了反向传播算法的数学理论基础",{"5":{"44":1}}],["常见的数值稳定性问题与解决方案",{"5":{"44":1}}],["常见的张量分解方法包括",{"5":{"50":2}}],["常见的策略包括",{"5":{"87":2,"152":2}}],["常见的学习率调度方法包括",{"5":{"48":2}}],["常见的学习率调度策略包括",{"5":{"97":2}}],["常用互信息作为正则化项来鼓励不同表示之间的信息共享",{"5":{"137":2}}],["常用值",{"5":{"134":2}}],["常用激活函数",{"5":{"47":2}}],["常用的噪声分布是高斯分布",{"5":{"159":2}}],["常用的互信息下界",{"5":{"133":2}}],["常用的策略包括",{"5":{"70":2}}],["常用的激活函数如sigmoid和tanh被选择是因为它们具有良好的数学性质",{"5":{"71":2}}],["常用的矩阵范数包括frobenius范数",{"5":{"50":2}}],["常用的初始化方法包括均匀初始化",{"5":{"89":2}}],["常用的正则化方法包括l2正则化",{"5":{"89":2}}],["常用的选择是或​​",{"5":{"89":1}}],["常用的选择是或",{"5":{"89":1}}],["常用的选择是",{"5":{"89":2}}],["常用的方法是通过张量重塑",{"5":{"93":2}}],["常用的效应量包括cohen",{"5":{"96":2}}],["常用于矩阵补全",{"5":{"50":2}}],["常用于多模态模型",{"5":{"86":2,"151":2}}],["常数基线",{"5":{"155":1}}],["常数被限制为",{"5":{"147":2}}],["常数的普适性",{"5":{"136":2}}],["常数的偏导数为零",{"5":{"48":2}}],["常数",{"2":{"144":1},"5":{"135":2,"136":2,"144":1,"146":2,"147":2}}],["常数矩阵",{"5":{"70":1}}],["常数因子的偏导数等于该因子乘以函数的偏导数",{"5":{"48":2}}],["常数项不影响优化结果",{"5":{"51":1}}],["常数项",{"5":{"51":1}}],["根据其内部策略选择一个动作",{"5":{"156":2}}],["根据独立同分布随机变量的求和性质",{"5":{"149":2}}],["根据独立随机变量之和的方差公式",{"5":{"95":2}}],["根据激活函数选择合适的初始化策略",{"5":{"148":2}}],["根据条件概率的定义",{"5":{"140":2}}],["根据标度律理论",{"5":{"138":2}}],["根据标准配置",{"5":{"93":2}}],["根据经验研究",{"5":{"138":2}}],["根据定理13",{"5":{"136":4}}],["根据定理2",{"5":{"44":2}}],["根据匹配程度分配信息贡献权重",{"5":{"132":2}}],["根据训练动态自动调整各损失项的权重",{"5":{"101":2}}],["根据4",{"5":{"101":2}}],["根据概率论的链式法则",{"5":{"101":2}}],["根据概率论的知识",{"5":{"95":2}}],["根据任务重要性赋值",{"5":{"70":2}}],["根据信息论中的性质",{"5":{"70":2,"97":2}}],["根据误差信号的定义和链式法则",{"5":{"44":2}}],["根据层映射的定义",{"5":{"45":2}}],["根据链式法则​",{"5":{"45":1}}],["根据链式法则",{"5":{"45":1,"89":2,"92":2,"94":2,"153":2}}],["根据大数定律",{"5":{"51":2,"96":4,"141":2}}],["根据eckart",{"5":{"50":2}}],["根据正交性",{"5":{"51":2}}],["根据统计学习理论",{"5":{"61":2}}],["根据硬件特性自适应调整",{"5":{"86":2,"151":2}}],["根据cauchy",{"5":{"87":2,"152":2}}],["根据矩阵微积分的链式法则",{"5":{"45":2}}],["根据矩阵范数的相容性",{"5":{"87":2,"152":2}}],["根据旋转矩阵的性质",{"5":{"88":2}}],["根据5",{"5":{"89":2}}],["根据公式",{"5":{"91":2}}],["根据注意力权重对值进行加权求和",{"5":{"95":2}}],["根据3σ原则",{"5":{"95":2}}],["根据取值方式的不同",{"5":{"96":2}}],["根据期望的线性性质",{"5":{"96":2,"149":2}}],["根据中心极限定理",{"5":{"96":2}}],["根据历史梯度的平方和调整学习率",{"5":{"97":2}}],["若迭代序列",{"5":{"136":1}}],["若迭代序列满足以下两个条件",{"5":{"136":1}}],["若损失函数",{"5":{"136":1}}],["若损失函数是",{"5":{"136":1}}],["若序列",{"5":{"136":1}}],["若序列收敛到",{"5":{"136":1}}],["若学习率调整满足以下robbins",{"5":{"136":2}}],["若学习率略大于临界值但未达到使系统真正发散的程度",{"5":{"136":2}}],["若系统的长期行为呈现以下三个特征",{"5":{"136":2}}],["若满足以下条件",{"5":{"136":2}}],["若但不存在上述线性收敛条件中的",{"5":{"136":1}}],["若对任意初始点",{"5":{"136":2}}],["若输入向量",{"5":{"135":2}}],["若存在区间",{"5":{"136":1}}],["若存在区间满足",{"5":{"136":1}}],["若存在严格递增的时间序列",{"5":{"136":1}}],["若存在严格递增的时间序列使得损失函数值交替变化",{"5":{"136":1}}],["若存在常数",{"5":{"136":1}}],["若存在常数和正整数",{"5":{"136":1}}],["若存在收缩因子",{"5":{"136":1}}],["若存在收缩因子和正整数",{"5":{"136":1}}],["若存在初始点",{"5":{"136":1}}],["若存在初始点的邻域",{"5":{"136":1}}],["若存在乘子满足",{"5":{"136":2}}],["若存在一条闭轨道",{"5":{"136":2}}],["若存在特征值满足",{"5":{"136":2}}],["若存在",{"5":{"134":2}}],["若真实标签",{"5":{"70":2}}],["若目标函数具有组合",{"5":{"71":1}}],["若目标函数具有",{"5":{"71":1}}],["若",{"5":{"44":6,"47":1,"96":6,"134":8,"135":26,"136":7}}],["若​",{"5":{"47":1}}],["若两个张量的维度数不同",{"5":{"50":2}}],["若干次",{"5":{"86":4,"151":4}}],["若和独立",{"5":{"96":1}}],["若服从多元高斯分布",{"5":{"96":1}}],["一刀切",{"5":{"159":2}}],["一半保持原值",{"5":{"148":2}}],["一旦一个神经元进入这种状态",{"5":{"148":2}}],["一旦这种指数增长开始发生",{"5":{"147":2}}],["一些研究对ics假设提出了质疑",{"5":{"146":2}}],["一些研究提出了自适应dropout概率的方法",{"5":{"145":2}}],["一张",{"5":{"143":1}}],["一张的灰度图像可以表示为一个维的向量",{"5":{"143":1}}],["一致性",{"5":{"139":2,"140":2}}],["一致性和渐近正态性",{"2":{"139":1},"5":{"139":1}}],["一致性保证",{"5":{"133":2}}],["一是中间推理步骤的分布",{"5":{"138":2}}],["一是梯度可以流回所有位置",{"5":{"92":2,"153":2}}],["一维系统的鞍结分岔可标准化为",{"5":{"136":2}}],["一维离散系统的极限环条件",{"5":{"136":2}}],["一维高斯分布的概率密度函数为",{"5":{"96":2}}],["一个具有",{"5":{"159":2}}],["一个专家被使用",{"5":{"158":2}}],["一个较低的",{"5":{"156":2}}],["一个mdp由一个五元组",{"5":{"156":2}}],["一个mhc块可以表示为",{"5":{"150":2}}],["一个简单但有效的选择是使用",{"5":{"155":1}}],["一个简单但有效的选择是使用常数基线",{"5":{"155":1}}],["一个简单的例子是",{"5":{"93":2}}],["一个有趣的开放问题是",{"5":{"147":2}}],["一个重要的观察是",{"5":{"147":2}}],["一个长度为",{"5":{"140":1}}],["一个长度为的文本序列定义为",{"5":{"140":1}}],["一个自回归语言模型定义为函数族",{"5":{"140":2}}],["一个更精确的标度律形式为",{"5":{"138":2}}],["一个离散时间动态系统定义为三元组",{"5":{"134":2}}],["一个解",{"5":{"101":2}}],["一个核心发现是",{"5":{"70":2}}],["一个强凸函数满足",{"5":{"70":1,"97":1}}],["一个映射是线性的",{"5":{"71":1}}],["一个映射",{"5":{"71":1}}],["一个仿射变换可以理解为",{"5":{"45":1}}],["一个仿射变换",{"5":{"45":1}}],["一个300维的词向量可以看作是一个从自然语言到连续向量空间的映射",{"5":{"50":2}}],["一个阶张量有个索引",{"5":{"50":1}}],["一个n维特征空间可以理解为一个n维欧几里得空间",{"5":{"50":2}}],["一个k维子空间是特征空间的一个k维线性子集",{"5":{"50":2}}],["一个线性",{"5":{"50":2}}],["一个线性变换可以用一个矩阵",{"5":{"50":1}}],["一个线性变换",{"5":{"50":1}}],["一个张量网络由若干个节点",{"5":{"50":2}}],["一个",{"5":{"50":1,"70":1,"97":1}}],["一个函数是凸函数",{"5":{"51":1}}],["一个函数",{"5":{"51":1}}],["一个关键的优化技巧是融合",{"5":{"61":1}}],["一个关键的优化技巧是",{"5":{"61":1}}],["一个关键问题是",{"5":{"92":2,"153":2}}],["一个全知的函数",{"5":{"92":2,"153":2}}],["一个常见的初始化方案是xavier初始化",{"5":{"94":2}}],["一个词是否被mask掉",{"5":{"96":2}}],["一个很大的模型可能在某些指标上有统计显著的改进",{"5":{"96":2}}],["一个凸函数具有这样的性质",{"5":{"70":2}}],["一个凸损失函数具有这样的性质",{"5":{"97":2}}],["一次前向和反向传播的计算量约为",{"5":{"138":2}}],["一次前向传播加上一次反向传播",{"5":{"44":2}}],["一次乘法或一次加法计为一次flops",{"5":{"45":2}}],["一次",{"5":{"87":2,"152":2}}],["一次性加载",{"5":{"92":2,"153":2}}],["一层中的个神经元实际上定义了个超平面",{"5":{"46":1}}],["一层中的",{"5":{"46":1}}],["一阶矩",{"5":{"145":1}}],["一阶矩估计",{"5":{"48":2,"157":2}}],["一阶项为零",{"5":{"137":2}}],["一阶张量是向量",{"5":{"50":2}}],["一阶张量",{"5":{"50":2}}],["一阶原点矩是期望",{"5":{"96":2}}],["一组向量如果两两正交且都是单位向量",{"5":{"50":2}}],["一般情形",{"2":{"148":1},"5":{"148":1}}],["一般情况下",{"5":{"87":1,"96":2,"152":1}}],["一般情况下​",{"5":{"87":1,"152":1}}],["一般任务",{"5":{"142":1}}],["一般条件",{"5":{"134":2}}],["一般来说",{"5":{"61":2}}],["一种改进的importance",{"5":{"158":2}}],["一种重要的变体是引入专家间的参数共享",{"5":{"158":2}}],["一种简单但有效的自适应策略是基于各任务梯度的范数调整权重",{"5":{"101":2}}],["一种常见策略是观察训练过程中梯度范数的分布",{"5":{"147":2}}],["一种常见的方法是在线性变换后引入非线性激活",{"5":{"159":2}}],["一种常见的优化方法是逐位置计算角度的正弦和余弦",{"5":{"88":1}}],["一种常见的优化方法是",{"5":{"88":1}}],["一种常见的设计是将正弦编码和可学习编码相加",{"5":{"89":2}}],["一种常用的技巧是使用gumbel",{"5":{"159":2}}],["一种常用的优势函数估计方法是使用td误差",{"5":{"155":2}}],["一种常用的压缩技术是权重分解",{"5":{"87":2,"152":2}}],["一种常用的可视化方法是将编码向量视为时间序列",{"5":{"91":2}}],["一种缓解边界效应的方法是使用循环位置编码",{"5":{"92":2,"153":2}}],["一步到位",{"5":{"92":2,"153":2}}],["再加上参数更新的少量计算",{"5":{"44":2}}],["再进行平移变换",{"5":{"47":2}}],["再用非线性方法进行二维或三维嵌入",{"5":{"50":2}}],["再次产生",{"5":{"61":2}}],["再到2019年均方根归一化",{"5":{"146":2}}],["再到实际应用中的性质和注意事项",{"5":{"51":2}}],["再到更长",{"5":{"88":2}}],["再到位置差2时的约2",{"5":{"91":2}}],["再到对齐阶段的rlhf和dpo损失",{"5":{"101":2}}],["再微调可学习残差",{"5":{"89":2}}],["再经过第二层注意力得到",{"5":{"94":2}}],["再通过子层",{"5":{"146":2}}],["再通过输入嵌入矩阵累积到​",{"5":{"94":1}}],["再通过输入嵌入矩阵",{"5":{"94":1}}],["再通过重参数化技巧进行采样",{"5":{"96":2}}],["掌握本节的数学基础后",{"5":{"44":2,"47":2}}],["额外",{"5":{"44":1,"45":1}}],["存在范数",{"5":{"148":1}}],["存在范数使得",{"5":{"148":1}}],["存在相容的矩阵范数",{"5":{"148":2}}],["存在某种",{"5":{"147":2}}],["存在某个维度使得相位差​不等于",{"5":{"91":1}}],["存在某个维度",{"5":{"91":1}}],["存在一定的统计偏差",{"5":{"146":2}}],["存在一个线性映射",{"5":{"141":2}}],["存在一个关键的过拟合边界",{"5":{"138":2}}],["存在一个最优的参数量和训练数据量组合",{"5":{"138":2}}],["存在一个单隐藏层神经网络",{"5":{"71":2}}],["存在一个更为微妙的问题",{"5":{"61":2}}],["存在大量的线性相关性",{"5":{"143":2}}],["存在大量的局部最小值",{"5":{"48":2}}],["存在低秩维度",{"5":{"141":2}}],["存在性",{"5":{"139":2}}],["存在模大于1的特征值",{"5":{"136":1}}],["存在稳定训练区域",{"5":{"136":2}}],["存在稳定的极限环环绕平衡点",{"5":{"136":2}}],["存在临界学习率",{"5":{"136":2}}],["存在使得对且对",{"5":{"136":1}}],["存在邻域使得所有轨道最终趋近于该闭轨道",{"5":{"136":2}}],["存在信息重叠",{"5":{"132":2}}],["存在信息损失",{"5":{"132":2}}],["存在介于和之间",{"5":{"45":1}}],["存在",{"5":{"45":1,"134":3,"136":5}}],["存在正交矩阵",{"5":{"50":2}}],["存在多个局部最优解",{"5":{"51":2,"89":2}}],["存在结构性冲突",{"5":{"88":2}}],["存在权衡关系",{"5":{"90":2}}],["存在潜在的语义关联",{"5":{"92":2,"153":2}}],["存在直接边",{"5":{"92":2,"153":2}}],["存在常数",{"5":{"92":2,"153":2}}],["存储以保持数值稳定性",{"5":{"157":2}}],["存储的权重为",{"5":{"157":2}}],["存储中间值",{"5":{"44":1,"45":1}}],["存储和加载注意力矩阵",{"5":{"86":2,"151":2}}],["存储和加载输出矩阵",{"5":{"86":2,"151":2}}],["存储输出",{"5":{"86":2,"151":2}}],["存储在sram",{"5":{"86":1,"151":1}}],["存储优化方面",{"5":{"89":2}}],["存储",{"5":{"89":2,"157":4}}],["存储位置编码",{"5":{"89":2}}],["适度震荡的积极意义",{"5":{"136":2}}],["适用于小型模型或容易过拟合的情况",{"5":{"145":2}}],["适用于大型模型或有限数据量的情况",{"5":{"145":2}}],["适用于不同的应用场景",{"5":{"143":2}}],["适用于不同的模型架构",{"5":{"132":2}}],["适用于rnn",{"5":{"41":1}}],["适用于transformer",{"5":{"41":1}}],["适用场景和数学性质上的不同",{"5":{"146":2}}],["适用场景分析",{"2":{"146":1},"5":{"146":1}}],["适用场景",{"5":{"44":1,"45":1,"132":1,"142":1,"143":1,"147":1,"157":1}}],["适应任务的局部位置模式",{"5":{"89":2}}],["适合程度",{"5":{"158":2}}],["适合于变长序列",{"5":{"150":2}}],["适合深层网络",{"5":{"134":2}}],["适合捕获整体位置信息",{"5":{"90":2}}],["适合区分很近的位置",{"5":{"90":2}}],["适合区分较远的位置",{"5":{"90":2}}],["适当的裁剪不会破坏优化算法的收敛性",{"5":{"147":2}}],["适当的裁剪策略仍然可以保持收敛性",{"5":{"147":2}}],["适当的权重初始化",{"5":{"44":1}}],["适当",{"5":{"136":2}}],["适当增大学习率",{"5":{"136":2}}],["适当标准化后",{"5":{"96":2}}],["空间dropout对每个空间位置独立地应用相同的掩码",{"5":{"145":2}}],["空间dropout的特征通道不变性",{"5":{"145":2}}],["空间dropout的数学模型为",{"5":{"145":2}}],["空间中寻找最近的邻居",{"5":{"141":1}}],["空间中由坐标轴截距为",{"5":{"70":1}}],["空间中",{"5":{"70":1,"141":1}}],["空间中的每个点代表一个数据样本在该空间中的表示",{"5":{"50":2}}],["空间中的一条直线",{"5":{"51":1}}],["空间中的一个子流形",{"5":{"51":1}}],["空间中的一个点",{"5":{"51":1}}],["空间中的向量映射到概率单纯形",{"5":{"96":1}}],["空间复杂度",{"5":{"44":1,"45":1}}],["空间复杂度同样降低到",{"5":{"86":2,"151":2}}],["空间复杂度为",{"5":{"86":4,"140":2,"151":4}}],["空间复杂度与单头注意力相当",{"5":{"93":2}}],["空间复杂度方面",{"5":{"95":2}}],["空间按",{"5":{"45":1}}],["空间",{"5":{"92":2,"153":2}}],["空间关系",{"5":{"93":2}}],["空间为",{"5":{"93":2}}],["空间需求会增加",{"5":{"93":2}}],["空间需求",{"5":{"93":2}}],["空间变换等多个数学视角",{"5":{"45":2}}],["空间变换与特征提取",{"2":{"94":1},"5":{"94":1}}],["仿射变换的几何意义",{"5":{"45":2,"47":2}}],["仿射变换负责旋转",{"5":{"45":2}}],["仿射变换",{"5":{"45":2,"47":5}}],["仿射变换是线性变换的推广",{"5":{"47":2}}],["仿射变换退化为线性变换",{"5":{"47":2}}],["仿射变换在几何上等价于线性变换",{"5":{"45":1}}],["仿射变换在几何上等价于先进行线性变换",{"5":{"47":1}}],["仿射变换将其映射为另一个超平面",{"5":{"47":2}}],["仿射变换承担着特征提取和信息整合的功能",{"5":{"47":1}}],["仿射变换本质上是对输入的线性投影",{"5":{"45":1}}],["仿射变换本身是线性变换",{"5":{"47":1}}],["仿射变换进行线性特征组合",{"5":{"47":2}}],["执行所有可能动作的价值的期望",{"5":{"156":2}}],["执行动作",{"5":{"156":2}}],["执行更多的非线性变换",{"5":{"150":2}}],["执行完整的前向传播",{"5":{"44":2}}],["执行旋转",{"5":{"45":2}}],["执行缩放",{"5":{"45":2}}],["执行另一个旋转",{"5":{"45":2}}],["先验分布提供了",{"5":{"133":2}}],["先验知识",{"5":{"91":2}}],["先将输入向量旋转到",{"5":{"45":2}}],["先通过前向传播保存必要的中间值",{"5":{"44":2}}],["先通过pca进行初步降维",{"5":{"50":2}}],["先学习简单样本再学习困难样本",{"5":{"51":2}}],["先预训练正弦编码",{"5":{"89":2}}],["先局部",{"5":{"92":2,"153":2}}],["主题或长度等元特征来初始化或偏置门控决策",{"5":{"159":2}}],["主题模型的推断和某些贝叶斯神经网络方法",{"5":{"96":2}}],["主成分方向",{"5":{"144":2}}],["主成分",{"5":{"143":2}}],["主成分分析",{"5":{"143":2}}],["主成分分析和谱聚类中都有应用",{"5":{"50":2}}],["主动学习",{"5":{"137":4}}],["主要由数据的内在熵决定",{"5":{"159":2}}],["主要问题",{"5":{"148":1}}],["主要体现在以下几个方面",{"5":{"146":2}}],["主要局限",{"5":{"140":1,"146":1}}],["主要优势",{"5":{"140":1,"146":1}}],["主要使用语言建模损失",{"5":{"101":2}}],["主要操作",{"5":{"44":1}}],["主要区别在于归一化和非线性",{"5":{"87":2,"152":2}}],["主要",{"5":{"87":2,"152":2}}],["主要捕获词汇级别和短语级别的特征",{"5":{"92":2,"153":2}}],["主要捕获句子级别和篇章级别的特征",{"5":{"92":2,"153":2}}],["主轴方向",{"5":{"45":2}}],["主流实现通常将d设为偶数",{"5":{"88":2}}],["主语",{"5":{"92":2,"153":2}}],["主语和谓语通常相距不太远",{"5":{"92":2,"153":2}}],["主导项是",{"5":{"93":4,"95":1}}],["主导项是的二次复杂度",{"5":{"95":1}}],["低方差",{"5":{"155":2}}],["低曲率方向",{"5":{"135":2}}],["低互补性",{"5":{"132":2}}],["低冗余度表示各头学习互补的信息",{"5":{"132":2}}],["低熵表示模型置信度高",{"5":{"137":2}}],["低熵",{"5":{"132":2}}],["低温度使得模型对正负样本的区别更加敏感",{"5":{"69":2}}],["低温度会给予这个负样本更高的概率",{"5":{"69":2}}],["低频模式需要更多的数据和更大的模型容量才能被可靠地学习",{"5":{"138":2}}],["低频",{"5":{"71":2}}],["低频成分编码粗粒度的位置关系",{"5":{"71":2}}],["低频成分编码远程依赖",{"5":{"88":2}}],["低频成分对应长程位置依赖",{"5":{"88":2}}],["低频成分使用正弦编码",{"5":{"89":2}}],["低频成分描述函数的整体趋势",{"5":{"90":2}}],["低频成分",{"5":{"90":6,"91":2}}],["低频成分覆盖长距离的位置关系",{"5":{"90":2}}],["低频成分捕获位置的整体结构和长距离关系",{"5":{"90":2}}],["低频成分捕获长距离的位置关系",{"5":{"90":2}}],["低频成分的间隔较大",{"5":{"90":2}}],["低频成分的相位差仍然较小",{"5":{"91":2}}],["低频端可分辨的最小频率间隔为",{"5":{"90":2}}],["低级特征",{"5":{"45":2}}],["低秩诱导",{"5":{"144":1}}],["低秩解通常对应于更简单的模型假设",{"5":{"142":2}}],["低秩近似是否会影响模型的泛化性能",{"5":{"142":2}}],["低秩近似是奇异值分解最重要的应用之一",{"5":{"50":2}}],["低秩近似带来的误差与泛化性能之间的关系是理解模型压缩的关键",{"5":{"142":2}}],["低秩近似如何在优化过程中自然涌现",{"5":{"142":2}}],["低秩近似技术",{"5":{"142":2}}],["低秩近似技术有着广泛的应用",{"5":{"50":2}}],["低秩近似的误差界",{"2":{"142":1},"5":{"142":1}}],["低秩近似的应用包括",{"5":{"89":2}}],["低秩近似可以去除这些噪声",{"5":{"89":2}}],["低秩近似等性质",{"5":{"89":2}}],["低秩近似",{"5":{"93":2,"143":1}}],["低秩更新是一种核心技术",{"5":{"50":2}}],["低秩结构和权重缩放等大模型特有的现象",{"5":{"135":2}}],["低秩结构的定义与直观解释",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["低秩结构与信息瓶颈",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["低秩结构是注意力矩阵最重要的数学特性之一",{"5":{"87":2,"152":2}}],["低秩结构对应于第二项的最小化",{"5":{"87":2,"152":2}}],["低秩结构对模型的泛化能力有积极影响",{"5":{"87":2,"152":2}}],["低秩结构为注意力计算的理论加速提供了依据",{"5":{"87":2,"152":2}}],["低秩",{"5":{"87":2,"89":2,"152":2}}],["低秩分解与计算优化",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["低秩分解",{"5":{"87":2,"152":2}}],["低秩矩阵可以用少数几个外积的和来近似表示",{"5":{"87":2,"152":2}}],["低秩矩阵的运算更快",{"5":{"89":2}}],["低秩参数化",{"5":{"89":2}}],["低维度",{"5":{"88":2,"90":2,"91":2}}],["低维度对位置变化敏感",{"5":{"90":2}}],["低维度捕获位置的粗粒度变化",{"5":{"91":2}}],["低维度区域呈现高频波动",{"5":{"91":2}}],["低层使用细粒度编码",{"5":{"89":2}}],["低",{"5":{"97":1,"132":4,"145":1}}],["纹理",{"5":{"45":2,"150":2}}],["靠近输入",{"5":{"45":2,"87":2,"92":2,"152":2,"153":2}}],["靠近输入的层",{"5":{"92":2,"153":2}}],["靠近输出",{"5":{"45":2,"87":2,"92":2,"152":2,"153":2}}],["靠近输出的层",{"5":{"92":2,"153":2}}],["边际贡献",{"5":{"132":2}}],["边际化",{"5":{"70":2}}],["边际化解释",{"5":{"70":2}}],["边际分布为",{"5":{"69":2}}],["边界行为由对数函数的性质决定",{"5":{"70":2}}],["边界行为由二次函数的性质决定",{"5":{"70":2}}],["边界行为分析",{"2":{"70":1},"5":{"70":1}}],["边界是由数据决定的",{"5":{"45":2}}],["边界效应",{"5":{"92":2,"153":2}}],["边界效应在注意力权重的分布上有所体现",{"5":{"92":2,"153":2}}],["边界位置需要",{"5":{"92":2,"153":2}}],["边",{"5":{"45":2}}],["边表示依赖关系",{"5":{"48":2}}],["边的权重为注意力分数",{"5":{"92":2,"153":2}}],["边缘分布仍然是高斯分布",{"5":{"96":2}}],["例如在早期使用较大的",{"5":{"159":2}}],["例如在sigmoid函数的中间区域",{"5":{"41":2}}],["例如当某些参数维度特别敏感时",{"5":{"147":2}}],["例如通过hosvd",{"5":{"143":2}}],["例如通过分析任务所需的最小计算量或通过测量模型对相关模式的记忆程度",{"5":{"138":2}}],["例如图像中相邻像素的相关性",{"5":{"143":2}}],["例如需要执行",{"5":{"138":2}}],["例如​",{"5":{"71":1}}],["例如",{"5":{"46":2,"50":20,"51":2,"69":2,"70":8,"71":1,"86":2,"87":4,"88":6,"89":6,"92":8,"93":4,"96":4,"97":2,"101":2,"138":4,"142":2,"143":2,"147":4,"148":2,"150":4,"151":2,"152":4,"153":8,"154":4,"156":4,"157":2,"158":6,"159":4}}],["例如将一个的二维张量重塑为的三维张量",{"5":{"50":1}}],["例如将一个",{"5":{"50":1}}],["例如神经网络输出的概率分布",{"5":{"61":2}}],["例2",{"5":{"45":4}}],["遍历",{"5":{"45":2}}],["总专家参数量为",{"5":{"158":2}}],["总专家数为",{"5":{"157":2}}],["总通信开销为两者之和",{"5":{"157":2}}],["总通信时间为",{"5":{"157":2}}],["总操作数",{"5":{"146":1}}],["总步数",{"5":{"133":2}}],["总损失函数的等高线是原始椭圆等高线与圆形的",{"5":{"144":2}}],["总损失",{"5":{"133":2}}],["总迭代",{"5":{"132":2}}],["总信息量最大化",{"5":{"132":2}}],["总结",{"2":{"139":1},"5":{"70":2,"139":1}}],["总",{"5":{"70":2}}],["总flops为各层flops之和",{"5":{"45":2}}],["总体mse与样本mse",{"2":{"51":1},"5":{"51":1}}],["总体mse",{"5":{"51":2}}],["总体mse是模型在",{"5":{"51":2}}],["总hbm访问量约为",{"5":{"86":4,"151":4}}],["总参数量为",{"5":{"86":2,"151":2}}],["总能在至少一个维度上区分任意两个不同的位置",{"5":{"91":2}}],["总复杂度为",{"5":{"92":2,"153":2}}],["总是从主语指向谓语",{"5":{"92":2,"153":2}}],["总时间复杂度为",{"5":{"92":2,"95":2,"153":2}}],["总计算量为稠密模型的",{"5":{"159":2}}],["总计算量约为",{"5":{"159":2}}],["总计算量保持不变",{"5":{"93":2}}],["总计4×",{"5":{"157":2}}],["总计约​次flops",{"5":{"45":1}}],["总计约",{"5":{"45":1}}],["总空间为",{"5":{"93":2}}],["总的投影参数数量为",{"5":{"94":2}}],["次采样的单样本梯度",{"5":{"149":1}}],["次前向传播",{"5":{"140":1}}],["次分岔发生的参数值",{"5":{"136":1}}],["次复合",{"5":{"136":1}}],["次线性收敛",{"5":{"136":2}}],["次非线性变换后到达输出层",{"5":{"45":1}}],["次乘法和",{"5":{"45":2}}],["次加法",{"5":{"45":1}}],["次迭代后",{"5":{"48":1,"135":2}}],["次标量乘法和加法",{"5":{"50":1}}],["次",{"5":{"87":2,"138":2,"152":2}}],["次要",{"5":{"87":2,"152":2}}],["次直积",{"5":{"88":1}}],["次谐波",{"5":{"90":1}}],["次谐波的角频率为",{"5":{"90":1}}],["次变换的结果",{"5":{"90":1}}],["次后的结果",{"5":{"90":1}}],["次矩阵乘法的累积",{"5":{"92":1,"153":1}}],["次顺序计算",{"5":{"92":1,"153":1}}],["行也保持相关",{"5":{"141":1}}],["行的softmax输出为",{"5":{"141":1}}],["行的范数满足",{"5":{"87":2,"152":2}}],["行为",{"5":{"42":2,"47":2,"132":2,"141":1}}],["行为样本",{"5":{"44":1}}],["行仅依赖于",{"5":{"45":1}}],["行和",{"5":{"45":1}}],["行和等于一",{"5":{"87":2,"152":2}}],["行和为1",{"5":{"92":2,"93":2,"153":2}}],["行数和列数",{"5":{"50":2}}],["行列式非零",{"5":{"50":2}}],["行列式为0",{"5":{"87":2,"152":2}}],["行归一化",{"5":{"87":2,"152":2}}],["行随机性",{"5":{"87":2,"152":2}}],["行",{"5":{"87":2,"89":1,"92":1,"95":3,"152":2,"153":1}}],["行第",{"5":{"89":2}}],["行对应序列中第",{"5":{"94":1}}],["列维度必须相同",{"5":{"143":1}}],["列仅依赖于",{"5":{"45":1}}],["列和",{"5":{"45":1}}],["列索引对应输入特征的索引",{"5":{"46":1}}],["列索引",{"5":{"46":1}}],["列",{"5":{"47":1,"89":2}}],["列空间",{"5":{"50":2}}],["列元素为",{"5":{"89":2}}],["寄存器",{"5":{"45":1}}],["左右",{"5":{"145":1}}],["左上",{"5":{"46":2}}],["左下",{"5":{"46":2}}],["左特征向量​满足",{"5":{"87":1,"152":1}}],["左特征向量和右特征向量之间满足正交关系",{"5":{"87":2,"152":2}}],["左特征向量",{"5":{"87":1,"152":1}}],["左边可以是任意小的数",{"5":{"91":2}}],["右下",{"5":{"46":2}}],["右上",{"5":{"46":2}}],["右特征向量满足",{"5":{"87":1,"152":1}}],["右特征向量",{"5":{"87":1,"152":1}}],["就很难获得足够的梯度信号来改进其门控权重",{"5":{"159":2}}],["就可以避免对无关参数的无效计算",{"5":{"159":2}}],["就可以实现异或逻辑",{"5":{"46":2}}],["就可以实现对大型语言模型的有效微调",{"5":{"50":2}}],["就会导致路由决策的极度不均衡",{"5":{"158":2}}],["就会观察到性能的",{"5":{"138":2}}],["就不再给予额外的奖励",{"5":{"154":2}}],["就能同时支持分类损失和注意力机制",{"5":{"69":2}}],["就具有通用逼近能力",{"5":{"71":2}}],["就将梯度向量乘以一个缩放因子使其范数等于阈值",{"5":{"50":2}}],["就依赖于这种思想",{"5":{"50":2}}],["就是",{"5":{"69":1}}],["就是保留前k个最大的奇异值及其对应的奇异向量",{"5":{"50":2}}],["就是典型的逐元素运算",{"5":{"50":1}}],["就是mse的平方根",{"5":{"51":2}}],["就是在这个预测空间中寻找最接近目标向量的点",{"5":{"51":1}}],["就是在这个预测空间中寻找最接近目标向量",{"5":{"51":1}}],["就是在保持自注意力并行计算优势的同时",{"5":{"88":2}}],["就是scaled",{"5":{"95":2}}],["就像一个旋转的指针",{"5":{"90":2}}],["就近似服从正态分布",{"5":{"96":2}}],["具有极高的吞吐量和内存合并访问模式",{"5":{"159":2}}],["具有均值为0",{"5":{"150":2}}],["具有零奇异值",{"5":{"148":2}}],["具有以下重要性质",{"5":{"140":1}}],["具有以下几个基本性质",{"5":{"87":1,"152":1}}],["具有一些重要的理论性质",{"5":{"140":2}}],["具有去噪和泛化效果",{"5":{"133":2}}],["具有明确的理论指导意义",{"5":{"132":2}}],["具有若干与概率分布类比的性质",{"5":{"70":1}}],["具有若干重要性质",{"5":{"61":2}}],["具有封闭形式的解",{"5":{"70":2}}],["具有相同的函数形式",{"5":{"101":1}}],["具有相同的形式",{"5":{"70":1}}],["具有相同的量纲",{"5":{"96":1}}],["具有记忆功能",{"5":{"46":2}}],["具有重要的几何意义和计算优势",{"5":{"50":2}}],["具有旋转不变性",{"5":{"50":2}}],["具有完整特征向量基",{"5":{"50":2}}],["具有了对数几率",{"5":{"61":2}}],["具有低秩结构的模型",{"5":{"87":2,"152":2}}],["具有与gram矩阵相似的性质",{"5":{"87":1,"152":1}}],["具有优美的理论性质和高效的算法",{"5":{"48":2}}],["具有优雅的数学形式和良好的理论性质",{"5":{"91":2}}],["具有天然的",{"5":{"91":2}}],["具有良好的对应关系",{"5":{"92":2,"153":2}}],["具体做法是",{"5":{"154":2}}],["具体做法是如果梯度的范数超过阈值",{"5":{"50":2}}],["具体值需要根据任务特点进行调整",{"5":{"154":2}}],["具体而言",{"5":{"41":2,"42":2,"71":2,"88":7,"93":2,"95":4,"96":4,"101":2,"133":2,"141":2,"146":6,"147":2,"148":2,"149":2,"159":2}}],["具体来说",{"5":{"50":6,"51":2,"96":4,"149":2,"155":2}}],["具体的",{"5":{"50":2}}],["具体推导略",{"5":{"51":2}}],["具体步骤如下",{"5":{"88":2}}],["具体选择取决于变分推断的目标",{"5":{"96":2}}],["齐次性",{"5":{"71":2}}],["齐次性和三角不等式等性质",{"5":{"50":2}}],["齐次坐标形式的单层网络",{"5":{"46":2}}],["齐次坐标表示在理论上具有优雅性",{"5":{"46":2}}],["齐次坐标表示",{"5":{"47":2}}],["齐次坐标表示的优势在于它将仿射变换转化为单一的矩阵乘法运算",{"5":{"47":2}}],["齐次坐标被广泛用于处理平移",{"5":{"47":2}}],["麦肯罗皮层",{"5":{"47":2}}],["麦肯罗皮层神经元模型",{"5":{"47":2}}],["麦肯罗皮层神经元模型虽然简单",{"5":{"47":2}}],["麦肯罗皮层神经元模型存在明显的局限性",{"5":{"47":2}}],["麦肯罗皮层神经元",{"5":{"47":2}}],["麦肯罗皮层神经元实际上是一个线性分类器",{"5":{"47":2}}],["麦肯罗皮层神经元的表达能力",{"5":{"47":2}}],["皮茨",{"5":{"47":2}}],["发生了显著变化",{"5":{"145":2}}],["发生在系统参数跨越临界点时",{"5":{"138":2}}],["发生时不动点处的",{"5":{"136":2}}],["发生",{"5":{"136":2}}],["发表了开创性的论文",{"5":{"47":2}}],["发现kaplan等人提出的标度律可能高估了参数量的重要性",{"5":{"138":2}}],["发现测试损失可以很好地拟合为参数量和数据量的幂律函数",{"5":{"138":2}}],["发现更优解",{"5":{"136":2}}],["发现性能更优的激活函数",{"5":{"41":2}}],["发现",{"5":{"97":2}}],["首先是去均值化",{"5":{"146":2}}],["首先是周期性对偶",{"5":{"90":2}}],["首先对每个模的切片矩阵进行svd",{"5":{"143":2}}],["首先",{"5":{"41":6,"42":2,"48":4,"50":10,"61":4,"69":2,"70":2,"71":2,"86":2,"87":2,"88":2,"89":2,"90":2,"91":2,"93":6,"95":2,"96":4,"143":2,"144":2,"146":6,"148":4,"149":2,"150":2,"151":2,"152":2,"154":2}}],["首先通过仿射变换进行线性投影",{"5":{"47":1}}],["首先通过仿射变换",{"5":{"47":1}}],["首先计算单个样本的hessian",{"5":{"70":2}}],["首先计算​",{"5":{"48":1}}],["首先计算",{"5":{"48":1,"61":2}}],["首先需要明确其统计本质",{"5":{"138":1}}],["首先需要判断是偏差问题还是方差问题",{"5":{"51":2}}],["首先需要回溯其理论基础",{"5":{"61":2}}],["首先需要根据查询与各个信息片段的匹配程度计算权重",{"5":{"95":2}}],["首先分析frobenius范数",{"5":{"87":2,"152":2}}],["首先重塑为的三维张量",{"5":{"93":1}}],["首先重塑为",{"5":{"93":1}}],["首先在输入空间中进行旋转和反射",{"5":{"94":2}}],["首次提出了人工神经元的数学模型",{"5":{"47":2}}],["树突",{"5":{"47":2}}],["树突负责接收来自其他神经元的信号",{"5":{"47":2}}],["等同于标准importance",{"5":{"158":2}}],["等同于普通梯度下降",{"5":{"136":2}}],["等现代rlhf算法的核心",{"5":{"155":2}}],["等现代深度学习框架均支持广播机制",{"5":{"50":2}}],["等效映射空间的扩展",{"5":{"150":1}}],["等效映射空间的扩展是残差连接的另一个数学贡献",{"5":{"150":1}}],["等效正则化强度",{"5":{"147":2}}],["等效地平滑了损失曲面的陡峭区域",{"5":{"147":2}}],["等效地改变损失函数的形状",{"5":{"147":2}}],["等效的",{"5":{"147":2}}],["等效的高斯dropout在权重上引入方差为的高斯噪声",{"5":{"96":1}}],["等效的高斯dropout在权重上引入方差为",{"5":{"96":1}}],["等效于在损失函数中添加了一个时变的正则化项",{"5":{"144":2}}],["等高线来改善条件数",{"5":{"144":2}}],["等高线恢复为原始椭圆",{"5":{"144":2}}],["等高线越接近圆形",{"5":{"144":2}}],["等高线的几何变换",{"5":{"144":2}}],["等高线的变形",{"2":{"144":1},"5":{"144":1}}],["等高线趋近于圆形",{"5":{"144":2}}],["等对比学习模型的损失函数可分解为信息论分量",{"5":{"137":2}}],["等号成立当且仅当",{"5":{"137":1}}],["等号成立当且仅当为常数",{"5":{"137":1}}],["等号成立当且仅当该行是one",{"5":{"87":2,"152":2}}],["等均可纳入动态系统的统一框架分析",{"5":{"134":2}}],["等分段线性激活创造了信息选择的机制",{"5":{"133":2}}],["等部分组成",{"5":{"47":2}}],["等",{"5":{"50":2,"93":2,"94":2}}],["等于矩阵的最大特征值的平方根",{"5":{"50":1}}],["等于矩阵",{"5":{"50":1}}],["等变",{"5":{"88":2}}],["等变的",{"5":{"88":1}}],["等价性的局限性",{"5":{"145":2}}],["等价性",{"5":{"139":2}}],["等价于该神经元被丢弃",{"5":{"145":2}}],["等价于损失",{"5":{"135":2}}],["等价于最小化其负值",{"5":{"139":2}}],["等价于最小化",{"5":{"61":2}}],["等价于最大化数据的似然函数",{"5":{"61":2}}],["等价但不同",{"5":{"89":2}}],["等式右边的第二和第三项是内容与位置的交叉项",{"5":{"88":2}}],["等式右边只依赖于相对位置",{"5":{"88":2}}],["等式不可能对所有同时成立",{"5":{"91":1}}],["等式不可能对所有",{"5":{"91":1}}],["等模式化的注意力策略",{"5":{"92":2,"153":2}}],["等的表示",{"5":{"92":2,"153":2}}],["等算法中非常重要",{"5":{"96":2}}],["等位置编码的数学性质",{"5":{"97":2}}],["假设top",{"5":{"157":2}}],["假设输入分布在零附近对称",{"5":{"148":2}}],["假设输入为",{"5":{"47":2}}],["假设各输入独立同分布",{"5":{"148":2}}],["假设激活函数导数对角矩阵满足",{"5":{"148":2}}],["假设网络每一层只有一个神经元",{"5":{"148":2}}],["假设网络中有大量独立的噪声源",{"5":{"96":2}}],["假设密切相关",{"5":{"146":2}}],["假设不同层或不同神经元之间的随机变量在期望意义上相互独立",{"5":{"145":2}}],["假设的各分量",{"5":{"145":1}}],["假设为凸二次函数",{"5":{"144":2}}],["假设信息均匀分布",{"5":{"133":2}}],["假设数据服从一个条件概率分布",{"5":{"70":2}}],["假设数据服从一个加性噪声模型",{"5":{"70":2}}],["假设数据生成分布为",{"5":{"69":2}}],["假设对于长度",{"5":{"140":1}}],["假设对于长度的序列有",{"5":{"140":1}}],["假设对于层网络",{"5":{"47":1}}],["假设对于",{"5":{"47":1}}],["假设给定输入时",{"5":{"47":1}}],["假设给定输入",{"5":{"47":1}}],["假设预训练模型的权重更新可以表示为低秩矩阵",{"5":{"50":2}}],["假设一个的权重矩阵被近似为​",{"5":{"50":1}}],["假设一个",{"5":{"50":1,"145":2}}],["假设个样本的误差约为1",{"5":{"51":1}}],["假设在给定输入的条件下",{"5":{"51":1}}],["假设在给定输入",{"5":{"51":1}}],["假设",{"5":{"51":1,"91":1,"94":2,"95":2,"145":1,"159":2}}],["假设样本之间相互独立",{"5":{"61":2}}],["假设样本是独立同分布的",{"5":{"61":2}}],["假设依赖关系主要存在于邻近位置之间",{"5":{"86":2,"151":2}}],["假设注意力权重与位置距离成反比",{"5":{"87":2,"152":2}}],["假设我们希望使用基于旧策略",{"5":{"154":2}}],["假设我们使用均方误差作为损失函数",{"5":{"51":2}}],["假设我们只关心某个特定的长程依赖关系",{"5":{"92":2,"153":2}}],["假设我们有一个二维向量",{"5":{"88":2}}],["假设我们有一个",{"5":{"92":2,"153":2}}],["假设我们有一个信息源",{"5":{"95":2}}],["假设我们需要位置和位置之间的关联强度依赖于某种复杂的非线性函数",{"5":{"93":1}}],["假设我们需要位置",{"5":{"93":1}}],["假设我们正在处理一个阅读理解任务",{"5":{"94":2}}],["假设d为偶数",{"5":{"88":2}}],["假设总参数量为",{"5":{"89":2}}],["假设每层的参数可以独立地近似其hessian块",{"5":{"48":2}}],["假设每次计算需要步",{"5":{"92":1,"153":1}}],["假设每次计算需要",{"5":{"92":1,"153":1}}],["假设每个专家的前馈网络需要进行两次矩阵乘法",{"5":{"159":2}}],["假设每个频率成分独立",{"5":{"90":2}}],["假设每个维度携带独立的信息",{"5":{"93":2}}],["假设存在非零系数​使得",{"5":{"90":1}}],["假设存在非零系数",{"5":{"90":1}}],["假设​",{"5":{"91":1}}],["假设模型族",{"5":{"149":2}}],["假设模型是一个简单的线性函数",{"5":{"51":2}}],["假设模型需要处理一个包含",{"5":{"93":2}}],["假设模型隐藏维度为",{"5":{"93":2}}],["假设有个流式多处理器",{"5":{"93":1}}],["假设有",{"5":{"93":1}}],["假设均值为0",{"5":{"95":2}}],["假设查询向量和键向量的分量都是独立同分布的随机变量",{"5":{"95":1}}],["假设查询向量",{"5":{"95":1}}],["假设这些数据独立同分布",{"5":{"96":2}}],["假设检验与模型评估",{"2":{"96":1},"5":{"96":1}}],["假设检验是统计推断的重要工具",{"5":{"96":2}}],["假设检验被用于比较不同模型的性能",{"5":{"96":2}}],["假设检验的基本框架包括原假设",{"5":{"96":2}}],["阈值过小",{"5":{"147":2}}],["阈值过大",{"5":{"147":2}}],["阈值选择策略",{"2":{"147":1},"5":{"147":1}}],["阈值化",{"5":{"71":2}}],["阈值",{"5":{"47":6,"133":2}}],["弗兰克",{"5":{"47":2}}],["提示时性能几乎没有提升",{"5":{"138":2}}],["提升了一倍",{"5":{"135":2}}],["提高收敛速度",{"5":{"150":2}}],["提高了计算效率",{"5":{"146":2}}],["提高数据多样性等",{"5":{"138":2}}],["提高表示的一致性和质量",{"5":{"137":2}}],["提高表示的任务相关性",{"5":{"133":2}}],["提高标注效率",{"5":{"137":2}}],["提高泛化性能",{"5":{"133":2,"136":2}}],["提高泛化能力",{"5":{"96":2}}],["提高多任务学习的稳定性和性能",{"5":{"133":2}}],["提高模型的泛化能力",{"5":{"145":2}}],["提高模型的鲁棒性",{"5":{"133":2}}],["提高模型对分布外样本的鲁棒性",{"5":{"133":2}}],["提高模型捕获长程依赖的能力",{"5":{"132":2}}],["提高对最重要位置的聚焦",{"5":{"132":2}}],["提高信息传递的保真度",{"5":{"132":2}}],["提供理论保证",{"5":{"133":2}}],["提供额外信息",{"5":{"133":2}}],["提供什么信息",{"5":{"132":2}}],["提供足够的非线性表达能力",{"2":{"41":1},"5":{"41":1}}],["提供了理论基础",{"5":{"156":2}}],["提供了另一个典型案例",{"5":{"138":1}}],["提供了量化工具",{"5":{"137":2}}],["提供了基于收敛性和稳定性理论的学习率调度",{"5":{"136":2}}],["提供了任务相关性和梯度冲突的信息论解释",{"5":{"133":2}}],["提供了比传统正则化更精细的控制",{"5":{"133":2}}],["提供了信息保护机制",{"5":{"132":1}}],["提供了自适应的激活特性",{"5":{"41":1}}],["提供了从",{"5":{"71":1}}],["提供了有力的数学工具",{"5":{"90":2}}],["提供稳定的全局位置信息",{"5":{"89":2}}],["提出的感知机",{"5":{"47":2}}],["提出了在固定计算预算下",{"5":{"159":2}}],["提出了基于互信息的正则化框架",{"5":{"133":2}}],["提出了基于熵正则化",{"5":{"132":2}}],["提出了各种改进",{"5":{"86":2,"151":2}}],["提出了一种更高效的隐式实现方式",{"5":{"88":2}}],["彭罗斯广义逆",{"5":{"47":2}}],["旋转后",{"5":{"143":2}}],["旋转后的向量为",{"5":{"88":2}}],["旋转到某个标准方向",{"5":{"143":1}}],["旋转",{"5":{"47":2,"88":6,"94":2,"143":8}}],["旋转保持向量的模长不变",{"5":{"50":2}}],["旋转操作不改变向量的长度",{"5":{"88":2}}],["旋转操作是线性变换的一种",{"5":{"88":2}}],["旋转操作",{"5":{"88":1}}],["旋转角度也只是落在实数轴的某个位置上",{"5":{"88":1}}],["旋转角度为",{"5":{"88":2}}],["旋转角度",{"5":{"88":3}}],["旋转角度较大",{"5":{"90":2}}],["旋转角度较小",{"5":{"90":2}}],["旋转角度由相对位置和频率共同决定",{"5":{"90":2}}],["旋转向量的直接实现需要遍历每个位置和每个维度",{"5":{"88":2}}],["旋转向量",{"5":{"88":2}}],["旋转矩阵仍然有定义",{"5":{"88":2}}],["旋转矩阵",{"5":{"88":1}}],["旋转位置编码",{"5":{"91":2,"92":2,"153":2}}],["旋转位置编码与相对位置建模",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["结构逐渐成为主流",{"5":{"146":2}}],["结构揭示了深度网络中信息流动的数学本质",{"5":{"135":2}}],["结构的互信息分析",{"5":{"132":2}}],["结构定义与基本形式",{"2":{"70":1},"5":{"70":1}}],["结构意味着激活值在0",{"5":{"42":2}}],["结构",{"2":{"135":1},"5":{"61":3,"71":2,"89":4,"92":2,"135":1,"146":2,"153":2}}],["结构高度相似",{"5":{"97":2}}],["结构包含类似的项",{"5":{"97":1}}],["结构包含类似",{"5":{"97":1}}],["结论显然成立",{"5":{"47":2}}],["结论成立",{"5":{"47":2}}],["结果维度",{"5":{"143":1}}],["结果显示",{"5":{"138":2}}],["结果不稳定",{"5":{"136":2}}],["结果等价于逐样本计算后按行堆叠",{"5":{"47":2}}],["结果包含了所有头的query向量",{"5":{"93":1}}],["结果",{"5":{"93":1}}],["结果是将向量的每个分量都乘以该标量",{"5":{"50":2}}],["结果是一个的注意力分数矩阵",{"5":{"95":1}}],["结果是一个",{"5":{"95":1}}],["结合残差连接",{"5":{"150":2}}],["结合联合熵和条件熵的链式法则可推导出所有等价形式",{"5":{"137":2}}],["结合全局位置和局部位置",{"5":{"86":2,"151":2}}],["结合性",{"5":{"88":2}}],["结合方式通常是在rope编码的基础上添加一个可学习的绝对位置嵌入",{"5":{"88":2}}],["结合了ntk",{"5":{"88":2}}],["结合了绝对和相对位置的优点",{"5":{"88":2}}],["结合",{"5":{"91":2}}],["结合前述的点积方差分析",{"5":{"95":2}}],["没有终止状态",{"5":{"156":2}}],["没有使用足够的数据量进行训练",{"5":{"138":2}}],["没有不确定性",{"5":{"137":2}}],["没有softmax的线性归一化将无法实现注意力的",{"5":{"71":2}}],["没有激活函数的神经网络无论多深",{"5":{"47":2}}],["没有激活函数的神经网络等价于单层线性变换",{"5":{"47":2}}],["没有归一化",{"5":{"87":2,"152":2}}],["没有破坏注意力机制的代数结构",{"5":{"88":2}}],["没有可训练参数",{"5":{"89":2}}],["没有连续的分量",{"5":{"90":2}}],["没有明显的主导方向",{"5":{"89":2}}],["没有明显的局部偏向",{"5":{"92":2,"153":2}}],["逻辑或",{"5":{"47":2}}],["逻辑回归模型正是使用sigmoid神经元进行二分类",{"5":{"47":2}}],["后与主损失",{"5":{"158":2,"159":2}}],["后经srivastava等人进一步阐述",{"5":{"145":2}}],["后期",{"5":{"134":2}}],["后期增加",{"5":{"133":1}}],["后期增加促进表示压缩",{"5":{"133":1}}],["后期使用学习率衰减进行精细调优",{"5":{"97":2}}],["后关于",{"5":{"69":1,"71":1,"137":1}}],["后",{"5":{"47":1,"89":1,"95":1,"96":1,"150":2,"156":8}}],["后续还需要batch",{"5":{"148":2}}],["后续研究发现",{"5":{"146":2}}],["后续奇异值迅速减小",{"5":{"87":4,"152":4}}],["后续的研究者开始探索乘法式的位置编码方式",{"5":{"88":2}}],["后续的研究表明",{"5":{"91":2}}],["后续的研究也提出了稀疏激活的多头注意力变体",{"5":{"93":2}}],["后续的模型如bert使用了类似的配置",{"5":{"93":2}}],["后个维度几乎不携带信息",{"5":{"89":1}}],["后全局",{"5":{"92":2,"153":2}}],["后者的性能通常在达到一定深度后就开始下降",{"5":{"150":2}}],["后者在推理时需要使用训练阶段计算的移动平均",{"5":{"146":2}}],["后者为负或零",{"5":{"70":2}}],["后者就是编码器分布与标准高斯先验之间的kl散度",{"5":{"96":2}}],["后的频率",{"5":{"88":2}}],["后的复合函数可能产生非凸的损失景观",{"5":{"97":2}}],["变长序列处理的适用性",{"5":{"150":1}}],["变长序列处理的适用性使得layer",{"5":{"150":1}}],["变得更负",{"5":{"154":2}}],["变得极大时",{"5":{"147":1}}],["变得很大",{"5":{"144":2}}],["变得很小",{"5":{"70":1}}],["变体类型",{"5":{"145":1}}],["变体",{"5":{"142":1}}],["变为长轴为",{"5":{"143":2}}],["变为稳定的",{"5":{"136":1}}],["变为",{"5":{"47":1,"88":3,"90":1}}],["变异性",{"5":{"51":2}}],["变换路径",{"5":{"150":2}}],["变换过程为",{"5":{"143":2}}],["变换的结构高度相似",{"5":{"101":2}}],["变换",{"5":{"70":2,"88":2,"135":2,"148":2}}],["变换映射到概率单纯形上",{"5":{"70":2}}],["变换得到类别概率",{"5":{"61":2}}],["变换到输出空间的过程",{"5":{"71":2}}],["变换到频域",{"5":{"90":2}}],["变化是",{"5":{"70":2}}],["变化是不对称且加速的",{"5":{"70":1}}],["变化是对称的",{"5":{"70":1}}],["变化",{"5":{"88":1,"158":2}}],["变大",{"5":{"92":1,"153":1}}],["变分rnn",{"5":{"145":2}}],["变分信息瓶颈训练",{"5":{"133":2}}],["变分信息瓶颈",{"5":{"133":2}}],["变分下界",{"5":{"133":2}}],["变分dropout的优化目标包含一个kl散度项",{"5":{"145":2}}],["变分dropout的kl散度正则化",{"5":{"145":2}}],["变分dropout的数学框架",{"2":{"145":1},"5":{"145":1}}],["变分dropout与高斯dropout",{"2":{"145":1},"5":{"145":1}}],["变分dropout",{"5":{"96":2,"145":3}}],["变分推断中的证据下界",{"5":{"137":2}}],["变分推断是一类近似贝叶斯推断的方法",{"5":{"96":2}}],["变分推断假设后验分布可以用某个简单的分布族",{"5":{"96":2}}],["变分推断被用于vae的训练",{"5":{"96":2}}],["点生成的轨道",{"5":{"136":1}}],["点和属于类别0",{"5":{"47":1}}],["点和属于类别1",{"5":{"47":1}}],["点",{"5":{"47":2,"97":1,"134":2,"136":1}}],["点迅速遍历整个圆周",{"5":{"90":2}}],["点运动缓慢",{"5":{"90":2}}],["点积关系也得到近似保持",{"5":{"141":2}}],["点积保持",{"5":{"141":2}}],["点积不依赖于绝对位置",{"5":{"90":2}}],["点积可以解释为各频率成分相位差余弦的叠加",{"5":{"90":2}}],["点积随呈周期性变化",{"5":{"90":1}}],["点积随",{"5":{"90":1}}],["点积总体上随位置距离增加而减小",{"5":{"91":2}}],["点积只依赖于相对位置",{"5":{"91":2}}],["点积呈现下降趋势",{"5":{"91":2}}],["点积与位置差的关系",{"5":{"91":2}}],["点积",{"5":{"94":2,"95":1}}],["点积作为相似度度量具有清晰的几何解释",{"5":{"95":2}}],["点积等于它们之间夹角的余弦值",{"5":{"95":2}}],["点积还包含了向量模长的影响",{"5":{"95":2}}],["点积是相对位置的函数",{"5":{"90":1}}],["点积是相对位置",{"5":{"90":1}}],["点积是各分量乘积之和",{"5":{"95":2}}],["点积的方差为",{"5":{"95":2}}],["点积的方差会以的速度线性增长",{"5":{"95":1}}],["点积的方差会以",{"5":{"95":1}}],["点积的分布特性",{"5":{"95":1}}],["点积的分布大致集中在区间内",{"5":{"95":1}}],["点积的分布大致集中在",{"5":{"95":1}}],["点积的量级与​​成正比",{"5":{"95":1}}],["点积的量级与",{"5":{"95":1}}],["点积结果需要通过softmax函数转换为概率分布",{"5":{"95":2}}],["点积分布的范围将变得非常宽",{"5":{"95":2}}],["点积运算隐含地测量了两个向量在各个维度上的",{"5":{"95":2}}],["点是函数的鞍点",{"5":{"97":1}}],["偏离越远",{"5":{"158":2}}],["偏离目标值",{"5":{"158":2}}],["偏离均匀分布时增大",{"5":{"158":2}}],["偏离原始的sft模型",{"5":{"154":2}}],["偏离1多远",{"5":{"154":2}}],["偏离1",{"5":{"148":1}}],["偏离1太远时",{"5":{"101":1}}],["偏离幂律",{"5":{"138":2}}],["偏好",{"5":{"101":2}}],["偏好程度",{"5":{"61":2}}],["偏置梯度",{"5":{"44":2}}],["偏置梯度为",{"5":{"44":2}}],["偏置项直接加到净输入上",{"5":{"44":1}}],["偏置项",{"5":{"44":1,"47":1}}],["偏置项则提供了灵活的阈值调节能力",{"5":{"47":1}}],["偏置项提供灵活的阈值调节",{"5":{"47":2}}],["偏置节点",{"5":{"45":2}}],["偏置的全连接层",{"5":{"45":1}}],["偏置加法",{"5":{"45":2}}],["偏置",{"5":{"45":1,"46":6}}],["偏置向量通常有特殊的初始化策略",{"5":{"46":2}}],["偏置向量为",{"5":{"47":2,"148":2}}],["偏导数与链式法则",{"2":{"48":1},"5":{"48":1}}],["偏导数是多元函数对单个变量的导数",{"5":{"48":2}}],["偏导数的计算遵循与单变量导数相同的规则",{"5":{"48":2}}],["偏导数可以理解为多元函数图像与包含坐标轴的平面相交得到的一元函数曲线的切线斜率",{"5":{"48":1}}],["偏导数可以理解为多元函数图像与包含坐标轴",{"5":{"48":1}}],["偏差可以忽略",{"5":{"146":2}}],["偏差可能较大但更接近真实值",{"5":{"133":2}}],["偏差校正",{"5":{"137":2}}],["偏差校正熵估计",{"5":{"137":2}}],["偏差小但上界可能远离真实值",{"5":{"133":2}}],["偏差",{"2":{"51":1,"139":1},"5":{"51":17,"139":1}}],["偏差反映了模型假设的局限性",{"5":{"51":2}}],["偏差主导了mse",{"5":{"51":2}}],["偏差和方差达到较好的平衡",{"5":{"51":2}}],["偏差平方和方差随模型复杂度的变化",{"5":{"51":2}}],["偏差的平方始终非负",{"5":{"51":1}}],["偏差的平方",{"5":{"51":1}}],["偏度",{"5":{"96":2}}],["接收输入特征",{"5":{"159":2}}],["接收到的数据量为",{"5":{"157":2}}],["接收到的激活为",{"5":{"157":2}}],["接近最大值时",{"5":{"158":2}}],["接近于恒等映射",{"5":{"150":2}}],["接近于零",{"5":{"150":2}}],["接近奇异",{"5":{"148":2}}],["接近奇异时尤为重要",{"5":{"144":2}}],["接近零",{"5":{"146":2,"159":2}}],["接近临界值",{"5":{"136":1}}],["接近球形",{"5":{"135":2}}],["接近单位矩阵",{"5":{"134":2}}],["接近收敛",{"5":{"134":2}}],["接近0时也有定义",{"5":{"158":2}}],["接近0",{"5":{"91":4,"95":1,"101":2}}],["接近0或1时",{"5":{"41":1,"70":1}}],["接近0或1",{"5":{"42":2}}],["接近0或显著小于邻居位置的权重",{"5":{"92":2,"153":2}}],["接近100",{"5":{"101":2}}],["接近1",{"5":{"69":2,"91":2}}],["接近1时",{"5":{"41":1}}],["接近饱和区域",{"5":{"41":2}}],["接近秩",{"5":{"41":2}}],["接近",{"5":{"42":2,"70":3,"89":2,"91":4,"135":4,"145":8,"159":2}}],["接近均匀",{"5":{"158":2}}],["接近均匀混合",{"5":{"41":1}}],["接近均匀分布",{"5":{"71":2,"132":2}}],["接近one",{"5":{"69":2,"71":2,"94":2}}],["接近置换矩阵",{"5":{"87":2,"152":2}}],["接近​",{"5":{"89":1}}],["接近​的某些维度",{"5":{"89":1}}],["接近序列开头",{"5":{"92":1,"153":1}}],["接着计算​",{"5":{"48":1}}],["接着计算",{"5":{"48":1}}],["接力",{"5":{"92":2,"153":2}}],["接下来的矩阵分析将把这个直觉推广到一般情形",{"5":{"148":2}}],["接下来的几节将严格分析这种累积效应的数学条件",{"5":{"148":2}}],["接下来计算第",{"5":{"146":2}}],["接下来计算第个样本的方差",{"5":{"146":1}}],["接下来计算第个特征的样本方差",{"5":{"146":1}}],["接下来",{"5":{"93":2}}],["范围从低频到高频",{"5":{"132":2}}],["范围内的信息",{"5":{"150":2}}],["范围内",{"5":{"44":2,"61":2,"148":1}}],["范数不等式",{"5":{"148":2}}],["范数裁剪通常优于值裁剪",{"5":{"147":2}}],["范数裁剪则均匀地压缩整个向量",{"5":{"147":2}}],["范数裁剪保持梯度的方向信息",{"5":{"147":2}}],["范数裁剪保持梯度的方向",{"5":{"147":2}}],["范数裁剪与值裁剪的比较",{"5":{"147":2}}],["范数裁剪的约束更加全局化",{"5":{"147":2}}],["范数裁剪的投影解释",{"5":{"147":2}}],["范数裁剪的非扩张性",{"5":{"147":2}}],["范数裁剪的数学形式",{"2":{"147":1},"5":{"147":1}}],["范数裁剪将梯度更新限制在以原点为中心",{"5":{"147":2}}],["范数裁剪可以表示为",{"5":{"147":2}}],["范数裁剪",{"5":{"147":2}}],["范数除以",{"5":{"146":1}}],["范数定义",{"5":{"144":1}}],["范数随深度指数增长",{"5":{"135":2}}],["范数随深度指数衰减",{"5":{"135":2}}],["范数与梯度流",{"2":{"135":1},"5":{"135":1}}],["范数的递推",{"5":{"135":2}}],["范数的某种极端形式",{"5":{"70":1}}],["范数的平方",{"5":{"70":1}}],["范数",{"5":{"48":2,"50":5,"87":2,"135":2,"146":1,"147":2,"152":2}}],["范数和∞",{"5":{"50":2}}],["范数分别是列范数和行范数",{"5":{"50":2}}],["范数​",{"5":{"50":1}}],["范数为1",{"5":{"87":1,"152":1}}],["驻点",{"5":{"48":2}}],["负载完全均衡",{"5":{"158":2}}],["负载均衡不仅依赖于噪声加入",{"5":{"159":2}}],["负载均衡可以通过监控专家分配概率的方差来量化",{"5":{"159":2}}],["负载均衡",{"5":{"159":3}}],["负载均衡策略的效果可以通过多个指标来监控",{"5":{"158":2}}],["负载均衡效果不明显",{"5":{"158":2,"159":2}}],["负载均衡是moe训练中最核心的挑战之一",{"5":{"158":2}}],["负载均衡辅助损失",{"2":{"158":1},"5":{"158":1}}],["负载均衡损失因此推动",{"5":{"158":2}}],["负载均衡损失通过双重正则化机制防止专家坍缩",{"5":{"158":2}}],["负载均衡损失通常乘以一个权重系数",{"5":{"158":2,"159":2}}],["负载均衡损失不仅有助于专家的均匀训练",{"5":{"157":2}}],["负载均衡损失对通信效率也有间接影响",{"5":{"157":2}}],["负奖励表示惩罚",{"5":{"156":2}}],["负定",{"5":{"134":2,"135":4}}],["负迁移",{"5":{"133":2}}],["负样本损失",{"5":{"137":2}}],["负样本评分",{"5":{"133":2}}],["负样本",{"5":{"69":2,"101":2,"137":4}}],["负样本对",{"5":{"69":2,"133":2,"137":2}}],["负对数似然损失函数",{"2":{"139":1},"5":{"139":1}}],["负对数似然正是交叉熵损失",{"5":{"101":1}}],["负对数似然",{"5":{"70":2,"101":1,"133":2,"137":2}}],["负梯度方向则是函数下降最快的方向",{"5":{"48":2}}],["负的最大值",{"5":{"48":2}}],["负责专家索引集合",{"5":{"157":2}}],["负责处理对应的一对维度",{"5":{"88":2}}],["负责组间的信息混合",{"5":{"93":2}}],["凹凸程度",{"5":{"48":2}}],["少量的活跃神经元可以编码大量的信息",{"5":{"71":2}}],["少数专家的",{"5":{"159":2}}],["少数专家的得分显著高于其他专家",{"5":{"158":2}}],["少数专家处理大多数输入",{"5":{"157":2}}],["少数几个特征值占据主导地位",{"5":{"149":2}}],["少数几个特征值很大",{"5":{"48":2}}],["少数位置权重较大",{"5":{"89":2}}],["显存优化与通信开销",{"2":{"157":1},"5":{"157":1}}],["显著影响模型的效率与效果",{"5":{"159":2}}],["显著高于目标值",{"5":{"158":2}}],["显著偏离",{"5":{"158":2}}],["显著缓解了梯度消失问题",{"5":{"148":2}}],["显著降低了计算复杂度",{"5":{"146":2}}],["显著降低了参数量和内存需求",{"5":{"93":2}}],["显著",{"5":{"41":2}}],["显著提高了训练效率",{"5":{"47":2}}],["显著提升了深层模型的可训练性",{"5":{"50":2}}],["显著提升了表示的丰富性和任务的性能",{"5":{"93":2}}],["显著提升了长序列注意力的计算效率",{"5":{"93":2}}],["显著小于",{"5":{"86":2,"151":2}}],["显著性检验的解读需要谨慎",{"5":{"96":2}}],["显式正则化",{"5":{"147":2}}],["显式正则化与优化景观",{"2":{"147":1},"5":{"147":1}}],["显式计算hessian是不现实的",{"5":{"48":2}}],["显式编码相对位置",{"5":{"91":2}}],["牛顿矩阵是hessian的一个常用近似",{"5":{"48":2}}],["牛顿矩阵为",{"5":{"48":2}}],["牛顿矩阵省略了二阶导数项",{"5":{"48":2}}],["基线的核心思想是在策略梯度中添加一个与动作无关的基线函数",{"5":{"155":2}}],["基底展开",{"5":{"143":1}}],["基张量",{"5":{"143":2}}],["基数估计",{"5":{"137":2}}],["基本低秩分解",{"5":{"142":1}}],["基本梯度下降算法的更新规则为",{"5":{"48":2}}],["基本运算与运算规则",{"2":{"50":1},"5":{"50":1}}],["基本定义与符号约定",{"2":{"51":1},"5":{"51":1}}],["基本思想是从原始数据中有放回地抽取与原数据等大的样本",{"5":{"96":2}}],["基于模型的强化学习",{"5":{"155":2}}],["基于这些人类偏好数据",{"5":{"154":2}}],["基于这一发现",{"5":{"138":2}}],["基于这一直觉",{"5":{"61":2}}],["基于kl散度约束的ppo",{"5":{"154":2}}],["基于数据",{"5":{"147":1}}],["基于损失函数变化的阈值调整",{"5":{"147":2}}],["基于梯度范数移动平均的动态阈值调整",{"5":{"147":2}}],["基于统计的阈值选择",{"5":{"147":2}}],["基于期望保持性质和方差分析",{"5":{"145":2}}],["基于jl引理",{"5":{"141":2}}],["基于fisher信息的自适应学习率",{"5":{"139":2}}],["基于历史梯度幅值",{"5":{"134":2}}],["基于信息瓶颈的剪枝方法通过评估每层的信息效率来决定保留哪些层",{"5":{"133":2}}],["基于信息论的注意力机制优化方法通过直接控制注意力分布的信息特性来实现更好的训练效果和泛化性能",{"5":{"132":2}}],["基于互信息的梯度过滤方法通过识别低信息量的梯度来减少任务间的干扰",{"5":{"133":2}}],["基于互信息的头重要性评估",{"5":{"132":2}}],["基于互信息的头重要性评估方法",{"5":{"132":2}}],["基于采样表示预测输出",{"5":{"133":2}}],["基于上述分析",{"5":{"70":2}}],["基于前文的分析",{"5":{"41":2}}],["基于",{"5":{"61":2,"137":2,"158":2}}],["基于重要性的稀疏化是最常见的自适应策略",{"5":{"86":2,"151":2}}],["基因组序列等",{"5":{"86":2,"151":2}}],["基础步骤",{"5":{"140":2}}],["基础",{"5":{"89":2}}],["基频为",{"5":{"90":2}}],["基准频率的选择是一个关键的设计决策",{"5":{"91":1}}],["基准频率为10000",{"5":{"91":2}}],["基准频率",{"5":{"91":3}}],["基函数展开",{"5":{"93":2}}],["基函数展开的理论保证告诉我们",{"5":{"93":2}}],["基函数",{"5":{"93":2}}],["步内完成",{"5":{"157":2}}],["步完成",{"5":{"157":2}}],["步更新后",{"5":{"144":2}}],["步后早停",{"5":{"144":2}}],["步逻辑推理",{"5":{"138":2}}],["步迭代后的参数值",{"5":{"134":2}}],["步依次传递信息",{"5":{"132":1}}],["步骤来高效地实现稀疏计算",{"5":{"159":2}}],["步骤5",{"5":{"155":2}}],["步骤4",{"5":{"155":2}}],["步骤3",{"5":{"155":4}}],["步骤2",{"5":{"155":4}}],["步骤1",{"5":{"155":4}}],["步骤",{"5":{"44":2}}],["步长",{"5":{"48":2}}],["步间接传播的比例",{"5":{"87":1,"152":1}}],["步的计算量为",{"5":{"138":2}}],["步的",{"5":{"133":1}}],["步的参数",{"5":{"48":1}}],["步的传递才能到达",{"5":{"92":1,"153":1}}],["步的顺序计算",{"5":{"92":2,"153":2}}],["步传播后",{"5":{"87":1,"152":1}}],["步传递后",{"5":{"92":1,"153":1}}],["步顺序计算",{"5":{"92":1,"153":1}}],["步",{"5":{"92":1,"153":1}}],["步使用的学习率",{"5":{"97":1}}],["阶跃激活函数的数学定义为",{"5":{"47":2}}],["阶张量有",{"5":{"50":1}}],["阶梯衰减",{"5":{"48":2,"97":2}}],["阶梯分布",{"5":{"89":2}}],["阶原点矩定义为",{"5":{"96":2}}],["阶中心矩定义为",{"5":{"96":2}}],["凸优化的裁剪收敛保证",{"5":{"147":2}}],["凸优化是研究凸函数在凸集上最小化问题的数学分支",{"5":{"48":2}}],["凸性保证了全局最小值的存在",{"5":{"70":2}}],["凸性是优化理论中最重要的概念之一",{"5":{"70":2}}],["凸性是损失函数最重要的性质之一",{"5":{"51":2}}],["凸性分析",{"2":{"70":1},"5":{"70":3}}],["凸性的充要条件是hessian矩阵半正定",{"5":{"48":2}}],["凸性与优化特性",{"2":{"51":1},"5":{"51":1}}],["凸性与全局最优性",{"2":{"97":1},"5":{"97":1}}],["凸性",{"5":{"97":1}}],["凸函数的全局收敛性",{"5":{"136":2}}],["凸函数的极值性质保证了",{"5":{"51":2}}],["凸函数",{"5":{"70":4,"97":2}}],["凸函数是一类特殊的函数",{"5":{"48":2}}],["凸",{"5":{"97":2}}],["要使损失减半",{"5":{"159":2}}],["要降低测试损失",{"5":{"159":2}}],["要比直接学习",{"5":{"150":2}}],["要么全部丢弃",{"5":{"145":2}}],["要么被完全丢弃",{"5":{"145":2}}],["要么完全跳过",{"5":{"86":2,"151":2}}],["要将这两个类别分开",{"5":{"71":2}}],["要求上述不等式在和时取严格小于号",{"5":{"48":1}}],["要求上述不等式在",{"5":{"48":1}}],["要覆盖长度为的序列",{"5":{"92":1,"153":1}}],["要覆盖长度为",{"5":{"92":1,"153":1}}],["利用谱半径的性质",{"5":{"148":2}}],["利用均方根统计量",{"5":{"146":2}}],["利用这些统计量",{"5":{"146":2}}],["利用计算得到的均值和方差",{"5":{"146":2}}],["利用正规形",{"5":{"136":2}}],["利用链式法则展开",{"5":{"135":4}}],["利用条件互信息分析多任务学习中的信息共享和迁移",{"5":{"133":2}}],["利用条件熵度量信息重叠",{"5":{"132":2}}],["利用随机矩阵理论的结果",{"5":{"133":2}}],["利用高斯分布的熵公式和期望公式",{"5":{"133":2}}],["利用高斯分布的互信息公式",{"5":{"133":2}}],["利用权衡中的作用",{"5":{"132":2}}],["利用权衡",{"5":{"132":2}}],["利用权衡很有价值",{"5":{"96":2}}],["利用了损失函数在参数空间中的几何结构",{"5":{"70":2}}],["利用​",{"5":{"42":1}}],["利用平方差公式",{"5":{"42":2}}],["利用",{"5":{"42":1,"132":2,"133":2,"137":2}}],["利用微分中值定理",{"5":{"45":2}}],["利用有限差分近似",{"5":{"45":2}}],["利用代数规则求导",{"5":{"45":2}}],["利用gpu的并行计算能力",{"5":{"86":2,"151":2}}],["利用gpu的异步执行能力隐藏内存访问延迟",{"5":{"86":2,"151":2}}],["利用低秩结构",{"5":{"87":2,"152":2}}],["利用旋转矩阵的正交性",{"5":{"88":2}}],["利用旋转矩阵的性质直接计算",{"5":{"88":2}}],["利用三角恒等式",{"5":{"91":2}}],["利用三角恒等式简化",{"5":{"92":2,"153":2}}],["利普希茨连续梯度的凸函数",{"5":{"48":2}}],["利普希茨连续的损失函数",{"5":{"97":1}}],["伪凸",{"5":{"48":2}}],["此时所有专家具有相等的平均路由概率",{"5":{"158":2}}],["此时噪声的等概率密度面是一个完美的球体",{"5":{"149":2}}],["此时梯度既不会消失也不会爆炸",{"5":{"148":2}}],["此时无法计算有意义的批量统计量",{"5":{"146":2}}],["此时正则化效果最强",{"5":{"145":4}}],["此时增加参数量或延长训练时间会更有效",{"5":{"138":2}}],["此时应该增加数据量或减少参数量",{"5":{"138":2}}],["此时二次迭代",{"5":{"136":1}}],["此时二次迭代在处的导数为",{"5":{"136":1}}],["此时系统满足线性收敛条件",{"5":{"136":2}}],["此时语言建模损失无法学习到序列中的位置模式",{"5":{"101":2}}],["此时",{"5":{"42":4,"51":4,"87":3,"91":2,"141":2,"148":2,"152":3}}],["此时网络的输出为",{"5":{"71":2}}],["此时mse达到理论最小值",{"5":{"51":2}}],["此时是置换矩阵",{"5":{"87":1,"152":1}}],["此时对角线元素趋向于0",{"5":{"94":1}}],["此时对角线元素",{"5":{"94":1}}],["此时缩放后点积的方差为1",{"5":{"95":2}}],["此外",{"5":{"48":2,"50":2,"86":4,"88":2,"92":2,"93":4,"95":2,"96":2,"97":2,"101":2,"138":2,"145":2,"146":2,"147":2,"151":4,"153":2}}],["允许专家的路由概率在一定范围内波动",{"5":{"158":2}}],["允许更大的策略变化",{"5":{"154":2}}],["允许使用更大的学习率",{"5":{"135":2,"150":2}}],["允许集中注意力",{"5":{"132":2}}],["允许动态调节注意力的集中程度",{"5":{"132":2}}],["允许关注前缀内的所有位置",{"5":{"101":2}}],["允许快速学习但可能容忍不稳定",{"5":{"41":1}}],["允许梯度下降在不同局部最小值之间",{"5":{"48":2}}],["允许每个位置与远距离的随机位置交互",{"5":{"86":2,"151":2}}],["允许信息直接从较浅的层流向较深的层",{"5":{"92":2,"153":2}}],["允许模型学习到位置",{"5":{"92":2,"153":2}}],["商的偏导数​​",{"5":{"48":1}}],["商的偏导数",{"5":{"48":1}}],["传统稠密模型的输出可以表示为",{"5":{"159":2}}],["传统的transformer语言模型遵循经验性的缩放定律",{"5":{"159":2}}],["传统的稠密模型在处理任何输入时都会激活全部参数",{"5":{"159":2}}],["传统的反向传播算法不能直接适用",{"5":{"157":2}}],["传统的神经网络试图直接学习这个映射",{"5":{"150":2}}],["传统的神经网络结构",{"5":{"132":2}}],["传统的全连接层将输入展平后进行线性变换",{"5":{"143":2}}],["传统的最大似然估计需要计算归一化常数",{"5":{"69":2}}],["传统的带动量sgd可能优于adam",{"5":{"48":2}}],["传统的循环神经网络",{"5":{"92":2,"95":2,"153":2}}],["传输通道的容量",{"5":{"133":2}}],["传导的",{"5":{"48":1}}],["传递和融合",{"5":{"132":2}}],["传递",{"5":{"92":2,"153":2}}],["衡量的是随机梯度在各参数方向上估计误差的二阶矩",{"5":{"149":2}}],["衡量了特征向量的",{"5":{"146":1}}],["衡量了模型对训练数据的",{"5":{"137":1}}],["衡量了编码向量的线性无关程度",{"5":{"89":1}}],["衡量震荡衰减速度的参数",{"5":{"136":2}}],["衡量震荡的强度范围",{"5":{"136":2}}],["衡量共享表示中与各任务相关的信息冗余程度",{"5":{"133":1}}],["衡量信息传递效率",{"5":{"133":2}}],["衡量假设对训练数据的",{"5":{"133":2}}],["衡量预测准确性",{"5":{"133":2}}],["衡量预测值与真实值之间差异的平方的平均值",{"5":{"51":2}}],["衡量三个变量之间的共同信息",{"5":{"132":2}}],["衡量该位置信息与查询需求的相关程度",{"5":{"132":2}}],["衡量位置",{"5":{"69":1}}],["衡量位置包含多少关于序列其他部分的信息",{"5":{"69":1}}],["衡量",{"5":{"69":1}}],["衡量和之间的依赖程度",{"5":{"69":1}}],["衡量函数沿特定方向的变化率",{"5":{"48":2}}],["衡量输出",{"5":{"48":1}}],["衡量模型预测的平均值与真实函数之间的差异",{"5":{"51":2}}],["衡量模型预测对训练数据的敏感程度",{"5":{"51":2}}],["衡量随机变量取值的离散程度",{"5":{"96":2}}],["衡量两个随机变量之间的线性相关程度",{"5":{"96":2}}],["衡量两个随机变量之间的信息共享程度",{"5":{"96":2}}],["衡量差异的实际大小",{"5":{"96":2}}],["沿着回报低的轨迹减少动作概率",{"5":{"155":2}}],["沿着回报高的轨迹增加动作概率",{"5":{"155":2}}],["沿椭球的长轴方向",{"5":{"149":2}}],["沿第一个模的",{"5":{"143":1}}],["沿坐标轴进行缩放",{"5":{"143":2}}],["沿网络深度",{"5":{"135":2}}],["沿稳定流形收敛",{"5":{"134":1}}],["沿轨迹单调非增",{"5":{"134":2}}],["沿轨迹的导数",{"5":{"134":2}}],["沿计算图的前向方向累积计算导数",{"5":{"44":2}}],["沿计算图的反向方向累积计算导数",{"5":{"44":2}}],["沿计算图的边反向传播梯度",{"5":{"44":2}}],["沿",{"5":{"48":1}}],["期望一致性",{"5":{"145":1}}],["期望损失可表示为",{"5":{"145":2}}],["期望保持",{"5":{"145":1}}],["期望保持只是说明输出的",{"5":{"145":1}}],["期望保持只是说明输出的一阶矩",{"5":{"145":1}}],["期望保持与方差控制的数学框架",{"5":{"145":2}}],["期望保持与信息保留",{"5":{"145":2}}],["期望保持性质",{"5":{"145":1}}],["期望保持性质使得dropout可以与其他正则化技术无缝组合",{"5":{"145":2}}],["期望保持性质为dropout的训练和推理提供了数学上的连接",{"5":{"145":2}}],["期望保持性质描述了dropout对输出均值的影响",{"5":{"145":2}}],["期望保持性质并不意味着dropout不改变输出的分布",{"5":{"145":2}}],["期望保持性质是dropout最核心的数学特性",{"5":{"145":2}}],["期望保持性质的实践意义",{"2":{"145":1},"5":{"145":1}}],["期望保持性质的严格表述",{"2":{"145":1},"5":{"145":1}}],["期望保持性质的数学证明",{"2":{"145":1},"5":{"145":1}}],["期望等于真实梯度",{"5":{"134":2}}],["期望mse的分解",{"2":{"51":1},"5":{"51":1}}],["期望获得的信息量",{"5":{"61":2}}],["期望范数为",{"5":{"89":2}}],["期望",{"2":{"96":1},"5":{"96":6,"135":4}}],["期望定义为",{"5":{"96":2}}],["期望具有线性性质",{"5":{"96":2}}],["期望传播",{"5":{"96":2}}],["期望传播在某些情况下比变分推断更准确",{"5":{"96":2}}],["期望传播提供了有用的近似工具",{"5":{"96":2}}],["期望有限",{"5":{"96":2}}],["词表与序列空间的建立",{"2":{"140":1},"5":{"140":1}}],["词袋",{"5":{"101":2}}],["词向量就是典型的向量表示",{"5":{"50":2}}],["词嵌入空间就是一个典型的高维特征空间",{"5":{"50":2}}],["词嵌入空间具有一些重要的几何性质",{"5":{"50":2}}],["词嵌入的降维可视化",{"5":{"50":2}}],["词嵌入维度为​",{"5":{"94":1}}],["词嵌入维度为",{"5":{"94":1}}],["词语的顺序承载着至关重要的语义信息",{"5":{"88":2}}],["词等不同层次",{"5":{"88":2}}],["词→短语→句子→篇章",{"5":{"92":2,"153":2}}],["词序信息等局部依赖",{"5":{"92":2,"153":2}}],["词序承载着重要的语法和语义信息",{"5":{"92":2,"153":2}}],["词索引是离散的",{"5":{"96":2}}],["词汇表通常包含数万个到数十万个词元",{"5":{"140":2}}],["词汇表中的词索引等",{"5":{"96":2}}],["词汇表上的概率分布就是典型的pmf",{"5":{"96":2}}],["困惑",{"5":{"70":2}}],["困惑度",{"5":{"96":2}}],["困难样本",{"5":{"70":2}}],["困难负样本",{"5":{"69":2}}],["困难",{"5":{"41":2}}],["便于通过反向传播进行优化",{"5":{"159":2}}],["便于梯度优化",{"5":{"158":2}}],["便于后续处理",{"5":{"61":2}}],["便于进行残差连接和层归一化操作",{"5":{"50":2}}],["便于进行理论分析",{"5":{"94":2}}],["便于进行理论分析和硬件优化",{"5":{"93":2}}],["便于进行理论分析和与贝叶斯方法的比较",{"5":{"95":2}}],["便于解释和比较",{"5":{"96":2}}],["许多损失函数都可以被视为某种形式的",{"5":{"69":2}}],["许多激活函数可以解释为某种概率分布的参数变换",{"5":{"71":2}}],["许多激活函数",{"5":{"71":2}}],["许多自然现象和测量误差都近似服从高斯分布",{"5":{"96":2}}],["某些专家如果初始时被选中的概率较低",{"5":{"159":2}}],["某些专家逐渐专业化",{"5":{"158":2}}],["某些方向上的噪声可能是其他方向上噪声的数百甚至数千倍",{"5":{"149":2}}],["某些空间位置的响应",{"5":{"145":2}}],["某些条件下唯一",{"5":{"143":1}}],["某些位置的权重接近1",{"5":{"141":2}}],["某些复杂的能力",{"5":{"138":2}}],["某些类别样本稀少",{"5":{"137":2}}],["某些",{"5":{"136":1}}],["某些临界点失稳",{"5":{"136":2}}],["某些参数组合使得增强系统的特征值位于单位圆上而非内部",{"5":{"136":1}}],["某些特定方向",{"5":{"50":2}}],["某些模型探索了将绝对位置编码",{"5":{"88":2}}],["某些模型在不同层使用不同的位置编码策略",{"5":{"88":2}}],["某些奇异值会增大",{"5":{"89":2}}],["某些会减小",{"5":{"89":2}}],["某些实践表明",{"5":{"89":2}}],["某些任务可能需要更精细的低频分辨率",{"5":{"90":2}}],["某些任务可能需要更精细的高频分辨率",{"5":{"90":2}}],["某些头的注意力矩阵几乎是秩1的",{"5":{"87":4,"152":4}}],["某些头的注意力矩阵有更多的非平凡奇异值",{"5":{"87":2,"152":2}}],["某些头的衰减较慢",{"5":{"87":2,"152":2}}],["某些头可能专注于学习近距离的依赖",{"5":{"92":2,"153":2}}],["某些头可能专注于学习远距离的依赖",{"5":{"92":2,"153":2}}],["某些头可能专注于学习语法结构",{"5":{"93":2}}],["某些头可能专注于学习语义角色",{"5":{"93":2}}],["某些头具有较高的熵",{"5":{"92":2,"153":2}}],["某些头具有较低的熵",{"5":{"92":2,"153":2}}],["某些头专门建模",{"5":{"92":6,"153":6}}],["某些头专注于学习语法依存关系",{"5":{"93":2}}],["某些语言依赖具有指数衰减的距离敏感性",{"5":{"88":2}}],["某些语言模型变体使用高斯混合模型来表示输出分布",{"5":{"96":2}}],["某一概率为1",{"5":{"61":2}}],["某个位置主导信息选择",{"5":{"132":2}}],["某个的概率为1",{"5":{"96":1}}],["某个",{"5":{"96":1,"132":2,"137":2}}],["尖锐化",{"5":{"141":2}}],["尖锐",{"5":{"69":4,"87":2,"89":2,"95":2,"96":2,"142":2,"149":2,"152":2}}],["尖锐性",{"5":{"92":2,"153":2}}],["选择的特定函数",{"5":{"159":2}}],["选择动作",{"5":{"156":2}}],["选择性收缩",{"5":{"144":2}}],["选择性强",{"5":{"132":2}}],["选择cp分解还是tucker分解取决于应用需求",{"5":{"143":2}}],["选择高熵样本进行标注",{"5":{"137":2}}],["选择概率",{"5":{"69":1}}],["选择正样本",{"5":{"69":2}}],["选择和互信息最大化作用",{"5":{"71":2}}],["选择",{"5":{"87":2,"95":2,"152":2,"155":2}}],["选择取决于具体任务和需求",{"5":{"89":2}}],["选择取决于具体任务需求和约束条件",{"5":{"89":2}}],["选择合适的使得初始编码向量具有适当的范数",{"5":{"89":1}}],["选择合适的",{"5":{"89":1}}],["选择逐元素相加而非拼接",{"5":{"91":2}}],["选择权重最大的个头参与计算",{"5":{"93":1}}],["选择权重最大的",{"5":{"93":1}}],["选择哪种kl形式取决于我们希望对近似分布施加什么样的约束",{"5":{"96":2}}],["服从均匀分布时取等号",{"5":{"137":1}}],["服从联合高斯分布时",{"5":{"133":1}}],["服从伯努利分布",{"5":{"47":1}}],["服从高斯分布",{"5":{"51":1}}],["服从以真实参数为均值",{"5":{"96":2}}],["服从多元高斯分布",{"5":{"96":1}}],["服从一维高斯分布",{"5":{"96":1}}],["重复",{"5":{"155":2}}],["重复但恰好符合奖励模型偏好的文本",{"5":{"154":2}}],["重复以下步骤直到收敛",{"5":{"143":2}}],["重复多次",{"5":{"96":2}}],["重加权",{"5":{"137":2}}],["重球法等方法的内在联系",{"5":{"134":2}}],["重球法的视角",{"5":{"134":2}}],["重构损失加正则化项",{"5":{"133":2}}],["重构损失",{"5":{"133":2}}],["重参数化采样",{"5":{"133":2}}],["重塑",{"5":{"50":2}}],["重塑操作改变张量的形状但不改变其包含的数据元素",{"5":{"50":2}}],["重写损失函数为",{"5":{"61":2}}],["重新参数化为",{"5":{"146":2}}],["重新分布",{"5":{"87":2,"152":2}}],["重新浮现",{"5":{"92":2,"153":2}}],["重新发现",{"5":{"92":2,"153":2}}],["重要性权重",{"5":{"158":2}}],["重要性采样比率",{"5":{"154":1}}],["重要性采样",{"5":{"154":1}}],["重要性采样也存在一个潜在问题",{"5":{"154":2}}],["重要性采样是一种从目标分布中估计期望值的方法",{"5":{"154":2}}],["重要性采样与比率",{"2":{"154":1},"5":{"154":1}}],["重要的是",{"5":{"93":2}}],["重要",{"5":{"96":2}}],["指门控网络将几乎所有输入都路由到少数几个专家",{"5":{"158":2}}],["指导策略的优化方向",{"5":{"155":2}}],["指导深度学习实践",{"5":{"136":2}}],["指令遵循任务",{"5":{"101":2}}],["指向某个方向",{"5":{"69":1}}],["指数敏感性",{"5":{"148":2}}],["指数为",{"5":{"136":2}}],["指数为正",{"5":{"136":2}}],["指数计算",{"5":{"136":2}}],["指数精确衡量",{"5":{"136":2}}],["指数的量化作用",{"5":{"136":2}}],["指数稳定",{"5":{"134":4}}],["指数增长",{"5":{"41":1,"148":2}}],["指数衰减",{"5":{"41":1,"48":2}}],["指数特征映射",{"5":{"86":2,"151":2}}],["指数核的数值稳定性较差",{"5":{"86":2,"151":2}}],["指数化",{"5":{"87":2,"152":2}}],["指数分布",{"5":{"90":2,"96":2}}],["指数分布提供了对数尺度的均匀分辨率",{"5":{"90":2}}],["指数分布是否是绝对最优的",{"5":{"90":2}}],["指数分布的频率确保了从低频到高频的广泛覆盖",{"5":{"90":2}}],["指数分布的期望为​",{"5":{"96":1}}],["指数分布的期望为",{"5":{"96":1}}],["指数分布在语言模型中的应用包括描述某些随机过程的到达时间间隔",{"5":{"96":2}}],["指数频率分布对应于某种特定的先验分布",{"5":{"90":2}}],["指数频率分布​的几何直觉是",{"5":{"91":1}}],["指数频率分布",{"5":{"91":1}}],["指数函数的非负性保证了输出概率的非负性",{"5":{"96":1}}],["指数函数",{"5":{"96":1}}],["指的是价值函数网络",{"5":{"155":2}}],["指的是策略网络",{"5":{"155":2}}],["指的是矩阵的秩",{"5":{"87":2,"152":2}}],["指的是序列中相距较远的位置之间存在的语义关联或结构关系",{"5":{"92":2,"153":2}}],["指出了震荡和周期行为可能带来的正则化效应",{"5":{"136":2}}],["指出rope的旋转操作可以在attention计算前完成",{"5":{"88":2}}],["指出",{"5":{"96":2}}],["趋于",{"5":{"137":1}}],["趋近于其渐近值",{"5":{"41":1}}],["趋向负无穷",{"5":{"158":2}}],["趋向无穷时",{"5":{"51":1}}],["趋向于均匀分布",{"5":{"141":2}}],["趋向于尖锐分布",{"5":{"94":2}}],["趋向于0",{"5":{"94":1}}],["趋向于正态分布",{"5":{"96":2}}],["独立地服从伯努利分布",{"5":{"145":1}}],["独立地进行注意力计算",{"5":{"93":3}}],["独立地进行scaled",{"5":{"93":1}}],["独立时取等号",{"5":{"137":2}}],["独立时互信息为零",{"5":{"71":1}}],["独立时互信息为0",{"5":{"96":1}}],["独立处理输入",{"5":{"93":2}}],["独立",{"5":{"95":1,"96":1,"145":3}}],["独立于无关选择的特性",{"5":{"69":1}}],["独立于样本量",{"5":{"96":2}}],["还受到其他专家得分的间接影响",{"5":{"159":2}}],["还要相应地增加训练数据量",{"5":{"159":2}}],["还取决于其他专家的得分",{"5":{"158":2}}],["还可能导致这些",{"5":{"159":2}}],["还可能通过额外的交叉连接进行信息交换",{"5":{"150":2}}],["还可以从概率论的角度进行分析",{"5":{"47":2}}],["还增强了网络的鲁棒性",{"5":{"150":2}}],["还保留了之前所有层的原始特征信息",{"5":{"150":2}}],["还包含一个由梯度协方差矩阵决定的正则化项",{"5":{"149":2}}],["还包括数据质量的优化",{"5":{"138":2}}],["还避免了计算",{"5":{"146":1}}],["还避免了计算​",{"5":{"146":1}}],["还归一化了特征的方向",{"5":{"146":2}}],["还起到平滑参数的作用",{"5":{"146":2}}],["还改变损失曲面的曲率分布",{"5":{"144":2}}],["还会改变极小值的位置",{"5":{"144":2}}],["还没有充分利用数据的全部信息",{"5":{"138":2}}],["还深刻影响了我们对大模型能力边界的理解",{"5":{"138":2}}],["还直接影响模型的最终性能",{"5":{"136":2}}],["还是来自分布",{"5":{"69":2}}],["还与激活值的存储需求有关",{"5":{"50":2}}],["还引入了位置插值",{"5":{"88":2}}],["还明确地包含查询和键之间的相对位置信息",{"5":{"92":2,"153":2}}],["还有某些头可能专注于学习位置相近词语之间的局部关联",{"5":{"93":2}}],["还有一种可视化方法是热力图展示",{"5":{"91":2}}],["还有一些头专注于学习语义相似性",{"5":{"93":2}}],["还容易引发梯度消失问题",{"5":{"92":2,"153":2}}],["还容易引发梯度消失或梯度爆炸问题",{"5":{"95":2}}],["还给出了差异大小的可能范围",{"5":{"96":2}}],["还为所有类别分配非零的概率值",{"5":{"96":2}}],["还能有效避免深度神经网络中常见的梯度消失问题",{"5":{"96":2}}],["维全1向量",{"5":{"158":2}}],["维流形",{"5":{"142":2}}],["维持学习能力",{"5":{"133":2}}],["维持数据点之间的相对关系",{"5":{"132":2}}],["维或",{"5":{"133":2}}],["维的权重向量",{"5":{"159":2}}],["维的紧致流形",{"5":{"70":1}}],["维的向量",{"5":{"88":1,"143":1}}],["维的",{"5":{"88":1}}],["维的编码向量",{"5":{"91":1}}],["维的编码向量投影到二维平面进行可视化",{"5":{"91":1}}],["维的子空间中",{"5":{"87":1,"152":1}}],["维的子空间",{"5":{"91":1}}],["维的键向量",{"5":{"95":1}}],["维的值向量",{"5":{"95":1}}],["维的概率向量",{"5":{"96":1}}],["维向量映射到概率单纯形",{"5":{"71":1}}],["维向量",{"5":{"47":1,"50":1}}],["维欧几里得空间",{"5":{"51":1}}],["维子空间信息",{"5":{"87":1,"152":1}}],["维度上",{"5":{"146":2}}],["维度缩减",{"5":{"143":1}}],["维度瓶颈",{"5":{"141":2}}],["维度瓶颈现象",{"5":{"141":2}}],["维度决定了位置编码的精度",{"5":{"132":2}}],["维度决定了能够编码的最大信息量",{"5":{"132":2}}],["维度需足够大以容纳需要编码的信息量",{"5":{"132":2}}],["维度的信息论下界",{"5":{"132":2}}],["维度的选择是一个重要的超参数",{"5":{"50":2}}],["维度对应不同的频率成分",{"5":{"90":1}}],["维度",{"5":{"90":1,"93":1,"146":4}}],["维度较小",{"5":{"91":2}}],["维度通常较大",{"5":{"91":2}}],["维度配对的设计",{"5":{"91":2}}],["维度配对",{"5":{"91":2}}],["维度​",{"5":{"93":1}}],["维度为列",{"5":{"91":2}}],["维度为",{"5":{"93":2}}],["维度设计与参数数量",{"2":{"94":1},"5":{"94":1}}],["维空间",{"5":{"93":1}}],["维随机向量",{"5":{"96":2}}],["覆盖",{"5":{"87":2,"96":1,"152":2}}],["依此类推",{"5":{"45":2,"158":2}}],["依赖批量大小",{"5":{"146":1}}],["依赖距离与信息损失",{"5":{"132":2}}],["依赖于不可微的top",{"5":{"157":2}}],["依赖于具体的损失函数和优化轨迹",{"5":{"147":2}}],["依赖于dropout参数和数据统计量",{"5":{"144":2}}],["依赖于完整的上下文",{"5":{"140":1}}],["依赖于预测值",{"5":{"70":2}}],["依赖于当前的预测概率",{"5":{"70":2,"97":2}}],["依赖于query和key",{"5":{"87":2,"152":2}}],["依赖传递的数学模型",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["依赖建模的计算复杂度下界",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["依赖",{"5":{"92":4,"153":4}}],["依赖的",{"5":{"92":2,"153":2}}],["依赖跨度的数学度量",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["依赖跨度较小",{"5":{"92":2,"153":2}}],["依赖跨度较大",{"5":{"92":2,"153":2}}],["依赖关系的最大跨度为",{"5":{"92":2,"153":2}}],["依赖关系的强度由内容相似度决定",{"5":{"92":2,"153":2}}],["依赖实现",{"5":{"97":1}}],["依概率收敛于",{"5":{"96":1}}],["键矩阵",{"5":{"140":2,"141":2}}],["键维度需足够大以编码",{"5":{"132":1}}],["键维度需足够大以编码个不同位置的信息标签",{"5":{"132":1}}],["键空间",{"5":{"132":2}}],["键空间和值空间的信息容量分别由其维度和取值范围决定",{"5":{"132":2}}],["键空间与值空间的语义分离",{"2":{"94":1},"5":{"94":1}}],["键值维度的信息容量",{"5":{"132":2}}],["键交互空间",{"5":{"132":2}}],["键",{"5":{"50":4,"69":2,"94":4,"95":1,"143":2}}],["键和值张量",{"5":{"50":2}}],["键向量定义了",{"5":{"132":2}}],["键向量",{"5":{"95":2,"132":2}}],["键​作为相关信息来源的概率",{"5":{"95":1}}],["键相似度计算",{"5":{"95":4}}],["键位置是谓语词汇时",{"5":{"92":2,"153":2}}],["键位置是其指代对象时",{"5":{"92":2,"153":2}}],["键位置对之间的相似度",{"5":{"95":2}}],["键的点积相似度度量了信息需求与信息标识之间的匹配程度",{"5":{"95":2}}],["转移到下一个状态",{"5":{"156":2}}],["转化为了一条轨迹上的期望梯度求和问题",{"5":{"155":2}}],["转化为鞍点或不稳定结点",{"5":{"136":2}}],["转换和被利用",{"5":{"137":2}}],["转换为随机输出",{"5":{"145":1}}],["转换为类别概率分布",{"5":{"70":2}}],["转换为后验概率的过程",{"5":{"71":2}}],["转换为显式",{"5":{"71":2}}],["转换为概率",{"5":{"71":1}}],["转换为有效概率分布的函数就是",{"5":{"61":2}}],["转换为有效的概率分布",{"5":{"69":2}}],["转换为有效的类别概率分布",{"5":{"96":2}}],["转换为标准高斯随机变量",{"5":{"96":1}}],["转置和求逆等",{"5":{"50":2}}],["转置操作也经常用于梯度的反向传播计算",{"5":{"50":2}}],["转置",{"5":{"50":2}}],["转置性质",{"5":{"50":2}}],["转置运算满足和",{"5":{"50":1}}],["转置运算满足",{"5":{"50":1}}],["转速由频率决定",{"5":{"90":2}}],["记",{"5":{"146":1}}],["记录当前信息平面位置",{"5":{"133":2}}],["记录训练mse",{"5":{"51":2}}],["记为第个样本的第个特征",{"5":{"146":1}}],["记为",{"5":{"46":2,"50":1,"96":7,"159":2}}],["记为或",{"5":{"50":1}}],["记为​",{"5":{"96":1}}],["记作或",{"5":{"50":1}}],["记作",{"5":{"50":1}}],["记忆",{"5":{"90":2,"133":2,"137":2}}],["零和性质",{"5":{"155":2}}],["零特征值方向对应",{"5":{"134":2}}],["零中心输出减少了梯度偏移问题",{"5":{"42":2}}],["零中心输出使得各层的输入分布更加稳定",{"5":{"42":2}}],["零中心版本",{"5":{"42":2}}],["零阶张量",{"5":{"50":2}}],["零空间",{"5":{"50":2}}],["零角度旋转是单位元",{"5":{"88":1}}],["零角度旋转",{"5":{"88":1}}],["零初始化",{"5":{"89":2,"148":1}}],["零初始化的优点是简单",{"5":{"89":2}}],["嵌入",{"5":{"101":2}}],["嵌入在",{"5":{"70":1}}],["嵌入在空间中",{"5":{"70":1}}],["嵌入层将词汇映射为向量",{"5":{"50":2}}],["嵌入空间中引入了一个额外的几何结构",{"5":{"70":2}}],["嵌入空间通常是密集的",{"5":{"50":2}}],["嵌入空间的维度通常远小于词表大小",{"5":{"50":2}}],["嵌入维度",{"5":{"50":2}}],["嵌入维度为​",{"5":{"89":1,"91":1}}],["嵌入维度为",{"5":{"89":1,"91":1}}],["嵌入之间的互信息",{"5":{"92":1,"153":1}}],["过度最小化方差可能导致专家的专业化程度降低",{"5":{"158":2}}],["过渡期",{"5":{"134":2}}],["过程",{"5":{"134":2}}],["过小",{"5":{"133":2,"136":2}}],["过拟合与欠拟合的边界分析",{"5":{"138":1}}],["过拟合与欠拟合的边界分析帮助我们理解何时应该增加参数量",{"5":{"138":1}}],["过拟合阶段",{"5":{"137":2}}],["过拟合风险增加",{"5":{"133":2}}],["过拟合的风险降低",{"5":{"93":2}}],["过大时增大",{"5":{"158":2}}],["过大的梯度也会导致优化过程震荡剧烈",{"5":{"148":2}}],["过大的权重差异可能导致某些任务的梯度主导",{"5":{"101":2}}],["过大",{"5":{"133":2,"136":2}}],["过于简单的激活函数可能限制网络的表达能力",{"5":{"41":2}}],["过于简单的激活函数",{"5":{"41":2}}],["过于复杂的激活函数可能难以训练或过拟合",{"5":{"41":2}}],["过于随机的初始化可能引入过多噪声",{"5":{"89":2}}],["过于确定的初始化",{"5":{"89":2}}],["过低的维度可能导致表达能力的损失",{"5":{"50":2}}],["过高的维度则可能导致过拟合和计算效率的下降",{"5":{"50":2}}],["过滤低质量内容",{"5":{"138":2}}],["过滤掉信息量小的梯度",{"5":{"133":2}}],["过滤掉噪声和不重要信息",{"5":{"94":2}}],["过滤后的梯度",{"5":{"133":2}}],["过滤",{"5":{"86":2,"151":2}}],["余弦退火调度",{"5":{"132":2}}],["余弦退火",{"5":{"48":2,"97":2}}],["余弦相似度测量两个向量方向的相似程度",{"5":{"50":1}}],["余弦相似度是最常用的相似度度量",{"5":{"50":2}}],["余弦相似度",{"5":{"50":1}}],["投影和剪切等",{"5":{"50":2}}],["投影将向量映射到低维子空间",{"5":{"50":2}}],["投影点就是最优预测",{"5":{"51":2}}],["投影部分",{"5":{"51":2}}],["投影矩阵满足幂等性",{"5":{"51":1}}],["投影矩阵",{"5":{"51":1,"94":2}}],["投影矩阵为和​",{"5":{"87":1,"152":1}}],["投影矩阵为",{"5":{"87":1,"152":1}}],["投影矩阵的并行计算",{"2":{"93":1},"5":{"93":1}}],["投影矩阵的行列式分析",{"2":{"94":1},"5":{"94":1}}],["投影矩阵的奇异值分解",{"2":{"94":1},"5":{"94":1}}],["投影矩阵的初始化策略对模型的训练动态有重要影响",{"5":{"94":2}}],["投影矩阵的列向量逐渐学习到输入数据的重要特征方向",{"5":{"94":2}}],["投影矩阵的列向量是通过数据驱动的学习得到的",{"5":{"94":2}}],["投影矩阵的学习率选择",{"5":{"97":2}}],["投影矩阵是方阵还是长矩阵取决于维度的选择",{"5":{"94":1}}],["投影矩阵是可逆的当且仅当其行列式",{"5":{"94":1}}],["投影矩阵通常通过适当的初始化",{"5":{"94":2}}],["投影",{"5":{"61":2,"91":2,"95":2,"132":2}}],["投影权重张量",{"5":{"93":2}}],["投影的几何视角",{"2":{"94":1},"5":{"94":1}}],["投影到低维空间",{"5":{"87":1,"152":1}}],["投影到一个",{"5":{"94":1}}],["遗忘",{"5":{"50":2,"86":2,"151":2}}],["遗忘机制确保了远处的旧信息逐渐衰减",{"5":{"86":2,"151":2}}],["谱的幂律分布",{"5":{"135":2}}],["谱分析与优化困难",{"2":{"135":1},"5":{"135":1}}],["谱",{"2":{"135":1},"5":{"135":1}}],["谱半径控制",{"5":{"144":1}}],["谱半径的关系",{"5":{"136":2}}],["谱半径小于",{"5":{"135":2}}],["谱半径满足",{"5":{"135":2}}],["谱半径直接决定了动态系统的局部稳定性",{"5":{"135":2}}],["谱半径时",{"5":{"133":1}}],["谱半径是特征值模的最大值",{"5":{"41":2}}],["谱半径等于1是临界状态",{"5":{"41":2}}],["谱半径等于最大奇异值",{"5":{"41":2}}],["谱半径可能大于或小于最大奇异值",{"5":{"41":2}}],["谱半径与矩阵范数之间存在重要关系",{"5":{"148":2}}],["谱半径与矩阵幂",{"5":{"135":2}}],["谱半径与矩阵幂的收敛性密切相关",{"5":{"87":2,"152":2}}],["谱半径与特征值的关系",{"5":{"41":2}}],["谱半径与收敛性",{"2":{"87":1,"148":1,"152":1},"5":{"87":1,"148":1,"152":1}}],["谱半径",{"5":{"87":2,"133":1,"135":4,"148":2,"152":2}}],["谱范数等于最大奇异值",{"5":{"148":2}}],["谱范数正则化在损失函数中添加",{"5":{"144":2}}],["谱范数与神经网络函数的lipschitz常数密切相关",{"5":{"144":2}}],["谱范数与",{"2":{"144":1},"5":{"144":1}}],["谱范数的幂迭代",{"2":{"135":1},"5":{"135":1}}],["谱范数的上界取决于注意力权重的分布",{"5":{"87":2,"152":2}}],["谱范数",{"5":{"50":6,"144":3}}],["谱范数在深度学习中有重要应用",{"5":{"50":2}}],["谱范数是核心概念",{"5":{"50":2}}],["谱范数最重要",{"5":{"50":2}}],["谱范数越小",{"5":{"87":2,"152":2}}],["谱范数越接近行数",{"5":{"87":2,"152":2}}],["谱范数不超过行数的平方根",{"5":{"87":2,"152":2}}],["谱范数为1",{"5":{"87":2,"152":2}}],["谱范数为",{"5":{"89":2}}],["谱归一化",{"5":{"50":4}}],["谱成分",{"5":{"87":2,"152":2}}],["谱正则化从矩阵谱范数的角度分析了权重矩阵的奇异值分布",{"5":{"144":2}}],["谱正则化通过限制权重矩阵的谱范数",{"5":{"144":2}}],["谱正则化与对抗鲁棒性",{"5":{"144":2}}],["谱正则化与hessian特征值",{"2":{"144":1},"5":{"144":1}}],["谱正则化的hessian下界",{"5":{"144":2}}],["谱正则化",{"5":{"144":2}}],["谱正则化惩罚小的奇异值或大的条件数",{"5":{"89":2}}],["谱正则化鼓励编码矩阵具有更好的条件数",{"5":{"89":2}}],["被冷落",{"5":{"159":2}}],["被路由到专家",{"5":{"158":2}}],["被激活的边际概率",{"5":{"159":2}}],["被激活的样本比例",{"5":{"158":2}}],["被激活的条件下",{"5":{"158":2}}],["被使用的频率",{"5":{"158":2}}],["被选中的概率",{"5":{"158":2}}],["被门控权重",{"5":{"158":2}}],["被移除",{"5":{"146":1}}],["被广泛应用于对比学习",{"5":{"137":2}}],["被缩放",{"5":{"135":2}}],["被人类偏好于",{"5":{"101":1}}],["被独立地选为掩码位置",{"5":{"101":1}}],["被掩码位置的双向表示",{"5":{"101":1}}],["被掩码位置的双向表示需要聚合来自左右两侧的token信息",{"5":{"101":1}}],["被掩码的概率分布",{"5":{"101":1}}],["被掩码的位置的位置编码被设为0或忽略",{"5":{"91":2}}],["被",{"5":{"70":1}}],["被拉向初始值",{"5":{"144":2}}],["被拉向",{"5":{"69":2}}],["被推离负样本的加权中心",{"5":{"69":2}}],["被誉为",{"5":{"50":2}}],["被正确分类",{"5":{"61":2}}],["被分片到",{"5":{"157":2}}],["被分解为多个低阶张量的组合",{"5":{"50":2}}],["被分为",{"5":{"93":1}}],["∞",{"5":{"50":2}}],["组进行计算",{"5":{"159":2}}],["组权重",{"5":{"159":2}}],["组合系数由输入决定的门控权重给出",{"5":{"158":2}}],["组合通信",{"5":{"157":1}}],["组合",{"5":{"71":1}}],["组合在梯度计算上的优美对称性",{"5":{"61":2}}],["组合的梯度计算",{"5":{"61":2}}],["组合后达到更高的有效表达能力",{"5":{"87":2,"152":2}}],["组合后达到更高的有效秩",{"5":{"87":2,"152":2}}],["组成",{"5":{"50":2}}],["组成部分",{"5":{"51":4}}],["组",{"5":{"86":1,"93":1,"151":1}}],["组织",{"5":{"90":1}}],["组间进行独立的计算",{"5":{"93":2}}],["组内进行全连接交互",{"5":{"93":2}}],["组内所有头的注意力输出并拼接",{"5":{"93":1}}],["查询与键的匹配决定信息传递路径",{"5":{"132":2}}],["查询与键之间的相似度度量为信息相关性的度量",{"5":{"132":1}}],["查询矩阵与键矩阵的乘积产生注意力分数",{"5":{"50":2}}],["查询",{"5":{"50":2,"69":4,"94":4,"95":6,"132":1}}],["查询向量和键向量都位于空间中",{"5":{"141":1}}],["查询向量定义了",{"5":{"132":2}}],["查询向量​和键向量​既编码了位置信息",{"5":{"92":1,"153":1}}],["查询向量",{"5":{"92":1,"95":2,"132":2,"141":1,"153":1}}],["查询空间",{"2":{"94":1},"5":{"94":1}}],["卷积网络和循环神经网络中的不同应用方式",{"5":{"145":2}}],["卷积网络中的dropout",{"2":{"145":1},"5":{"145":1}}],["卷积网络的函数类是",{"5":{"92":2,"153":2}}],["卷积神经网络的张量分解",{"2":{"143":1},"5":{"143":1}}],["卷积层",{"5":{"46":2}}],["卷积运算和批处理操作",{"5":{"50":2}}],["卷积矩阵",{"5":{"87":2,"152":2}}],["卷积矩阵是稀疏的",{"5":{"87":2,"152":2}}],["卷积矩阵与输入无关",{"5":{"87":2,"152":2}}],["卷积矩阵具有平移不变性",{"5":{"87":2,"152":2}}],["卷积矩阵的结构是规则的",{"5":{"87":2,"152":2}}],["卷积矩阵可以视为具有特殊结构",{"5":{"93":2}}],["卷积结构",{"5":{"88":2}}],["卷积的路径长度与层数成正比",{"5":{"92":2,"153":2}}],["卷积核的张量表示",{"5":{"143":2}}],["卷积核的感受野是固定的",{"5":{"93":2}}],["卷积核固定",{"5":{"87":2,"152":2}}],["卷积核定义了一组可学习的滤波器",{"5":{"93":2}}],["卷积可以写为矩阵乘法形式",{"5":{"93":2}}],["×",{"5":{"50":2,"136":2}}],["又可以利用学习到的价值函数来减少梯度的方差",{"5":{"155":2}}],["又控制了模型的参数量",{"5":{"150":2}}],["又能在平坦区域稳定收敛",{"5":{"149":2}}],["又称为张量点积或广义矩阵乘法",{"5":{"50":2}}],["又不受单批次大小的限制",{"5":{"50":2}}],["又不会过于密集或稀疏",{"5":{"91":2}}],["代码token可能被路由到具有编程知识专家",{"5":{"158":2}}],["代价是需要额外的",{"5":{"157":2}}],["代价是增加额外的计算量",{"5":{"50":2}}],["代替",{"5":{"157":2}}],["代入缩放定律",{"5":{"159":2}}],["代入和",{"5":{"145":1}}],["代入归纳假设即可得证",{"5":{"140":2}}],["代入不动点条件",{"5":{"134":2}}],["代入对数几率的表达式",{"5":{"71":1}}],["代入得",{"5":{"44":2}}],["代入并化简",{"5":{"44":1}}],["代入",{"5":{"44":1,"136":2,"145":1}}],["代入rope编码后的向量",{"5":{"88":2}}],["代表的线性变换可以分解为三个基本变换的复合",{"5":{"143":1}}],["代表任务所需的某个子能力或子步骤",{"5":{"138":2}}],["代表在该任务上可能达到的最小损失",{"5":{"138":2}}],["代表向量的长度",{"5":{"88":2}}],["代表向量的方向",{"5":{"88":2}}],["代表平移",{"5":{"88":2}}],["代表了当前生成位置对信息的需求",{"5":{"95":2}}],["代表了源语言各个词语的",{"5":{"95":2}}],["代表模型对第",{"5":{"96":1}}],["代词与其指代对象的关联",{"5":{"92":2,"153":2}}],["代词可能回指句子开头提到的实体",{"5":{"88":2}}],["代词可能距离指代对象很远",{"5":{"92":2,"153":2}}],["流动",{"5":{"134":2}}],["流多处理器",{"5":{"45":1}}],["流水线并行",{"5":{"50":2}}],["流形等",{"5":{"51":2}}],["流式多处理器",{"5":{"93":2}}],["各元素的方差为",{"5":{"148":1}}],["各元素的概率值保持在合理的范围内",{"5":{"95":2}}],["各坐标分别缩放",{"5":{"143":2}}],["各向异性的噪声结构还决定了逃离鞍点的",{"5":{"149":1}}],["各向异性的噪声结构还决定了逃离鞍点的效率方向",{"5":{"149":1}}],["各向异性",{"5":{"135":2,"149":1}}],["各向同性噪声",{"5":{"149":2}}],["各向同性",{"5":{"135":2}}],["各方向收敛速度一致",{"5":{"135":2}}],["各层",{"5":{"133":1}}],["各层和都很小",{"5":{"133":1}}],["各层表示在信息平面上遵循特定的演化轨迹",{"5":{"133":2}}],["各层参数的梯度​",{"5":{"44":1}}],["各层参数的梯度",{"5":{"44":1}}],["各注意力头可以视为不同的信息分槽",{"5":{"132":2}}],["各维度需满足以下信息论下界约束",{"5":{"132":2}}],["各位置几乎均匀被关注",{"5":{"132":2}}],["各位置的注意力权重趋于均匀",{"5":{"95":2}}],["各输入位置的信息贡献概率分布",{"5":{"132":2}}],["各分解项的数学意义",{"2":{"51":1},"5":{"51":1}}],["各种稀疏注意力和线性注意力算法都利用了注意力矩阵的低秩结构",{"5":{"87":2,"152":2}}],["各种稀疏注意力算法",{"5":{"87":2,"152":2}}],["各频率成分在对数尺度上是均匀分布的",{"5":{"90":2}}],["各项正是这个函数的傅里叶基函数",{"5":{"90":1}}],["各项",{"5":{"90":1}}],["各项的物理意义如下",{"5":{"91":2}}],["各头学习到的信息可分解为各头的独立贡献减去冗余部分",{"5":{"132":2}}],["各头的输出被简单地拼接起来",{"5":{"93":2}}],["各头独立计算后简单拼接",{"5":{"93":2}}],["各行之间的",{"5":{"94":2}}],["却带来了深远的数学意义和实践效果",{"5":{"150":2}}],["却抓住了生物神经元信息处理的核心特征",{"5":{"47":2}}],["却蕴含着深刻的统计学内涵",{"5":{"51":2}}],["却表达着截然相反的事实",{"5":{"88":2}}],["却因主客关系的颠倒而意义迥异",{"5":{"88":2}}],["却与自注意力的核心运算",{"5":{"88":2}}],["却在数学上严格保证了平移等变性",{"5":{"88":2}}],["取这两部分的最小值意味着",{"5":{"154":2}}],["取前",{"5":{"143":5}}],["取前个左奇异向量更新",{"5":{"143":3}}],["取前个最大的奇异值",{"5":{"143":1}}],["取前个非零奇异值对应的特征向量构成​",{"5":{"143":1}}],["取对数后得到",{"5":{"138":1}}],["取对数后得到是一条直线",{"5":{"138":1}}],["取对数后得到对数似然函数",{"5":{"51":2}}],["取极限操作可交换",{"5":{"136":2}}],["取交集得证",{"5":{"135":2}}],["取交集得",{"5":{"134":2}}],["取负号后最小化内部表达式",{"5":{"133":2}}],["取平方得到",{"5":{"51":2}}],["取期望",{"5":{"51":1}}],["取范数",{"5":{"87":2,"152":2}}],["取决于其质量分数",{"5":{"138":2}}],["取决于具体实现",{"5":{"91":2}}],["取决于投影矩阵的学习结果",{"5":{"93":2}}],["取特定值",{"5":{"96":1}}],["欧几里得距离测量两个向量之间的直线距离",{"5":{"50":1}}],["欧几里得距离",{"5":{"50":1,"70":1}}],["欧几里得范数",{"5":{"51":2}}],["给定专家",{"5":{"158":2}}],["给定损失函数",{"5":{"148":2}}],["给定样本",{"5":{"146":2}}],["给定三个向量",{"5":{"143":2}}],["给定个点和误差参数",{"5":{"141":1}}],["给定原始概率分布",{"5":{"140":2}}],["给定词汇表",{"5":{"140":2}}],["给定数据集",{"5":{"139":2}}],["给定观测数据集",{"5":{"139":2}}],["给定总计算预算",{"5":{"138":2}}],["给定计算预算",{"5":{"138":2}}],["给定真实分布",{"5":{"137":1}}],["给定真实分布和模型分布",{"5":{"137":1}}],["给定",{"5":{"137":2,"141":1}}],["给定条件下和之间的条件互信息定义为在已知的条件下",{"5":{"137":1}}],["给定条件下的条件熵定义为在该已知信息下剩余的不确定性",{"5":{"137":1}}],["给定偏好对",{"5":{"101":2}}],["给定一个提示",{"5":{"154":2}}],["给定一个包含",{"5":{"101":1}}],["给定一个包含个序列的训练集",{"5":{"101":1}}],["给定一个正样本对",{"5":{"69":2}}],["给定一个样本",{"5":{"69":2}}],["给定一个数据分布",{"5":{"69":2}}],["给定一个方阵",{"5":{"50":2}}],["给定输入随机变量",{"5":{"133":2}}],["给定输入序列",{"5":{"101":2,"140":2}}],["给定输入和其上下文",{"5":{"71":1}}],["给定输入",{"5":{"45":1,"51":1,"71":1}}],["给定输入​",{"5":{"45":1}}],["给定输入的最优预测是关于的条件期望",{"5":{"51":1}}],["给定网络结构和输入",{"5":{"45":2}}],["给定独立同分布的样本集​",{"5":{"51":1}}],["给定独立同分布的样本集",{"5":{"51":1}}],["给定角度",{"5":{"88":2}}],["给定query向量",{"5":{"88":2}}],["给定训练好的自回归模型",{"5":{"140":2}}],["给定训练数据集",{"5":{"51":2,"140":4}}],["给定训练时见过的序列长度",{"5":{"88":2}}],["给定秩为的编码矩阵",{"5":{"89":1}}],["给定秩为",{"5":{"89":1}}],["给定足够多的频率成分",{"5":{"90":2}}],["给定两个概率分布",{"5":{"61":2}}],["给定两个点在各自平面上的位置",{"5":{"90":2}}],["给定的序列长度限制了可分析的频率范围和频率分辨率",{"5":{"90":1}}],["给定的序列长度",{"5":{"90":1}}],["给定编码向量",{"5":{"91":2}}],["给定序列长度和模型维度",{"5":{"88":1}}],["给定序列长度",{"5":{"88":1}}],["给定序列计算所有位置对的某种关联度量",{"5":{"92":1,"153":1}}],["给定序列",{"5":{"92":1,"153":1}}],["给定查询向量",{"5":{"69":2}}],["给定查询",{"5":{"95":2,"132":2}}],["给定前文的条件下",{"5":{"96":2}}],["给出了策略价值函数",{"5":{"155":2}}],["给出",{"5":{"87":1,"152":1}}],["给出的最优损失函数就是mse",{"5":{"51":2}}],["给出的点积决定",{"5":{"91":2}}],["给予位置编码稍高的学习率可以加速其学习",{"5":{"89":2}}],["降低了过拟合风险",{"5":{"158":2}}],["降低曲面的曲率",{"5":{"147":2}}],["降低表示对输入的依赖",{"5":{"133":2}}],["降低熵",{"5":{"132":2}}],["降低到",{"5":{"50":1,"86":1,"87":2,"90":1,"93":1,"151":1,"152":2,"157":2,"159":6}}],["降低对异常值的敏感性",{"5":{"51":2}}],["降低",{"5":{"93":2}}],["唯一性",{"5":{"51":2,"91":2,"92":2,"139":2,"143":1,"153":2}}],["唯一性的另一个角度是相位唯一性",{"5":{"90":2}}],["垂足就是最优预测向量",{"5":{"51":2}}],["垂线的长度",{"5":{"51":2}}],["复现困难",{"5":{"136":2}}],["复合而成的函数",{"5":{"146":1}}],["复合后仍保持凸性",{"5":{"70":2}}],["复合后的结果仍然是线性变换",{"5":{"47":2}}],["复合函数视角下的前向传播",{"2":{"45":1},"5":{"45":1}}],["复合函数",{"5":{"45":2}}],["复合函数的梯度",{"5":{"45":2}}],["复合函数的雅可比矩阵",{"5":{"45":2}}],["复合函数的偏导数法则与单变量情况类似",{"5":{"48":2}}],["复杂训练任务",{"5":{"147":1}}],["复杂的模型",{"5":{"51":2}}],["复杂的模型通常具有低偏差但高方差",{"5":{"51":2}}],["复杂度显著降低",{"5":{"141":2}}],["复杂度对比",{"5":{"140":2}}],["复杂度更精细的复杂度度量",{"5":{"133":2}}],["复杂度",{"5":{"86":2,"140":1,"151":2}}],["复杂度只能建模",{"5":{"92":1,"153":1}}],["复杂度在超长序列场景下仍然是瓶颈",{"5":{"92":1,"153":1}}],["复杂度的局限性及相应的优化方向",{"5":{"92":1,"153":1}}],["复杂度为",{"5":{"93":2}}],["复杂",{"5":{"97":1,"150":2}}],["复数的实部和虚部可以看作是在二维平面上的坐标",{"5":{"88":2}}],["复数乘法可以自然地表示为二维旋转矩阵的乘法",{"5":{"88":2}}],["复数乘以得到",{"5":{"88":1}}],["复数",{"5":{"88":1}}],["复数系数",{"5":{"90":2}}],["复数形式的统一分析",{"2":{"90":1},"5":{"90":1}}],["复数形式的优点在于",{"5":{"90":2}}],["复数形式的优雅之处在于",{"5":{"90":2}}],["复数编码的内积具有简洁的形式",{"5":{"90":2}}],["复数编码提供了理解位置变换的另一个视角",{"5":{"90":2}}],["复内积",{"5":{"90":2}}],["测量两个向量之间的直线距离",{"5":{"50":1}}],["测量两个向量方向的相似程度",{"5":{"50":1}}],["测试损失",{"5":{"159":2}}],["测试时高熵可能表示输入来自分布外样本",{"5":{"137":2}}],["测试误差更稳定",{"5":{"136":2}}],["测试数据分布为",{"5":{"137":2}}],["测试数据",{"5":{"132":2}}],["测试mse达到最小值",{"5":{"51":2}}],["测试mse",{"5":{"51":2}}],["测试序列长度显著超过训练序列长度",{"5":{"89":2}}],["良好",{"5":{"51":2,"97":1,"148":2}}],["良好的表示",{"5":{"71":1}}],["良好的表示应该最大化",{"5":{"71":1}}],["良好的表示应该在压缩输入信息的同时保留与任务相关的信息",{"5":{"87":2,"152":2}}],["良好的理论性质和天然的长度外推能力",{"5":{"91":2}}],["好的表示应该能用较少的信息描述数据",{"5":{"137":2}}],["好的表示应该保持输入数据的拓扑结构",{"5":{"133":2}}],["好的设计不是添加复杂性",{"5":{"88":2}}],["好",{"5":{"51":2,"97":2,"101":2,"154":4}}],["融合计算",{"2":{"61":1},"5":{"61":2}}],["融合计算的核心思想是直接计算损失值",{"5":{"61":2}}],["融合计算不仅适用于前向传播",{"5":{"61":2}}],["融合梯度计算",{"2":{"61":1},"5":{"61":1}}],["融合",{"5":{"61":1,"91":2}}],["融合实现",{"5":{"97":2}}],["克罗内克函数",{"5":{"44":2}}],["克劳德",{"5":{"61":2}}],["香农熵估计",{"5":{"137":2}}],["香农熵",{"2":{"61":1},"5":{"61":4}}],["香农",{"5":{"61":2}}],["惊喜",{"5":{"61":2}}],["稀释",{"5":{"144":2}}],["稀疏度",{"5":{"159":2}}],["稀疏的奖励信号中学习最优的行为策略",{"5":{"156":2}}],["稀疏因子",{"5":{"143":1}}],["稀疏",{"5":{"141":2}}],["稀疏性诱导的秩缩减",{"5":{"141":2}}],["稀疏softmax",{"5":{"69":2,"101":2}}],["稀疏编码通常更高效地利用参数",{"5":{"71":2}}],["稀疏表示在信息论上具有几个优势",{"5":{"71":2}}],["稀疏表示对噪声更加鲁棒",{"5":{"71":2}}],["稀疏表示提供了更好的可解释性",{"5":{"71":2}}],["稀疏注意力和头剪枝的信息论方法",{"5":{"132":2}}],["稀疏注意力和分层注意力",{"5":{"50":2}}],["稀疏注意力引入的信息损失反映了稀疏模式的有效性",{"5":{"132":2}}],["稀疏注意力模式",{"5":{"132":2}}],["稀疏注意力等方法都建立在信息论原理之上",{"5":{"132":2}}],["稀疏注意力的信息损失",{"5":{"132":2}}],["稀疏注意力的数学框架",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["稀疏注意力的复杂度分析",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["稀疏注意力的关键是设计合适的稀疏模式​",{"5":{"86":1,"151":1}}],["稀疏注意力的关键是设计合适的稀疏模式",{"5":{"86":1,"151":1}}],["稀疏注意力的谱性质",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["稀疏注意力",{"5":{"86":2,"87":2,"151":2,"152":2}}],["稀疏注意力显著降低了计算复杂度",{"5":{"86":2,"151":2}}],["稀疏注意力也引入了新的计算挑战",{"5":{"86":2,"151":2}}],["稀疏注意力矩阵的非零元素比例为",{"5":{"86":2,"151":2}}],["稀疏注意力矩阵的谱性质取决于稀疏模式",{"5":{"87":2,"152":2}}],["稀疏模式可以分为以下几类",{"5":{"86":2,"151":2}}],["稀疏模式的学习与优化",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["稀疏模式的不规则内存访问可能导致缓存命中率降低",{"5":{"86":2,"151":2}}],["稀疏模式的选择对谱性质有重要影响",{"5":{"87":2,"152":2}}],["稀疏矩阵运算在现代硬件",{"5":{"86":2,"151":2}}],["稀疏激活通过top",{"5":{"159":2}}],["稀疏激活导致的非连续内存访问可能无法充分利用硬件的并行计算能力",{"5":{"159":2}}],["稀疏激活也带来了新的挑战",{"5":{"159":2}}],["稀疏激活将其从",{"5":{"159":2}}],["稀疏激活的另一个重要优势体现在内存带宽利用率上",{"5":{"159":2}}],["稀疏激活的效率优势",{"2":{"159":1},"5":{"159":1}}],["稀疏激活是moe架构实现计算效率提升的关键机制",{"5":{"159":2}}],["稀疏激活是moe区别于其他架构的关键特征",{"5":{"159":2}}],["稀疏激活可以表示为引入一个选择函数",{"5":{"159":2}}],["稀疏激活",{"5":{"93":6}}],["稀疏激活引入了一种非线性",{"5":{"93":2}}],["稀有事件",{"5":{"61":2}}],["越小",{"5":{"144":2}}],["越大允许的偏离越大",{"5":{"158":2}}],["越大",{"5":{"144":2,"149":2,"158":2,"159":2}}],["越大表示压缩越强",{"5":{"137":2}}],["越高表示学习越高效",{"5":{"133":2}}],["越高表示知识越紧凑",{"5":{"133":2}}],["越不确定",{"5":{"61":2}}],["越确定",{"5":{"61":2}}],["越接近",{"5":{"61":3}}],["简化形式",{"5":{"101":2}}],["简单的线性门控可能无法找到一条直线将两个簇的路由决策分开",{"5":{"159":2}}],["简单的线性softmax门控存在明显的局限性",{"5":{"159":2}}],["简单的策略梯度方法往往面临严峻的训练稳定性问题",{"5":{"154":2}}],["简单的模型",{"5":{"51":2}}],["简单的模型通常具有高偏差但低方差",{"5":{"51":2}}],["简单但通常导致梯度消失",{"5":{"148":1}}],["简单",{"5":{"147":1,"150":4}}],["简单地堆砌参数量而不相应增加数据量",{"5":{"138":2}}],["简单rnn的梯度分析",{"2":{"41":1},"5":{"41":1}}],["简单性与拟合度的平衡",{"5":{"139":2}}],["简单性与表达能力之间存在权衡",{"5":{"41":2}}],["简单性有时比数学上的",{"5":{"71":2}}],["简称svd",{"5":{"143":2}}],["简称dpo",{"5":{"101":2}}],["简称ppo",{"5":{"101":2}}],["简称rlhf",{"5":{"101":2}}],["简称mlm",{"5":{"101":2}}],["简称llms",{"5":{"101":2}}],["简称nce",{"5":{"69":2}}],["简称kl散度",{"5":{"61":2}}],["简称",{"5":{"61":2}}],["$m或",{"5":{"146":1}}],["$a",{"5":{"143":1}}],["$r",{"5":{"142":1}}],["$是fisher信息矩阵",{"5":{"142":2}}],["$t",{"5":{"141":1}}],["$$硬阈值产生更加稀疏的结果",{"5":{"144":2}}],["$$",{"5":{"46":2,"61":8,"86":2,"151":2}}],["$",{"5":{"61":2,"86":1,"97":2,"141":1,"142":5,"143":1,"146":5,"151":1}}],["$dhd",{"5":{"86":1,"151":1}}],["展开期望损失",{"5":{"145":2}}],["展开运算",{"5":{"143":2}}],["展开",{"5":{"137":2}}],["展开后得到四项",{"5":{"91":2}}],["展示它与注意力机制中",{"5":{"70":2}}],["展示这些看似不同的数学结构如何殊途同归",{"5":{"69":2}}],["展示了权重衰减如何通过",{"5":{"144":2}}],["展示了其",{"5":{"143":2}}],["展示了损失函数理论在大模型架构设计中的核心地位",{"5":{"70":2}}],["展示了",{"5":{"61":2,"146":2}}],["展示了如何使用复指数函数简洁地表示位置编码",{"5":{"90":2}}],["展示了它们如何支持相对位置相关的注意力模式",{"5":{"92":2,"153":2}}],["展现了深度表示学习的数学基础",{"5":{"45":2}}],["默认实现已经采用了融合计算策略",{"5":{"61":2}}],["整理得",{"5":{"144":2}}],["整幅图像可以用远少于",{"5":{"143":1}}],["整幅图像可以用远少于个参数来有效描述",{"5":{"143":1}}],["整个乘积的范数也可能很小",{"5":{"148":2}}],["整个数据集的似然函数可以分解为所有位置",{"5":{"140":2}}],["整个数据集的似然函数为",{"5":{"61":2}}],["整个网络的所有参数都会被激活并参与计算",{"5":{"159":2}}],["整个网络的各层在信息平面上形成一条轨迹",{"5":{"133":2}}],["整个网络仍然等价于一个简单的线性变换",{"5":{"71":2}}],["整个编码向量是不同频率复指数序列的实部和虚部的组合",{"5":{"90":2}}],["整个计算可以视为对输入进行一系列线性变换和非线性变换的组合",{"5":{"93":1}}],["整个计算可以视为对输入",{"5":{"93":1}}],["整个序列的信息",{"5":{"92":2,"153":2}}],["整个序列或批次的损失是这些位置损失的均值或和",{"5":{"96":2}}],["整数",{"5":{"61":2}}],["整体任务的性能就会突然跃升",{"5":{"138":2}}],["整体预测性能接近随机",{"5":{"138":2}}],["整体缩放梯度的大小",{"5":{"69":2}}],["整体位置",{"5":{"91":2}}],["编码真实分布",{"5":{"137":1}}],["编码在保留关于输入的有用信息与压缩关于输入的冗余之间取得平衡",{"5":{"137":1}}],["编码器输出均值和对数方差",{"5":{"133":2}}],["编码器输出的变分后验分布",{"5":{"133":2}}],["编码器层输出",{"5":{"132":2}}],["编码器表示的信息分析",{"5":{"132":2}}],["编码器",{"5":{"132":2,"133":2}}],["编码函数",{"5":{"69":1}}],["编码函数定义了相似度函数",{"5":{"69":1}}],["编码",{"5":{"61":2,"90":2,"91":2,"137":1}}],["编码来自分布",{"5":{"137":1}}],["编码来自",{"5":{"61":2}}],["编码后计算attention",{"5":{"88":2}}],["编码成",{"5":{"88":2}}],["编码矩阵中的所有元素都是可学习的参数",{"5":{"89":1}}],["编码矩阵的具体数值完全由训练数据决定",{"5":{"89":1}}],["编码矩阵的秩",{"5":{"89":1}}],["编码矩阵的秩会逐渐增加",{"5":{"89":2}}],["编码矩阵的秩与长度外推能力之间存在重要的关系",{"5":{"89":2}}],["编码矩阵的奇异值分布揭示了编码的结构特性",{"5":{"89":2}}],["编码矩阵的低秩近似是分析和压缩可学习位置编码的重要工具",{"5":{"89":2}}],["编码矩阵通常可以达到满秩",{"5":{"89":2}}],["编码矩阵秩的上界为",{"5":{"89":2}}],["编码矩阵接近奇异",{"5":{"89":2}}],["编码矩阵",{"5":{"89":3}}],["编码向量应该形成一个能够唯一标识每个位置的配置",{"5":{"89":2}}],["编码向量之间的角度关系可以通过gram矩阵分析",{"5":{"89":2}}],["编码向量的维度决定了它最多能够区分个位置",{"5":{"89":1}}],["编码向量的维度决定了它最多能够区分",{"5":{"89":1}}],["编码向量的偶数维度",{"5":{"91":2}}],["编码向量的分散程度可以用方差矩阵来度量",{"5":{"89":2}}],["编码向量的分布具有某种",{"5":{"91":2}}],["编码向量越",{"5":{"89":4}}],["编码向量序列是线性无关的",{"5":{"90":2}}],["编码向量可以张成整个​空间",{"5":{"89":1}}],["编码向量可以张成整个",{"5":{"89":1}}],["编码向量可以视为高维空间中的点",{"5":{"91":2}}],["编码向量在球面上均匀分布",{"5":{"91":2}}],["编码向量在二维投影空间中形成螺旋或圆形分布",{"5":{"91":2}}],["编码向量集合张成一个维的子空间",{"5":{"91":1}}],["编码向量集合",{"5":{"91":1}}],["编码被限制在某个低维子空间中",{"5":{"89":2}}],["编码的有效表达能力受到训练数据分布的影响",{"5":{"89":2}}],["编码的表达能力与泛化能力之间存在权衡",{"5":{"89":2}}],["编码的唯一性由各频率成分的线性无关性保证",{"5":{"90":2}}],["编码的唯一性与可辨识性",{"2":{"91":1},"5":{"91":1}}],["编码的生成和计算应当高效",{"5":{"91":2}}],["编码有足够的容量表示所有位置",{"5":{"89":2}}],["编码容量不足",{"5":{"89":2}}],["编码主要捕获位置的主要变化模式",{"5":{"89":2}}],["编码可以预先计算并缓存",{"5":{"89":2}}],["编码维度​是表达能力的硬上限",{"5":{"89":1}}],["编码维度",{"5":{"89":1}}],["编码充分利用了​维空间",{"5":{"89":1}}],["编码充分利用了",{"5":{"89":1}}],["编码公式中的正弦和余弦函数不是随意选择的",{"5":{"90":2}}],["编码差可以表示为",{"5":{"90":2}}],["编码就能够为每个位置生成唯一的表示",{"5":{"90":2}}],["编码应当能够处理训练时未见过的序列长度",{"5":{"91":2}}],["编码语义相似性",{"5":{"91":2}}],["编码位置之间的几何关系",{"5":{"91":2}}],["编码接近",{"5":{"91":2}}],["编码点沿螺旋轨迹移动",{"5":{"91":2}}],["下选择动作",{"5":{"156":2}}],["下选择特定动作",{"5":{"155":2}}],["下执行动作",{"5":{"156":6}}],["下可以执行的操作",{"5":{"156":2}}],["下被选中的概率",{"5":{"155":2}}],["下采取特定动作",{"5":{"155":2}}],["下确定的常数",{"5":{"155":2}}],["下生成回复",{"5":{"154":2}}],["下面给出三种归一化方法的完整公式汇总",{"5":{"146":2}}],["下面我们分别考虑凸函数和非凸函数两种情形",{"5":{"136":2}}],["下的表示",{"5":{"150":2}}],["下的似然函数为",{"5":{"140":1}}],["下的期望",{"5":{"70":1}}],["下映射到自身",{"5":{"136":1}}],["下界估计的保守性",{"5":{"133":2}}],["下界估计",{"5":{"133":2}}],["下界",{"5":{"132":2,"137":2}}],["下界性质表明",{"5":{"61":1}}],["下界性质",{"5":{"61":1}}],["下",{"5":{"69":2,"93":2,"97":2,"133":2,"141":1,"154":2,"156":4,"159":2}}],["下一状态序列",{"5":{"155":2}}],["下一个token预测的数学框架",{"2":{"101":1},"5":{"101":1}}],["下一个词的选择遵循某种概率分布",{"5":{"96":2}}],["下一个词的条件概率分布的期望特性",{"5":{"96":2}}],["下一节将从",{"5":{"70":2}}],["下一节我们将讨论损失函数与优化目标",{"5":{"45":2}}],["下一节",{"5":{"89":2,"91":2,"92":2,"93":2,"95":2,"153":2}}],["下一层的输入分布会发生偏移",{"5":{"42":2}}],["下一步无法开始",{"5":{"92":2,"153":2}}],["下观察到该样本集的概率",{"5":{"61":2}}],["下降",{"5":{"97":2,"133":1,"136":4}}],["下进行大部分计算来加速训练",{"5":{"97":2}}],["下计算",{"5":{"97":2}}],["经典的朗之万动力学描述了粒子在热浴中的布朗运动",{"5":{"149":2}}],["经过某种变换",{"5":{"150":2}}],["经过深层累积后也会产生巨大的影响",{"5":{"148":2}}],["经过dropout处理后",{"5":{"145":2}}],["经过旋转后",{"5":{"143":2}}],["经过训练的网络权重矩阵通常具有较低的",{"5":{"143":2}}],["经过代数推导可得收敛界",{"5":{"136":2}}],["经过交叉熵的概率论推导",{"5":{"101":2}}],["经过层后",{"5":{"41":1,"42":1,"148":1}}],["经过",{"5":{"41":1,"42":1,"61":2,"87":2,"92":1,"143":4,"144":2,"148":1,"152":2,"153":1}}],["经过激活函数的输出为",{"5":{"71":2}}],["经过线性变换后",{"5":{"47":1}}],["经过线性变换",{"5":{"47":1}}],["经过线性变换得到的",{"5":{"87":1,"152":1}}],["经过线性投影并拆分注意力头后",{"5":{"50":2}}],["经过注意力计算后的输出张量会与输入张量形状相同",{"5":{"50":2}}],["经过sigmoid变换后",{"5":{"71":2}}],["经过softmax归一化后",{"5":{"87":2,"152":2}}],["经过softmax变换后",{"5":{"87":2,"152":2}}],["经过足够多次的",{"5":{"87":2,"152":2}}],["经过rope编码后变为",{"5":{"88":2}}],["经过rope编码后的query",{"5":{"88":2}}],["经过步传播后",{"5":{"87":1,"152":1}}],["经过步传递后",{"5":{"92":1,"153":1}}],["经过第一层注意力后得到",{"5":{"94":2}}],["经过投影矩阵​变换后得到query向量​",{"5":{"94":1}}],["经过投影矩阵",{"5":{"94":1}}],["经过缩放后",{"5":{"95":2}}],["经验观察",{"2":{"142":1},"5":{"142":1}}],["经验熵以",{"5":{"137":1}}],["经验熵以的速率收敛到真实熵",{"5":{"137":1}}],["经验熵估计存在系统性偏差",{"5":{"137":2}}],["经验风险与泛化风险的差距更小",{"5":{"136":2}}],["经验风险的最小化在一定条件下",{"5":{"61":2}}],["经验上",{"5":{"69":2}}],["经验分布收敛于真实数据分布",{"5":{"61":2}}],["经验证据表明",{"5":{"87":2,"152":2}}],["尽可能接近",{"5":{"158":2}}],["尽可能接近1",{"5":{"148":1}}],["尽可能接近真实数据分布",{"5":{"61":2}}],["尽可能大",{"5":{"137":1}}],["尽管模型定义了",{"5":{"159":2}}],["尽管存在这些挑战",{"5":{"154":2}}],["尽管程度较sigmoid轻",{"5":{"148":2}}],["尽管有放缩版本的dropout在期望上保持一致",{"5":{"145":2}}],["尽管如此",{"5":{"145":2}}],["尽管只能保证收敛到局部极小值",{"5":{"143":2}}],["尽管信息瓶颈理论提供了深刻的理论洞见",{"5":{"133":2}}],["尽管softmax是当前损失函数设计的主流选择",{"5":{"101":2}}],["尽管整体上是非线性的",{"5":{"45":2}}],["尽管单个神经元结构极其简单",{"5":{"47":2}}],["尽管adam在实践中表现优异",{"5":{"48":2}}],["尽管超长上下文",{"5":{"88":2}}],["尽管rope具有诸多优势",{"5":{"88":2}}],["尽管编码矩阵有个参数",{"5":{"89":1}}],["尽管编码矩阵有",{"5":{"89":1}}],["尽管昨天下了很大的雨",{"5":{"92":2,"153":2}}],["尽管",{"5":{"92":2,"153":2}}],["尽管多头注意力有个头",{"5":{"93":1}}],["尽管多头注意力有个注意力头",{"5":{"93":1}}],["尽管多头注意力有",{"5":{"93":2}}],["关闭",{"5":{"145":2}}],["关系",{"5":{"138":2}}],["关于门控网络参数",{"5":{"159":2}}],["关于门控权重和专家参数的梯度计算需要应用链式法则",{"5":{"157":2}}],["关于负样本键",{"5":{"69":1}}],["关于负样本键的梯度",{"5":{"69":1}}],["关于键向量",{"5":{"69":1}}],["关于键向量的梯度",{"5":{"69":1}}],["关于查询向量",{"5":{"69":1}}],["关于查询向量的梯度",{"5":{"69":1}}],["关于第一层权重",{"5":{"41":1}}],["关于输入",{"5":{"71":1,"87":2,"152":2}}],["关于为什么深度神经网络能够有效优化",{"5":{"48":2}}],["关于变量",{"5":{"48":1}}],["关于",{"5":{"48":1,"51":1,"70":2,"92":2,"96":1,"97":1,"139":1,"153":2,"158":2}}],["关于参数和的梯度为",{"5":{"51":1}}],["关于参数",{"5":{"51":1}}],["关于特征值1的特征向量",{"5":{"87":1,"152":1}}],["关注更多位置",{"5":{"132":2}}],["关注程度",{"5":{"61":2,"69":2}}],["关注",{"5":{"86":4,"89":2,"95":2,"151":4}}],["关注的循环依赖",{"5":{"86":1,"151":1}}],["关注的是位置之间的相对关系",{"5":{"88":2}}],["关注位置",{"5":{"91":1,"92":2,"153":2}}],["关注输入的局部窗口",{"5":{"92":2,"153":2}}],["关键是调整",{"5":{"136":2}}],["关键学习率定义",{"5":{"134":2}}],["关键发现",{"5":{"70":2}}],["关键的结果是",{"5":{"149":2}}],["关键的难点在于估计",{"5":{"148":1}}],["关键的难点在于估计的上界或下界",{"5":{"148":1}}],["关键的理解在于",{"5":{"88":2}}],["关键的超参数包括",{"5":{"89":2}}],["关键的区别在于卷积的局部性和注意力的全局性",{"5":{"93":2}}],["关键在于模型是否能够学习到正确的注意力权重模式",{"5":{"92":2,"153":2}}],["关键在于",{"5":{"94":2}}],["机制直接限制策略比率的变化范围",{"5":{"154":2}}],["机制使得网络能够同时利用不同抽象层次的特征",{"5":{"150":2}}],["机制与均方误差中的梯度裁剪有类似的功能",{"5":{"101":2}}],["机制的几何效果是",{"5":{"69":2}}],["机制可以被理解为一种",{"5":{"61":2}}],["机制中",{"5":{"61":2}}],["机制",{"5":{"87":2,"152":2}}],["机制通过并行运行多个独立的注意力头",{"5":{"93":2}}],["得证",{"5":{"144":2}}],["得",{"5":{"134":2}}],["得到因子矩阵",{"5":{"143":2}}],["得到截断svd",{"5":{"143":2}}],["得到二元语法",{"5":{"140":2}}],["得到样本属于正类的概率估计",{"5":{"71":2}}],["得到期望mse",{"5":{"51":2}}],["得到奇异值序列",{"5":{"87":2,"152":2}}],["得到",{"5":{"87":2,"88":1,"90":1,"136":2,"152":2}}],["得到的注意力矩阵定义为",{"5":{"87":1,"152":1}}],["得到的注意力矩阵",{"5":{"87":1,"152":1}}],["得到两个维的向量3",{"5":{"88":1}}],["得到两个",{"5":{"88":1}}],["得到各自的输出",{"5":{"93":2}}],["得到多头注意力的最终输出",{"5":{"93":2}}],["得到query矩阵",{"5":{"94":2}}],["得到查询向量​",{"5":{"94":1}}],["得到查询向量",{"5":{"94":1}}],["得到生成当前词所需的信息",{"5":{"95":2}}],["得到注意力权重矩阵",{"5":{"95":2}}],["得到最终的聚合结果",{"5":{"95":2}}],["得到最终的输出矩阵",{"5":{"95":2}}],["得到词汇表上各词元的生成概率分布",{"5":{"96":2}}],["得分最高的专家仍然获得最高的权重",{"5":{"159":2}}],["得分最高的专家仍然获得最高的概率",{"5":{"158":2}}],["得分",{"5":{"61":2}}],["固有随机性",{"5":{"51":2}}],["固定而",{"5":{"159":2}}],["固定阈值",{"5":{"147":1}}],["固定为",{"5":{"145":1}}],["固定n",{"5":{"140":1}}],["固定时",{"5":{"137":1}}],["固定的",{"5":{"61":2}}],["固定的形式",{"5":{"91":2}}],["固定稀疏模式",{"5":{"86":2,"151":2}}],["固定",{"5":{"89":2,"159":2}}],["固定+可学习",{"5":{"89":2}}],["哪个分布是",{"5":{"61":2}}],["递推关系",{"5":{"134":2}}],["递推关系的等价性",{"5":{"134":2}}],["递推地",{"5":{"45":2}}],["递归形式的线性注意力",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["递归形式的另一个优势是支持",{"5":{"86":2,"151":2}}],["递归函数",{"5":{"92":2,"153":2}}],["长对话",{"5":{"86":2,"151":2}}],["长对话建模",{"5":{"95":2}}],["长距离依赖丢失",{"5":{"140":2}}],["长距离回指",{"5":{"88":2}}],["长距离衰减问题是一个实践中的挑战",{"5":{"88":1}}],["长距离衰减问题",{"5":{"88":1}}],["长度外推",{"5":{"91":2,"92":2,"153":2}}],["长程依赖与位置编码",{"5":{"132":2}}],["长程依赖是序列建模中的核心挑战",{"5":{"132":2}}],["长程依赖的信息论分析",{"2":{"132":1},"5":{"132":1}}],["长程依赖的建模存在",{"5":{"92":2,"153":2}}],["长程依赖建模机制",{"5":{"86":2,"151":2}}],["长程依赖建模和位置编码等多个角度深入分析了注意力机制的数学原理",{"5":{"87":2,"152":2}}],["长程依赖",{"5":{"92":2,"153":2}}],["长序列中的效率优势",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["长序列是检验长程依赖建模能力的关键场景",{"5":{"92":2,"153":2}}],["探测器",{"5":{"71":2}}],["探讨计算最优配置的策略",{"5":{"138":2}}],["探讨了深度学习训练中可能出现的混沌现象及其对结果复现性的影响",{"5":{"136":2}}],["探讨它们的数学原理",{"5":{"86":2,"151":2}}],["探讨其建模长程依赖的数学原理",{"5":{"92":2,"153":2}}],["探讨其低秩结构及其对模型行为的影响",{"5":{"93":2}}],["探索根据输入内容自适应地调整位置编码",{"5":{"88":1}}],["探索",{"5":{"92":2,"132":2,"153":2}}],["读者应能够理解反向传播的工作原理",{"5":{"44":2}}],["读者应该能够从线性代数和数值分析的角度深入理解注意力矩阵的数学本质",{"5":{"87":2,"152":2}}],["读者应该能够从矩阵分析的角度深入理解可学习位置编码的性质",{"5":{"89":2}}],["读者应该能够从数学层面深入理解注意力机制为何能够有效建模长程依赖",{"5":{"92":2,"153":2}}],["读者应该能够从数学层面深入理解多头注意力的架构设计",{"5":{"93":2}}],["读者应该建立起对mse的全面深入理解",{"5":{"51":2}}],["读者应该建立起对位置编码频率空间本质的深入理解",{"5":{"90":2}}],["读者将建立起对位置编码频率空间本质的深入理解",{"5":{"90":2}}],["读者将建立起对多头注意力数学本质的完整理解",{"5":{"93":2}}],["读者将建立起对注意力变体全景图的理解",{"5":{"86":2,"151":2}}],["读者将建立起对注意力机制内部运作机制的完整理解",{"5":{"94":2}}],["远超任何计算设备的处理能力",{"5":{"140":2}}],["远大于隐藏维度",{"5":{"141":1}}],["远大于",{"5":{"138":2}}],["远离",{"5":{"70":1}}],["远小于标准注意力的100",{"5":{"86":2,"151":2}}],["远小于标准注意力的",{"5":{"86":2,"151":2}}],["远小于其维度",{"5":{"87":2,"152":2}}],["远小于原始的​",{"5":{"89":1}}],["远小于原始的",{"5":{"89":1}}],["远距离位置的梯度贡献决定了模型能否有效学习长程依赖",{"5":{"132":2}}],["远距离位置的编码向量相距较远",{"5":{"91":2}}],["远距离梯度的重要性",{"5":{"132":2}}],["远距离依赖关系可以更快速地建立",{"5":{"132":2}}],["远距离关系",{"5":{"91":2}}],["远程依赖的边界效应",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["滑动窗口注意力",{"5":{"86":2,"151":2}}],["滑动窗口注意力类似于带状矩阵",{"5":{"86":2,"151":2}}],["膨胀窗口注意力",{"5":{"86":2,"151":2}}],["缺点",{"5":{"147":1}}],["缺点是理论解释相对复杂",{"5":{"146":2}}],["缺点是计算量略大于批归一化",{"5":{"146":2}}],["缺点是对批量大小敏感",{"5":{"146":2}}],["缺点是仍然需要计算完整的注意力分数矩阵",{"5":{"86":2,"151":2}}],["缺点是训练初期位置信息完全缺失",{"5":{"89":2}}],["缺乏概率分布差异的语义解释",{"5":{"61":2}}],["令导数为零得",{"5":{"145":2}}],["令",{"5":{"86":2,"94":2,"143":2,"151":2}}],["傅里叶特征映射",{"5":{"86":2,"151":2}}],["傅里叶级数是分析周期函数的经典工具",{"5":{"90":2}}],["傅里叶级数展开定义为",{"5":{"90":2}}],["傅里叶级数的深刻意义在于",{"5":{"90":2}}],["傅里叶级数可以更加简洁地表示",{"5":{"90":2}}],["傅里叶分析揭示了时间域",{"5":{"90":2}}],["傅里叶变换的核心是对偶性",{"5":{"90":2}}],["傅里叶变换和正交基扩展是信号处理和自然语言处理中的重要工具",{"5":{"50":2}}],["傅里叶变换和微分方程等运算下保持封闭形式",{"5":{"96":2}}],["傅里叶系数和​通过以下积分计算",{"5":{"90":1}}],["傅里叶系数",{"5":{"90":1}}],["旧信息的能力",{"5":{"86":2,"151":2}}],["累积",{"5":{"148":1}}],["累积雅可比矩阵的范数也不会指数级地增长或衰减",{"5":{"148":2}}],["累积层后的梯度范数上界为",{"5":{"148":1}}],["累积的期望缩放因子可能变得非常小",{"5":{"145":2}}],["累积能量比满足",{"5":{"142":1}}],["累积能量比",{"5":{"142":3}}],["累积历史梯度信息",{"5":{"136":2}}],["累积衰减模型",{"5":{"41":2}}],["累积量可以写为",{"5":{"86":2,"151":2}}],["累积到",{"5":{"94":1}}],["造成内存带宽瓶颈",{"5":{"86":2,"151":2}}],["算法正是为了解决这一问题而被提出的",{"5":{"154":2}}],["算法中的自适应更新可视为对",{"5":{"135":2}}],["算法的完整更新为",{"5":{"134":2}}],["算法的正确性依赖于softmax的数学性质",{"5":{"86":2,"151":2}}],["算法2",{"5":{"44":2}}],["算法流程如下",{"5":{"86":2,"151":2}}],["算法",{"5":{"97":4,"132":2,"133":6,"135":4,"136":2,"137":8,"143":2}}],["≈",{"5":{"86":2,"151":2}}],["异常检测",{"5":{"137":2}}],["异常检测等场景",{"5":{"137":2}}],["异常样本的误差为",{"5":{"51":2}}],["异常值的影响是线性的",{"5":{"51":2}}],["异步执行",{"5":{"86":2,"151":2}}],["条完整的轨迹",{"5":{"155":2}}],["条轨迹中时刻",{"5":{"155":2}}],["条轨迹的长度",{"5":{"155":2}}],["条件计算将传统的确定性函数映射",{"5":{"159":2}}],["条件计算的核心洞见在于",{"5":{"159":2}}],["条件计算",{"2":{"159":1},"5":{"159":3}}],["条件计算与稀疏模型",{"2":{"131":1},"4":{"106":1,"107":1,"108":1,"157":1,"158":1,"159":1},"5":{"131":7}}],["条件的隐式引入来稳定训练过程",{"5":{"147":2}}],["条件与裁剪",{"5":{"147":2}}],["条件可能不成立或",{"5":{"147":2}}],["条件下输出词元",{"5":{"140":1}}],["条件下",{"5":{"137":3}}],["条件下的条件熵是对所有可能的条件熵的期望",{"5":{"137":1}}],["条件",{"2":{"147":1},"5":{"136":2,"143":2,"147":3}}],["条件互信息满足链式法则",{"5":{"137":2}}],["条件互信息的链式法则",{"5":{"137":2}}],["条件互信息",{"5":{"137":2}}],["条件互信息正则化和分层信息瓶颈等方法",{"5":{"133":2}}],["条件互信息为分析和优化多任务学习提供了信息论工具",{"5":{"133":2}}],["条件互信息与多任务学习",{"2":{"133":1},"5":{"133":1}}],["条件众数",{"5":{"70":1}}],["条件概率",{"5":{"140":1}}],["条件概率中的历史上下文包含了生成第个词元所需的所有相关信息",{"5":{"140":1}}],["条件概率生成",{"5":{"140":2}}],["条件概率最大的类别",{"5":{"70":2}}],["条件概率密度函数为",{"5":{"51":2}}],["条件数的病态性",{"5":{"149":2}}],["条件数的改善直接转化为优化速度的提升",{"5":{"144":2}}],["条件数的改善",{"5":{"144":2}}],["条件数的几何意义",{"5":{"135":2}}],["条件数保持不变",{"5":{"135":2}}],["条件数即为最大与最小曲率之比",{"5":{"135":2}}],["条件数衡量了损失",{"5":{"135":2}}],["条件数影响显现",{"5":{"134":2}}],["条件数对收敛速度的影响",{"5":{"134":2}}],["条件数对训练稳定性有重要影响",{"5":{"87":2,"152":2}}],["条件数越大",{"5":{"41":2,"97":4,"136":2}}],["条件数与梯度稳定性",{"5":{"41":2}}],["条件数与数值稳定性",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["条件数定义为最大奇异值与最小奇异值的比值",{"5":{"87":2,"152":2}}],["条件数接近1",{"5":{"87":2,"152":2}}],["条件数为无穷大",{"5":{"87":4,"152":4}}],["条件数为1",{"5":{"87":2,"152":2}}],["条件数为",{"5":{"89":2,"144":2}}],["条件数较小",{"5":{"87":2,"152":2}}],["条件数较大",{"5":{"87":2,"152":2}}],["条件数反映了编码矩阵的数值稳定性",{"5":{"89":2}}],["条件数",{"5":{"97":2,"134":2,"135":2}}],["条件期望",{"5":{"51":2,"96":1}}],["条件期望与最优预测",{"2":{"51":1},"5":{"51":1}}],["条件期望也被称为回归函数",{"5":{"51":2}}],["条件期望是在给定某些信息的条件下对随机变量的期望",{"5":{"96":2}}],["条件期望是的函数",{"5":{"96":1}}],["条件期望的这一性质奠定了mse在回归任务中的理论基础",{"5":{"51":2}}],["条件期望的一个重要性质是全期望公式",{"5":{"96":2}}],["条件期望用于描述在给定上下文的条件下",{"5":{"96":2}}],["条件分布是多元高斯分布最优美也最实用的性质之一",{"5":{"96":2}}],["条件分布在高斯过程回归",{"5":{"96":2}}],["条件熵小于等于无条件熵表明已知",{"5":{"137":1}}],["条件熵小于等于无条件熵表明已知的信息不会增加关于的不确定性",{"5":{"137":1}}],["条件熵满足",{"5":{"137":1}}],["条件熵满足且",{"5":{"137":1}}],["条件熵的非负性与单调性",{"5":{"137":2}}],["条件熵越低",{"5":{"132":2}}],["条件熵越高",{"5":{"132":2}}],["条件熵与注意力",{"5":{"132":2}}],["条件熵",{"5":{"96":2,"137":2}}],["条件协方差为​",{"5":{"96":1}}],["条件协方差为",{"5":{"96":1}}],["条对角线上",{"5":{"86":1,"87":1,"151":1,"152":1}}],["溢出",{"5":{"86":1,"151":1}}],["字节",{"5":{"86":2,"151":2}}],["泛化与低秩平坦性",{"5":{"142":2}}],["泛化保证的理论分析",{"2":{"142":1},"5":{"142":1}}],["泛化波动",{"5":{"136":2}}],["泛化理论与信息瓶颈",{"5":{"133":2}}],["泛化对应",{"5":{"133":2}}],["泛化误差最小",{"5":{"133":2}}],["泛化误差可表示为输入输出互信息的函数",{"5":{"133":2}}],["泛化误差上界越大",{"5":{"133":2}}],["泛化误差满足以下信息论上界",{"5":{"133":2}}],["泛化误差通常较小",{"5":{"87":2,"152":2}}],["泛化界之间的理论联系",{"5":{"133":2}}],["泛化界的形式一致",{"5":{"133":2}}],["泛化界",{"5":{"133":2}}],["泛化能力的理论分析",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["泛化性能通常更好",{"5":{"134":2}}],["泛化性讨论",{"2":{"61":1},"5":{"61":1}}],["泛化性",{"5":{"91":2}}],["始终保留全部输入",{"5":{"150":2}}],["始终非负",{"5":{"51":1}}],["始终为正",{"5":{"87":2,"152":2}}],["头的注意力权重为",{"5":{"132":1}}],["头的互补性和冗余度分析为多头注意力的设计提供了理论依据",{"5":{"132":2}}],["头的剪枝建议",{"5":{"132":2}}],["头的剪枝排序",{"5":{"132":2}}],["头学习冗余信息",{"5":{"132":2}}],["头学习不同信息",{"5":{"132":2}}],["头和之间的互补性定义为联合信息与各自信息之和的差",{"5":{"132":1}}],["头互补性度量",{"5":{"132":2}}],["头对预测输出的直接贡献",{"5":{"132":2}}],["头剪枝的信息论准则",{"5":{"132":2}}],["头部专业化",{"5":{"70":2}}],["头选择",{"5":{"87":2,"152":2}}],["头间差异",{"5":{"87":2,"152":2}}],["头间的信息流动分析",{"2":{"93":1},"5":{"93":1}}],["头数与头维度的权衡",{"2":{"93":1},"5":{"93":1}}],["头数为",{"5":{"93":4}}],["头数和头维度",{"5":{"93":1}}],["头数",{"5":{"93":1}}],["头",{"5":{"93":4,"132":1}}],["头维度",{"5":{"93":2}}],["头维度128",{"5":{"93":2}}],["秩亏性或近秩亏性",{"5":{"149":2}}],["秩近似",{"5":{"143":2}}],["秩一张量和",{"5":{"143":1}}],["秩一张量",{"5":{"143":2}}],["秩一张量是外积形式定义的向量三元组的乘积",{"5":{"143":2}}],["秩一张量分解",{"2":{"143":1},"5":{"143":1}}],["秩约束的泛化界",{"5":{"142":2}}],["秩可能略有增加",{"5":{"87":2,"152":2}}],["秩较低的矩阵参数化空间较小",{"5":{"87":2,"152":2}}],["秩的低秩矩阵有个自由度",{"5":{"87":1,"152":1}}],["秩",{"5":{"87":1,"89":1,"142":2,"152":1}}],["秩衡量了编码向量的线性无关程度",{"5":{"89":1}}],["秩越低",{"5":{"87":2,"89":2,"152":2}}],["秩越高",{"5":{"89":2}}],["秩为的逼近",{"5":{"89":1}}],["秩为",{"5":{"89":1}}],["版本",{"5":{"87":2,"152":2}}],["足够小时",{"5":{"141":1}}],["足够的数据",{"5":{"140":2}}],["足够宽的单层网络可以逼近任意函数",{"5":{"71":2}}],["足够大时",{"5":{"69":1,"90":1,"149":1}}],["足够大",{"5":{"91":2}}],["足够",{"5":{"91":2}}],["足以捕获大部分语义信息",{"5":{"87":2,"152":2}}],["足以唯一标识远超过任何实际序列长度的位置",{"5":{"90":2}}],["堆叠多层注意力可以",{"5":{"87":2,"152":2}}],["突触是神经元之间传递信息的连接点",{"5":{"47":2}}],["突破线性限制",{"5":{"133":2}}],["突破",{"5":{"87":2,"152":2}}],["说明在相同计算预算下",{"5":{"159":2}}],["说明在给定取值空间大小时",{"5":{"137":2}}],["说明存在严重的负载不均衡",{"5":{"159":2}}],["说明负载越均衡",{"5":{"158":2}}],["说明专家之间的负载越不均衡",{"5":{"158":2}}],["说明专家",{"5":{"158":4}}],["说明这个动作比平均水平更好",{"5":{"155":2}}],["说明惩罚过于保守",{"5":{"154":2}}],["说明惩罚力度不够",{"5":{"154":2}}],["说明新策略不太倾向于选择这个动作",{"5":{"154":2}}],["说明新策略更倾向于选择这个动作",{"5":{"154":2}}],["说明了动量参数如何通过相位滞后和阻尼比影响震荡行为",{"5":{"136":2}}],["说明了压缩表示如何改善泛化性能",{"5":{"133":2}}],["说明周期行为具有一定的鲁棒性",{"5":{"136":2}}],["说明",{"5":{"134":2}}],["说明模型的表达能力主要集中在少数方向上",{"5":{"50":2}}],["说明编码的有效秩远低于满秩",{"5":{"89":2}}],["说明该头主要关注单一位置",{"5":{"87":2,"152":2}}],["说明该头的注意力分布更均匀",{"5":{"87":2,"152":2}}],["说明该位置的注意力是",{"5":{"92":4,"153":4}}],["说明底层信息能够有效传递到高层",{"5":{"92":2,"153":2}}],["枢纽",{"5":{"87":2,"152":2}}],["另外",{"5":{"50":2}}],["另一类重要的裁剪策略是逐元素裁剪",{"5":{"147":2}}],["另一种增强门控能力的方法是引入输入的部分先验信息",{"5":{"159":2}}],["另一种常用的负载均衡损失是平方偏差损失",{"5":{"158":2,"159":2}}],["另一种常见配置是令",{"5":{"94":2}}],["另一种常见的初始化是kaiming初始化",{"5":{"94":2}}],["另一种压缩技术是知识蒸馏",{"5":{"87":2,"152":2}}],["另一种混合设计是使用可学习编码替代正弦编码的某些频率成分",{"5":{"89":2}}],["另一种可视化方法是使用主成分分析",{"5":{"91":2}}],["另一种方法是使用绝对位置编码的扩展",{"5":{"92":2,"153":2}}],["另一种多头注意力的变体是",{"5":{"93":2}}],["另一个联系是通过",{"5":{"93":2}}],["学术论文",{"5":{"138":2}}],["学习和决策的主体",{"5":{"156":2}}],["学习速度更快但可能面临训练不稳定",{"5":{"154":2}}],["学习序列的联合概率分布",{"5":{"140":2}}],["学习复杂依赖关系的阶段性特征",{"5":{"138":1}}],["学习复杂依赖关系的阶段性特征是涌现能力的数学基础",{"5":{"138":1}}],["学习相变",{"5":{"138":2}}],["学习信号相互干扰",{"5":{"138":2}}],["学习阶段",{"5":{"137":2}}],["学习",{"5":{"133":2}}],["学习的稀疏模式",{"5":{"132":2}}],["学习效率低下",{"5":{"99":2,"101":2}}],["学习token之间的注意力模式",{"5":{"71":2}}],["学习率控制着",{"5":{"144":2}}],["学习率衰减的隐式正则化",{"5":{"144":2}}],["学习率衰减可以视为一种随时间变化的隐性正则化",{"5":{"144":2}}],["学习率衰减与震荡衰减",{"5":{"136":2}}],["学习率序列",{"5":{"136":2}}],["学习率引起的鞍结分岔",{"5":{"136":2}}],["学习率周期调度",{"5":{"136":2}}],["学习率设计",{"5":{"134":2}}],["学习率选择受限",{"5":{"134":2,"135":2}}],["学习率选择不当可能导致训练不稳定",{"5":{"97":2}}],["学习率稳定性条件",{"5":{"134":2}}],["学习率必须满足",{"5":{"134":2}}],["学习率与系统动力学",{"2":{"134":1},"5":{"134":1}}],["学习率太小会导致收敛缓慢",{"5":{"48":2}}],["学习率太大可能导致跳过最优解甚至发散",{"5":{"48":2}}],["学习率按指数函数递减",{"5":{"48":2}}],["学习率按余弦曲线从初始值逐渐减小到零",{"5":{"48":2}}],["学习率预热",{"5":{"48":2}}],["学习率调度与正则化之间存在深刻的相互作用",{"5":{"144":2}}],["学习率调度与收敛性",{"2":{"97":1},"5":{"97":1}}],["学习率调度中的正则化效应",{"2":{"144":1},"5":{"144":1}}],["学习率调度的理论指导",{"2":{"139":1},"5":{"139":1}}],["学习率调度",{"5":{"48":2,"97":4,"147":2}}],["学习率调度策略在训练过程中动态调整学习率",{"5":{"97":2}}],["学习率调度定义了一个函数",{"5":{"97":2}}],["学习率调整和梯度裁剪中都有应用",{"5":{"50":2}}],["学习率的选择方面",{"5":{"89":2}}],["学习率敏感性分析",{"2":{"97":1},"5":{"97":1}}],["学习率",{"5":{"97":2,"134":2,"137":2,"144":2,"155":2}}],["学习率是一个关键的超参数",{"5":{"48":2}}],["学习率是梯度下降中最重要的超参数",{"5":{"97":2}}],["学习率被自动减小",{"5":{"97":2}}],["学习率保持不变",{"5":{"97":2}}],["学习率过大",{"5":{"97":2}}],["学习率过小",{"5":{"97":2}}],["学习喜欢我",{"5":{"88":2}}],["学习到对任务最有帮助的位置表示",{"5":{"89":2}}],["学生模型",{"5":{"137":2}}],["学生模型可以学习到教师模型注意力矩阵的低秩近似",{"5":{"87":2,"152":2}}],["学生表示的提取能力",{"5":{"133":2}}],["学生",{"5":{"87":2,"152":2}}],["教师模型",{"5":{"137":2}}],["教师表示的信息密度",{"5":{"133":2}}],["教师",{"5":{"87":2,"152":2}}],["工程实现及其在现代大语言模型中的核心地位",{"5":{"88":2}}],["狗咬人",{"5":{"88":2,"91":2,"92":2,"153":2}}],["我们都应该增加其在状态",{"5":{"155":2}}],["我们介绍了策略梯度方法和actor",{"5":{"154":2}}],["我们介绍了scaled",{"5":{"94":2}}],["我们严格定义了",{"5":{"149":2}}],["我们立即得到以下重要推论",{"5":{"149":2}}],["我们应该减少其概率",{"5":{"155":2}}],["我们应该",{"5":{"148":2}}],["我们揭示了梯度稳定性的核心判据",{"5":{"148":2}}],["我们揭示了非线性激活函数打破表达瓶颈的数学必然性",{"5":{"71":2}}],["我们无法简单地将乘积的下界与各因子的下界联系起来",{"5":{"148":2}}],["我们无法直接访问真实的数据分布",{"5":{"140":2}}],["我们看到了自回归建模思想如何在不同历史阶段以不同形式得到体现",{"5":{"140":2}}],["我们按照以下条件概率逐步生成",{"5":{"140":2}}],["我们只能通过有限的训练样本来近似这个分布",{"5":{"140":2}}],["我们能够得到该信号在不同",{"5":{"150":2}}],["我们能够更有针对性地调整超参数",{"5":{"136":2}}],["我们能够诊断训练问题",{"5":{"135":2}}],["我们证明了凸函数的",{"5":{"136":1}}],["我们证明了凸函数的收敛界和强凸函数的线性收敛速率",{"5":{"136":1}}],["我们证明了位置差的编码分辨率与频率​成反比",{"5":{"90":1}}],["我们证明了位置差",{"5":{"90":1}}],["我们经常需要控制生成文本的随机性和多样性",{"5":{"140":2}}],["我们经常需要度量两个概率分布之间的差异",{"5":{"61":2}}],["我们经常观察到损失函数值在一定范围内波动",{"5":{"136":2}}],["我们来分析损失函数关于参数",{"5":{"70":1}}],["我们来分析损失函数关于参数的梯度",{"5":{"70":1}}],["我们发现",{"5":{"70":2}}],["我们发现均方误差的梯度与预测误差",{"5":{"69":1}}],["我们发现均方误差的梯度与预测误差成正比",{"5":{"69":1}}],["我们最小化",{"5":{"70":2}}],["我们详细推导了交叉熵与最大似然估计的等价性",{"5":{"69":2}}],["我们详细推导了期望mse的分解公式",{"5":{"51":2}}],["我们详细推导了注意力矩阵的特征值和特征向量结构",{"5":{"87":2,"152":2}}],["我们详细推导了编码矩阵的秩",{"5":{"89":2}}],["我们详细分析了注意力机制和位置编码的数学基础",{"5":{"70":2}}],["我们详细分析了位置编码的频谱结构",{"5":{"90":2}}],["我们详细探讨了scaled",{"5":{"93":2}}],["我们不再依赖于完整的轨迹回报",{"5":{"155":2}}],["我们不会同时处理所有感知信息",{"5":{"132":2}}],["我们不需要存储或重新计算导数",{"5":{"42":2}}],["我们不需要任何显式的相对位置计算",{"5":{"88":2}}],["我们不是从离散集合中",{"5":{"69":2}}],["我们不是通过dft从数据中学习频率",{"5":{"90":2}}],["我们并不显式定义正负样本",{"5":{"69":2}}],["我们现在可以讨论它在大语言模型训练中的应用",{"5":{"154":2}}],["我们现在可以构建从infonce到注意力的直接桥梁",{"5":{"69":2}}],["我们现在展示softmax操作如何统一分类损失",{"5":{"69":2}}],["我们已经讨论了dropout与l2正则化的等价性",{"5":{"144":2}}],["我们已经介绍了softmax函数的定义和基本性质",{"5":{"69":2}}],["我们已经从计算单元和结构组织的角度介绍了神经网络的数学描述",{"5":{"45":2}}],["我们实际上在优化表示之间的互信息",{"5":{"69":2}}],["我们实际上是在优化策略的价值与控制策略变化幅度之间寻求平衡",{"5":{"154":2}}],["我们实际上是在寻找一个能够拟合目标函数的数学表达",{"5":{"45":2}}],["我们实际上是在问",{"5":{"48":2}}],["我们实际上是在最小化模型分布与真实分布之间的kl散度",{"5":{"61":2}}],["我们分别深入探讨了均方误差",{"5":{"69":2,"70":2}}],["我们分析了dropout对输出方差的影响",{"5":{"145":2}}],["我们分析了位置编码的数学性质",{"5":{"70":2}}],["我们分析了多头注意力的矩阵推导",{"5":{"69":2}}],["我们分析了sigmoid",{"5":{"71":2}}],["我们分析了激活函数与transformer架构",{"5":{"71":2}}],["我们分析了mse的性质",{"5":{"51":2}}],["我们分析了正弦位置编码和旋转位置编码的数学性质",{"5":{"92":2,"153":2}}],["我们分析了梯度爆炸的数学机制",{"5":{"41":2}}],["我们分析了梯度下降动力学",{"5":{"97":2}}],["我们可以训练一个奖励模型",{"5":{"155":2}}],["我们可以训练一个神经网络模型",{"5":{"154":2}}],["我们可以在偏差和方差之间找到最优的平衡点",{"5":{"155":2}}],["我们可以使用td误差",{"5":{"155":2}}],["我们可以控制整个网络的lipschitz常数",{"5":{"144":2}}],["我们可以显著加速优化过程",{"5":{"144":2}}],["我们可以利用这条直线规律外推到更大的参数范围",{"5":{"138":2}}],["我们可以预测不同参数配置下的系统行为",{"5":{"136":2}}],["我们可以系统地理解和预测优化算法的行为",{"5":{"136":2}}],["我们可以建立训练稳定性的精确判据",{"5":{"135":2}}],["我们可以建立一个简化的数学模型",{"5":{"92":2,"153":2}}],["我们可以量化各组件之间的信息传递效率",{"5":{"132":2}}],["我们可以总结出激活函数设计应遵循的数学原则",{"5":{"41":2}}],["我们可以清楚地看到损失信息如何从输出层反向流动到输入层",{"5":{"44":2}}],["我们可以计算损失函数对各层参数的梯度",{"5":{"44":2}}],["我们可以构建任意复杂的非线性函数",{"5":{"46":2}}],["我们可以构建置信区间来量化估计的不确定性",{"5":{"96":2}}],["我们可以实现降维同时最小化信息损失",{"5":{"50":2}}],["我们可以了解模型使用了多少",{"5":{"50":2}}],["我们可以对少数类样本赋予更高权重以改善模型性能",{"5":{"51":2}}],["我们可以对近期数据赋予更高权重以使模型更关注最新的模式",{"5":{"51":2}}],["我们可以将优势函数改写为",{"5":{"155":2}}],["我们可以将新策略的期望回报用旧策略采样的数据来表示",{"5":{"154":2}}],["我们可以将导数表示为",{"5":{"42":2}}],["我们可以将目标向量和预测向量​视为空间中的两个点",{"5":{"51":1}}],["我们可以将目标向量",{"5":{"51":1}}],["我们可以将位置的编码向量视为一个离散序列",{"5":{"90":1}}],["我们可以将位置",{"5":{"90":1}}],["我们可以将query生成过程理解为",{"5":{"94":2}}],["我们可以给出另一种等价的解释",{"5":{"61":2}}],["我们可以根据样本的难度赋予不同权重",{"5":{"51":2}}],["我们可以根据重要性分数选择top",{"5":{"86":2,"151":2}}],["我们可以更好地掌握这一强大技术的本质",{"5":{"154":2}}],["我们可以更清晰地理解数据如何在神经网络中流动",{"5":{"137":2}}],["我们可以更清晰地看到两种范式的优势和局限性",{"5":{"89":2}}],["我们可以更深入地理解深度学习的本质",{"5":{"133":2}}],["我们可以更深入地理解注意力机制的本质",{"5":{"132":2}}],["我们可以更深入地理解",{"5":{"61":2}}],["我们可以更深入地理解这一性质的来源",{"5":{"90":2}}],["我们可以分析位置编码的自由度",{"5":{"89":2}}],["我们可以分析编码向量的以下性质",{"5":{"89":2}}],["我们可以分析注意力权重的分布特性",{"5":{"92":2,"153":2}}],["我们可以分析它能够表示的函数复杂度",{"5":{"92":2,"153":2}}],["我们可以分析多头注意力能够表示的函数类",{"5":{"93":2}}],["我们可以比较两种编码的表达能力",{"5":{"89":2}}],["我们可以理解为什么位置编码能够帮助模型更好地泛化",{"5":{"90":2}}],["我们可以区分",{"5":{"92":2,"153":2}}],["我们可以通过梯度累积来模拟更大的有效批大小",{"5":{"50":2}}],["我们可以通过分析的模式来推断该头负责的依赖类型",{"5":{"92":1,"153":1}}],["我们可以通过分析",{"5":{"92":1,"153":1}}],["我们可以证明注意力机制在建模长程依赖方面的效率优势",{"5":{"92":2,"153":2}}],["我们可以发现一个有趣的结构",{"5":{"93":2}}],["我们可以考虑以下优化策略",{"5":{"89":2}}],["我们可以考虑注意力头的输出表示之间的相关性",{"5":{"93":2}}],["我们可以考虑注意力头的有效容量",{"5":{"93":2}}],["我们可以得到一个统一的矩阵表达式",{"5":{"94":2}}],["我们终将知道",{"2":{"21":1,"85":1,"131":1},"5":{"21":1,"85":1,"131":1}}],["我们必须知道",{"2":{"21":1,"85":1,"131":1},"5":{"21":1,"85":1,"131":1}}],["我们首先建立归一化技术的统一数学框架",{"5":{"146":2}}],["我们首先建立了dropout的数学模型",{"5":{"145":2}}],["我们首先通过定义词表和序列空间",{"5":{"140":2}}],["我们首先回顾经典的n",{"5":{"140":2}}],["我们首先回顾了傅里叶分析的基础知识",{"5":{"90":2}}],["我们首先详细推导了sigmoid和tanh函数的饱和区域和饱和深度",{"5":{"41":2}}],["我们首先给出了mse的严格数学定义",{"5":{"51":2}}],["我们首先严格定义了注意力矩阵",{"5":{"87":2,"152":2}}],["我们首先定义了可学习位置编码的参数化形式",{"5":{"89":2}}],["我们首先从位置编码的动机出发",{"5":{"91":2}}],["我们首先需要建立sgd中梯度噪声的数学模型",{"5":{"149":2}}],["我们首先需要建立饱和现象的严格数学定义",{"5":{"41":2}}],["我们首先需要分析传统循环神经网络在长程依赖建模方面的局限性",{"5":{"92":2,"153":2}}],["我们首先需要理解引入多头注意力的动机",{"5":{"93":2}}],["我们首先探讨了优化景观的几何结构",{"5":{"97":2}}],["我们总结了激活函数设计的数学原则",{"5":{"41":2}}],["我们需要定义价值函数",{"5":{"156":2}}],["我们需要定义一些数学度量",{"5":{"92":2,"153":2}}],["我们需要使用",{"5":{"155":1}}],["我们需要使用时序差分学习",{"5":{"155":1}}],["我们需要验证",{"5":{"155":2}}],["我们需要大量的采样来覆盖不同的状态",{"5":{"155":2}}],["我们需要解决一个核心问题",{"5":{"155":2}}],["我们需要训练一个价值函数网络",{"5":{"154":2}}],["我们需要分析",{"5":{"149":1}}],["我们需要分析的特征值分解",{"5":{"149":1}}],["我们需要引入fisher信息矩阵",{"5":{"149":2}}],["我们需要",{"5":{"148":2}}],["我们需要更精确地刻画梯度爆炸现象的数学本质",{"5":{"147":2}}],["我们需要将pca的思想推广到高阶张量",{"5":{"143":2}}],["我们需要将链式法则从标量情形推广到向量和矩阵情形",{"5":{"44":2}}],["我们需要在给定历史上下文的情况下预测下一个词元",{"5":{"140":2}}],["我们需要一个巧妙的分解策略来将这个高维问题转化为可计算的形式",{"5":{"140":2}}],["我们需要预测的是这个条件分布本身",{"5":{"70":2}}],["我们需要显式计算其梯度",{"5":{"69":2}}],["我们需要从根本上重新理解语言建模的本质任务",{"5":{"140":2}}],["我们需要从",{"5":{"69":1}}],["我们需要从个候选样本",{"5":{"69":1}}],["我们需要从线性映射的局限性出发",{"5":{"71":2}}],["我们需要组合多个神经元",{"5":{"46":2}}],["我们需要找到一组模型参数",{"5":{"48":2}}],["我们需要平衡验证集大小和计算成本",{"5":{"96":2}}],["我们需要首先引入",{"5":{"154":1}}],["我们需要首先引入重要性采样",{"5":{"154":1}}],["我们需要首先详细分析反向传播中链式法则的作用方式",{"5":{"148":2}}],["我们需要首先理解nce的基本思想及其局限性",{"5":{"69":2}}],["我们需要首先理解生物神经元的基本结构及其数学抽象过程",{"5":{"47":2}}],["我们需要首先明确几个基本定义",{"5":{"50":2}}],["我们需要明确哪个分布是",{"5":{"61":2}}],["我们需要构造一个位置感知的表示函数",{"5":{"88":2}}],["我们需要计算损失函数对每一层参数的梯度",{"5":{"148":2}}],["我们需要计算梯度",{"5":{"61":2}}],["我们需要计算每个位置",{"5":{"88":2}}],["我们需要计算旋转矩阵",{"5":{"88":1}}],["我们需要计算",{"5":{"88":1}}],["我们需要理解",{"5":{"89":2}}],["我们需要证明",{"5":{"91":2}}],["我们需要考察当向量维度​较大时",{"5":{"95":1}}],["我们需要考察当向量维度",{"5":{"95":1}}],["我们将目标重新定义为学习残差",{"5":{"150":2}}],["我们将揭示正则化技术的深层机制",{"5":{"144":2}}],["我们将建立对梯度动力学的系统性理解",{"5":{"41":2}}],["我们将讨论反向传播与自动微分的关系",{"5":{"44":2}}],["我们将复杂的复合函数求导问题分解为误差信号的前向传播和参数梯度的局部计算两个步骤",{"5":{"44":2}}],["我们将看到如何利用链式法则推导出反向传播的梯度计算公式",{"5":{"45":2}}],["我们将在第2",{"5":{"45":2}}],["我们将在下一节讨论如何将多个神经元组织成层",{"5":{"47":2}}],["我们将深入探讨mse在向量空间中的几何解释",{"5":{"51":2}}],["我们将详细分析损失函数的优化性质",{"5":{"71":2}}],["我们将详细推导mse的偏差",{"5":{"51":2}}],["我们将分析mse的性质",{"5":{"51":2}}],["我们将mse置于向量空间的框架下分析",{"5":{"51":2}}],["我们将可学习编码与正弦编码进行了比较",{"5":{"89":2}}],["我们将从链式法则的矩阵形式出发",{"5":{"44":2}}],["我们将从群论的角度详细推导旋转位置编码",{"5":{"89":2}}],["我们将从频率空间的角度进一步分析位置编码的理论基础",{"5":{"91":2}}],["我们将从谱性质的角度进一步分析注意力矩阵的数学特性",{"5":{"92":2,"153":2}}],["我们将从谱性质的角度分析注意力矩阵的数学特性",{"5":{"93":2}}],["我们将正弦余弦编码与可学习编码",{"5":{"91":2}}],["我们将所有头的query投影矩阵堆叠成一个大的投影矩阵",{"5":{"93":2}}],["我们将个头的投影矩阵按行堆叠",{"5":{"93":1}}],["我们将",{"5":{"93":1}}],["我们将进一步探讨query",{"5":{"95":2}}],["我们论证了激活函数作为通用逼近器核心组件的数学基础",{"5":{"71":2}}],["我们伴随值",{"5":{"45":2}}],["我们主要关注全连接层",{"5":{"46":2}}],["我们通常最大化对数似然函数",{"5":{"140":2}}],["我们通常最小化负对数似然",{"5":{"61":2}}],["我们通常基于已经说过的话来思考下一步要说什么",{"5":{"140":2}}],["我们通常关注算法能否收敛到临界点",{"5":{"136":2}}],["我们通常使用广义优势估计",{"5":{"155":2}}],["我们通常使用有限时长的轨迹或者使用蒙特卡洛采样来估计轨迹的总回报",{"5":{"155":2}}],["我们通常使用较窄但较深的网络来达到相同的表达能力",{"5":{"71":2}}],["我们通常使用小批量随机梯度下降",{"5":{"44":2}}],["我们通常使用随机梯度下降或其变体",{"5":{"96":2}}],["我们通常保持权重矩阵和偏置向量的分离形式",{"5":{"46":2}}],["我们通常需要同时计算多个神经元的输出",{"5":{"47":2}}],["我们通常假设对于所有",{"5":{"92":1,"153":1}}],["我们通常假设",{"5":{"92":1,"153":1}}],["我们通常有",{"5":{"93":2}}],["我们通常不直接建模随机变量的概率分布",{"5":{"96":2}}],["我们通过最大化数据在模型下的似然来学习参数",{"5":{"140":2}}],["我们通过构造具体的权重和阈值来说明",{"5":{"47":2}}],["我们通过旋转来",{"5":{"88":2}}],["我们通过数值计算来验证这一性质",{"5":{"91":2}}],["我们通过信息传递路径分析",{"5":{"92":2,"153":2}}],["我们引入齐次坐标",{"5":{"47":2}}],["我们建立了",{"5":{"149":1}}],["我们建立了与fisher信息矩阵",{"5":{"149":1}}],["我们建立了模n乘积",{"5":{"143":2}}],["我们建立了损失函数的数学基础",{"5":{"99":2,"101":2}}],["我们建立了单个神经元的数学模型",{"5":{"46":2}}],["我们建立了仿射变换的数学框架",{"5":{"47":2}}],["我们建立了一个完整的损失函数优化理论框架",{"5":{"97":2}}],["我们会发现它蕴含着丰富的理论内涵",{"5":{"45":2}}],["我们会以最快的速度离开当前的等值面",{"5":{"48":2}}],["我们同时处理这两类随机变量",{"5":{"96":2}}],["我们希望",{"5":{"140":2,"158":4}}],["我们希望学习一个参数化的模型分布",{"5":{"69":2}}],["我们希望最大化",{"5":{"71":2}}],["我们希望用简单的近似分布来逼近复杂的后验分布",{"5":{"96":1}}],["我们希望用简单的近似分布",{"5":{"96":1}}],["我们希望找到一个预测值​来最小化期望损失",{"5":{"51":1}}],["我们希望找到一个预测值",{"5":{"51":1}}],["我们希望输出序列​满足​",{"5":{"92":1,"153":1}}],["我们希望输出序列",{"5":{"92":1,"153":1}}],["我们希望计算​和​",{"5":{"94":1}}],["我们希望计算",{"5":{"94":1}}],["我们根据样本数据计算检验统计量",{"5":{"96":2}}],["我们计算在该状态下采取该动作的对数概率梯度",{"5":{"155":2}}],["我们计算每个节点对其直接后继的梯度",{"5":{"45":2}}],["我们计算两个模型的性能差异",{"5":{"96":2}}],["我们很少直接计算矩阵的逆",{"5":{"50":2}}],["我们使用",{"5":{"155":1}}],["我们使用优势函数",{"5":{"155":1}}],["我们使用ppo算法来优化语言模型策略",{"5":{"154":2}}],["我们使用模型之前生成的词元作为上下文",{"5":{"140":2}}],["我们使用真实的上下文来预测下一个词元",{"5":{"140":2}}],["我们使用雅可比矩阵",{"5":{"48":2}}],["我们使用迭代算法如幂迭代",{"5":{"50":2}}],["我们使用离散的时间步长",{"5":{"97":2}}],["我们在优化目标中加入正交性惩罚项",{"5":{"50":2}}],["我们关心如何用少量的奇异值最好地近似原始矩阵",{"5":{"50":2}}],["我们得到",{"5":{"149":2}}],["我们得到以下关键结果",{"5":{"148":2}}],["我们得到了一个可以近似离散采样的连续分布",{"5":{"71":2}}],["我们得到形状为的张量",{"5":{"50":1}}],["我们得到形状为",{"5":{"50":1}}],["我们先计算若干个小批次的梯度",{"5":{"50":2}}],["我们对所有可能的样本集取期望",{"5":{"51":1}}],["我们对所有可能的样本集",{"5":{"51":1}}],["我们假设",{"5":{"61":2}}],["我们的优化目标是最大化期望累积奖励",{"5":{"155":2}}],["我们的优化目标是最小化真实分布",{"5":{"61":2}}],["我们的目标是通过梯度下降更新参数以最小化",{"5":{"148":2}}],["我们的目标是计算和对所有层",{"5":{"44":1}}],["我们的目标是计算",{"5":{"44":1}}],["我们系统地探讨了标准transformer中注意力机制的数学原理",{"5":{"86":2,"151":2}}],["我们从反向传播的链式法则出发",{"5":{"148":2}}],["我们从归一化的统一范式出发",{"5":{"146":2}}],["我们从l2正则化的梯度表达式出发",{"5":{"144":2}}],["我们从svd的严格数学定义出发",{"5":{"143":2}}],["我们从统计学角度分析了mse的来源",{"5":{"51":2}}],["我们从计算流程",{"5":{"87":2,"152":2}}],["我们从数学上证明了注意力矩阵的秩最多为​",{"5":{"87":1,"152":1}}],["我们从数学上证明了注意力矩阵的秩最多为",{"5":{"87":1,"152":1}}],["我们从未显式地告诉模型",{"5":{"88":2}}],["我们从频率域的角度重新分析了相对位置的编码",{"5":{"90":2}}],["我们从rnn的梯度消失问题出发",{"5":{"92":2,"153":2}}],["我们从函数空间和计算复杂度的角度进行了理论分析",{"5":{"92":2,"153":2}}],["我们从多头注意力的设计动机出发",{"5":{"93":2}}],["我们从子空间学习的角度论证了多头注意力能够表示比单头注意力更丰富的函数类",{"5":{"93":2}}],["我们从注意力机制的核心思想出发",{"5":{"95":2}}],["我们从信息论",{"5":{"97":2}}],["我们有基本的不等式关系",{"5":{"148":2}}],["我们有",{"5":{"88":4,"149":2}}],["我们定义一个关键中间变量",{"5":{"44":2}}],["我们定义",{"5":{"88":2}}],["我们定义信息传递路径长度的数学概念",{"5":{"92":2,"153":2}}],["我们试图",{"5":{"88":2}}],["我们探讨了这种特殊噪声结构对优化动力学的影响",{"5":{"149":2}}],["我们探讨了激活函数的信息压缩",{"5":{"71":2}}],["我们探讨了可学习编码能够表示的函数空间",{"5":{"89":2}}],["我们讨论了dropout在全连接网络",{"5":{"145":2}}],["我们讨论了正则化与优化算法的相互作用",{"5":{"144":2}}],["我们讨论了前向传播的数学本质",{"5":{"44":2}}],["我们讨论了初始化策略",{"5":{"89":2}}],["我们讨论了频率域与位置域的对偶性",{"5":{"90":2}}],["我们讨论了数值稳定性问题和实际实现中的关键技术",{"5":{"97":2}}],["我们还需要知道噪声在不同方向上的分布情况",{"5":{"149":2}}],["我们还将讨论梯度裁剪与显式正则化策略之间的关系",{"5":{"147":2}}],["我们还探讨了dropout与l2正则化之间的深层联系",{"5":{"145":2}}],["我们还探讨了隐式正则化效应",{"5":{"144":2}}],["我们还讨论了条件数对数值稳定性的影响",{"5":{"87":2,"152":2}}],["我们还讨论了低秩结构与信息瓶颈理论的关系",{"5":{"87":2,"152":2}}],["我们还讨论了注意力机制的矩阵形式",{"5":{"95":2}}],["我们还介绍了复数形式的统一分析",{"5":{"90":2}}],["我们还详细分析了多头注意力的计算复杂度",{"5":{"93":2}}],["我们还深入探讨了正则化与泛化的数学联系",{"5":{"97":2}}],["我们给出具体的数值示例",{"5":{"91":2}}],["我们深入探讨了频率选择与编码分辨率的关系",{"5":{"90":2}}],["我们深入分析了位置编码的唯一性",{"5":{"91":2}}],["我们深入分析了多头注意力的参数结构",{"5":{"93":2}}],["我们进一步讨论了梯度流与信息保持的关系",{"5":{"41":2}}],["我们进一步分析了位置编码如何与注意力计算交互",{"5":{"91":2}}],["我们进一步分析了注意力权重的分布特性",{"5":{"92":2,"153":2}}],["我们展示了期望保持性质",{"5":{"145":2}}],["我们展示了如何通过神经网络来逼近复杂的条件概率函数",{"5":{"140":2}}],["我们展示了交叉熵与kl散度的等价关系",{"5":{"61":2}}],["我们展示了编码向量的数值分布",{"5":{"91":2}}],["我们展示了这些损失函数的数学本质",{"5":{"101":2}}],["我们重点分析了缩放因子的统计学原理",{"5":{"95":1}}],["我们重点分析了缩放因子",{"5":{"95":1}}],["我喜欢学习",{"5":{"88":2}}],["我需要什么信息",{"5":{"94":2}}],["我这里有什么信息",{"5":{"94":2}}],["至少到原来的四分之一",{"5":{"148":2}}],["至少保留",{"5":{"133":1}}],["至少保留位关于的信息",{"5":{"133":1}}],["至少有一个等于",{"5":{"61":2}}],["至少有一个被拒绝的概率也会大大增加",{"5":{"96":2}}],["至关重要",{"5":{"88":2}}],["句法关系",{"5":{"71":2}}],["句末的标点符号常常标志着句子的结束",{"5":{"88":2}}],["句子开头的词与句子结尾的词之间可能存在语法上的呼应",{"5":{"88":2}}],["句子",{"5":{"88":2}}],["污染",{"5":{"88":2}}],["绝对位置编码",{"5":{"71":2,"88":2}}],["绝对位置编码能力",{"5":{"88":2}}],["绝对位置编码的数学形式为",{"5":{"91":2}}],["绝对位置编码的优点",{"5":{"91":2}}],["绝对位置编码的缺点",{"5":{"91":2}}],["绝对位置编码与相对位置编码进行了比较",{"5":{"91":2}}],["绝对位置和分别出现在不同的项中",{"5":{"88":1}}],["绝对位置",{"5":{"88":3,"101":2,"132":2}}],["绝对位置信息的丢失是一个理论上的局限",{"5":{"88":1}}],["绝对位置信息的丢失",{"5":{"88":1}}],["出现周期2震荡",{"5":{"136":2}}],["出现震荡收敛现象",{"5":{"136":2}}],["出发的时间序列",{"5":{"134":2}}],["出发",{"5":{"61":2}}],["出纯粹的位置信息",{"5":{"88":2}}],["共同塑造优化景观和最终的模型性能",{"5":{"147":2}}],["共同引导优化过程朝向具有更好泛化性质的方向进行",{"5":{"147":2}}],["共同决定了不同维度上的旋转频率",{"5":{"88":2}}],["共同作为注意力层的输入",{"5":{"91":2}}],["共同建模输入序列中的各种依赖关系",{"5":{"92":2,"153":2}}],["共同构成完整的变换",{"5":{"88":2}}],["共同构成了大语言模型训练的数学基础",{"5":{"101":2}}],["共享部分的计算只需要执行一次",{"5":{"158":2}}],["共享部分或全部参数",{"5":{"150":2}}],["共享参数减少了总参数量",{"5":{"158":2}}],["共享的信息量",{"5":{"137":1}}],["共享表示损害任务",{"5":{"133":1}}],["共享表示损害任务学习",{"5":{"133":1}}],["共享表示帮助任务",{"5":{"133":1}}],["共享表示帮助任务学习",{"5":{"133":1}}],["共享表示",{"5":{"133":1}}],["共享表示与各任务的关系可用条件互信息描述",{"5":{"133":1}}],["共享位置编码",{"5":{"89":2}}],["共享编码将参数量从减少到​",{"5":{"89":1}}],["共享编码将参数量从",{"5":{"89":1}}],["共享同一个key和value",{"5":{"93":2}}],["共轭梯度法利用hessian的信息来加速收敛",{"5":{"48":2}}],["共轭先验是使后验分布与先验分布同分布族的先验选择",{"5":{"96":2}}],["产生最终的模型输出",{"5":{"158":2}}],["产生周期4",{"5":{"136":4}}],["产生新的不动点对",{"5":{"136":2}}],["产生较大的梯度",{"5":{"69":1}}],["产生较小的梯度",{"5":{"69":2}}],["产生了完全不同的数学性质",{"5":{"88":2}}],["弧度",{"5":{"88":2}}],["温度选择的实践意义",{"5":{"140":2}}],["温度效应",{"5":{"140":2}}],["温度缩放后的分布定义为",{"5":{"140":2}}],["温度缩放的概率分布",{"5":{"140":2}}],["温度序列",{"5":{"132":2}}],["温度调节使得注意力可以在不同任务中灵活适应",{"5":{"132":2}}],["温度调节的",{"5":{"132":2}}],["温度调度策略",{"5":{"132":2}}],["温度控制探索",{"5":{"132":1}}],["温度的信息论解释",{"5":{"132":2}}],["温度与注意力熵单调相关",{"5":{"132":1}}],["温度对熵的影响",{"5":{"132":2}}],["温度",{"5":{"132":6,"144":2}}],["温度参数是平衡生成质量和多样性的重要超参数",{"5":{"140":2}}],["温度参数是infonce中的一个关键超参数",{"5":{"69":1}}],["温度参数与概率分布控制",{"2":{"140":1},"5":{"140":1}}],["温度参数使得教师模型的",{"5":{"137":2}}],["温度参数通常设置在较小的值",{"5":{"69":1}}],["温度参数对梯度有双重影响",{"5":{"69":1}}],["温度参数对梯度的影响",{"5":{"69":2}}],["温度参数的数学效应",{"5":{"69":2}}],["温度参数的作用与影响",{"2":{"69":1},"5":{"69":1}}],["温度参数提供了从",{"5":{"71":1}}],["温度参数",{"5":{"69":3,"71":1,"95":1}}],["温度参数控制着注意力分布的",{"5":{"95":1}}],["温和",{"5":{"88":2}}],["频谱特征与各向异性",{"2":{"149":1},"5":{"149":1}}],["频谱效率高的优点",{"5":{"88":2}}],["频率覆盖",{"5":{"132":2}}],["频率参数选择10000作为底数",{"5":{"88":2}}],["频率参数为10000",{"5":{"88":2}}],["频率分布",{"5":{"70":2,"91":2}}],["频率分解",{"5":{"89":2}}],["频率分辨率",{"5":{"90":2}}],["频率选择与编码分辨率",{"2":{"90":1},"5":{"90":1}}],["频率选择与编码分辨率的关系",{"5":{"90":2}}],["频率空间与位置空间的对偶性",{"2":{"90":1},"5":{"90":1}}],["频率空间分析为我们理解位置编码提供了全新的视角",{"5":{"90":2}}],["频率空间的数学分析",{"0":{"90":1},"4":{"90":1},"5":{"85":4,"90":1,"131":4}}],["频率空间的数学分析<",{"5":{"85":1,"131":1}}],["频率空间的视角不仅帮助我们理解正弦余弦编码的数学原理",{"5":{"90":2}}],["频率称为第次谐波",{"5":{"90":1}}],["频率对应的角频率为",{"5":{"90":1}}],["频率成分不变",{"5":{"90":2}}],["频率间隔近似为",{"5":{"90":2}}],["频率不变",{"5":{"90":2}}],["频率域的稀疏性",{"5":{"90":2}}],["频率",{"5":{"90":2,"136":2}}],["频域位置编码从信号处理的角度重新思考位置编码",{"5":{"88":1}}],["频域位置编码",{"5":{"88":1}}],["添加基线不会改变策略梯度估计量的期望值",{"5":{"155":2}}],["添加kl散度惩罚",{"5":{"154":2}}],["添加谱正则化后的hessian矩阵满足",{"5":{"144":2}}],["添加l2正则化后",{"5":{"144":8}}],["添加小常数防止",{"5":{"132":2}}],["添加",{"5":{"88":2,"95":1}}],["添加后",{"5":{"95":1}}],["漂移",{"5":{"88":2}}],["封闭有界集合",{"5":{"71":2}}],["封闭性",{"5":{"88":2}}],["运算",{"5":{"143":1,"159":2}}],["运算在数学形式上完全一致",{"5":{"61":2}}],["运算按分量独立进行",{"5":{"88":2}}],["运算可以利用高度优化的矩阵乘法核心",{"5":{"95":2}}],["群论完整推导",{"0":{"88":1},"4":{"88":1},"5":{"85":4,"88":1,"131":4}}],["群论完整推导<",{"5":{"85":1,"131":1}}],["群论视角",{"2":{"88":1},"5":{"88":1}}],["群论的核心概念是群作用",{"5":{"88":1}}],["群论的核心概念是",{"5":{"88":1}}],["群作用描述了如何将群元素",{"5":{"88":2}}],["群作用",{"5":{"88":1}}],["附近对损失函数进行二阶近似",{"5":{"149":1}}],["附近不会压缩梯度",{"5":{"148":1}}],["附近的局部拓扑结构与其线性化系统",{"5":{"135":2}}],["附近的局部线性近似",{"5":{"135":2}}],["附近",{"5":{"42":1,"69":1,"149":2}}],["附加信号",{"5":{"88":2}}],["压缩比与精度权衡",{"5":{"143":2}}],["压缩比约为",{"5":{"142":1,"143":2}}],["压缩比约为倍",{"5":{"142":1}}],["压缩比为",{"5":{"142":2}}],["压缩关于",{"5":{"137":1}}],["压缩关于的信息",{"5":{"137":1}}],["压缩程度",{"5":{"137":1}}],["压缩程度与泛化性能正相关",{"5":{"137":1}}],["压缩程度随层深的变化",{"5":{"132":2}}],["压缩与泛化的关系",{"5":{"137":2}}],["压缩到最小表示",{"5":{"133":1}}],["压缩量",{"5":{"133":2}}],["压缩机制鼓励提取本质特征而非记忆细节",{"5":{"133":2}}],["压缩阶段",{"5":{"133":6}}],["压缩输入信息同时保留输出信息",{"5":{"132":2}}],["压缩",{"5":{"87":2,"132":2,"133":2,"148":2,"152":2}}],["压低",{"5":{"88":2}}],["压扁",{"5":{"94":2}}],["模的投影矩阵",{"5":{"143":1}}],["模3切片矩阵的左奇异向量矩阵",{"5":{"143":2}}],["模2",{"5":{"143":2}}],["模型规模和数据规模应该同步扩大",{"5":{"159":2}}],["模型进行系统性的对比分析",{"5":{"159":2}}],["模型拥有海量的专业参数",{"5":{"159":2}}],["模型不确定性的体现",{"5":{"156":2}}],["模型会学习到最优的缩放和偏移",{"5":{"150":2}}],["模型会学习到最能支持任务目标的位置表示",{"5":{"89":2}}],["模型面临着梯度消失",{"5":{"150":2}}],["模型集成视角",{"5":{"145":2}}],["模型变为",{"5":{"145":2}}],["模型压缩和高效适应成为核心挑战",{"5":{"142":2}}],["模型演进的趋势",{"5":{"140":2}}],["模型族通常远大于数据",{"5":{"140":2}}],["模型对上下文的理解越充分",{"5":{"140":2}}],["模型对预测越来越确定",{"5":{"137":2}}],["模型架构特性",{"5":{"138":2}}],["模型从",{"5":{"138":2}}],["模型逐渐获得拟合更多层依赖关系的能力",{"5":{"138":2}}],["模型逐渐调整和​",{"5":{"92":1,"153":1}}],["模型逐渐调整",{"5":{"92":1,"153":1}}],["模型处于欠拟合状态",{"5":{"138":2}}],["模型处于随机状态",{"5":{"133":2}}],["模型突然展现出新的能力",{"5":{"137":2}}],["模型参数量是稠密模型的",{"5":{"159":2}}],["模型参数量巨大",{"5":{"48":2}}],["模型参数为",{"5":{"137":2}}],["模型容量增大",{"5":{"138":2}}],["模型容量足以记忆训练数据",{"5":{"137":2}}],["模型容量",{"5":{"137":2,"159":1}}],["模型容量和优化过程",{"5":{"92":2,"153":2}}],["模型开始区分不同类别",{"5":{"137":2}}],["模型开始丢弃冗余信息",{"5":{"133":2}}],["模型开始过拟合训练数据中的噪声",{"5":{"51":2}}],["模型快速学习预测任务",{"5":{"133":2}}],["模型更容易沿着噪声较强的方向移动",{"5":{"149":2}}],["模型更关注保留关于",{"5":{"133":1}}],["模型更关注保留关于的信息",{"5":{"133":1}}],["模型更强调压缩输入信息",{"5":{"133":2}}],["模型更偏好",{"5":{"99":1,"101":1}}],["模型更偏好而非",{"5":{"99":1,"101":1}}],["模型在该任务上的性能接近随机",{"5":{"138":2}}],["模型在该任务上表现不佳",{"5":{"99":2,"101":2}}],["模型在误差大的样本上接收更大的梯度信号",{"5":{"51":2}}],["模型倾向于学习更",{"5":{"145":2}}],["模型倾向于过拟合",{"5":{"138":2}}],["模型倾向于在该任务上表现良好",{"5":{"99":2,"101":2}}],["模型倾向于选择高概率的词元",{"5":{"96":2}}],["模型难以学习有效的表示",{"5":{"99":2,"101":2}}],["模型被训练来根据给定的提示生成内容",{"5":{"99":2,"101":2}}],["模型被迫学习最相关的特征表示",{"5":{"94":2}}],["模型将退化为",{"5":{"99":2,"101":2}}],["模型将退化为简单的加权平均",{"5":{"71":2}}],["模型首先计算一个未归一化的logit向量",{"5":{"99":2,"101":2}}],["模型中",{"5":{"70":2}}],["模型常常需要同时优化多个目标",{"5":{"70":2}}],["模型越错误",{"5":{"70":2}}],["模型预测接近均匀分布",{"5":{"137":2}}],["模型预测分布的熵通常随训练时间呈现以下变化规律",{"5":{"137":2}}],["模型预测和泛化能力方面",{"5":{"137":2}}],["模型预测概率分布为",{"5":{"70":2}}],["模型预测为",{"5":{"70":4}}],["模型预测",{"5":{"69":2}}],["模型预测的概率分布为",{"5":{"61":2}}],["模型预测的概率分布记为",{"5":{"61":2}}],["模型的总参数量是",{"5":{"159":2}}],["模型的参数规模可以大幅扩展",{"5":{"159":2}}],["模型的最终输出是专家输出的加权组合",{"5":{"159":2}}],["模型的最终输出为",{"5":{"159":2}}],["模型的核心思想是将复杂的联合概率建模问题转化为一系列简单的条件概率建模问题",{"5":{"140":2}}],["模型的自注意力机制决定了每个位置关注哪些前面的token",{"5":{"99":2,"101":2}}],["模型的输出分布满足",{"5":{"99":2,"101":2}}],["模型的输入通常是一个变长的词元序列",{"5":{"94":2}}],["模型的不同层",{"5":{"45":1}}],["模型的注意力权重分布",{"5":{"87":2,"152":2}}],["模型的泛化误差与模型的复杂度",{"5":{"87":2,"152":2}}],["模型的泛化能力可能提升",{"5":{"93":2}}],["模型的表示能力可能受到限制",{"5":{"93":2}}],["模型并行",{"5":{"45":2}}],["模型并行化将大模型分布在多个计算设备上",{"5":{"50":2}}],["模型级并行",{"5":{"45":1}}],["模型克服了这些局限性",{"5":{"47":2}}],["模型为词汇表中的每个词分配一个概率",{"5":{"96":2}}],["模型a优于模型b",{"5":{"96":2}}],["模型层面",{"5":{"61":2}}],["模型",{"5":{"61":2,"95":2,"99":2,"101":2,"133":2,"138":2,"140":2}}],["模型分布",{"5":{"61":3}}],["模型分布由softmax函数给出",{"5":{"61":1}}],["模型最后一层",{"5":{"61":2}}],["模型通过参数定义了一个预测函数",{"5":{"51":1}}],["模型通过参数",{"5":{"51":1}}],["模型通过学习",{"5":{"87":2,"152":2}}],["模型通过学习低秩的注意力模式",{"5":{"87":2,"152":2}}],["模型通过学习这些投影矩阵",{"5":{"94":2}}],["模型复杂度与似然之间的权衡",{"5":{"139":2}}],["模型复杂度可以用多项式的阶数",{"5":{"51":2}}],["模型复杂度较低",{"5":{"87":2,"152":2}}],["模型需要在保留任务相关信息的同时丢弃输入中的冗余信息",{"5":{"133":2}}],["模型需要基于前缀预测后续token",{"5":{"99":2,"101":2}}],["模型需要更少的训练样本来学习良好的参数",{"5":{"87":2,"152":2}}],["模型需要从复杂的混合信号中",{"5":{"88":2}}],["模型需要",{"5":{"88":2,"89":2}}],["模型需要根据问题",{"5":{"94":2}}],["模型来模仿较大的",{"5":{"87":2,"152":2}}],["模型无法捕捉数据的真实模式",{"5":{"51":2}}],["模型无法简单地",{"5":{"88":2}}],["模型能否在推理时处理更长的序列",{"5":{"88":2}}],["模型能够准确估计其生成概率",{"5":{"140":2}}],["模型能够高效地并行处理大量数据",{"5":{"50":2}}],["模型能够在底层捕获局部特征",{"5":{"92":2,"153":2}}],["模型能够确定应该",{"5":{"95":2}}],["模型只能近似前几层依赖关系",{"5":{"138":2}}],["模型只能学习到近似分布",{"5":{"138":2}}],["模型只能学习到一种类型的关联模式",{"5":{"93":2}}],["模型只能感知相对位置",{"5":{"88":2}}],["模型只能使用插值或外推策略",{"5":{"89":2}}],["模型学习的过程",{"5":{"51":2}}],["模型学习到的是位置的",{"5":{"90":2}}],["模型学习到的重要特征会对应于较大的奇异值",{"5":{"94":2}}],["模型学习到的决定了什么样的输入模式会产生较高的注意力分数",{"5":{"94":1}}],["模型学习到的",{"5":{"94":1}}],["模型必须学习平滑的位置表示",{"5":{"90":2}}],["模型可能仍然陷入负载不均衡",{"5":{"159":2}}],["模型可能学会生成冗长",{"5":{"154":2}}],["模型可能对这些样本产生过度自信的预测",{"5":{"99":2,"101":2}}],["模型可能需要处理比训练时更长的序列",{"5":{"88":2}}],["模型可能无法学习到有区分性的表示",{"5":{"69":2}}],["模型可能无法学习到这些位置的可靠编码",{"5":{"89":2}}],["模型可能过拟合或学习到不合理的依赖模式",{"5":{"92":2,"153":2}}],["模型可以学习恢复任意的均值和方差",{"5":{"150":2}}],["模型可以学习到当查询位置是主语词汇",{"5":{"92":2,"153":2}}],["模型可以学习到当查询位置是代词",{"5":{"92":2,"153":2}}],["模型可以学习到",{"5":{"92":2,"153":2}}],["模型可以学习到与距离相关的注意力权重模式",{"5":{"92":2,"153":2}}],["模型可以在不同的子空间中计算其与其他位置的关联强度",{"5":{"93":2}}],["模型就能够捕获任意位置对之间的依赖关系",{"5":{"92":2,"153":2}}],["模型再根据这个分布进行词元的采样或选择",{"5":{"96":2}}],["模型输出的概率分布",{"5":{"137":1}}],["模型输出的概率分布的熵量化了模型预测的不确定性",{"5":{"137":1}}],["模型输出的未归一化分数",{"5":{"61":2}}],["模型输出logits而非直接预测概率具有深刻的理论依据",{"5":{"96":2}}],["模型有足够的样本和容量来提取和记忆它们",{"5":{"138":2}}],["模型有足够的容量来拟合训练数据",{"5":{"97":2}}],["模型有着有趣的类比",{"5":{"93":2}}],["模糊化",{"5":{"88":2}}],["模式",{"5":{"90":2}}],["模长较大的向量更容易获得较大的点积值",{"5":{"95":2}}],["思想可以迁移到注意力机制中",{"5":{"70":2}}],["思路是",{"5":{"88":2}}],["目标秩",{"5":{"143":2}}],["目标随机变量",{"5":{"133":1}}],["目标随机变量和编码变量",{"5":{"133":1}}],["目标可重写为更直观的形式",{"5":{"133":2}}],["目标函数定义为",{"5":{"154":2}}],["目标函数也不会再增加",{"5":{"154":2}}],["目标函数就是",{"5":{"154":4}}],["目标函数由两部分取最小值组成",{"5":{"154":2}}],["目标函数不会被进一步优化",{"5":{"99":2,"101":2}}],["目标函数值与最优值的差距为",{"5":{"48":2}}],["目标则需要基于前缀进行自回归生成",{"5":{"99":2,"101":2}}],["目标",{"5":{"99":2,"101":2,"132":2,"133":2}}],["目标是最大化所有未来奖励的无折扣总和",{"5":{"156":2}}],["目标是最大化预测性",{"5":{"137":2}}],["目标是最小化重构误差",{"5":{"69":2}}],["目标是高效传输教师模型中的知识",{"5":{"133":2}}],["目标是学习一个从输入",{"5":{"70":2}}],["目标是学习一个从输入到离散类别标签的映射",{"5":{"70":1}}],["目标是学习一个从输入到连续输出的映射函数",{"5":{"70":1}}],["目标是在中寻找与目标向量距离最小的点",{"5":{"51":1}}],["目标是在",{"5":{"51":1}}],["目标向量",{"5":{"51":3}}],["目标向量对应空间中的一个点",{"5":{"51":1}}],["目标向量向预测空间",{"5":{"51":2}}],["目标向量的平方范数等于可解释部分",{"5":{"51":2}}],["目标变量服从高斯分布",{"5":{"51":1}}],["目标变量",{"5":{"51":1}}],["目标分布",{"5":{"61":2}}],["目前主流的大语言模型",{"5":{"88":2}}],["仍然能够保持至少",{"5":{"150":2}}],["仍然保持正值",{"5":{"146":2}}],["仍然是一个活跃的研究领域",{"5":{"88":2}}],["仍保持有界",{"5":{"135":2}}],["仍为超平面",{"5":{"47":2}}],["性能优异",{"5":{"146":1}}],["性能相当",{"5":{"146":3}}],["性能波动剧烈",{"5":{"138":2}}],["性能迅速提升到远高于随机的水平",{"5":{"138":2}}],["性能提升速度显著加快或减慢",{"5":{"138":2}}],["性能上的竞争力是第四个因素",{"5":{"88":1}}],["性能上的竞争力",{"5":{"88":1}}],["性",{"5":{"137":2}}],["性质或数据处理不等式的特殊情况",{"5":{"137":2}}],["性质2的证明利用",{"5":{"136":1}}],["性质2的证明利用的连续性和极限的交换性",{"5":{"136":1}}],["性质3是连续函数的基本性质",{"5":{"136":2}}],["性质1的直接证明利用三角不等式",{"5":{"136":2}}],["性质",{"5":{"48":2,"97":2,"136":2,"147":4}}],["性质分析",{"5":{"61":2}}],["性质及其在分类任务中的应用",{"5":{"61":4}}],["性质及其设计原理",{"5":{"91":2}}],["技术的出现",{"5":{"150":2}}],["技术",{"5":{"144":2,"155":2}}],["技术来处理长上下文场景",{"5":{"88":2}}],["技巧",{"5":{"61":4}}],["技巧的核心思想是",{"5":{"61":2}}],["段落的首句往往承担着引言或总结的功能",{"5":{"88":2}}],["段落结构",{"5":{"88":2}}],["动作价值函数",{"5":{"156":2}}],["动作就是生成下一个词",{"5":{"156":2}}],["动作空间",{"5":{"155":2,"156":2}}],["动作对的动作价值",{"5":{"155":2}}],["动作",{"5":{"154":4,"155":6,"156":2}}],["动力学解释",{"5":{"134":2}}],["动量来平衡收敛速度和震荡抑制",{"5":{"136":2}}],["动量优化和稳定性控制方法",{"5":{"136":2}}],["动量优化器的分岔图结构",{"5":{"136":2}}],["动量通过先预测后校正的策略减少震荡",{"5":{"136":2}}],["动量通过扩大稳定区域",{"5":{"135":2}}],["动量参数空间中",{"5":{"136":1}}],["动量参数的选择对系统的震荡行为有重要影响",{"5":{"136":2}}],["动量与学习率的特定组合",{"5":{"136":2}}],["动量变量",{"5":{"136":2}}],["动量变量可类比于物理中的速度",{"5":{"134":2}}],["动量系数与震荡行为的关系",{"5":{"136":2}}],["动量系数与震荡的关系",{"5":{"136":2}}],["动量系数选择",{"5":{"134":2}}],["动量系统的有效阻尼比可表示为",{"5":{"136":2}}],["动量系统的特征方程与震荡判据",{"5":{"136":2}}],["动量系统的扩展相空间",{"5":{"136":2}}],["动量的改进机制",{"5":{"136":2}}],["动量的等效相位滞后效应",{"5":{"136":2}}],["动量的等效形式",{"5":{"134":2}}],["动量的加速效果",{"5":{"135":2}}],["动量的稳定性条件",{"5":{"134":2}}],["动量的连续时间极限",{"5":{"134":2}}],["动量更新可以展开为历史梯度的指数加权平均",{"5":{"136":2}}],["动量更新可重写为二阶系统",{"5":{"134":2}}],["动量更新对应于惯性效应",{"5":{"134":2}}],["动量更新规则",{"5":{"134":2}}],["动量更新的公式为",{"5":{"48":2}}],["动量等超参数可解释为系统动力学参数",{"5":{"134":2}}],["动量方法将原始的状态空间从",{"5":{"136":1}}],["动量方法将原始的状态空间从扩展到",{"5":{"136":1}}],["动量方法通过累积历史梯度信息来加速收敛并抑制震荡",{"5":{"136":2}}],["动量方法等优化算法纳入统一的分析框架",{"5":{"135":2}}],["动量方法退化为重球法",{"5":{"134":2}}],["动量方法的特征方程为",{"5":{"136":2}}],["动量方法的稳定性条件",{"5":{"135":2}}],["动量方法的",{"2":{"135":1},"5":{"135":1}}],["动量方法的收敛条件为",{"5":{"134":2}}],["动量方法的连续极限满足",{"5":{"134":2}}],["动量方法的动力学分析",{"2":{"134":1},"5":{"134":1}}],["动量方法",{"5":{"48":2,"134":2}}],["动量方法可以看作是对梯度进行指数移动平均",{"5":{"48":2}}],["动量方法和噪声梯度有助于算法逃离鞍点",{"5":{"48":2}}],["动量",{"5":{"51":2,"136":1}}],["动态路由还需要考虑设备间的通信延迟",{"5":{"157":2}}],["动态路由",{"5":{"157":2}}],["动态秩调整",{"5":{"142":1}}],["动态系统称为在区间内呈现震荡行为",{"5":{"136":1}}],["动态系统视角揭示了学习率与系统特征值之间的关系",{"5":{"134":2}}],["动态系统",{"5":{"134":2,"136":1}}],["动态系统是描述系统状态随时间演化的数学模型",{"5":{"134":2}}],["动态系统与训练稳定性",{"2":{"131":1},"4":{"124":1,"125":1,"126":1,"134":1,"135":1,"136":1},"5":{"131":7}}],["动态调整",{"5":{"133":1}}],["动态调整可以适应不同训练阶段的需求",{"5":{"133":1}}],["动态地决定哪些信息需要被关注",{"5":{"132":2}}],["动态位置编码探索根据输入内容自适应地调整位置编码",{"5":{"88":1}}],["动态位置编码",{"5":{"88":1}}],["动态损失缩放通常效果更好",{"5":{"97":2}}],["识别信息压缩的瓶颈位置",{"5":{"132":2}}],["识别信息瓶颈",{"5":{"132":2}}],["识别正样本",{"5":{"69":1}}],["识别句子边界并在这些位置给予更强的位置标记",{"5":{"88":2}}],["启发式设计",{"5":{"88":2}}],["启发式",{"5":{"88":2}}],["绕原点逆时针旋转了",{"5":{"88":1}}],["乘积的范数也可能很小",{"5":{"148":2}}],["乘子和吸引域",{"5":{"136":2}}],["乘子满足",{"5":{"136":2}}],["乘子",{"5":{"133":2,"136":4}}],["乘以一个正数",{"5":{"146":1}}],["乘以",{"5":{"88":1}}],["乘法变换",{"5":{"88":1}}],["质心位置反映了编码向量的",{"5":{"89":2}}],["满足归一化条件",{"5":{"158":2}}],["满足零均值和单位方差",{"5":{"146":1}}],["满足零均值和单位方差的统计性质",{"5":{"146":2}}],["满足所谓的",{"5":{"143":1}}],["满足特定特征值条件",{"5":{"136":2}}],["满足但对任意有的点",{"5":{"136":1}}],["满足以下两条基本公理",{"5":{"134":2}}],["满足以下两个条件",{"5":{"71":2,"136":1}}],["满足以下自洽方程",{"5":{"133":1}}],["满足",{"5":{"41":1,"42":2,"50":2,"87":5,"88":2,"134":4,"135":2,"136":5,"142":1,"147":2,"150":2,"152":5,"155":2}}],["满足大规模训练的需求",{"5":{"47":2}}],["满足概率分布的所有公理",{"5":{"71":1}}],["满足概率的基本要求",{"5":{"47":2}}],["满足和",{"5":{"50":1,"87":1,"152":1}}],["满足非负性",{"5":{"50":2}}],["满足一个关键的性质",{"5":{"51":1}}],["满足幂等性",{"5":{"51":1}}],["满秩情况",{"5":{"89":2}}],["满秩",{"5":{"89":2}}],["逼近目标函数",{"5":{"45":1}}],["逼近误差为零",{"5":{"89":2}}],["逼近的方向很重要",{"5":{"89":2}}],["难度",{"5":{"51":2}}],["难以收敛到好的解",{"5":{"148":2}}],["难以通过梯度下降求解",{"5":{"144":2}}],["难以预测收敛点",{"5":{"136":2}}],["难以捕捉动态训练过程中的信息流动变化",{"5":{"133":2}}],["难以泛化到未见过的位置",{"5":{"89":2}}],["难以表达相对位置信息",{"5":{"91":2}}],["想象",{"5":{"89":2}}],["干扰内容信息的学习",{"5":{"89":2}}],["评分低的优先剪枝",{"5":{"132":2}}],["评分函数可以是神经网络",{"5":{"86":2,"151":2}}],["评估每层保留了多少关于目标的信息",{"5":{"132":2}}],["评估指标为验证集准确率或困惑度",{"5":{"89":2}}],["占总参数量的比例不可忽略",{"5":{"89":2}}],["占比约3",{"5":{"89":2}}],["太大",{"5":{"158":2,"159":4}}],["太大的初始化可能导致训练不稳定",{"5":{"89":2}}],["太小",{"5":{"158":2,"159":4}}],["太小的初始化可能限制位置信息的作用",{"5":{"89":2}}],["太远",{"5":{"99":1,"101":1,"154":4}}],["讨论了互信息估计的实际方法",{"5":{"133":2}}],["讨论了位置编码的信息容量限制",{"5":{"132":2}}],["讨论了数值稳定性问题及其解决方案",{"5":{"61":2}}],["讨论了混合位置编码的设计",{"5":{"89":2}}],["讨论了训练良好的模型中不同头会发展出",{"5":{"93":2}}],["奠定了理论基础",{"5":{"61":2,"70":2,"71":2}}],["奠定了坚实的基础",{"5":{"51":2}}],["奠定了坚实的理论基础",{"5":{"90":2}}],["奠定坚实的理论基础",{"5":{"90":2}}],["周期",{"5":{"136":2}}],["周期学习率使参数在多个解之间震荡",{"5":{"136":2}}],["周期学习率产生的参数震荡起到类似随机扰动的正则化效果",{"5":{"136":2}}],["周期学习率调度",{"5":{"136":2}}],["周期学习率的正则化机制",{"5":{"136":2}}],["周期倍增序列",{"5":{"136":2}}],["周期倍增分岔的间隔比趋于普适常数",{"5":{"136":2}}],["周期倍增分岔的发生机制",{"5":{"136":2}}],["周期倍增分岔是最典型的分岔类型之一",{"5":{"136":2}}],["周期2区",{"5":{"136":2}}],["周期2轨道也会失稳",{"5":{"136":2}}],["周期8等轨道",{"5":{"136":4}}],["周期轨道的吸引域",{"5":{"136":2}}],["周期轨道的稳定性决定了训练过程中周期行为的持久性",{"5":{"136":2}}],["周期轨道稳定的充要条件是所有",{"5":{"136":2}}],["周期轨道稳定性的",{"5":{"136":2}}],["周期轨道",{"5":{"136":1}}],["周期点在系统的吸引子集合中稠密",{"5":{"136":2}}],["周期点的稠密性",{"5":{"136":2}}],["周期点与周期轨道",{"5":{"136":2}}],["周期点",{"5":{"134":2,"136":1}}],["周期行为与极限环",{"5":{"136":2}}],["周期行为<",{"5":{"131":1}}],["周期行为",{"0":{"136":1},"4":{"126":1,"136":1},"5":{"131":4,"136":1}}],["周期为",{"5":{"88":2,"90":2}}],["周期为1",{"5":{"90":2}}],["周期为10000",{"5":{"90":2}}],["周期函数可以展开为",{"5":{"90":2}}],["周期性扰动的正则化原理",{"5":{"136":2}}],["周期性数据增强",{"5":{"136":2}}],["周期性",{"5":{"90":2}}],["周期很长",{"5":{"91":2}}],["周期很短",{"5":{"91":2}}],["逆矩阵的谱范数等于最小奇异值的倒数",{"5":{"148":2}}],["逆元",{"5":{"88":2}}],["逆变换为",{"5":{"90":2}}],["连续变量的熵估计",{"5":{"137":2}}],["连续",{"5":{"136":2,"139":1}}],["连续性是分析优化算法的重要工具",{"5":{"147":2}}],["连续性",{"5":{"136":2}}],["连续性可得梯度下降的二阶泰勒展开上界",{"5":{"136":2}}],["连续性条件",{"5":{"136":2}}],["连续极限",{"5":{"134":2}}],["连续极限理解",{"5":{"134":2}}],["连续对应关系",{"5":{"134":2}}],["连续可微",{"5":{"47":2}}],["连续情况",{"5":{"90":2}}],["连续随机变量可以取任意实数值或实数区间内的值",{"5":{"96":2}}],["连续随机变量由概率密度函数",{"5":{"96":2}}],["冲激",{"5":{"90":2}}],["答案是否定的",{"5":{"51":2}}],["答案取决于具体任务和评估标准",{"5":{"90":2}}],["呈现混沌特性",{"5":{"136":2}}],["呈现对初始条件的敏感性",{"5":{"136":2}}],["呈现交替震荡行为",{"5":{"136":2}}],["呈现稳定的静态行为",{"5":{"136":2}}],["呈现出某种",{"5":{"91":2}}],["呈周期性变化",{"5":{"90":1}}],["呈指数级增长",{"5":{"41":2}}],["呈指数级变化",{"5":{"91":1}}],["互补性度量反映了不同头学习信息的关系",{"5":{"132":2}}],["互补性与冗余",{"5":{"132":2}}],["互信息建立了输入",{"5":{"137":2}}],["互信息下界",{"5":{"137":2}}],["互信息衡量了模型对训练数据的",{"5":{"137":1}}],["互信息衡量的是知道后关于的信息量",{"5":{"69":1,"71":1}}],["互信息衡量的是知道",{"5":{"69":1,"71":1}}],["互信息与泛化的关系",{"5":{"137":2}}],["互信息正则化",{"5":{"137":2}}],["互信息正则化损失",{"5":{"133":2}}],["互信息满足以下重要性质",{"5":{"137":2}}],["互信息等于",{"5":{"137":2}}],["互信息可用熵和条件熵表示为多种等价形式",{"5":{"137":2}}],["互信息可用于分析不同位置或不同层之间的信息流动",{"5":{"96":2}}],["互信息越大表示任务",{"5":{"133":1}}],["互信息越大表示任务与共享参数的依赖越强",{"5":{"133":1}}],["互信息上界为理解和预测模型泛化能力提供了工具",{"5":{"133":2}}],["互信息估计值",{"5":{"137":2}}],["互信息估计的偏差",{"5":{"133":2}}],["互信息估计",{"5":{"133":2,"137":2}}],["互信息呈指数衰减",{"5":{"133":2}}],["互信息图",{"5":{"132":2}}],["互信息的视角",{"2":{"139":1},"5":{"139":1}}],["互信息的表示学习",{"5":{"137":2}}],["互信息的性质",{"5":{"137":2}}],["互信息的等价表达式",{"5":{"137":2}}],["互信息的深层理论",{"2":{"137":1},"5":{"137":1}}],["互信息的链式关系",{"5":{"132":2}}],["互信息的次可加性和冗余的定义",{"5":{"132":2}}],["互信息的下界估计",{"5":{"71":2}}],["互信息最大化",{"5":{"69":1}}],["互信息最大化的视角",{"2":{"69":1},"5":{"69":1}}],["互信息是衡量两个随机变量之间依赖程度的指标",{"5":{"137":2}}],["互信息是比传统",{"5":{"133":1}}],["互信息是信息论中度量两个随机变量之间依赖程度的基本概念",{"5":{"69":2}}],["互信息是非负的",{"5":{"71":2}}],["互信息",{"5":{"69":2,"71":2,"92":1,"96":2,"133":1,"137":7,"139":2,"153":1}}],["互信息为1",{"5":{"92":2,"153":2}}],["互信息逐渐降低",{"5":{"92":2,"153":2}}],["互信息在高维空间难以精确估计",{"5":{"133":2}}],["互信息在各层都较高",{"5":{"92":1,"153":1}}],["互信息在底层较低",{"5":{"92":2,"153":2}}],["互信息始终非负",{"5":{"96":2}}],["互不相同",{"5":{"90":1}}],["辐角增加​",{"5":{"90":1}}],["辐角增加",{"5":{"90":1}}],["兼容线性代数结构",{"5":{"88":2}}],["兼容性",{"5":{"91":2}}],["奇数维度",{"5":{"91":2}}],["奇异值之和",{"5":{"144":1}}],["奇异值对齐",{"5":{"142":2}}],["奇异值衰减的规律是什么",{"5":{"142":2}}],["奇异值衰减的速度与层深",{"5":{"87":2,"152":2}}],["奇异值大于1的方向会导致梯度放大",{"5":{"41":2}}],["奇异值小于1的方向会导致梯度衰减",{"5":{"41":2}}],["奇异值决定了梯度在对应方向上的缩放因子",{"5":{"41":1}}],["奇异值和权重初始化的数学原理",{"5":{"41":2}}],["奇异值",{"5":{"41":1,"143":2}}],["奇异值为对角线上特征值的平方根",{"5":{"45":1}}],["奇异值为",{"5":{"45":1}}],["奇异值与梯度范数分析",{"2":{"41":1},"5":{"41":1}}],["奇异值与低秩近似",{"2":{"50":1},"5":{"50":1}}],["奇异值分解视角",{"2":{"148":1},"5":{"148":1}}],["奇异值分解定理",{"5":{"45":2}}],["奇异值分解揭示了线性变换的几何本质",{"5":{"45":2}}],["奇异值分解",{"2":{"143":1},"5":{"50":2,"87":2,"94":2,"143":3,"152":2}}],["奇异值分解与特征值分解有密切关系",{"5":{"50":2}}],["奇异值分解之所以强大",{"5":{"50":2}}],["奇异值分解可以用于理解模型的表达能力",{"5":{"50":2}}],["奇异值分布通常呈现以下模式",{"5":{"87":2,"152":2}}],["奇异值分布更均匀",{"5":{"87":2,"152":2}}],["奇异值分布的实证分析",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["奇异值分布的分析对于理解模型行为有重要价值",{"5":{"87":2,"152":2}}],["奇异值分布",{"5":{"89":2}}],["奇异值分布揭示了编码的结构特性",{"5":{"89":2}}],["奇异值的数量",{"5":{"41":2}}],["奇异值的大小决定了各个方向上的",{"5":{"94":2}}],["奇异矩阵",{"5":{"94":2}}],["写成向量形式即",{"5":{"44":2}}],["写成更紧凑的形式",{"5":{"91":2}}],["角度二",{"5":{"145":2}}],["角度一",{"5":{"145":2}}],["角度理解",{"5":{"137":2}}],["角度可以无限增长",{"5":{"88":2}}],["角度变化不会过快",{"5":{"88":2}}],["角频率​随维度呈指数级变化",{"5":{"91":1}}],["角频率",{"5":{"91":1}}],["精度下溢",{"5":{"44":1}}],["精确位置",{"5":{"91":2}}],["精确率等指标的置信区间",{"5":{"96":2}}],["注记",{"5":{"140":7,"145":6,"146":6,"148":6,"149":4}}],["注意到",{"5":{"158":2}}],["注意力和线性注意力在信息论特性上有不同的权衡",{"5":{"132":2}}],["注意力熵正则化对注意力分布有不同的调节效果",{"5":{"132":2}}],["注意力熵损失",{"5":{"132":2}}],["注意力熵满足",{"5":{"132":2}}],["注意力熵",{"5":{"132":2}}],["注意力熵与信息选择的集中程度之间存在直接的对应关系",{"5":{"132":2}}],["注意力熵与信息选择的关系",{"5":{"132":2}}],["注意力信息瓶颈",{"5":{"132":2}}],["注意力高度集中",{"5":{"132":2}}],["注意力作为软选择的数学表达",{"5":{"69":2}}],["注意力作为广义对比学习",{"2":{"69":1},"5":{"69":1}}],["注意力分散",{"5":{"132":2}}],["注意力分布的熵定义为分布不确定性的度量",{"5":{"132":2}}],["注意力分布",{"5":{"69":2,"132":2}}],["注意力分布较为均匀",{"5":{"92":2,"153":2}}],["注意力分布较为集中",{"5":{"92":2,"153":2}}],["注意力分数来自",{"5":{"70":2}}],["注意力分数或",{"5":{"70":2}}],["注意力分数被缩放",{"5":{"69":1}}],["注意力分数被缩放​​",{"5":{"69":1}}],["注意力分数",{"5":{"41":2,"94":1}}],["注意力分数的熵正则化",{"5":{"132":2}}],["注意力分数的方差会很大",{"5":{"41":2}}],["注意力分数的计算可以看作是一个张量收缩操作",{"5":{"50":2}}],["注意力分数的计算在批处理情况下变成的三维张量运算",{"5":{"50":1}}],["注意力分数的计算需要的计算量",{"5":{"50":1}}],["注意力分数的计算",{"5":{"50":2}}],["注意力分数的矩阵分解表示",{"2":{"94":1},"5":{"94":1}}],["注意力分数矩阵​​可以视为一种核矩阵",{"5":{"87":1,"152":1}}],["注意力分数矩阵",{"5":{"87":1,"92":2,"93":2,"94":1,"95":1,"152":1,"153":2}}],["注意力分数矩阵为",{"5":{"91":2}}],["注意力分数矩阵可以近似为​",{"5":{"87":1,"152":1}}],["注意力分数矩阵可以近似为",{"5":{"87":1,"152":1}}],["注意力分数矩阵可以表示为",{"5":{"94":1}}],["注意力分数矩阵的几何意义",{"2":{"95":1},"5":{"95":1}}],["注意力分数矩阵的元素为",{"5":{"95":2}}],["注意力分数矩阵和注意力权重矩阵都需要的存储空间",{"5":{"95":1}}],["注意力分数只依赖于相对位置",{"5":{"91":2}}],["注意力分数不仅取决于查询和键的内容",{"5":{"92":2,"153":2}}],["注意力分数特别高",{"5":{"92":2,"153":2}}],["注意力分数实际上是在定义的二次型下",{"5":{"94":1}}],["注意力模式",{"5":{"92":2,"99":2,"101":2,"153":2}}],["注意力权重趋向于在最大相似性附近集中",{"5":{"141":2}}],["注意力权重趋向于某种极限分布",{"5":{"141":2}}],["注意力权重逐渐",{"5":{"141":2}}],["注意力权重通过",{"5":{"132":2}}],["注意力权重定义了在给定查询条件下",{"5":{"132":2}}],["注意力权重向量为",{"5":{"132":2}}],["注意力权重与概率分布的类比",{"2":{"70":1},"5":{"70":1}}],["注意力权重与值矩阵的乘积产生加权输出",{"5":{"50":2}}],["注意力权重与值向量的乘积同样需要",{"5":{"50":2}}],["注意力权重与位置距离有明确的关系",{"5":{"92":2,"153":2}}],["注意力权重矩阵定义为",{"5":{"141":1}}],["注意力权重矩阵定义了一种依赖关系图",{"5":{"92":1,"153":1}}],["注意力权重矩阵定义了一个有向加权图",{"5":{"92":1,"153":1}}],["注意力权重矩阵具有若干与概率分布类比的性质",{"5":{"70":1}}],["注意力权重矩阵的计算涉及softmax操作",{"5":{"70":1}}],["注意力权重矩阵的谱性质决定了梯度如何在不同位置之间传播",{"5":{"41":2}}],["注意力权重矩阵的谱性质决定了梯度如何被",{"5":{"41":1}}],["注意力权重矩阵",{"5":{"41":2,"50":1,"61":2,"70":2,"92":2,"141":1,"153":2}}],["注意力权重矩阵可以看作是查询张量和键张量的收缩",{"5":{"50":1}}],["注意力权重矩阵是非负行随机矩阵",{"5":{"41":1}}],["注意力权重矩阵是softmax作用于的结果",{"5":{"91":1}}],["注意力权重矩阵是softmax作用于",{"5":{"91":1}}],["注意力权重",{"5":{"61":2,"69":3,"92":2,"96":2,"153":2}}],["注意力权重本身是一种概率分布",{"5":{"41":2}}],["注意力权重的稳定性分析可以借鉴",{"5":{"70":2}}],["注意力权重的谱结构",{"5":{"41":2}}],["注意力权重的计算公式为",{"5":{"61":2,"71":2}}],["注意力权重的计算将query",{"5":{"71":1}}],["注意力权重的计算",{"5":{"71":1}}],["注意力权重的计算结果只依赖于新的顺序",{"5":{"92":2,"153":2}}],["注意力权重的归一化与信息聚合",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["注意力权重的分布特性",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["注意力权重的空间分布",{"5":{"92":2,"153":2}}],["注意力权重的概率解释",{"2":{"95":1},"5":{"95":1,"132":2}}],["注意力权重的温度参数推广",{"2":{"95":1},"5":{"95":1}}],["注意力权重的梯度可能具有特殊结构",{"5":{"97":2}}],["注意力权重的",{"5":{"97":2}}],["注意力权重应该较大",{"5":{"92":4,"153":4}}],["注意力权重分布在语义相关的位置上",{"5":{"92":2,"153":2}}],["注意力权重​同时依赖于位置和内容​",{"5":{"92":1,"153":1}}],["注意力权重​正确地反映了位置对位置的重要性",{"5":{"92":1,"153":1}}],["注意力权重可以满足关注",{"5":{"86":1,"151":1}}],["注意力权重可以满足",{"5":{"86":1,"151":1}}],["注意力权重可以被解释为一种条件概率分布",{"5":{"95":2}}],["注意力输出",{"5":{"70":1,"92":1,"93":1,"153":1}}],["注意力输出为",{"5":{"41":2,"69":2,"86":2,"87":2,"92":2,"151":2,"152":2,"153":2}}],["注意力输出需要经过前馈网络处理",{"5":{"41":2}}],["注意力输出可以理解为value向量在注意力权重分布下的期望",{"5":{"70":1}}],["注意力输出可以视为对值向量的",{"5":{"92":1,"153":1}}],["注意力输出的第个元素为",{"5":{"92":1,"153":1}}],["注意力输出的第",{"5":{"92":1,"153":1}}],["注意力输出的信息量受到其维度​和注意力权重的约束",{"5":{"93":1}}],["注意力机制分析",{"5":{"143":2}}],["注意力机制有多种扩展形式",{"5":{"132":2}}],["注意力机制实现了信息路由功能",{"5":{"132":2}}],["注意力机制可视为信息瓶颈的实例",{"5":{"132":2}}],["注意力机制可以被视为一种广义的对对比学习",{"5":{"69":2}}],["注意力机制是现代深度学习的核心创新之一",{"5":{"132":2}}],["注意力机制提供了动态调整关注焦点的机制",{"5":{"99":2,"101":2}}],["注意力机制提供了一种",{"5":{"92":2,"153":2}}],["注意力机制虽然不是直接源于信息论",{"5":{"69":2}}],["注意力机制使用这个分布对value向量进行加权平均",{"5":{"69":2}}],["注意力机制使用softmax计算query与各key之间的注意力权重",{"5":{"61":2}}],["注意力机制",{"5":{"69":2,"71":4,"95":2,"101":2}}],["注意力机制中的互信息分析",{"2":{"132":1},"5":{"132":1}}],["注意力机制中的softmax和ffn中的gelu形成了",{"5":{"71":2}}],["注意力机制中的查询",{"5":{"50":2}}],["注意力机制的张量视角",{"2":{"143":1},"5":{"143":1}}],["注意力机制的信息论分析不仅深化了我们对这一核心组件的理解",{"5":{"132":2}}],["注意力机制的信息论基础",{"2":{"132":1},"5":{"132":1}}],["注意力机制的目标是在容量约束下最大化信息保留",{"5":{"132":2}}],["注意力机制的本质可以理解为一种自适应信息处理系统",{"5":{"132":2}}],["注意力机制的扩展与前沿",{"2":{"132":1},"5":{"132":1}}],["注意力机制的优化与正则化",{"2":{"132":1},"5":{"132":1}}],["注意力机制的批处理实现是理解这一概念的良好例子",{"5":{"50":2}}],["注意力机制的计算过程就包含了多个矩阵乘法操作",{"5":{"50":2}}],["注意力机制的计算复杂度与序列长度的平方成正比",{"5":{"50":2}}],["注意力机制的梯度流可以视为一种信息瓶颈",{"5":{"41":2}}],["注意力机制的梯度计算可以借鉴分类任务中",{"5":{"61":2}}],["注意力机制的变体",{"0":{"86":1,"151":1},"4":{"86":1,"105":1,"151":1},"5":{"85":4,"86":1,"131":4,"151":1}}],["注意力机制的变体<",{"5":{"85":1,"131":1}}],["注意力机制的归纳偏置",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["注意力机制的归纳偏置包括",{"5":{"92":2,"153":2}}],["注意力机制的核心功能是在大量信息中选择性地提取相关部分",{"5":{"132":2}}],["注意力机制的核心是一个的注意力矩阵",{"5":{"87":1,"152":1}}],["注意力机制的核心是一个",{"5":{"87":1,"152":1}}],["注意力机制的核心特性是全连接",{"5":{"92":2,"153":2}}],["注意力机制的核心特性是是一个随机矩阵",{"5":{"92":1,"153":1}}],["注意力机制的核心特性是",{"5":{"92":1,"153":1}}],["注意力机制的核心思想",{"2":{"95":1},"5":{"95":1}}],["注意力机制的核心思想可以概括为",{"5":{"95":2}}],["注意力机制的一个重要特性是它可以同时建模位置相关的依赖和内容相关的依赖",{"5":{"92":2,"153":2}}],["注意力机制的内存访问模式更加规则",{"5":{"92":2,"153":2}}],["注意力机制的中间结果",{"5":{"92":2,"153":2}}],["注意力机制的函数空间",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["注意力机制的函数类是",{"5":{"92":2,"153":2}}],["注意力机制的学习目标是学习这个oracle的权重分配",{"5":{"92":2,"153":2}}],["注意力机制的复杂度在超长序列场景下仍然是瓶颈",{"5":{"92":1,"153":1}}],["注意力机制的",{"5":{"92":1,"153":1,"159":2}}],["注意力机制数学",{"2":{"85":1,"131":1},"4":{"86":1,"87":1,"92":1,"93":1,"94":1,"95":1,"102":1,"103":1,"104":1,"105":1,"150":1,"151":1,"152":1,"153":1},"5":{"85":13,"131":15}}],["注意力机制与信息流量<",{"5":{"131":1}}],["注意力机制与信息流量",{"0":{"132":1},"4":{"129":1,"132":1},"5":{"131":4,"132":1}}],["注意力机制与激活函数的协同",{"2":{"71":1},"5":{"71":1}}],["注意力机制与核方法有着深刻的数学关联",{"5":{"87":2,"152":2}}],["注意力机制允许序列中任意两个位置之间直接建立联系",{"5":{"91":2}}],["注意力机制从根本上改变了这一结构",{"5":{"92":2,"153":2}}],["注意力机制通过全局信息访问有效解决了这一问题",{"5":{"132":2}}],["注意力机制通过全连接结构从根本上解决了这一问题",{"5":{"92":2,"153":2}}],["注意力机制通过一种革命性的设计解决了这一问题",{"5":{"92":2,"153":2}}],["注意力机制通过softmax归一化将原始的注意力分数转换为有效的权重分布",{"5":{"92":2,"153":2}}],["注意力机制为",{"5":{"92":2,"153":2}}],["注意力机制在scaled",{"5":{"69":1}}],["注意力机制在内存带宽效率方面也具有优势",{"5":{"92":2,"153":2}}],["注意力机制定义了一类特定的函数",{"5":{"92":2,"153":2}}],["注意力机制能够表示的函数类与传统的循环网络或卷积网络有显著差异",{"5":{"92":2,"153":2}}],["注意力机制具有强大的表达能力",{"5":{"92":2,"153":2}}],["注意力机制达到了这个任务的下界",{"5":{"92":2,"153":2}}],["注意力机制没有",{"5":{"92":4,"153":4}}],["注意力机制和位置编码则提供了实现这些目标的计算框架",{"5":{"101":2}}],["注意力计算本质上测量的是这些向量之间的相似性",{"5":{"141":1}}],["注意力计算",{"5":{"45":2,"132":2,"141":1}}],["注意力计算的时间复杂度和空间复杂度均为",{"5":{"86":2,"151":2}}],["注意力计算的雅可比矩阵涉及softmax的梯度",{"5":{"89":2}}],["注意力计算的矩阵运算流程",{"2":{"95":1},"5":{"95":1}}],["注意力层会优先学习对该任务有利的表示",{"5":{"70":2}}],["注意力层",{"5":{"46":2}}],["注意力层通常被多次堆叠",{"5":{"94":2}}],["注意力如何建模长程依赖",{"0":{"92":1,"153":1},"4":{"92":1,"103":1,"153":1},"5":{"85":4,"92":1,"131":4,"153":1}}],["注意力如何建模长程依赖<",{"5":{"85":1,"131":1}}],["注意力窗口",{"5":{"86":2,"151":2}}],["注意力矩阵",{"5":{"86":1,"87":3,"141":4,"151":1,"152":3}}],["注意力矩阵的计算涉及查询",{"5":{"143":2}}],["注意力矩阵的理论秩上界为",{"5":{"141":2}}],["注意力矩阵的秩上界",{"5":{"141":2}}],["注意力矩阵的秩受query",{"5":{"87":2,"152":2}}],["注意力矩阵的行向量趋向于标准基向量",{"5":{"141":2}}],["注意力矩阵的数学定义",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["注意力矩阵的范数性质",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["注意力矩阵的frobenius范数定义为",{"5":{"87":2,"152":2}}],["注意力矩阵的谱分析与奇异值分布",{"2":{"141":1},"5":{"141":1}}],["注意力矩阵的谱范数定义为",{"5":{"87":2,"152":2}}],["注意力矩阵的谱性质与低秩结构",{"0":{"87":1,"152":1},"4":{"87":1,"104":1,"152":1},"5":{"85":4,"87":1,"131":4,"152":1}}],["注意力矩阵的谱性质与低秩结构<",{"5":{"85":1,"131":1}}],["注意力矩阵的谱性质与gram矩阵有显著不同",{"5":{"87":2,"152":2}}],["注意力矩阵的谱可以展开为特征函数的加权和",{"5":{"87":2,"152":2}}],["注意力矩阵的核范数可以用于度量其有效秩",{"5":{"87":2,"152":2}}],["注意力矩阵的特征值分析",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["注意力矩阵的特征值具有以下性质",{"5":{"87":2,"152":2}}],["注意力矩阵的特征向量揭示了其结构特性",{"5":{"87":2,"152":2}}],["注意力矩阵的条件数取决于注意力权重的分布",{"5":{"87":2,"152":2}}],["注意力矩阵的低秩近似与模型压缩",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["注意力矩阵的低秩结构<",{"5":{"131":1}}],["注意力矩阵的低秩结构",{"0":{"141":1},"4":{"120":1,"141":1},"5":{"131":4,"141":1}}],["注意力矩阵的低秩结构具有直观的解释",{"5":{"87":2,"152":2}}],["注意力矩阵的低秩结构可以解释为一种",{"5":{"87":2,"152":2}}],["注意力矩阵的低秩结构意味着只使用了的维子空间信息",{"5":{"87":1,"152":1}}],["注意力矩阵的低秩结构对其表达能力有重要影响",{"5":{"87":2,"152":2}}],["注意力矩阵的低秩结构为计算优化提供了理论基础",{"5":{"87":2,"152":2}}],["注意力矩阵的低秩结构为模型压缩提供了理论基础",{"5":{"87":2,"152":2}}],["注意力矩阵的低秩结构是transformer高效计算的理论基础",{"5":{"87":2,"152":2}}],["注意力矩阵的奇异值呈现",{"5":{"87":2,"152":2}}],["注意力矩阵的结构是数据依赖的",{"5":{"87":2,"152":2}}],["注意力矩阵的病态性问题",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["注意力矩阵的病态性是指其条件数过大",{"5":{"87":2,"152":2}}],["注意力矩阵的第个元素为",{"5":{"87":1,"152":1}}],["注意力矩阵的第",{"5":{"87":1,"152":1}}],["注意力矩阵是query和key矩阵点积后经过softmax归一化得到的",{"5":{"87":2,"152":2}}],["注意力矩阵是scaled",{"5":{"87":2,"152":2}}],["注意力矩阵是稠密的",{"5":{"87":2,"152":2}}],["注意力矩阵是非线性的",{"5":{"87":2,"152":2}}],["注意力矩阵具有以下几个基本性质",{"5":{"87":1,"152":1}}],["注意力矩阵与gram矩阵的关系",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["注意力矩阵与gram矩阵有着密切的关系",{"5":{"87":2,"152":2}}],["注意力矩阵与gram矩阵有以下相似之处",{"5":{"87":2,"152":2}}],["注意力矩阵与卷积矩阵有几个关键区别",{"5":{"87":2,"152":2}}],["注意力矩阵与输入相关",{"5":{"87":2,"152":2}}],["注意力矩阵与核矩阵有以下联系",{"5":{"87":2,"152":2}}],["注意力矩阵经过softmax归一化",{"5":{"87":2,"152":2}}],["注意力矩阵不具有平移不变性",{"5":{"87":2,"152":2}}],["注意力矩阵不一定对称",{"5":{"87":2,"152":2}}],["注意力矩阵可以视为一个核矩阵",{"5":{"87":2,"152":2}}],["注意力矩阵可能在这两种极端情况之间波动",{"5":{"87":2,"152":2}}],["注意力矩阵接近置换矩阵",{"5":{"87":2,"152":2}}],["注意力矩阵接近均匀矩阵",{"5":{"87":2,"152":2}}],["注意力矩阵仍然可能出现病态性",{"5":{"87":2,"152":2}}],["注意力矩阵都可以用个参数完全描述",{"5":{"87":1,"152":1}}],["注意力矩阵都可以用",{"5":{"87":1,"152":1}}],["注意力传递",{"5":{"87":2,"152":2}}],["注意力头数为32",{"5":{"45":2}}],["注意力头的专业化分工",{"2":{"92":1,"153":1},"5":{"92":1,"153":1}}],["注意力头的重参数化视角",{"2":{"93":1},"5":{"93":1}}],["注意力头的稀疏激活",{"2":{"93":1},"5":{"93":1}}],["注意力头的分槽",{"5":{"132":2}}],["注意力头的分工与专业化",{"2":{"93":1},"5":{"93":1}}],["注意力头的分组与跨头交互",{"2":{"93":1},"5":{"93":1}}],["注意力头的注意力",{"5":{"93":2}}],["注意力的信息论基础",{"5":{"132":2}}],["注意力的信息论特性",{"2":{"132":1},"5":{"132":1}}],["注意力的信息瓶颈目标",{"5":{"132":2}}],["注意力的信息瓶颈解释",{"5":{"132":2}}],["注意力的计算也可以从概率分布的角度理解",{"5":{"61":2}}],["注意力的是一次性建模所有位置对之间的依赖",{"5":{"92":1,"153":1}}],["注意力的",{"5":{"92":1,"153":1}}],["注意力的张量形式强调了多头的结构",{"5":{"93":2}}],["注意力具有明显的效率优势",{"5":{"92":2,"153":2}}],["注意力策略",{"5":{"92":2,"153":2}}],["注意力可以通过学习实现",{"5":{"92":2,"153":2}}],["注意力集中于少数位置",{"5":{"95":2}}],["注意矩阵乘法的方向",{"5":{"44":2}}],["注意位置1与位置2的点积和位置0与位置1的点积相等",{"5":{"91":2}}],["注意这个空间需求与头数成正比",{"5":{"93":2}}],["注意这里矩阵堆叠的方向",{"5":{"93":2}}],["注意这里的梯度不依赖于​的具体值",{"5":{"94":1}}],["注意这里的梯度不依赖于",{"5":{"94":1}}],["注意这里对每一行独立进行softmax归一化",{"5":{"95":2}}],["注意条件协方差不依赖于观测值",{"5":{"96":2}}],["注入效率取决于查询",{"5":{"132":1}}],["注入效率取决于查询与键的匹配程度",{"5":{"132":1}}],["注入效率受匹配程度限制",{"5":{"132":2}}],["注入",{"5":{"91":2}}],["既可以直接优化策略",{"5":{"155":2}}],["既可以为正也可以为负",{"5":{"96":2}}],["既能逃离鞍点",{"5":{"149":2}}],["既能覆盖足够多的不同尺度",{"5":{"91":2}}],["既有正特征值又有负特征值",{"5":{"135":2}}],["既保证了一次更新使用的样本数量",{"5":{"50":2}}],["既不震荡也不迟缓",{"5":{"136":2}}],["既不会太小导致分布过于平坦",{"5":{"95":2}}],["既不是局部极小值也不是局部极大值",{"5":{"97":1}}],["矛盾",{"5":{"91":2}}],["看到",{"5":{"91":2,"92":4,"153":4}}],["看到未来",{"5":{"95":2}}],["观察发现",{"5":{"91":2}}],["绘制各维度随位置变化的曲线",{"5":{"91":2}}],["绘制编码矩阵",{"5":{"91":2}}],["生物神经元在膜电位超过阈值时发放动作电位",{"5":{"71":2}}],["生物神经元是一种复杂的细胞系统",{"5":{"47":2}}],["生物序列",{"5":{"89":2}}],["生成整体质量更高的段落",{"5":{"156":2}}],["生成的",{"5":{"154":2}}],["生成过程的马尔可夫性质",{"5":{"140":2}}],["生成质量通常越高",{"5":{"140":2}}],["生成",{"5":{"132":2}}],["生成一个长度为",{"5":{"140":1}}],["生成一个长度为的序列需要次前向传播",{"5":{"140":1}}],["生成一个",{"5":{"91":1}}],["生成一个独立的编码向量",{"5":{"91":1}}],["生成任务要求模型在预测位置的输出时只能依赖于位置到的信息",{"5":{"95":1}}],["生成任务要求模型在预测位置",{"5":{"95":1}}],["张成的空间维度远小于矩阵的维度",{"5":{"143":2}}],["张成一个",{"5":{"91":1}}],["张量维度处理对比",{"2":{"146":1},"5":{"146":1}}],["张量回归的参数效率",{"5":{"143":2}}],["张量回归层通过因子分解",{"5":{"143":1}}],["张量回归层通过因子分解来减少参数量",{"5":{"143":1}}],["张量回归层设计等问题提供了统一的数学框架",{"5":{"143":2}}],["张量回归层定义为",{"5":{"143":2}}],["张量回归层",{"2":{"143":1},"5":{"143":5}}],["张量链",{"5":{"143":2}}],["张量",{"5":{"143":3}}],["张量是最基本的数据结构",{"5":{"50":2}}],["张量是多维数组的自然推广",{"5":{"50":2}}],["张量是多维数组在数学上的抽象表示",{"5":{"50":2}}],["张量的高阶svd定义为",{"5":{"143":1}}],["张量的表示与运算",{"2":{"50":1},"5":{"50":1}}],["张量的维度通常具有明确的语义含义",{"5":{"50":2}}],["张量收缩是矩阵乘法在高维张量上的推广",{"5":{"50":2}}],["张量收缩指定两个张量的若干维度进行配对相乘并求和",{"5":{"50":2}}],["张量分解作为处理高维数据的核心数学工具",{"5":{"143":2}}],["张量分解为卷积核压缩",{"5":{"143":2}}],["张量分解技术为模型压缩和加速提供了有效手段",{"5":{"143":2}}],["张量分解可以看作是pca在张量形式上的推广",{"5":{"143":2}}],["张量分解的统一视角",{"5":{"143":2}}],["张量分解是处理高维张量的重要技术",{"5":{"50":2}}],["张量分解在模型压缩",{"5":{"50":2}}],["张量网络与高维运算",{"2":{"50":1},"5":{"50":1}}],["张量网络是用低维张量通过网络结构表示高维张量的方法",{"5":{"50":2}}],["张量网络的核心思想是用多个小张量的乘积之和来表示一个巨大的高维张量",{"5":{"50":2}}],["张量网络的思维方式有助于理解复杂的计算结构",{"5":{"50":2}}],["张量网络在模型压缩和高效推理中有着重要的应用价值",{"5":{"50":2}}],["张量运算的基本操作包括逐元素运算",{"5":{"50":2}}],["张量运算的高效实现是现代深度学习框架的核心能力之一",{"5":{"50":2}}],["张量并行",{"5":{"50":2}}],["张量可重排为的张量",{"5":{"50":1}}],["张量可重排为",{"5":{"50":1}}],["张量形式的表示虽然符号上更为复杂",{"5":{"93":2}}],["开始分化但最终收敛到一个相对均衡的分布",{"5":{"158":2}}],["开始的未来折扣奖励之和",{"5":{"156":2}}],["开始到轨迹结束的折扣累积回报",{"5":{"155":2}}],["开始下降",{"5":{"133":2}}],["开始",{"5":{"41":2,"44":2,"156":2}}],["开头的",{"5":{"92":2,"153":2}}],["爬山",{"5":{"92":2,"153":2}}],["日出",{"5":{"92":2,"153":2}}],["扁平化",{"5":{"92":2,"153":2}}],["感知机与仿射变换",{"2":{"47":1},"5":{"47":1}}],["感知机是麦肯罗皮层神经元的重要推广",{"5":{"47":2}}],["感知机通常采用连续可微的激活函数",{"5":{"47":2}}],["感知机模型",{"5":{"47":2}}],["感知机的数学定义为",{"5":{"47":2}}],["感知机",{"5":{"47":2}}],["感受野的数学分析",{"5":{"150":1}}],["感受野的数学分析揭示了mhc在捕捉多尺度信息方面的优势",{"5":{"150":1}}],["感受野",{"5":{"92":2,"153":2}}],["换一种说法",{"5":{"92":2,"153":2}}],["聚合信息",{"5":{"69":1}}],["聚合",{"5":{"44":1,"87":2,"152":2}}],["聚合的信息",{"5":{"92":1,"153":1}}],["聚焦最重要位置",{"5":{"132":2}}],["聚焦",{"5":{"92":2,"153":2}}],["篇章级别的指代关系等",{"5":{"92":2,"153":2}}],["浅层先进入压缩阶段",{"5":{"133":2}}],["浅层网络的线性组合较为简单",{"5":{"45":2}}],["浅层网络需要指数级数量的神经元才能逼近",{"5":{"71":2}}],["浅层",{"5":{"92":2,"153":2}}],["谓语关系",{"5":{"92":2,"93":2,"153":2}}],["谓语依赖",{"5":{"92":2,"153":2}}],["名词依赖",{"5":{"92":2,"153":2}}],["修改为",{"5":{"147":2}}],["修改后的公式为",{"5":{"96":2}}],["修正线性单元",{"5":{"42":2,"47":2}}],["修饰语",{"5":{"92":2,"153":2}}],["专家容量限制",{"5":{"158":2}}],["专家坍缩的发生可以用梯度消失的机制来解释",{"5":{"158":2}}],["专家坍缩的数学本质是门控网络陷入了不良的局部最优",{"5":{"158":2}}],["专家坍缩",{"5":{"158":2,"159":2}}],["专家分块",{"5":{"159":2}}],["专家分配频率的标准差",{"5":{"158":2}}],["专家分配的时间动态也是需要关注的方面",{"5":{"158":2}}],["专家分配概率",{"5":{"158":4}}],["专家分配由路由概率的大小决定",{"5":{"158":2}}],["专家分布",{"5":{"157":1}}],["专家输出也是逐位置计算的",{"5":{"158":2}}],["专家集合可以数学上表示为一个映射",{"5":{"158":2}}],["专家的计算过程可以分解为两个连续的线性变换",{"5":{"158":2}}],["专家的梯度是基于被分配到的样本计算的",{"5":{"157":2}}],["专家网络通过第二个公式学习更好的特征变换",{"5":{"158":2}}],["专家网络的内部结构设计有多种变体",{"5":{"158":2}}],["专家网络的数学表示",{"2":{"158":1},"5":{"158":1}}],["专家网络",{"5":{"158":2,"159":4}}],["专家网络是moe架构中负责实际计算的核心组件",{"5":{"158":2}}],["专家网络与负载均衡<",{"5":{"131":1}}],["专家网络与负载均衡",{"0":{"158":1},"4":{"107":1,"158":1},"5":{"131":4,"158":1}}],["专家激活",{"5":{"157":1}}],["专家激活的all",{"5":{"157":2}}],["专家数多",{"5":{"157":1}}],["专家数量为",{"5":{"157":4}}],["专家权重分片",{"5":{"157":2}}],["专家权重",{"5":{"157":2}}],["专家选择策略对通信效率有重要影响",{"5":{"157":2}}],["专家并行的实现是分布式训练moe模型的关键技术",{"5":{"159":2}}],["专家并行的通信计算重叠是减少通信开销的关键技术",{"5":{"157":2}}],["专家并行的通信量可以通过以下方式估算",{"5":{"157":2}}],["专家并行的通信开销主要来源于专家激活的all",{"5":{"157":2}}],["专家并行引入的额外通信主要来自两个方面",{"5":{"157":2}}],["专家并行",{"5":{"157":3}}],["专家并行策略",{"2":{"157":1},"5":{"157":1}}],["专家梯度的并行计算也是需要考虑的重要问题",{"5":{"157":2}}],["专家",{"5":{"93":4,"157":2,"158":4}}],["专业模块",{"5":{"159":2}}],["专业知识",{"5":{"159":2}}],["专业书籍",{"5":{"138":2}}],["专业化",{"5":{"92":2,"93":2,"153":2}}],["专业化分工",{"5":{"92":2,"153":2}}],["专业技能",{"5":{"93":2}}],["专注于学习局部上下文信息",{"5":{"92":2,"153":2}}],["专注于q",{"5":{"94":2}}],["专门为relu激活函数设计",{"5":{"94":2}}],["桥接",{"5":{"92":2,"153":2}}],["间接地稳定梯度流",{"5":{"147":2}}],["间接限制了网络的lipschitz常数",{"5":{"144":2}}],["间接影响fisher信息度量的几何结构",{"5":{"71":2}}],["间接传递到了位置",{"5":{"92":2,"153":2}}],["底层",{"5":{"92":2,"153":2}}],["底层主要进行局部特征提取",{"5":{"92":2,"153":2}}],["顶层",{"5":{"92":2,"153":2}}],["承担着特征提取和信息整合的功能",{"5":{"47":1}}],["承担",{"5":{"92":2,"153":2}}],["延迟",{"5":{"92":2,"153":2}}],["必然导致某种程度的梯度压缩",{"5":{"148":2}}],["必然事件的发生几乎不带来任何新信息",{"5":{"61":2}}],["必须满足以下下界",{"5":{"133":1}}],["必须至少检查每一对位置一次",{"5":{"92":2,"153":2}}],["必须是对称半正定的",{"5":{"96":2}}],["归纳步骤",{"5":{"140":2}}],["归纳偏置",{"5":{"92":2,"153":2}}],["归一化方法",{"5":{"146":1}}],["归一化方法的适用场景对比",{"5":{"146":1}}],["归一化方法的适用场景对比本节系统地介绍了深度学习中三种主流归一化方法的数学原理与公式推导",{"5":{"146":1}}],["归一化除法",{"5":{"146":1}}],["归一化操作使得层的输出对于输入分布的变化更加鲁棒",{"5":{"150":2}}],["归一化操作会将所有激活值限制在均值为0",{"5":{"150":2}}],["归一化操作会将层的输出强制限制在固定范围",{"5":{"146":2}}],["归一化操作的核心思想是相同的",{"5":{"146":2}}],["归一化类型",{"5":{"132":1}}],["归一化位置的信息论选择",{"5":{"132":2}}],["归一化的零均值单位方差性质",{"5":{"146":2}}],["归一化的通用数学表达式可以写为",{"5":{"146":2}}],["归一化的统一范式",{"2":{"146":1},"5":{"146":1}}],["归一化的信息论解释",{"5":{"132":2}}],["归一化的信息损失",{"5":{"132":2}}],["归一化的数学效应",{"5":{"41":2}}],["归一化技术也是控制梯度流的重要手段",{"5":{"148":2}}],["归一化技术在实践中的有效性已被广泛验证",{"5":{"146":2}}],["归一化技术通过将每层的输入分布稳定在固定范围内",{"5":{"146":2}}],["归一化技术通过多种机制稳定梯度传播",{"5":{"41":2}}],["归一化技术的设计提供了理论依据",{"5":{"148":2}}],["归一化技术的理论基础与内部协变量偏移",{"5":{"146":2}}],["归一化技术的梯度稳定机制",{"2":{"41":1},"5":{"41":1}}],["归一化技术经历了持续的演进与优化",{"5":{"146":2}}],["归一化技术是深度学习中最具影响力的技术创新之一",{"5":{"146":2}}],["归一化技术是现代深度学习训练的核心组件",{"5":{"41":2}}],["归一化技术与梯度流",{"2":{"148":1},"5":{"148":1}}],["归一化技术与梯度稳定",{"2":{"41":1},"5":{"41":1}}],["归一化技术与位置编码在transformer中共同工作",{"5":{"41":2}}],["归一化技术",{"5":{"41":2}}],["归一化确保了不同位置的位置编码具有相似的数值尺度",{"5":{"41":2}}],["归一化将激活值的分布规范化到固定范围",{"5":{"41":2}}],["归一化",{"5":{"41":2,"69":2}}],["归一化层的梯度计算涉及除以方差",{"5":{"41":2}}],["归一化层会将其缩放",{"5":{"41":2}}],["归一化层",{"5":{"44":1,"148":2}}],["归一化维度",{"5":{"41":1}}],["归一化后的激活值具有稳定的均值和方差",{"5":{"150":2}}],["归一化后的输出",{"5":{"150":2}}],["归一化后的结果保持不变",{"5":{"146":2}}],["归一化后",{"5":{"51":2}}],["归一化性质确保输出是一个有效的概率分布",{"5":{"61":1}}],["归一化性质",{"5":{"61":1,"70":2}}],["量化到4",{"5":{"142":1}}],["量化",{"5":{"142":1}}],["量化分析了远距离位置间的信息传递效率",{"5":{"132":2}}],["量化地展示了这一优势",{"5":{"92":2,"153":2}}],["量级",{"5":{"45":1}}],["事实上",{"5":{"159":2}}],["事件与其论元的关联等",{"5":{"92":2,"153":2}}],["事先就知道序列中任意两个位置之间是否",{"5":{"92":1,"153":1}}],["达到数百万甚至数十亿时",{"5":{"149":2}}],["达到最大值",{"5":{"61":2}}],["达到百万甚至十亿级别时",{"5":{"92":1,"153":1}}],["猫坐在垫子上",{"5":{"93":2}}],["猫",{"5":{"93":6}}],["坐在",{"5":{"93":4}}],["坐",{"5":{"93":4}}],["垫子",{"5":{"93":2}}],["公式对比汇总",{"2":{"146":1},"5":{"146":1}}],["公式可得",{"5":{"134":2}}],["公式化的",{"5":{"89":2}}],["公式",{"5":{"93":2,"95":4}}],["免费",{"5":{"93":4}}],["切断",{"5":{"148":2}}],["切割",{"5":{"45":2}}],["切片",{"5":{"93":2}}],["拼接后的输出矩阵",{"5":{"93":2}}],["综合优化策略的选择需要根据具体的硬件配置和模型规模进行权衡",{"5":{"157":2}}],["综合前向和反向的需求",{"5":{"148":2}}],["综合评分",{"5":{"132":2}}],["综合评估每个注意力头的重要性",{"5":{"132":2}}],["综合以上结果",{"5":{"44":2}}],["综合以上三个阶段",{"5":{"93":2}}],["综合以上分析",{"5":{"94":2}}],["综合来看",{"5":{"93":2}}],["线性层后的期望保持",{"2":{"145":1},"5":{"145":1}}],["线性层的",{"5":{"133":1}}],["线性层的受输入维度限制",{"5":{"133":1}}],["线性时间生成",{"5":{"140":2}}],["线性收敛的充要条件是",{"5":{"136":2}}],["线性收敛意味着误差以固定比例递减",{"5":{"136":2}}],["线性收敛",{"5":{"136":2}}],["线性化稳定性理论",{"5":{"135":2}}],["线性化稳定性分析",{"2":{"135":1},"5":{"135":1}}],["线性化无法判断",{"5":{"134":1}}],["线性投影",{"5":{"132":2}}],["线性投影的矩阵形式",{"2":{"94":1},"5":{"94":1}}],["线性最小二乘问题",{"5":{"70":1}}],["线性代数基础",{"2":{"21":1,"85":1,"131":1},"5":{"21":1,"85":1,"131":1}}],["线性代数与张量运算<",{"5":{"21":1,"85":1,"131":1}}],["线性代数与张量运算",{"0":{"50":1},"4":{"50":1},"5":{"21":4,"50":1,"85":4,"131":4}}],["线性代数作为现代数学的重要分支",{"5":{"50":2}}],["线性代数为理解和构建深度学习模型提供了坚实的数学框架",{"5":{"50":2}}],["线性代数的核心地位",{"2":{"50":1},"5":{"50":1}}],["线性代数的基本运算包括向量加法",{"5":{"50":2}}],["线性代数的瑞士军刀",{"5":{"50":2}}],["线性映射的数学结构",{"2":{"71":1},"5":{"71":1}}],["线性映射",{"5":{"71":2}}],["线性映射可以用矩阵乘法完全描述",{"5":{"71":2}}],["线性映射描述了输入空间通过旋转",{"5":{"71":2}}],["线性模型表达能力的根本局限",{"2":{"71":1},"5":{"71":1}}],["线性模型虽然具有优美的数学性质",{"5":{"71":2}}],["线性模型的学习能力可以用函数空间的维度来刻画",{"5":{"71":2}}],["线性模型的函数空间是所有仿射函数的集合",{"5":{"71":2}}],["线性模型必然产生系统性误差",{"5":{"71":2}}],["线性模型只能学习形如的函数",{"5":{"71":1}}],["线性模型只能学习形如",{"5":{"71":1}}],["线性分类器",{"5":{"71":2}}],["线性",{"5":{"70":1,"71":6,"132":2}}],["线性变换不变性",{"5":{"70":2,"97":2}}],["线性变换可以分解为旋转",{"5":{"45":1}}],["线性变换的复合仍然是线性变换",{"5":{"71":2}}],["线性变换的奇异值分解",{"5":{"45":2}}],["线性变换",{"5":{"45":1}}],["线性变换后的点集满足",{"5":{"47":2}}],["线性变换是理解特征空间中数据变换的关键概念",{"5":{"50":2}}],["线性变换对特征空间的作用包括旋转",{"5":{"50":2}}],["线性变换保持高斯性",{"5":{"96":2}}],["线性回归的闭式解就是通过求解正规方程得到的",{"5":{"50":2}}],["线性而非平方增长",{"5":{"51":2}}],["线性算子",{"5":{"51":2}}],["线性注意力使用核函数近似",{"5":{"132":2}}],["线性注意力的信息容量受限于核函数的秩",{"5":{"132":2}}],["线性注意力的信息容量",{"5":{"132":2}}],["线性注意力的数学定义",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["线性注意力的关键在于设计合适的特征映射函数",{"5":{"86":2,"151":2}}],["线性注意力的表达能力分析",{"2":{"86":1,"151":1},"5":{"86":1,"151":1}}],["线性注意力的表达能力受限于所选的核函数",{"5":{"86":2,"151":2}}],["线性注意力的递归更新只需要维护累积量和",{"5":{"86":1,"151":1}}],["线性注意力的递归更新只需要维护累积量",{"5":{"86":1,"151":1}}],["线性注意力的谱分析",{"2":{"87":1,"152":1},"5":{"87":1,"152":1}}],["线性注意力的核心思想是用核函数替换softmax",{"5":{"87":1,"152":1}}],["线性注意力的核心思想是用核函数",{"5":{"87":1,"152":1}}],["线性注意力的核矩阵具有与gram矩阵相似的性质",{"5":{"87":1,"152":1}}],["线性注意力的核矩阵",{"5":{"87":1,"152":1}}],["线性注意力的一个重要优势是支持递归",{"5":{"86":2,"151":2}}],["线性注意力的一个常见问题是表达能力受限",{"5":{"87":2,"152":2}}],["线性注意力",{"5":{"86":4,"87":2,"132":2,"151":4,"152":2}}],["线性注意力用核函数近似softmax",{"5":{"86":2,"151":2}}],["线性注意力通过核函数近似softmax",{"5":{"86":2,"151":2}}],["线性注意力可能无法表示某些复杂的注意力模式",{"5":{"87":2,"152":2}}],["线性注意力避免了的注意力矩阵计算",{"5":{"87":1,"152":1}}],["线性注意力避免了",{"5":{"87":1,"152":1}}],["线性注意力变体的动机所在",{"5":{"95":2}}],["线性核",{"5":{"87":2,"152":2}}],["线性rope探索了非均匀旋转的可能性",{"5":{"88":2}}],["线性rope的公式为",{"5":{"88":2}}],["线性rope在实践中并未展现出显著优势",{"5":{"88":2}}],["线性偏置的方式可能限制了对局部模式的建模能力",{"5":{"88":2}}],["线性增加",{"5":{"90":1}}],["线性增长",{"5":{"95":1}}],["线性无关",{"5":{"96":1}}],["线程块",{"5":{"93":2}}],["活跃的神经元可以被解释为检测到特定模式的",{"5":{"71":2}}],["活跃",{"5":{"93":2,"148":2}}],["跨尺度信息流动",{"5":{"150":1}}],["跨尺度信息流动是mhc区别于简单并联结构的关键",{"5":{"150":1}}],["跨临界分岔和倍周期分岔",{"5":{"136":2}}],["跨临界分岔的标准形式为",{"5":{"136":2}}],["跨临界分岔的正规形",{"5":{"136":2}}],["跨临界分岔的定义",{"5":{"136":2}}],["跨临界分岔是两个不动点交换稳定性的分岔",{"5":{"136":2}}],["跨临界分岔",{"5":{"136":2}}],["跨注意力层实现了不同模块之间的信息传递",{"5":{"132":2}}],["跨注意力信息流",{"5":{"132":2}}],["跨度",{"5":{"92":2,"153":2}}],["跨度为10",{"5":{"92":2,"153":2}}],["跨头交互",{"5":{"93":2}}],["跨头交互的另一种方式是使用",{"5":{"93":2}}],["让模型专注于主任务优化",{"5":{"158":2,"159":2}}],["让模型快速学习",{"5":{"133":1}}],["让语言模型生成两个不同的回复",{"5":{"154":2}}],["让我们深入分析这个目标函数的数学原理和优化动机",{"5":{"154":2}}],["让我们首先建立复数与向量之间的对应关系",{"5":{"88":2}}],["让我们考虑一个简化的二维例子",{"5":{"88":2}}],["让我们给出rope平移等变性的严格数学证明",{"5":{"88":2}}],["让我们具体分析两种编码方式下query",{"5":{"88":2}}],["让我们分析这个选择的数学意义",{"5":{"88":2}}],["让我们分析几个代表性大语言模型中rope的具体实现细节",{"5":{"88":2}}],["让一个注意力机制学习如何组合不同头的输出",{"5":{"93":2}}],["小步慢走",{"5":{"154":2}}],["小模型的容量不足以同时学习这两个分布",{"5":{"138":2}}],["小模型在使用链式思考",{"5":{"138":2}}],["小于某个临界值",{"5":{"138":2}}],["小于",{"5":{"138":2,"154":2}}],["小特征值占主导",{"5":{"135":2}}],["小特征值使",{"5":{"134":2}}],["小",{"5":{"135":2}}],["小批量",{"5":{"134":2}}],["小球沿损失曲面的梯度方向滚落",{"5":{"134":2}}],["小扰动后系统最终回到平衡点",{"5":{"134":2}}],["小扰动不会导致系统偏离平衡点太远",{"5":{"134":2}}],["小的奇异值可能对应噪声",{"5":{"89":2}}],["小的",{"5":{"90":5,"91":2}}],["小的​",{"5":{"90":1}}],["小波基",{"5":{"93":2}}],["扮演着类似的角色",{"5":{"93":2}}],["地通过恒等路径流向较浅的层",{"5":{"150":2}}],["地回传到第一层",{"5":{"92":2,"153":2}}],["地增加表示的丰富性",{"5":{"93":2}}],["携带了多少关于输入",{"5":{"93":1}}],["什么内容出现在什么位置",{"5":{"91":2}}],["什么样的特征应该作为",{"5":{"94":2}}],["捕获更丰富的信息结构",{"5":{"133":2}}],["捕获比更抽象",{"5":{"71":1}}],["捕获比",{"5":{"71":1}}],["捕获词汇级别和短语级别的特征",{"5":{"92":2,"153":2}}],["捕获句子级别和篇章级别的特征",{"5":{"92":2,"153":2}}],["捕获跨越长距离的语义关联",{"5":{"92":2,"153":2}}],["捕获该词元的语义信息",{"5":{"94":2}}],["捕捉输入的不同方面特征",{"5":{"94":2}}],["文本分类任务",{"5":{"99":2,"101":2}}],["文本生成本质上是一个随机过程",{"5":{"96":2}}],["文章中的各个词元",{"5":{"94":2}}],["文章中各位置的上下文信息将被编码为value向量",{"5":{"94":2}}],["竹子",{"5":{"94":2}}],["肉",{"5":{"94":2}}],["苹果",{"5":{"94":2}}],["需设置μ",{"5":{"145":1}}],["需参数化",{"5":{"145":1}}],["需高阶分析",{"5":{"134":1}}],["需满足",{"5":{"134":2,"135":2}}],["需编码以下两类位置信息",{"5":{"132":1}}],["需要辅助损失正则化",{"5":{"159":1}}],["需要学习路由策略",{"5":{"159":1}}],["需要从计算复杂度",{"5":{"159":2}}],["需要不成比例地增加模型规模",{"5":{"159":2}}],["需要引入辅助损失来正则化门控网络",{"5":{"159":2}}],["需要引入正则化来促进专家多样化",{"5":{"158":2}}],["需要引入特定的近似技巧来处理不可微的路由决策",{"5":{"157":2}}],["需要通过all",{"5":{"157":2}}],["需要采用分布式并行策略来在多个设备上训练和推理",{"5":{"157":2}}],["需要更多的采样才能获得稳定的更新方向",{"5":{"155":2}}],["需要更大的模型才能涌现",{"5":{"138":2}}],["需要更大的更新来纠正",{"5":{"99":2,"101":2}}],["需要增加",{"5":{"154":2}}],["需要手动调参",{"5":{"147":1}}],["需要放缩",{"5":{"145":1}}],["需要推理时缩放",{"5":{"145":1}}],["需要补偿训练时的随机缩放",{"5":{"145":2}}],["需要建模",{"5":{"140":1}}],["需要综合考虑多个因素",{"5":{"138":1}}],["需要考虑参数规模",{"5":{"138":1}}],["需要四倍的样本量",{"5":{"137":2}}],["需要进行偏差校正",{"5":{"137":2}}],["需要进一步发展",{"5":{"133":2}}],["需要高阶分析",{"5":{"135":2}}],["需要足够编码",{"5":{"132":1}}],["需要足够编码个位置",{"5":{"132":1}}],["需要编码位置间的相对关系",{"5":{"132":2}}],["需要步依次传递信息",{"5":{"132":1}}],["需要步顺序计算",{"5":{"92":1,"153":1}}],["需要什么信息",{"5":{"132":2}}],["需要聚合来自左右两侧的token信息",{"5":{"99":1,"101":1}}],["需要检测梯度爆炸并采取应对措施",{"5":{"41":2}}],["需要​",{"5":{"41":1}}],["需要",{"5":{"41":1,"50":1,"86":1,"92":2,"132":1,"145":2,"148":2,"151":1,"153":2}}],["需要一条非线性的决策边界",{"5":{"71":2}}],["需要一种中间表示来桥接理论模型和计算程序",{"5":{"45":2}}],["需要大量gpu并行计算才能在合理时间内完成",{"5":{"45":2}}],["需要优化的",{"5":{"61":2}}],["需要经过一系列具体的数学推导",{"5":{"61":2}}],["需要在数学特性和计算效率之间进行权衡",{"5":{"41":2}}],["需要在每个新标记到来时更新注意力输出",{"5":{"86":2,"151":2}}],["需要字节mb的存储",{"5":{"86":1,"151":1}}],["需要计算和存储完整的注意力矩阵",{"5":{"86":1,"151":1}}],["需要计算和存储完整的",{"5":{"86":1,"151":1}}],["需要额外的正则化技术",{"5":{"87":2,"152":2}}],["需要很多步才能遍历整个圆周",{"5":{"90":2}}],["需要使用重要性采样权重进行修正",{"5":{"154":2}}],["需要使用估计方法",{"5":{"137":2}}],["需要使用离散傅里叶变换",{"5":{"90":2}}],["需要使用因果掩码",{"5":{"91":2}}],["需要使用适当的多重比较校正方法",{"5":{"96":2}}],["需要特别区分的是",{"5":{"149":2}}],["需要特别强调的是",{"5":{"149":2}}],["需要特别注意数值稳定性问题",{"5":{"44":2}}],["需要特殊处理",{"5":{"91":2}}],["需要修改注意力计算",{"5":{"91":2}}],["需要多层累积的",{"5":{"92":2,"153":2}}],["需要分析信息在层间传递的数学机制",{"5":{"92":2,"153":2}}],["需要两步",{"5":{"92":2,"153":2}}],["需要堆叠约层",{"5":{"92":1,"153":1}}],["需要堆叠约",{"5":{"92":1,"153":1}}],["需要读取完整的和矩阵",{"5":{"92":2,"153":2}}],["需要读取完整的",{"5":{"92":2,"153":2}}],["需要将模型规模增加约",{"5":{"159":2}}],["需要将其与传统的稠密",{"5":{"159":2}}],["需要将其激活发送到负责处理该位置的专家所在设备",{"5":{"157":2}}],["需要将批量大小增加四倍",{"5":{"146":2}}],["需要将参数量增加约",{"5":{"138":4}}],["需要将按头进行拆分",{"5":{"93":1}}],["需要将",{"5":{"93":1}}],["需要注意的是",{"5":{"96":2,"146":2}}],["需要仔细分析",{"5":{"41":2}}],["需要仔细调参",{"5":{"97":2}}],["需要处理",{"5":{"97":1}}],["需求",{"5":{"94":2}}],["供给",{"5":{"94":2}}],["回答了",{"5":{"156":4}}],["回报",{"5":{"156":4}}],["回归任务选择",{"5":{"70":2}}],["回归任务的数学结构",{"5":{"70":2}}],["回归任务和分类任务在数学本质上的差异决定了它们需要不同类型的损失函数",{"5":{"70":2}}],["回归任务与分类任务的本质差异",{"2":{"70":1},"5":{"70":1}}],["回归任务",{"5":{"44":2}}],["回顾7",{"5":{"94":2}}],["回顾",{"5":{"95":2}}],["填充token的数学处理",{"5":{"99":2,"101":2}}],["填充位置不应该参与注意力计算",{"5":{"95":2}}],["填充掩码的实现",{"2":{"95":1},"5":{"95":1}}],["填充掩码",{"5":{"95":2}}],["填充掩码和因果掩码的组合展示了注意力机制的可扩展性",{"5":{"95":2}}],["面临着长程依赖难以建模的困境",{"5":{"95":2}}],["匹配以最小化损失",{"5":{"158":2}}],["匹配的正样本",{"5":{"69":1}}],["匹配",{"5":{"95":2}}],["索引为",{"5":{"91":4}}],["索引",{"5":{"95":2}}],["式中",{"5":{"95":2}}],["舒适区",{"5":{"95":2}}],["去均值化",{"5":{"146":1}}],["去均值化操作相当于在特征向量上施加约束",{"5":{"146":2}}],["去均值化操作的信息损失",{"5":{"146":2}}],["去均值化操作",{"5":{"146":2}}],["去重",{"5":{"138":2}}],["去掉该头后信息损失多少",{"5":{"132":2}}],["去除关于输入的冗余信息",{"5":{"71":2}}],["去相关",{"5":{"95":2}}],["灵活的可扩展性",{"5":{"95":2}}],["涉及隐藏状态在不同时间步之间的统计依赖关系",{"5":{"145":2}}],["涉及两个不动点交换稳定性的情况",{"5":{"136":2}}],["涉及不动点的成对出现或消失",{"5":{"136":2}}],["涉及与的矩阵乘法",{"5":{"95":2}}],["涉及",{"5":{"95":2}}],["掷骰子的点数",{"5":{"96":2}}],["落在饱和区域",{"5":{"42":2}}],["落在区间的概率为",{"5":{"96":1}}],["落在区间",{"5":{"96":1}}],["矩阵积的对比",{"5":{"143":1}}],["矩阵积的对比cp分解",{"5":{"143":1}}],["矩阵代表的线性变换可以分解为三个基本变换的复合",{"5":{"143":1}}],["矩阵乘积的秩满足",{"5":{"141":2}}],["矩阵乘法退化为逐元素乘法",{"5":{"44":2}}],["矩阵乘法",{"5":{"44":3,"50":2,"95":1,"143":1}}],["矩阵乘法节点",{"5":{"45":2}}],["矩阵乘法tile",{"5":{"45":1}}],["矩阵乘法满足结合律",{"5":{"50":2}}],["矩阵乘法是最重要也是计算量最大的运算",{"5":{"50":2}}],["矩阵乘法是连接两个张量的最基本操作",{"5":{"50":2}}],["矩阵乘法的flops约为",{"5":{"44":1}}],["矩阵乘法的计算复杂度是",{"5":{"50":2}}],["矩阵乘法的计算结果是一个的矩阵",{"5":{"95":1}}],["矩阵存在模大于1的特征值",{"5":{"136":1}}],["矩阵可近似为低秩分解",{"5":{"135":2}}],["矩阵可分解为两部分",{"5":{"135":2}}],["矩阵描述了损失函数的局部曲率",{"5":{"135":2}}],["矩阵描述了映射",{"5":{"135":2}}],["矩阵正定",{"5":{"134":2}}],["矩阵与张量分解占据着极为重要的地位",{"5":{"143":2}}],["矩阵与张量分解",{"0":{"143":1},"2":{"131":1},"4":{"118":2,"119":1,"120":1,"141":1,"142":1,"143":2},"5":{"131":12,"143":1}}],["矩阵更为复杂",{"5":{"70":2,"97":2}}],["矩阵为",{"5":{"70":2,"97":2,"134":2,"135":2}}],["矩阵",{"5":{"41":4,"45":1,"50":2,"61":2,"70":2,"86":1,"87":2,"92":2,"94":4,"97":4,"133":2,"134":6,"135":18,"136":4,"143":2,"144":2,"151":1,"152":2,"153":2}}],["矩阵谱半径与稳定性",{"5":{"41":2}}],["矩阵的第",{"5":{"149":2}}],["矩阵的svd存在性",{"5":{"143":2}}],["矩阵的行",{"5":{"143":2}}],["矩阵的秩不超过",{"5":{"141":2}}],["矩阵的秩受限于唯一非零元素的位置数量",{"5":{"141":2}}],["矩阵的秩最大为​",{"5":{"94":1}}],["矩阵的谱半径小于1",{"5":{"136":2}}],["矩阵的谱半径",{"5":{"135":2}}],["矩阵的谱半径决定了系统的稳定性",{"5":{"41":1}}],["矩阵的角度深入分析了神经网络训练的稳定性问题",{"5":{"135":2}}],["矩阵的近似预条件化",{"5":{"135":2}}],["矩阵的转置乘以输出梯度",{"5":{"135":2}}],["矩阵的转置是将矩阵的行和列互换",{"5":{"50":2}}],["矩阵的定义与性质",{"2":{"135":1},"5":{"135":1}}],["矩阵的定义与结构",{"2":{"135":1},"5":{"135":1}}],["矩阵的理论框架",{"2":{"135":2},"5":{"135":2}}],["矩阵的最大特征值",{"5":{"134":2,"136":2}}],["矩阵的形状",{"5":{"50":2}}],["矩阵的求逆是找到一个矩阵使得",{"5":{"50":1}}],["矩阵的求逆是找到一个矩阵",{"5":{"50":1}}],["矩阵的条件数定义为",{"5":{"41":1}}],["矩阵的条件数",{"5":{"87":2,"152":2}}],["矩阵的大部分能量",{"5":{"87":2,"152":2}}],["矩阵的每一行进行归一化",{"5":{"95":1}}],["矩阵的特征值",{"5":{"136":3}}],["矩阵的特征值分析",{"5":{"135":2}}],["矩阵的特征值分解揭示了损失函数在不同方向上的曲率特性",{"5":{"97":2}}],["矩阵的特征值对应于数据主成分方向的曲率",{"5":{"97":2}}],["矩阵运算级和模型级等多个层次进行并行优化",{"5":{"45":2}}],["矩阵运算级并行",{"5":{"45":1}}],["矩阵运算与批量处理的等价性",{"5":{"47":2}}],["矩阵运算一次性处理了所有个样本",{"5":{"47":1}}],["矩阵运算一次性处理了所有",{"5":{"47":1}}],["矩阵形式的链式法则在深度学习中尤为重要",{"5":{"48":1}}],["矩阵是对称的",{"5":{"135":2}}],["矩阵是研究动态系统局部稳定性的核心数学工具",{"5":{"135":2}}],["矩阵是半正定对称矩阵",{"5":{"45":1}}],["矩阵是二维数组",{"5":{"50":2}}],["矩阵是query投影和key投影的某种",{"5":{"94":1}}],["矩阵范数与谱半径",{"2":{"148":1},"5":{"148":1}}],["矩阵范数与度量",{"2":{"50":1},"5":{"50":1}}],["矩阵范数正则化的对比",{"5":{"144":2}}],["矩阵范数将矩阵映射到非负实数",{"5":{"50":2}}],["矩阵范数的选择取决于具体问题的需求",{"5":{"50":2}}],["矩阵范数等多个数学工具出发",{"5":{"87":2,"152":2}}],["矩阵范数是衡量矩阵",{"5":{"50":2}}],["矩阵范数是分析矩阵性质的强大工具",{"5":{"87":2,"152":2}}],["矩阵分解的基石",{"2":{"143":1},"5":{"143":1}}],["矩阵分解的深化理解",{"2":{"50":1},"5":{"50":1}}],["矩阵分解还包括lu分解",{"5":{"50":2}}],["矩阵接近奇异",{"5":{"87":2,"152":2}}],["矩阵补全等",{"5":{"87":2,"152":2}}],["矩阵有",{"5":{"87":1,"152":1}}],["矩阵秩与奇异值分析",{"2":{"89":1},"5":{"89":1}}],["矩阵定义为所有二阶偏导数组成的对称矩阵",{"5":{"135":2}}],["矩阵定义为所有一阶偏导数组成的矩阵",{"5":{"135":2}}],["矩阵定义为",{"5":{"97":2}}],["矩阵和",{"5":{"135":2}}],["矩阵和张量运算的计算复杂度和内存效率是核心关注点",{"5":{"50":2}}],["矩阵和鞍点分析",{"5":{"97":2}}],["矩",{"5":{"96":2}}],["矩母函数",{"5":{"96":2}}],["峰度",{"5":{"96":2}}],["峰度为3",{"5":{"96":2}}],["源于信号处理中的多分辨率分析理论",{"5":{"150":1}}],["源于多个方面的原因",{"5":{"96":2}}],["源",{"5":{"134":1}}],["源语言句子中的相关词语来获取信息",{"5":{"95":2}}],["源语言句子中的哪些位置",{"5":{"95":2}}],["稍后详述",{"5":{"96":2}}],["卡尔曼滤波和贝叶斯线性回归中有着核心应用",{"5":{"96":2}}],["确保网络在训练初期具有稳定的梯度流",{"5":{"148":2}}],["确保裁剪阈值不低于训练初期观察到的梯度范数",{"5":{"147":2}}],["确保表示包含预测所需的信息",{"5":{"133":2}}],["确保信息跨层传递",{"5":{"132":2}}],["确保必要信息不丢失",{"5":{"132":2}}],["确保注意力权重构成有效的概率分布",{"5":{"132":2}}],["确保新策略不会与旧策略相差太远",{"5":{"99":2,"101":2}}],["确保新策略不会偏离旧策略太远",{"5":{"96":2}}],["确保了价值函数的计算在数学上是收敛和有意义的",{"5":{"156":2}}],["确保了每次更新都能带来正向的改进",{"5":{"154":2}}],["确保了梯度可以直接通过恒等映射传递",{"5":{"41":2}}],["确保了主路径",{"5":{"41":2}}],["确保了特征的",{"5":{"86":2,"151":2}}],["确保稳定性但可能限制学习速度",{"5":{"41":1}}],["确保参数更新不会过大",{"5":{"44":2}}],["确保误差为非负值",{"5":{"51":2}}],["确保数学上的等价性",{"5":{"86":1,"151":1}}],["确保位置",{"5":{"140":1}}],["确保位置的查询不会注意到位置及之后的键",{"5":{"140":1}}],["确保位置信息在深层网络中稳定传递",{"5":{"41":2}}],["确保位置编码不会过度主导内容信息",{"5":{"88":2}}],["确保自回归生成时的",{"5":{"91":2}}],["确保梯度信号可以有效传播",{"5":{"95":2}}],["确保比较的公平性",{"5":{"96":2}}],["确保输出是一个有效的概率分布",{"5":{"61":1}}],["确保输出的概率分布满足归一性条件",{"5":{"96":2}}],["确定性时",{"5":{"137":1}}],["确定性高",{"5":{"96":2}}],["确定在原假设下观察到该统计量或更极端情况的概率",{"5":{"96":2}}],["联合概率分布的建模目标",{"2":{"140":1},"5":{"140":1}}],["联合训练允许可学习编码调整正弦编码的影响程度",{"5":{"89":2}}],["联合熵描述了",{"5":{"137":1}}],["联合熵描述了作为整体的不确定性",{"5":{"137":1}}],["联合熵与条件熵满足链式法则",{"5":{"137":2}}],["联合熵",{"5":{"96":2,"137":2}}],["于",{"5":{"133":2}}],["于少数关键位置",{"5":{"92":2,"153":2}}],["于某个概率分布",{"5":{"96":2}}],["费希尔信息矩阵逆为方差的正态分布",{"5":{"96":2}}],["罗森布拉特",{"5":{"47":2}}],["罗下界",{"5":{"96":2}}],["弱",{"5":{"145":2}}],["弱归纳偏置",{"5":{"92":2,"153":2}}],["弱大数定律保证样本均值以概率收敛于期望",{"5":{"96":2}}],["验证了highway",{"5":{"150":1}}],["验证一下复数乘法与矩阵乘法的对应关系",{"5":{"88":2}}],["验证集越大",{"5":{"96":2}}],["验证改进措施的有效性以及检测数据中的系统性偏差",{"5":{"96":2}}],["备择假设是我们想要支持的假设",{"5":{"96":2}}],["报告效应量可以帮助读者理解改进的实际意义",{"5":{"96":2}}],["置信区间的上界超过随机水平时",{"5":{"138":2}}],["置信区间比p值提供更多信息",{"5":{"96":2}}],["置信度",{"5":{"69":2}}],["置换检验",{"5":{"96":2}}],["置换检验通过随机打乱标签来生成置换样本",{"5":{"96":2}}],["置换检验在比较两个模型在特定测试集上的性能时很有用",{"5":{"96":2}}],["倾向于互斥",{"5":{"158":2}}],["倾向于同时被选中",{"5":{"158":2}}],["倾向于同向变化",{"5":{"96":1}}],["倾向于收敛到平坦最小值",{"5":{"136":2}}],["倾向于使用纯rope或纯alibi",{"5":{"88":2}}],["倾向于反向变化",{"5":{"96":1}}],["划分为两个区域",{"5":{"45":1}}],["划分为两个子向量",{"5":{"96":1}}],["鼓励模型在错误类别上保持一定的预测概率",{"5":{"144":2}}],["鼓励表示对对抗扰动不敏感",{"5":{"133":1}}],["鼓励编码分布接近先验",{"5":{"133":2}}],["鼓励相反的效果",{"5":{"133":2}}],["鼓励最大化输出信息",{"5":{"133":2}}],["鼓励压缩输入信息",{"5":{"133":2}}],["鼓励不同头之间的熵差异",{"5":{"132":2}}],["鼓励各头学习不同类型的注意力模式",{"5":{"132":2}}],["鼓励分散注意力",{"5":{"132":2}}],["鼓励信息互补",{"5":{"132":2}}],["鼓励多样性",{"5":{"132":2}}],["鼓励权重矩阵的列向量相互正交",{"5":{"50":2}}],["鼓励",{"5":{"96":1}}],["找到一个主要的模式并紧密地拟合它",{"5":{"96":1}}],["彩票假设",{"5":{"97":2}}],["初始学习率",{"5":{"136":2}}],["初始向量",{"5":{"135":4}}],["初始快速下降期",{"5":{"134":2}}],["初始条件",{"5":{"134":2}}],["初始",{"5":{"133":2}}],["初始阶段",{"5":{"133":2,"137":2}}],["初始稀疏度",{"5":{"132":2}}],["初始温度",{"5":{"132":2}}],["初始化actor网络参数",{"5":{"155":2}}],["初始化为0",{"5":{"150":2}}],["初始化只是解决梯度不稳定性问题的第一步",{"5":{"148":2}}],["初始化方式为",{"5":{"148":4}}],["初始化方法",{"5":{"89":2,"148":1}}],["初始化与",{"2":{"135":1},"5":{"135":1}}],["初始化与梯度流",{"2":{"41":1},"5":{"41":1}}],["初始化",{"5":{"133":4,"135":6,"137":2,"143":2,"155":2}}],["初始化行最大值​为",{"5":{"86":1,"151":1}}],["初始化行最大值",{"5":{"86":1,"151":1}}],["初始化行和​为零",{"5":{"86":1,"151":1}}],["初始化行和",{"5":{"86":1,"151":1}}],["初始化输出块​为零",{"5":{"86":1,"151":1}}],["初始化输出块",{"5":{"86":1,"151":1}}],["初始化应该在",{"5":{"89":2}}],["初始化尺度等",{"5":{"89":2}}],["初始化尺度的选择影响训练初期的梯度流动",{"5":{"89":2}}],["初始化的稳定条件",{"5":{"135":2}}],["初始化的",{"5":{"135":4}}],["初始化的编码矩阵",{"5":{"89":2}}],["初始化的选择会影响训练初期的梯度流动和收敛速度",{"5":{"94":2}}],["初始化策略的目标是使网络在训练初期具有",{"5":{"148":2}}],["初始化策略总结",{"2":{"148":1},"5":{"148":1}}],["初始化策略与训练动态",{"2":{"89":1},"5":{"89":1}}],["初始化策略和批量归一化的效果",{"5":{"96":2}}],["初始编码范数与内容嵌入范数相当是合理的选择",{"5":{"89":2}}],["初始的微小差异通过训练过程的放大",{"5":{"93":2}}],["初期",{"5":{"134":2,"158":2}}],["初期的小学习率允许模型逐渐学习位置信息的细微模式",{"5":{"97":2}}],["初期使用小",{"5":{"133":1}}],["初期使用小让模型快速学习",{"5":{"133":1}}],["初期使用较小的学习率有助于优化器",{"5":{"97":2}}],["初期使用",{"5":{"97":2}}],["帮助研究者诊断和调整moe训练策略",{"5":{"158":2}}],["帮助我们理解何时应该增加参数量",{"5":{"138":1}}],["帮助我们理解模型性能评估的可靠性",{"5":{"96":2}}],["帮助我们揭开人工智能系统内部运作机制的神秘面纱",{"5":{"137":2}}],["帮助我们更好地分析和优化人工智能系统",{"5":{"133":2}}],["帮助模型快速收敛到最优解",{"5":{"97":2}}],["建议采用以下配置",{"5":{"157":2}}],["建议采用自适应学习率调度结合梯度裁剪的策略",{"5":{"136":2}}],["建立梯度消失与爆炸的精确判别条件",{"5":{"148":2}}],["建立其与优化理论之间的联系",{"5":{"147":2}}],["建立了梯度范数与层间雅可比矩阵乘积之间的联系",{"5":{"148":2}}],["建立了梯度消失和梯度爆炸的精确判据",{"5":{"135":2}}],["建立了理论分析与实际训练策略的联系",{"5":{"136":2}}],["建立了理论收敛速度与实际优化行为之间的联系",{"5":{"97":2}}],["建立了从全局收敛到线性收敛的完整理论框架",{"5":{"136":2}}],["建立了从经典信息瓶颈到变分信息瓶颈的完整数学框架",{"5":{"133":2}}],["建立了系统的理论框架",{"5":{"136":2}}],["建立了离散时间动态系统的局部稳定性判据",{"5":{"135":2}}],["建立了完整的数学理论框架",{"5":{"135":2}}],["建立了信息论与优化理论的深层联系",{"5":{"137":2}}],["建立了信息论与深度学习之间的重要联系",{"5":{"133":2}}],["建立了信息瓶颈目标与",{"5":{"133":2}}],["建立了信息瓶颈框架下的注意力优化目标",{"5":{"132":2}}],["建立了注意力熵与信息集中度之间的量化关系",{"5":{"132":2}}],["建立了注意力权重与概率分布",{"5":{"132":2}}],["建立了深度学习与信息论之间的重要联系",{"5":{"132":2}}],["建模为随机微分方程",{"5":{"135":2}}],["建模能力使其在理解任务上具有优势",{"5":{"99":2,"101":2}}],["建模是一个核心挑战",{"5":{"92":2,"153":2}}],["建模全局长程依赖",{"5":{"92":2,"153":2}}],["建模成本都是固定的",{"5":{"92":2,"153":2}}],["建模这种依赖需要步的顺序计算",{"5":{"92":1,"153":1}}],["建模这种依赖需要",{"5":{"92":1,"153":1}}],["演化的梯度流方程为",{"5":{"97":1}}]],"serializationVersion":2}