{"documentCount":4,"nextId":8,"documentIds":{"4":"index.html","5":"第1章-数学基础/1.3-微积分与优化基础.html","6":"第1章-数学基础/1.2-概率论与统计.html","7":"第1章-数学基础/1.1-线性代数与张量运算.html"},"fieldIds":{"title":0,"aliases":1,"headers":2,"tags":3,"path":4,"content":5},"fieldLength":{"4":[1,1,2,1,2,31],"5":[3,1,6,1,6,500],"6":[3,1,15,1,6,891],"7":[2,1,26,1,5,871]},"averageFieldLength":[2.25,1,12.25,1,4.75,573.25],"storedFields":{"4":{"title":"大模型中的数学","aliases":[],"headers":["第一章 线性代数基础"],"tags":[],"path":"index.html"},"5":{"title":"1.3 微积分与优化基础","aliases":[],"headers":["1.3.1 偏导数与链式法则","1.3.2 多元函数梯度与Hessian","1.3.3 梯度下降与凸优化基础"],"tags":[],"path":"第1章-数学基础/1.3-微积分与优化基础.html"},"6":{"title":"1.2 概率论与统计","aliases":[],"headers":["1.2.1 随机变量与概率分布","1.2.2 期望、方差与协方差","1.2.3 高斯分布与多元高斯","1.2.4 KL散度与交叉熵","1.2.5 统计推断与参数估计","1.2.6 大数定律与中心极限定理","1.2.7 假设检验与模型评估"],"tags":[],"path":"第1章-数学基础/1.2-概率论与统计.html"},"7":{"title":"1.1 线性代数与张量运算","aliases":[],"headers":["1.1.1 线性代数的核心地位","1.1.2 基本运算与运算规则","1.1.3 张量的表示与运算","1.1.4 特征空间的几何直觉","1.1.5 特征值与特征向量","1.1.6 奇异值与低秩近似","1.1.7 正交性与正交矩阵","1.1.8 矩阵范数与度量","1.1.9 矩阵分解的深化理解","1.1.10 Kronecker积与向量化","1.1.11 张量网络与高维运算","1.1.12 批处理与并行计算的张量视图","1.1.13 计算复杂度与内存效率"],"tags":[],"path":"第1章-数学基础/1.1-线性代数与张量运算.html"}},"dirtCount":4,"index":[["隐藏维度",{"5":{"7":1}}],["隐藏维度通常在数千量级",{"5":{"7":1}}],["次标量乘法和加法",{"5":{"7":1}}],["次迭代后",{"5":{"5":1}}],["降低到",{"5":{"7":1}}],["到",{"5":{"7":1}}],["减少到",{"5":{"7":1}}],["测量两个向量方向的相似程度",{"5":{"7":1}}],["测量两个向量之间的直线距离",{"5":{"7":1}}],["欧几里得距离",{"5":{"7":1}}],["欧几里得距离测量两个向量之间的直线距离",{"5":{"7":1}}],["视为被重复",{"5":{"7":1}}],["流水线并行",{"5":{"7":2}}],["代价是增加额外的计算量",{"5":{"7":2}}],["激活值的内存消耗可能超过参数本身",{"5":{"7":2}}],["还与激活值的存储需求有关",{"5":{"7":2}}],["还给出了差异大小的可能范围",{"5":{"6":2}}],["内存效率不仅与参数数量有关",{"5":{"7":2}}],["稀疏注意力和分层注意力",{"5":{"7":2}}],["又不受单批次大小的限制",{"5":{"7":2}}],["又称为张量点积或广义矩阵乘法",{"5":{"7":2}}],["既保证了一次更新使用的样本数量",{"5":{"7":2}}],["微批处理",{"5":{"7":2}}],["微积分是研究变化率和累积效应的数学分支",{"5":{"5":2}}],["微积分与优化基础<",{"5":{"4":1}}],["微积分与优化基础",{"0":{"5":1},"4":{"2":1,"5":1},"5":{"4":4,"5":1}}],["单个样本的注意力计算涉及形状为",{"5":{"7":1}}],["单个样本的注意力计算涉及形状为的查询",{"5":{"7":1}}],["单位为比特",{"5":{"6":2}}],["现代gpu的并行架构非常适合这种批处理计算",{"5":{"7":2}}],["现代gpu架构专门优化了张量运算",{"5":{"7":2}}],["×",{"5":{"7":2}}],["序列长度",{"5":{"7":2}}],["序列长度和嵌入维度",{"5":{"7":2}}],["卷积运算和批处理操作",{"5":{"7":2}}],["更复杂的网络结构如树状张量网络",{"5":{"7":2}}],["更新规则为",{"5":{"5":2}}],["被分解为多个低阶张量的组合",{"5":{"7":2}}],["被誉为",{"5":{"7":2}}],["值的交互",{"5":{"7":2}}],["值投影都是通过矩阵乘法实现的",{"5":{"7":2}}],["查询",{"5":{"7":2}}],["查询矩阵与键矩阵的乘积产生注意力分数",{"5":{"7":2}}],["组成",{"5":{"7":2}}],["按列优先的顺序排列矩阵元素",{"5":{"7":2}}],["混合精度利用了现代gpu对低精度运算的专门优化",{"5":{"7":2}}],["混合精度训练可以将内存消耗减半",{"5":{"7":2}}],["混合精度训练是另一种重要的效率优化技术",{"5":{"7":2}}],["混合精度训练使用半精度浮点数进行计算以节省内存和加速",{"5":{"5":2}}],["混合积性质",{"5":{"7":2}}],["主成分分析和谱聚类中都有应用",{"5":{"7":2}}],["主题模型的推断和某些贝叶斯神经网络方法",{"5":{"6":2}}],["才能进行这种分解",{"5":{"7":2}}],["带置换矩阵",{"5":{"7":2}}],["qr分解将矩阵分解为正交矩阵",{"5":{"7":1}}],["qr分解将矩阵分解为正交矩阵和上三角矩阵的乘积",{"5":{"7":1}}],["qr分解在线性最小二乘问题中有重要应用",{"5":{"7":2}}],["qr分解有多种计算方法",{"5":{"7":2}}],["qr分解",{"5":{"7":2}}],["除了前面介绍的奇异值分解",{"5":{"7":2}}],["核范数正则化可以鼓励解的低秩性质",{"5":{"7":2}}],["核范数是frobenius范数和谱范数之间的折中",{"5":{"7":2}}],["核范数",{"5":{"7":2}}],["核范数和1",{"5":{"7":2}}],["∞",{"5":{"7":2}}],["齐次性和三角不等式等性质",{"5":{"7":2}}],["满足",{"5":{"7":2}}],["满足非负性",{"5":{"7":2}}],["满足和",{"5":{"7":1}}],["虽然自然语言处理很少直接使用傅里叶变换",{"5":{"7":2}}],["傅里叶变换和正交基扩展是信号处理和自然语言处理中的重要工具",{"5":{"7":2}}],["傅里叶变换和微分方程等运算下保持封闭形式",{"5":{"6":2}}],["构成标准正交基",{"5":{"7":2}}],["说明模型的表达能力主要集中在少数方向上",{"5":{"7":2}}],["典型的低秩适配方法如lora",{"5":{"7":2}}],["典型的操作包括矩阵乘法",{"5":{"5":2}}],["低秩更新是一种核心技术",{"5":{"7":2}}],["低秩近似技术有着广泛的应用",{"5":{"7":2}}],["低秩近似是奇异值分解最重要的应用之一",{"5":{"7":2}}],["用秩不超过k的矩阵对",{"5":{"7":1}}],["用秩不超过k的矩阵对的最佳近似",{"5":{"7":1}}],["用于处理超大序列或有限gpu内存的情况",{"5":{"7":2}}],["用于比较两个版本",{"5":{"6":2}}],["用于根据样本数据判断关于总体的假设是否成立",{"5":{"6":2}}],["用于建模连续的随机过程",{"5":{"6":2}}],["young",{"5":{"7":2}}],["无论是否满秩",{"5":{"7":2}}],["无论是方阵还是矩形阵",{"5":{"7":2}}],["无论原始随机变量的分布是什么",{"5":{"6":2}}],["另外",{"5":{"7":2}}],["存在正交矩阵",{"5":{"7":2}}],["存在大量的局部最小值",{"5":{"5":2}}],["奇异值分解可以用于理解模型的表达能力",{"5":{"7":2}}],["奇异值分解之所以强大",{"5":{"7":2}}],["奇异值分解与特征值分解有密切关系",{"5":{"7":2}}],["奇异值分解",{"5":{"7":2}}],["奇异值与低秩近似",{"2":{"7":1},"5":{"7":1}}],["谱归一化",{"5":{"7":4}}],["谱范数最重要",{"5":{"7":2}}],["谱范数是核心概念",{"5":{"7":2}}],["谱范数在深度学习中有重要应用",{"5":{"7":2}}],["谱范数",{"5":{"7":6}}],["类似的稳定性分析有助于理解和解决梯度消失或梯度爆炸问题",{"5":{"7":2}}],["类别分布",{"5":{"6":2}}],["再用非线性方法进行二维或三维嵌入",{"5":{"7":2}}],["再通过重参数化技巧进行采样",{"5":{"6":2}}],["先通过pca进行初步降维",{"5":{"7":2}}],["iteration",{"5":{"7":4}}],["int8",{"5":{"7":2}}],["internal",{"5":{"4":3}}],["inverse",{"5":{"7":2}}],["information",{"5":{"6":2}}],["index",{"4":{"3":1,"4":1}}],["直接求解特征方程通常是不切实际的",{"5":{"7":2}}],["直观上",{"5":{"6":2}}],["给定一个方阵",{"5":{"7":2}}],["给定前文的条件下",{"5":{"6":2}}],["遗忘",{"5":{"7":2}}],["零空间",{"5":{"7":2}}],["零阶张量",{"5":{"7":2}}],["列空间",{"5":{"7":2}}],["子空间是特征空间中的重要结构",{"5":{"7":2}}],["全连接层就是典型的线性变换加上非线性激活函数的组合",{"5":{"7":2}}],["剪切则是一种保持体积但改变形状的变换",{"5":{"7":2}}],["投影将向量映射到低维子空间",{"5":{"7":2}}],["投影和剪切等",{"5":{"7":2}}],["旋转保持向量的模长不变",{"5":{"7":2}}],["缩放的倍数就是特征值",{"5":{"7":2}}],["缩放改变向量的模长",{"5":{"7":2}}],["缩放",{"5":{"7":2}}],["余弦相似度",{"5":{"7":1}}],["余弦相似度是最常用的相似度度量",{"5":{"7":2}}],["余弦相似度测量两个向量方向的相似程度",{"5":{"7":1}}],["余弦退火",{"5":{"5":2}}],["过高的维度则可能导致过拟合和计算效率的下降",{"5":{"7":2}}],["过低的维度可能导致表达能力的损失",{"5":{"7":2}}],["嵌入维度",{"5":{"7":2}}],["嵌入空间的维度通常远小于词表大小",{"5":{"7":2}}],["嵌入空间通常是密集的",{"5":{"7":2}}],["嵌入层将词汇映射为向量",{"5":{"7":2}}],["语义不同的词则距离较远",{"5":{"7":2}}],["语义相近的词在空间中距离较近",{"5":{"7":2}}],["语言模型预测下一个词的概率分布为",{"5":{"6":2}}],["语言模型的训练目标是最大化训练语料库的对数似然",{"5":{"6":2}}],["语言模型的训练目标就是学习这个条件概率分布",{"5":{"6":2}}],["语言模型的输出层正是类别分布的一个典型应用",{"5":{"6":2}}],["空间中的每个点代表一个数据样本在该空间中的表示",{"5":{"7":2}}],["否则形状不兼容",{"5":{"7":2}}],["否则为0",{"5":{"6":2}}],["将形状较小的张量在某些维度上",{"5":{"7":1}}],["将形状较小的张量在某些维度上视为被重复",{"5":{"7":1}}],["将它们累加起来",{"5":{"7":2}}],["将离散的符号表示转化为连续的数值表示",{"5":{"7":2}}],["逐元素运算对张量中的每个独立元素应用相同的函数",{"5":{"7":2}}],["广播规则通常是从最后一个维度开始逐维比较",{"5":{"7":2}}],["广播是张量逐元素运算中的一种自动对齐机制",{"5":{"7":2}}],["广播",{"5":{"7":2}}],["便于进行残差连接和层归一化操作",{"5":{"7":2}}],["便于解释和比较",{"5":{"6":2}}],["经过注意力计算后的输出张量会与输入张量形状相同",{"5":{"7":2}}],["经过线性投影并拆分注意力头后",{"5":{"7":2}}],["输出",{"5":{"7":2}}],["输出层的预测分布是一个",{"5":{"6":1}}],["输出层的预测分布是一个维的概率向量",{"5":{"6":1}}],["输入张量的形状通常是",{"5":{"7":2}}],["行列式非零",{"5":{"7":2}}],["行数和列数",{"5":{"7":2}}],["记作",{"5":{"7":1}}],["记作或",{"5":{"7":1}}],["记为或",{"5":{"7":1}}],["记为​",{"5":{"6":1}}],["记为",{"5":{"6":7,"7":1}}],["就是典型的逐元素运算",{"5":{"7":1}}],["就是保留前k个最大的奇异值及其对应的奇异向量",{"5":{"7":2}}],["就可以实现对大型语言模型的有效微调",{"5":{"7":2}}],["就依赖于这种思想",{"5":{"7":2}}],["就将梯度向量乘以一个缩放因子使其范数等于阈值",{"5":{"7":2}}],["就近似服从正态分布",{"5":{"6":2}}],["结果是将向量的每个分量都乘以该标量",{"5":{"7":2}}],["残差连接是向量加法的典型应用",{"5":{"7":2}}],["中",{"5":{"7":6}}],["中心极限定理提供了重要的洞见",{"5":{"6":2}}],["中心极限定理的重要性在于它表明正态分布在自然界中无处不在",{"5":{"6":2}}],["中心极限定理",{"5":{"6":2}}],["中心极限定理表明大量独立随机变量之和趋向于高斯分布",{"5":{"6":2}}],["中心",{"5":{"6":2}}],["两个向量",{"5":{"7":1}}],["两个向量和如果满足",{"5":{"7":1}}],["两个三阶张量",{"5":{"7":1}}],["两个三阶张量和在第二和第三维度上的收缩产生一个的矩阵",{"5":{"7":1}}],["两个矩阵",{"5":{"7":1}}],["两个矩阵和的乘积",{"5":{"7":1}}],["两个二阶张量",{"5":{"7":2}}],["两个维度相同的向量可以逐分量相加",{"5":{"7":2}}],["两个模型没有差异",{"5":{"6":2}}],["weight",{"5":{"7":2}}],["wise",{"5":{"7":2}}],["warmup策略在训练初期逐渐增加学习率",{"5":{"5":2}}],["warmup",{"5":{"5":4}}],["向量的乘积需要",{"5":{"7":1}}],["向量加法满足交换律和结合律",{"5":{"7":1}}],["向量加法是逐分量",{"5":{"7":2}}],["向量乘积",{"5":{"7":2}}],["向量化操作与kronecker积结合可以简化矩阵方程的表示",{"5":{"7":2}}],["向量化",{"5":{"7":2}}],["向量距离和相似度是度量特征空间中样本关系的基本工具",{"5":{"7":2}}],["向量",{"5":{"7":2}}],["向量是线性空间中最基本的元素",{"5":{"7":2}}],["转置运算满足",{"5":{"7":1}}],["转置运算满足和",{"5":{"7":1}}],["转置性质",{"5":{"7":2}}],["转置",{"5":{"7":2}}],["转置操作也经常用于梯度的反向传播计算",{"5":{"7":2}}],["转置和求逆等",{"5":{"7":2}}],["转换为标准高斯随机变量",{"5":{"6":1}}],["转换为有效的类别概率分布",{"5":{"6":2}}],["几乎所有的计算都在张量之上进行",{"5":{"7":2}}],["几何上",{"5":{"5":2}}],["三阶及更高阶的张量则可以表示更复杂的数据结构",{"5":{"7":2}}],["决定了它所代表的线性变换的类型",{"5":{"7":2}}],["前馈神经网络中的全连接层更是由多个权重矩阵堆叠而成",{"5":{"7":2}}],["键和值张量",{"5":{"7":2}}],["键",{"5":{"7":4}}],["权重矩阵与",{"5":{"7":1}}],["权重矩阵的低秩近似可以显著减少参数量和计算量",{"5":{"7":2}}],["权重矩阵的列空间决定了该层能够表示的特征空间的范围",{"5":{"7":2}}],["权重矩阵无处不在",{"5":{"7":2}}],["权重初始化的尺度选择",{"5":{"6":2}}],["批处理相当于将多个独立的矩阵运算堆叠成一个大的张量运算",{"5":{"7":2}}],["批处理的数学优势来自于矩阵运算的并行性",{"5":{"7":2}}],["批处理是在现有维度之外增加一个新的批处理维度",{"5":{"7":2}}],["批处理",{"5":{"7":2}}],["批处理与并行计算的张量视图",{"2":{"7":1},"5":{"7":1}}],["批量归一化对每一层的输入进行标准化",{"5":{"6":2}}],["批量归一化",{"5":{"6":2}}],["张量可重排为",{"5":{"7":1}}],["张量可重排为的张量",{"5":{"7":1}}],["张量并行",{"5":{"7":2}}],["张量运算的高效实现是现代深度学习框架的核心能力之一",{"5":{"7":2}}],["张量运算的基本操作包括逐元素运算",{"5":{"7":2}}],["张量网络在模型压缩和高效推理中有着重要的应用价值",{"5":{"7":2}}],["张量网络的思维方式有助于理解复杂的计算结构",{"5":{"7":2}}],["张量网络的核心思想是用多个小张量的乘积之和来表示一个巨大的高维张量",{"5":{"7":2}}],["张量网络是用低维张量通过网络结构表示高维张量的方法",{"5":{"7":2}}],["张量网络与高维运算",{"2":{"7":1},"5":{"7":1}}],["张量分解在模型压缩",{"5":{"7":2}}],["张量分解是处理高维张量的重要技术",{"5":{"7":2}}],["张量收缩指定两个张量的若干维度进行配对相乘并求和",{"5":{"7":2}}],["张量收缩是矩阵乘法在高维张量上的推广",{"5":{"7":2}}],["张量的维度通常具有明确的语义含义",{"5":{"7":2}}],["张量的表示与运算",{"2":{"7":1},"5":{"7":1}}],["张量是多维数组在数学上的抽象表示",{"5":{"7":2}}],["张量是多维数组的自然推广",{"5":{"7":2}}],["张量是最基本的数据结构",{"5":{"7":2}}],["9",{"2":{"7":1},"5":{"7":1}}],["8",{"2":{"7":1},"5":{"7":1}}],["依概率收敛于",{"5":{"6":1}}],["后",{"5":{"6":1}}],["后者就是编码器分布与标准高斯先验之间的kl散度",{"5":{"6":2}}],["找到一个主要的模式并紧密地拟合它",{"5":{"6":1}}],["覆盖",{"5":{"6":1}}],["鼓励权重矩阵的列向量相互正交",{"5":{"7":2}}],["鼓励",{"5":{"6":1}}],["及其概率分布",{"5":{"6":1}}],["划分为两个子向量",{"5":{"6":1}}],["服从一维高斯分布",{"5":{"6":1}}],["服从多元高斯分布",{"5":{"6":1}}],["服从以真实参数为均值",{"5":{"6":2}}],["处求值",{"5":{"6":1}}],["处的梯度定义为",{"5":{"5":1}}],["处的偏导数定义为",{"5":{"5":1}}],["个秩一张量的和",{"5":{"7":1}}],["个参数",{"5":{"7":1}}],["个参数而不是",{"5":{"7":1}}],["个索引",{"5":{"7":1}}],["个类别的logit",{"5":{"6":1}}],["个类别上服从类别分布",{"5":{"6":1}}],["个元素为",{"5":{"6":1}}],["维向量",{"5":{"7":1}}],["维度的选择是一个重要的超参数",{"5":{"7":2}}],["维随机向量",{"5":{"6":2}}],["维的概率向量",{"5":{"6":1}}],["倾向于反向变化",{"5":{"6":1}}],["倾向于同向变化",{"5":{"6":1}}],["独立时互信息为0",{"5":{"6":1}}],["独立",{"5":{"6":1}}],["独立于样本量",{"5":{"6":2}}],["落在区间",{"5":{"6":1}}],["落在区间的概率为",{"5":{"6":1}}],["成立",{"5":{"6":2,"7":1}}],["取特定值",{"5":{"6":1}}],["任务完成率等指标",{"5":{"6":2}}],["任何高斯随机变量都可以通过标准化变换",{"5":{"6":1}}],["任何高斯随机变量都可以通过标准化变换转换为标准高斯随机变量",{"5":{"6":1}}],["任何局部最优解都是全局最优解",{"5":{"5":2}}],["置换检验在比较两个模型在特定测试集上的性能时很有用",{"5":{"6":2}}],["置换检验通过随机打乱标签来生成置换样本",{"5":{"6":2}}],["置换检验",{"5":{"6":2}}],["置信区间比p值提供更多信息",{"5":{"6":2}}],["需要",{"5":{"7":1}}],["需要使用适当的多重比较校正方法",{"5":{"6":2}}],["需要注意的是",{"5":{"6":2}}],["至少有一个被拒绝的概率也会大大增加",{"5":{"6":2}}],["报告效应量可以帮助读者理解改进的实际意义",{"5":{"6":2}}],["效应量",{"5":{"6":2}}],["样本量很小时",{"5":{"6":2}}],["样本量很大时",{"5":{"6":2}}],["样本均值趋向于随机变量的期望",{"5":{"6":2}}],["重塑操作改变张量的形状但不改变其包含的数据元素",{"5":{"7":2}}],["重塑",{"5":{"7":2}}],["重要",{"5":{"6":2}}],["重复多次",{"5":{"6":2}}],["实数",{"5":{"7":2}}],["实际上是一个巨大的查找矩阵",{"5":{"7":2}}],["实际显著",{"5":{"6":2}}],["实践表明深度学习模型通常能够找到性能不错的解",{"5":{"5":2}}],["统计显著",{"5":{"6":2}}],["统计推断的思想贯穿于模型训练",{"5":{"6":2}}],["统计推断是利用样本数据对总体特征进行推断的过程",{"5":{"6":2}}],["统计推断与参数估计",{"2":{"6":1},"5":{"6":1}}],["显著提升了深层模型的可训练性",{"5":{"7":2}}],["显著性检验的解读需要谨慎",{"5":{"6":2}}],["显式计算hessian是不现实的",{"5":{"5":2}}],["配对t检验可以判断两个模型性能的差异是否显著",{"5":{"6":2}}],["配对t检验是比较两个相关样本均值的常用方法",{"5":{"6":2}}],["05",{"5":{"6":2}}],["备择假设是我们想要支持的假设",{"5":{"6":2}}],["原假设通常是我们想要拒绝的假设",{"5":{"6":2}}],["验证改进措施的有效性以及检测数据中的系统性偏差",{"5":{"6":2}}],["验证集越大",{"5":{"6":2}}],["同时理解评估结果的统计显著性",{"5":{"6":2}}],["该指标是总体性能的一个估计",{"5":{"6":2}}],["该批量内样本的均值和方差是整体数据均值和方差的良好估计",{"5":{"6":2}}],["该位置的交叉熵损失为",{"5":{"6":2}}],["初始化策略和批量归一化的效果",{"5":{"6":2}}],["趋向于正态分布",{"5":{"6":2}}],["适当标准化后",{"5":{"6":2}}],["tensor",{"5":{"7":4}}],["test",{"5":{"6":2}}],["tree",{"5":{"7":2}}],["transpose",{"5":{"7":2}}],["transformer中的注意力机制可以被看作一个三阶张量",{"5":{"7":2}}],["transformer",{"5":{"7":2}}],["tuning",{"5":{"7":2}}],["tucker",{"5":{"7":6}}],["theorem",{"5":{"6":2}}],["target=",{"5":{"4":3,"5":3,"6":3,"7":2}}],["优化目标就是最小化",{"5":{"6":1}}],["优化目标就是最小化或",{"5":{"6":1}}],["优化过程变得更加稳定",{"5":{"6":2}}],["随着批量大小的增加",{"5":{"6":2}}],["随机采样策略的设计",{"5":{"6":2}}],["随机变量分为离散随机变量和连续随机变量",{"5":{"6":2}}],["随机变量是样本空间到实数集的映射",{"5":{"6":2}}],["随机变量与概率分布",{"2":{"6":1},"5":{"6":1}}],["随机梯度下降的隐式正则化效应",{"5":{"5":2}}],["弱大数定律保证样本均值以概率收敛于期望",{"5":{"6":2}}],["指出",{"5":{"6":2}}],["指数分布的期望为",{"5":{"6":1}}],["指数分布的期望为​",{"5":{"6":1}}],["指数分布在语言模型中的应用包括描述某些随机过程的到达时间间隔",{"5":{"6":2}}],["指数分布",{"5":{"6":2}}],["指数衰减",{"5":{"5":2}}],["帮助我们理解模型性能评估的可靠性",{"5":{"6":2}}],["精确率等指标的置信区间",{"5":{"6":2}}],["基本运算与运算规则",{"2":{"7":1},"5":{"7":1}}],["基本思想是从原始数据中有放回地抽取与原数据等大的样本",{"5":{"6":2}}],["基本梯度下降算法的更新规则为",{"5":{"5":2}}],["特别适用于样本量不大或分布未知的情况",{"5":{"6":2}}],["特别是当目标分布是高度非高斯的时候",{"5":{"6":2}}],["特征向量",{"5":{"7":2}}],["特征向量表示这些曲率对应的方向",{"5":{"5":2}}],["特征值是实数",{"5":{"7":2}}],["特征值可以是负数",{"5":{"7":2}}],["特征值分解在动力系统分析",{"5":{"7":2}}],["特征值分解总是可行的",{"5":{"7":2}}],["特征值分解将方阵分解为特征向量矩阵和特征值对角矩阵的乘积",{"5":{"7":2}}],["特征值分解和cholesky分解等多种方法",{"5":{"7":2}}],["特征值分解只适用于方阵且要求矩阵可对角化",{"5":{"7":2}}],["特征值分解用于提取数据中方差最大的方向",{"5":{"7":2}}],["特征值与网络稳定性密切相关",{"5":{"7":2}}],["特征值与特征向量",{"2":{"7":1},"5":{"7":1}}],["特征值和特征向量有着多方面的应用",{"5":{"7":2}}],["特征值和特征向量是线性代数中最深刻也最有应用价值的概念之一",{"5":{"7":2}}],["特征值的计算需要求解特征方程",{"5":{"7":2}}],["特征空间是机器学习和深度学习中描述数据表示的核心概念",{"5":{"7":2}}],["特征空间的几何直觉",{"2":{"7":1},"5":{"7":1}}],["特征函数提供了有用的分析工具",{"5":{"6":2}}],["特征函数是矩母函数的复数形式",{"5":{"6":2}}],["来逼近复杂的后验分布",{"5":{"6":1}}],["来编码来自分布",{"5":{"6":1}}],["来控制总体错误率",{"5":{"6":2}}],["来近似后验分布",{"5":{"6":2}}],["来近似",{"5":{"6":2}}],["来组织偏导数",{"5":{"5":2}}],["变分推断被用于vae的训练",{"5":{"6":2}}],["变分推断假设后验分布可以用某个简单的分布族",{"5":{"6":2}}],["变分推断是一类近似贝叶斯推断的方法",{"5":{"6":2}}],["变分dropout",{"5":{"6":2}}],["共轭先验是使后验分布与先验分布同分布族的先验选择",{"5":{"6":2}}],["共轭梯度法利用hessian的信息来加速收敛",{"5":{"5":2}}],["贝叶斯方法可用于模型选择",{"5":{"6":2}}],["贝叶斯估计使用后验分布进行预测",{"5":{"6":2}}],["贝叶斯估计是另一种参数估计方法",{"5":{"6":2}}],["贝叶斯估计和期望传播等统计推断方法",{"5":{"6":2}}],["参数复杂度从",{"5":{"7":1}}],["参数复杂度从降低到",{"5":{"7":1}}],["参数高效微调和特定网络结构的设计",{"5":{"7":2}}],["参数高效微调以及模型蒸馏等场景中具有重要应用",{"5":{"7":2}}],["参数的后验分布为",{"5":{"6":2}}],["参数优化和性能评估的全过程",{"5":{"6":2}}],["罗下界",{"5":{"6":2}}],["费希尔信息矩阵逆为方差的正态分布",{"5":{"6":2}}],["渐近正态的",{"5":{"6":2}}],["收敛于真实参数值",{"5":{"6":2}}],["训练过程就是使用随机梯度下降等优化算法来最大化这个目标函数",{"5":{"6":2}}],["似然函数定义为",{"5":{"6":2}}],["于某个概率分布",{"5":{"6":2}}],["选择哪种kl形式取决于我们希望对近似分布施加什么样的约束",{"5":{"6":2}}],["提高泛化能力",{"5":{"6":2}}],["替换为软标签",{"5":{"6":2}}],["one",{"5":{"6":2}}],["标量乘法是将一个向量与一个标量",{"5":{"7":2}}],["标量乘法",{"5":{"7":2}}],["标签平滑相当于在训练时使用真实分布与均匀分布的混合作为目标分布",{"5":{"6":2}}],["标签平滑可以防止模型对训练数据过度自信",{"5":{"6":2}}],["标签平滑",{"5":{"6":2}}],["标准化均值差异",{"5":{"6":2}}],["标准vae假设潜在变量服从标准高斯分布",{"5":{"6":2}}],["标准高斯分布",{"5":{"6":2}}],["标准梯度下降以",{"5":{"5":1}}],["标准梯度下降以的速度收敛",{"5":{"5":1}}],["标准优化算法的内存开销巨大",{"5":{"5":2}}],["真实标签为",{"5":{"6":2}}],["整个序列或批次的损失是这些位置损失的均值或和",{"5":{"6":2}}],["交叉熵损失相对于模型输出",{"5":{"6":2}}],["交叉熵损失的优势在于它直接与对数似然相关",{"5":{"6":2}}],["交叉熵损失是最常用的损失函数",{"5":{"6":2}}],["交叉熵可以分解为熵与kl散度之和",{"5":{"6":2}}],["交叉熵定义为",{"5":{"6":2}}],["交叉熵与kl散度密切相关",{"5":{"6":2}}],["等于矩阵",{"5":{"7":1}}],["等于矩阵的最大特征值的平方根",{"5":{"7":1}}],["等现代深度学习框架均支持广播机制",{"5":{"7":2}}],["等",{"5":{"7":2}}],["等效的高斯dropout在权重上引入方差为",{"5":{"6":1}}],["等效的高斯dropout在权重上引入方差为的高斯噪声",{"5":{"6":1}}],["等算法中非常重要",{"5":{"6":2}}],["确保比较的公平性",{"5":{"6":2}}],["确保新策略不会偏离旧策略太远",{"5":{"6":2}}],["确定在原假设下观察到该统计量或更极端情况的概率",{"5":{"6":2}}],["确定性高",{"5":{"6":2}}],["严格来说不是真正的距离",{"5":{"6":2}}],["严格凸函数至多有一个全局最小值点",{"5":{"5":2}}],["严格凸函数是凸性更强的形式",{"5":{"5":2}}],["距离",{"5":{"6":2}}],["互信息可用于分析不同位置或不同层之间的信息流动",{"5":{"6":2}}],["互信息始终非负",{"5":{"6":2}}],["互信息",{"5":{"6":2}}],["joint",{"5":{"6":2}}],["jacobian",{"5":{"5":2}}],["联合熵",{"5":{"6":2}}],["某些特定方向",{"5":{"7":2}}],["某些语言模型变体使用高斯混合模型来表示输出分布",{"5":{"6":2}}],["某个",{"5":{"6":1}}],["某个的概率为1",{"5":{"6":1}}],["尖锐",{"5":{"6":2}}],["不当使用容易引入隐蔽且难以察觉的",{"5":{"7":2}}],["不相关时等号才成立",{"5":{"6":1}}],["不等于",{"5":{"6":2}}],["不确定性量化和小样本学习",{"5":{"6":2}}],["不确定性高",{"5":{"6":2}}],["不同",{"5":{"5":2}}],["分解则将张量表示为一个核心张量与多个因子矩阵在各模上的乘积",{"5":{"7":2}}],["分解将一个张量表示为若干个秩一张量的和",{"5":{"7":2}}],["分解",{"5":{"7":4}}],["分布用pdf表示",{"5":{"6":2}}],["分布越",{"5":{"6":4}}],["分析不同层之间隐藏状态的相关性",{"5":{"6":2}}],["熵为0",{"5":{"6":2}}],["熵越小",{"5":{"6":2}}],["熵越大",{"5":{"6":2}}],["熵衡量了随机变量不确定性的大小",{"5":{"6":2}}],["熵定义为",{"5":{"6":2}}],["信息论的基本概念始于熵",{"5":{"6":2}}],["也是为什么许多研究致力于开发高效的注意力变体",{"5":{"7":2}}],["也是证明神经网络泛化性质的关键工具",{"5":{"7":2}}],["也决定了它在网络中的具体作用方式",{"5":{"7":2}}],["也指导着我们理解和解释模型训练过程中的各种现象",{"5":{"6":2}}],["也采用了类似的思想",{"5":{"6":2}}],["也称为正态分布",{"5":{"6":2}}],["也称为多项分布",{"5":{"6":2}}],["最大似然估计具有一些重要的渐近性质",{"5":{"6":2}}],["最大似然估计就是找到使对数似然最大的参数值",{"5":{"6":2}}],["最大似然估计",{"5":{"6":2}}],["最小化交叉熵等价于最大化数据的对数似然",{"5":{"6":2}}],["最后通过解码器重建原始输入",{"5":{"6":2}}],["最终达到较好的收敛效果",{"5":{"5":2}}],["利用权衡很有价值",{"5":{"6":2}}],["利普希茨连续梯度的凸函数",{"5":{"5":2}}],["神经架构搜索和语言模型微调中有着应用",{"5":{"6":2}}],["神经网络的激活值等",{"5":{"6":2}}],["神经网络本质上是一个嵌套的复合函数",{"5":{"5":2}}],["噪声的参数通过变分推断学习得到",{"5":{"6":2}}],["进行大部分计算",{"5":{"7":2}}],["进行梯度估计时",{"5":{"6":2}}],["进一步将dropout解释为贝叶斯推断",{"5":{"6":2}}],["进入函数值更高的区域",{"5":{"5":2}}],["时kl散度为0",{"5":{"6":1}}],["时",{"5":{"6":7,"7":1}}],["时取严格小于号",{"5":{"5":1}}],["具有完整特征向量基",{"5":{"7":2}}],["具有旋转不变性",{"5":{"7":2}}],["具有重要的几何意义和计算优势",{"5":{"7":2}}],["具有相同的量纲",{"5":{"6":1}}],["具有优美的理论性质和高效的算法",{"5":{"5":2}}],["具体的",{"5":{"7":2}}],["具体做法是如果梯度的范数超过阈值",{"5":{"7":2}}],["具体选择取决于变分推断的目标",{"5":{"6":2}}],["具体来说",{"5":{"6":4,"7":6}}],["具体而言",{"5":{"6":4}}],["xavier初始化和he初始化都假设权重服从某种高斯分布",{"5":{"6":2}}],["卡尔曼滤波和贝叶斯线性回归中有着核心应用",{"5":{"6":2}}],["注意力分数的计算",{"5":{"7":2}}],["注意力分数的计算需要的计算量",{"5":{"7":1}}],["注意力分数的计算在批处理情况下变成的三维张量运算",{"5":{"7":1}}],["注意力分数的计算可以看作是一个张量收缩操作",{"5":{"7":2}}],["注意力机制的计算复杂度与序列长度的平方成正比",{"5":{"7":2}}],["注意力机制的计算过程就包含了多个矩阵乘法操作",{"5":{"7":2}}],["注意力机制的批处理实现是理解这一概念的良好例子",{"5":{"7":2}}],["注意力机制中的查询",{"5":{"7":2}}],["注意力权重矩阵",{"5":{"7":1}}],["注意力权重矩阵可以看作是查询张量和键张量的收缩",{"5":{"7":1}}],["注意力权重与值向量的乘积同样需要",{"5":{"7":2}}],["注意力权重与值矩阵的乘积产生加权输出",{"5":{"7":2}}],["注意力权重",{"5":{"6":2}}],["注意条件协方差不依赖于观测值",{"5":{"6":2}}],["均值和对数方差",{"5":{"6":2}}],["均值和协方差相应地分块为",{"5":{"6":2}}],["均匀分布可用于权重初始化",{"5":{"6":2}}],["均匀分布的期望为",{"5":{"6":2}}],["均匀分布",{"5":{"6":2}}],["考虑将",{"5":{"6":1}}],["考虑将划分为两个子向量和​",{"5":{"6":1}}],["考虑一个简单的两层神经网络",{"5":{"5":2}}],["条件协方差为",{"5":{"6":1}}],["条件协方差为​",{"5":{"6":1}}],["条件熵",{"5":{"6":2}}],["条件分布在高斯过程回归",{"5":{"6":2}}],["条件分布是多元高斯分布最优美也最实用的性质之一",{"5":{"6":2}}],["条件期望",{"5":{"6":1}}],["条件期望用于描述在给定上下文的条件下",{"5":{"6":2}}],["条件期望的一个重要性质是全期望公式",{"5":{"6":2}}],["条件期望是的函数",{"5":{"6":1}}],["条件期望是在给定某些信息的条件下对随机变量的期望",{"5":{"6":2}}],["线性变换对特征空间的作用包括旋转",{"5":{"7":2}}],["线性变换是理解特征空间中数据变换的关键概念",{"5":{"7":2}}],["线性变换保持高斯性",{"5":{"6":2}}],["线性回归的闭式解就是通过求解正规方程得到的",{"5":{"7":2}}],["线性无关",{"5":{"6":1}}],["线性代数的瑞士军刀",{"5":{"7":2}}],["线性代数的基本运算包括向量加法",{"5":{"7":2}}],["线性代数的核心地位",{"2":{"7":1},"5":{"7":1}}],["线性代数为理解和构建深度学习模型提供了坚实的数学框架",{"5":{"7":2}}],["线性代数作为现代数学的重要分支",{"5":{"7":2}}],["线性代数基础",{"2":{"4":1},"5":{"4":1}}],["线性代数与张量运算<",{"5":{"4":1}}],["线性代数与张量运算",{"0":{"7":1},"4":{"0":1,"7":1},"5":{"4":4,"7":1}}],["边缘分布仍然是高斯分布",{"5":{"6":2}}],["边表示依赖关系",{"5":{"5":2}}],["称为奇异值",{"5":{"7":2}}],["称为欠拟合",{"5":{"7":2}}],["称为马氏距离",{"5":{"6":2}}],["称为精度矩阵",{"5":{"6":2}}],["必须是对称半正定的",{"5":{"6":2}}],["若两个张量的维度数不同",{"5":{"7":2}}],["若",{"5":{"6":6}}],["若服从多元高斯分布",{"5":{"6":1}}],["若和独立",{"5":{"6":1}}],["稍后详述",{"5":{"6":2}}],["相乘",{"5":{"7":2}}],["相对于标准正态分布",{"5":{"6":2}}],["相关系数可用于分析不同词嵌入维度之间的冗余程度",{"5":{"6":2}}],["相关系数消除了量纲的影响",{"5":{"6":2}}],["相关系数的取值范围为",{"5":{"6":2}}],["相关系数",{"5":{"6":2}}],["完全对称",{"5":{"6":2}}],["许多自然现象和测量误差都近似服从高斯分布",{"5":{"6":2}}],["源于多个方面的原因",{"5":{"6":2}}],["且特征向量矩阵是正交的",{"5":{"7":2}}],["且当且仅当",{"5":{"6":1}}],["且当且仅当时kl散度为0",{"5":{"6":1}}],["且",{"5":{"6":1}}],["且同样唯一确定概率分布",{"5":{"6":2}}],["且最优解集是凸集",{"5":{"5":2}}],["gpu可以同时计算批次中所有样本的注意力分数",{"5":{"7":2}}],["gan",{"5":{"7":2}}],["gauss",{"5":{"6":4}}],["gaussian",{"5":{"6":2}}],["generating",{"5":{"6":2}}],["gram",{"5":{"7":2}}],["gradient",{"5":{"5":2,"7":2}}],["grad",{"5":{"5":4}}],["graph",{"5":{"5":9,"6":9,"7":6}}],["峰度为3",{"5":{"6":2}}],["峰度",{"5":{"6":2}}],["由于训练数据通常被组织为序列",{"5":{"6":2}}],["由于参数数量巨大",{"5":{"5":2}}],["由jensen不等式保证",{"5":{"6":2}}],["由四阶中心矩归一化得到",{"5":{"6":2}}],["由三阶中心矩归一化得到",{"5":{"6":2}}],["偏度",{"5":{"6":2}}],["偏导数可以理解为多元函数图像与包含坐标轴",{"5":{"5":1}}],["偏导数可以理解为多元函数图像与包含坐标轴的平面相交得到的一元函数曲线的切线斜率",{"5":{"5":1}}],["偏导数的计算遵循与单变量导数相同的规则",{"5":{"5":2}}],["偏导数是多元函数对单个变量的导数",{"5":{"5":2}}],["偏导数与链式法则",{"2":{"5":1},"5":{"5":1}}],["阶张量有",{"5":{"7":1}}],["阶中心矩定义为",{"5":{"6":2}}],["阶原点矩定义为",{"5":{"6":2}}],["阶梯衰减",{"5":{"5":2}}],["矩阵和张量运算的计算复杂度和内存效率是核心关注点",{"5":{"7":2}}],["矩阵",{"5":{"7":2}}],["矩阵分解还包括lu分解",{"5":{"7":2}}],["矩阵分解的深化理解",{"2":{"7":1},"5":{"7":1}}],["矩阵范数的选择取决于具体问题的需求",{"5":{"7":2}}],["矩阵范数将矩阵映射到非负实数",{"5":{"7":2}}],["矩阵范数是衡量矩阵",{"5":{"7":2}}],["矩阵范数与度量",{"2":{"7":1},"5":{"7":1}}],["矩阵的求逆是找到一个矩阵",{"5":{"7":1}}],["矩阵的求逆是找到一个矩阵使得",{"5":{"7":1}}],["矩阵的转置是将矩阵的行和列互换",{"5":{"7":2}}],["矩阵的形状",{"5":{"7":2}}],["矩阵乘法的计算复杂度是",{"5":{"7":2}}],["矩阵乘法是连接两个张量的最基本操作",{"5":{"7":2}}],["矩阵乘法是最重要也是计算量最大的运算",{"5":{"7":2}}],["矩阵乘法满足结合律",{"5":{"7":2}}],["矩阵乘法",{"5":{"7":2}}],["矩阵是二维数组",{"5":{"7":2}}],["矩阵形式的链式法则在深度学习中尤为重要",{"5":{"5":1}}],["矩母函数",{"5":{"6":2}}],["矩",{"5":{"6":2}}],["当批大小受限于gpu内存时",{"5":{"7":2}}],["当批量太小时",{"5":{"6":2}}],["当批量大小足够大时",{"5":{"6":4}}],["当单个样本太大以至于无法在gpu内存中容纳时",{"5":{"7":2}}],["当维度匹配时",{"5":{"7":2}}],["当两个形状不同的张量进行逐元素运算时",{"5":{"7":2}}],["当样本量",{"5":{"6":1}}],["当样本量时",{"5":{"6":1}}],["当且仅当",{"5":{"6":1}}],["当且仅当和独立时互信息为0",{"5":{"6":1}}],["当",{"5":{"6":6,"7":1}}],["当我们比较多个模型或在多个测试集上评估时",{"5":{"6":2}}],["当我们计算批次中所有样本的前向传播时",{"5":{"7":2}}],["当我们计算验证集上的性能指标时",{"5":{"6":2}}],["当我们计算损失函数关于某个特定参数的偏导数时",{"5":{"5":2}}],["当独立同分布的随机变量样本量趋向无穷时",{"5":{"6":2}}],["当是真实分布而是模型预测分布时",{"5":{"6":1}}],["当使用自然对数时",{"5":{"6":2}}],["当使用以2为底的对数时",{"5":{"6":2}}],["当dropout率为",{"5":{"6":2}}],["当时",{"5":{"6":3,"7":1}}],["协方差矩阵在大语言模型中的应用包括",{"5":{"6":2}}],["协方差矩阵的对角线元素是各随机变量的方差",{"5":{"6":2}}],["协方差矩阵是多维随机变量的二阶中心矩描述",{"5":{"6":2}}],["协方差可以展开为",{"5":{"6":2}}],["协方差定义为",{"5":{"6":2}}],["协方差",{"5":{"6":2}}],["只在关键步骤使用全精度",{"5":{"7":2}}],["只适用于对称正定矩阵",{"5":{"7":2}}],["只需要训练",{"5":{"7":1}}],["只需要训练个参数而不是个参数",{"5":{"7":1}}],["只需进行转置操作",{"5":{"7":2}}],["只改变方向",{"5":{"7":2}}],["只有可对角化矩阵",{"5":{"7":2}}],["只有满秩",{"5":{"7":2}}],["只有方阵才可能存在逆矩阵",{"5":{"7":2}}],["只有当",{"5":{"6":1}}],["只有当和不相关时等号才成立",{"5":{"6":1}}],["只要样本量足够大",{"5":{"6":2}}],["只是需要明确哪个变量被当作中间变量",{"5":{"5":2}}],["根据eckart",{"5":{"7":2}}],["根据中心极限定理",{"5":{"6":2}}],["根据大数定律",{"5":{"6":4}}],["根据期望的线性性质",{"5":{"6":2}}],["根据取值方式的不同",{"5":{"6":2}}],["定义为所有元素平方和的平方根",{"5":{"7":2}}],["定义为​",{"5":{"6":1}}],["定义为",{"5":{"6":3}}],["vectorization",{"5":{"7":2}}],["value",{"5":{"7":2}}],["variational",{"5":{"6":2}}],["variance",{"5":{"6":2}}],["v0",{"5":{"5":4}}],["困惑度",{"5":{"6":2}}],["例如将一个",{"5":{"7":1}}],["例如将一个的二维张量重塑为的三维张量",{"5":{"7":1}}],["例如",{"5":{"6":4,"7":20}}],["下一个词的条件概率分布的期望特性",{"5":{"6":2}}],["下一个词的选择遵循某种概率分布",{"5":{"6":2}}],["因此单个层的计算量是相当可观的",{"5":{"7":2}}],["因此现代gpu都针对矩阵运算进行了专门的硬件优化",{"5":{"7":2}}],["因此深入掌握线性代数的基本概念和运算规则是理解模型工作原理的必要前提",{"5":{"7":2}}],["因此标准化操作是合理的",{"5":{"6":2}}],["因此最小化交叉熵等价于最小化kl散度",{"5":{"6":2}}],["因此共轭梯度法和拟牛顿法等方法在处理大规模优化问题时具有优势",{"5":{"5":2}}],["因为前向和后向替换的复杂度是",{"5":{"7":1}}],["因为前向和后向替换的复杂度是而不是高斯消元的",{"5":{"7":1}}],["因为梯度是线性的",{"5":{"7":2}}],["因为长序列可能消耗大量内存",{"5":{"7":2}}],["因为正规方程",{"5":{"7":1}}],["因为正规方程的解等价于求解",{"5":{"7":1}}],["因为正交矩阵的奇异值全部为1",{"5":{"7":2}}],["因为在数值上计算高阶多项式的根非常困难",{"5":{"7":2}}],["因为它可以将不同样本的计算分配到不同的处理单元上同时执行",{"5":{"7":2}}],["因为它将复杂的矩阵运算转化为标准的矩阵",{"5":{"7":2}}],["因为它决定了信息放大或衰减的上界",{"5":{"7":2}}],["因为它能够捕捉语义方向的相似性而不受词频等因素的影响",{"5":{"7":2}}],["因为它不满足对称性和三角不等式",{"5":{"6":2}}],["因为数值稳定性较差且计算成本高昂",{"5":{"7":2}}],["因为真实分布固定",{"5":{"6":2}}],["因为我们可以将期望操作与梯度操作交换次序",{"5":{"6":2}}],["位置",{"5":{"6":2}}],["都离不开这些基本概念",{"5":{"6":2}}],["离散程度和相互关系",{"5":{"6":2}}],["离散随机变量只能取有限或可数无穷个值",{"5":{"6":2}}],["uniform",{"5":{"6":2}}],["描述分布的尾部厚度",{"5":{"6":2}}],["描述分布的对称性",{"5":{"6":2}}],["描述词嵌入向量的分布特性",{"5":{"6":2}}],["描述了随机变量的",{"5":{"6":2}}],["描述独立事件发生的时间间隔",{"5":{"6":2}}],["描述",{"5":{"6":2}}],["描述单次二元试验的成功概率",{"5":{"6":2}}],["连续随机变量由概率密度函数",{"5":{"6":2}}],["连续随机变量可以取任意实数值或实数区间内的值",{"5":{"6":2}}],["模型并行化将大模型分布在多个计算设备上",{"5":{"7":2}}],["模型能够高效地并行处理大量数据",{"5":{"7":2}}],["模型a优于模型b",{"5":{"6":2}}],["模型为词汇表中的每个词分配一个概率",{"5":{"6":2}}],["模型参数量巨大",{"5":{"5":2}}],["二次型",{"5":{"6":2}}],["二阶张量",{"5":{"7":2}}],["二阶张量是矩阵",{"5":{"7":2}}],["二阶中心矩是方差",{"5":{"6":2}}],["二阶矩估计",{"5":{"5":2}}],["二项分布的共轭先验是beta分布",{"5":{"6":2}}],["二项分布的期望为",{"5":{"6":2}}],["二项分布在语言模型中的应用包括描述一批样本中被mask的词的数量",{"5":{"6":2}}],["二项分布",{"5":{"6":2}}],["或更低精度",{"5":{"7":2}}],["或行向量",{"5":{"7":2}}],["或其中一个维度为",{"5":{"7":2}}],["或",{"5":{"6":3,"7":2}}],["或和",{"5":{"6":2}}],["或平均值",{"5":{"6":2}}],["或证据",{"5":{"6":2}}],["或奈特",{"5":{"6":2}}],["或均匀分布",{"5":{"6":2}}],["或称单位高斯分布",{"5":{"6":2}}],["或经过dropout操作后某个神经元是否被激活等",{"5":{"6":2}}],["或等值线",{"5":{"5":2}}],["一组向量如果两两正交且都是单位向量",{"5":{"7":2}}],["一个",{"5":{"7":1}}],["一个张量网络由若干个节点",{"5":{"7":2}}],["一个线性变换",{"5":{"7":1}}],["一个线性变换可以用一个矩阵",{"5":{"7":1}}],["一个线性",{"5":{"7":2}}],["一个k维子空间是特征空间的一个k维线性子集",{"5":{"7":2}}],["一个n维特征空间可以理解为一个n维欧几里得空间",{"5":{"7":2}}],["一个阶张量有个索引",{"5":{"7":1}}],["一个300维的词向量可以看作是一个从自然语言到连续向量空间的映射",{"5":{"7":2}}],["一个很大的模型可能在某些指标上有统计显著的改进",{"5":{"6":2}}],["一个词是否被mask掉",{"5":{"6":2}}],["一维高斯分布的概率密度函数为",{"5":{"6":2}}],["一阶张量",{"5":{"7":2}}],["一阶张量是向量",{"5":{"7":2}}],["一阶原点矩是期望",{"5":{"6":2}}],["一阶矩估计",{"5":{"5":2}}],["一般情况下",{"5":{"6":2}}],["bug",{"5":{"7":2}}],["broadcasting",{"5":{"7":2}}],["b测试可以用于评估模型对用户交互的实际影响",{"5":{"6":2}}],["b测试的核心是随机分组和控制变量",{"5":{"6":2}}],["b测试是互联网公司和研究机构常用的实验方法",{"5":{"6":2}}],["batch",{"5":{"6":4,"7":6}}],["backprop",{"5":{"5":4}}],["bootstrap",{"5":{"6":2}}],["beta先验和dirichlet先验可用于建模词汇概率的不确定性",{"5":{"6":2}}],["bernoulli",{"5":{"6":2}}],["bias",{"5":{"7":2}}],["bit",{"5":{"6":2}}],["binomial",{"5":{"6":2}}],["伯努利分布可用于描述二元随机事件",{"5":{"6":2}}],["伯努利分布的期望为",{"5":{"6":2}}],["伯努利分布",{"5":{"6":2}}],["非对角线元素是变量之间的协方差",{"5":{"6":2}}],["非负性",{"5":{"6":3}}],["非负性对所有成立",{"5":{"6":1}}],["非凸性意味着梯度下降可能收敛到局部最小值而非全局最小值",{"5":{"5":2}}],["非凸优化是深度学习面临的现实挑战",{"5":{"5":2}}],["表示完全正相关",{"5":{"6":1}}],["表示完全负相关",{"5":{"6":2}}],["表示",{"5":{"6":1,"7":2}}],["表示线性无关",{"5":{"6":2}}],["表示生成该词的可能性",{"5":{"6":2}}],["表示取特定值的概率",{"5":{"6":1}}],["表示逐元素乘法",{"5":{"5":2}}],["fp32",{"5":{"7":2}}],["fp16",{"5":{"7":2}}],["frobenius范数因其可导性好而最常用",{"5":{"7":2}}],["frobenius范数常用于权重衰减",{"5":{"7":2}}],["frobenius范数是最直观的矩阵范数",{"5":{"7":2}}],["fine",{"5":{"7":2}}],["flow",{"5":{"7":4}}],["function",{"5":{"6":6}}],["fac基于层间独立性假设",{"5":{"5":2}}],["factored",{"5":{"5":2}}],["fac",{"5":{"5":2}}],["概率密度最大的点",{"5":{"6":2}}],["概率由pdf在区间上的积分给出",{"5":{"6":2}}],["概率质量函数",{"5":{"6":2}}],["概率论是研究随机现象规律性的数学分支",{"5":{"6":2}}],["概率论与统计<",{"5":{"4":1}}],["概率论与统计",{"0":{"6":1},"4":{"1":1,"6":1},"5":{"4":4,"6":1}}],["词嵌入的降维可视化",{"5":{"7":2}}],["词嵌入空间具有一些重要的几何性质",{"5":{"7":2}}],["词嵌入空间就是一个典型的高维特征空间",{"5":{"7":2}}],["词向量就是典型的向量表示",{"5":{"7":2}}],["词汇表上的概率分布就是典型的pmf",{"5":{"6":2}}],["词汇表中的词索引等",{"5":{"6":2}}],["词索引是离散的",{"5":{"6":2}}],["掷骰子的点数",{"5":{"6":2}}],["正则化",{"5":{"7":2}}],["正是因为它对任何矩阵都成立",{"5":{"7":2}}],["正向",{"5":{"6":1}}],["正向鼓励覆盖的所有模式",{"5":{"6":1}}],["正确答案",{"5":{"6":2}}],["正面或反面",{"5":{"6":2}}],["正交变换仍然是有价值的分析工具",{"5":{"7":2}}],["正交约束在优化中被用于防止权重矩阵的秩退化",{"5":{"7":2}}],["正交初始化将权重矩阵初始化为正交矩阵",{"5":{"7":2}}],["正交初始化是一种常用的权重初始化方法",{"5":{"7":2}}],["正交矩阵是满足",{"5":{"7":1}}],["正交矩阵是满足的方阵",{"5":{"7":1}}],["正交矩阵的行列式的绝对值为1",{"5":{"7":2}}],["正交矩阵的逆矩阵等于其转置矩阵",{"5":{"7":2}}],["正交矩阵保持向量的范数不变",{"5":{"7":2}}],["正交矩阵有几个重要的性质",{"5":{"7":2}}],["正交性有着多方面的应用",{"5":{"7":2}}],["正交性是线性代数中一个核心概念",{"5":{"7":2}}],["正交性与正交矩阵",{"2":{"7":1},"5":{"7":1}}],["正交",{"5":{"5":2}}],["理解这些并行策略的数学基础",{"5":{"7":2}}],["理解不同运算的资源需求对于优化模型设计和硬件利用至关重要",{"5":{"7":2}}],["理解张量运算的数学本质有助于编写高效的代码和诊断性能瓶颈",{"5":{"7":2}}],["理解它们的数学性质和计算特性对于优化模型性能和调试模型行为至关重要",{"5":{"7":2}}],["理解最大似然估计",{"5":{"6":2}}],["理解随机变量和概率分布的概念",{"5":{"6":2}}],["理解偏导数",{"5":{"5":2}}],["文本生成本质上是一个随机过程",{"5":{"6":2}}],["假设一个",{"5":{"7":1}}],["假设一个的权重矩阵被近似为​",{"5":{"7":1}}],["假设预训练模型的权重更新可以表示为低秩矩阵",{"5":{"7":2}}],["假设检验的基本框架包括原假设",{"5":{"6":2}}],["假设检验被用于比较不同模型的性能",{"5":{"6":2}}],["假设检验是统计推断的重要工具",{"5":{"6":2}}],["假设检验与模型评估",{"2":{"6":1},"5":{"6":1}}],["假设网络中有大量独立的噪声源",{"5":{"6":2}}],["假设这些数据独立同分布",{"5":{"6":2}}],["假设每层的参数可以独立地近似其hessian块",{"5":{"5":2}}],["7",{"2":{"6":1,"7":1},"5":{"6":1,"7":1}}],["6",{"2":{"6":1,"7":1},"5":{"6":1,"7":1}}],["5",{"2":{"6":1,"7":1},"5":{"6":1,"7":1}}],["4",{"2":{"6":1,"7":1},"5":{"6":1,"7":1}}],["方差接近1",{"5":{"6":2}}],["方差有限",{"5":{"6":1}}],["方差达到克拉美",{"5":{"6":2}}],["方差用于分析梯度的波动",{"5":{"6":2}}],["方差的性质包括",{"5":{"6":2}}],["方差的平方根称为标准差",{"5":{"6":2}}],["方差可以展开为",{"5":{"6":2}}],["方差",{"5":{"6":3}}],["方差和协方差是概率论中最重要的一阶和二阶统计量",{"5":{"6":2}}],["方差为的高斯分布",{"5":{"6":1}}],["方差为1的特殊高斯分布",{"5":{"6":2}}],["方差为​",{"5":{"6":2}}],["方差为",{"5":{"6":9}}],["方差与协方差",{"2":{"6":1},"5":{"6":1}}],["方向",{"5":{"5":4}}],["方向导数在梯度方向",{"5":{"5":2}}],["方向导数",{"5":{"5":2}}],["期望有限",{"5":{"6":2}}],["期望传播提供了有用的近似工具",{"5":{"6":2}}],["期望传播在某些情况下比变分推断更准确",{"5":{"6":2}}],["期望传播",{"5":{"6":2}}],["期望具有线性性质",{"5":{"6":2}}],["期望定义为",{"5":{"6":2}}],["期望",{"2":{"6":1},"5":{"6":6}}],["要求上述不等式在",{"5":{"5":1}}],["要求上述不等式在和时取严格小于号",{"5":{"5":1}}],["步的参数",{"5":{"5":1}}],["步长",{"5":{"5":2}}],["之间的夹角",{"5":{"5":1}}],["之间的差异",{"5":{"5":1}}],["沿",{"5":{"5":1}}],["形状相同的三阶张量",{"5":{"5":1}}],["接着计算",{"5":{"5":1}}],["接着计算​",{"5":{"5":1}}],["衡量差异的实际大小",{"5":{"6":2}}],["衡量两个随机变量之间的信息共享程度",{"5":{"6":2}}],["衡量两个随机变量之间的线性相关程度",{"5":{"6":2}}],["衡量随机变量取值的离散程度",{"5":{"6":2}}],["衡量输出",{"5":{"5":1}}],["衡量函数沿特定方向的变化率",{"5":{"5":2}}],["传导的",{"5":{"5":1}}],["传统的带动量sgd可能优于adam",{"5":{"5":2}}],["商的偏导数",{"5":{"5":1}}],["商的偏导数​​",{"5":{"5":1}}],["的计算量",{"5":{"7":1}}],["的计算就隐含了条件期望的概念",{"5":{"6":2}}],["的查询",{"5":{"7":1}}],["的三阶张量表示为",{"5":{"7":1}}],["的三维张量运算",{"5":{"7":1}}],["的三维张量",{"5":{"7":3}}],["的列向量",{"5":{"7":1}}],["的列向量称为右奇异向量",{"5":{"7":2}}],["的列向量称为左奇异向量",{"5":{"7":2}}],["的大矩阵",{"5":{"7":1}}],["的解等价于求解",{"5":{"7":1}}],["的线性变换中最大的拉伸因子",{"5":{"7":1}}],["的权重矩阵被近似为",{"5":{"7":1}}],["的最佳近似",{"5":{"7":1}}],["的最大特征值的平方根",{"5":{"7":1}}],["的特征值的平方根",{"5":{"7":1}}],["的特征值决定",{"5":{"7":1}}],["的特征向量",{"5":{"7":1}}],["的稳定性由矩阵",{"5":{"7":1}}],["的稳定性由矩阵的特征值决定",{"5":{"7":1}}],["的多项式方程",{"5":{"7":1}}],["的二维张量",{"5":{"7":1}}],["的二维张量重塑为",{"5":{"7":1}}],["的二阶泰勒展开为",{"5":{"5":1}}],["的二阶偏导数连续",{"5":{"5":1}}],["的张量",{"5":{"7":2}}],["的乘积",{"5":{"7":3}}],["的乘法可以看作是收缩掉两个张量的一个公共维度",{"5":{"7":2}}],["的标准",{"5":{"7":2}}],["的对角线元素",{"5":{"7":2}}],["的对称半正定矩阵",{"5":{"6":1}}],["的信息",{"5":{"7":2}}],["的概念推广到任意维度",{"5":{"7":2}}],["的概率为1",{"5":{"6":1}}],["的概率为",{"5":{"6":1}}],["的概率",{"5":{"6":3}}],["的方阵",{"5":{"7":1}}],["的方阵才是可逆的",{"5":{"7":2}}],["的方向导数为",{"5":{"5":1}}],["的运算",{"5":{"7":2}}],["的分布趋向于标准正态分布",{"5":{"6":1}}],["的支持集",{"5":{"6":1}}],["的支持集必须包含",{"5":{"6":1}}],["的所有模式",{"5":{"6":1}}],["的梯度为",{"5":{"6":1}}],["的梯度具有简洁的形式",{"5":{"6":2}}],["的样本时所需要的额外信息量",{"5":{"6":1}}],["的高斯噪声",{"5":{"6":1}}],["的高斯分布",{"5":{"6":1}}],["的条件分布仍然是高斯分布",{"5":{"6":1}}],["的函数",{"5":{"6":1}}],["的pdf为",{"5":{"6":1}}],["的语言模型",{"5":{"6":1}}],["的效果",{"5":{"6":2}}],["的渐近行为",{"5":{"6":2}}],["的先验分布为",{"5":{"6":2}}],["的one",{"5":{"6":2}}],["的定义",{"5":{"6":2}}],["的一次试验版本",{"5":{"6":2}}],["的单调性来保证收敛性",{"5":{"5":1}}],["的速度收敛",{"5":{"5":1}}],["的等值面上",{"5":{"5":1}}],["的矩阵组合成一个",{"5":{"7":1}}],["的矩阵与一个",{"5":{"7":1}}],["的矩阵",{"5":{"5":1,"7":3}}],["的偏导数的乘积之和",{"5":{"5":1}}],["的偏导数与它对",{"5":{"5":1}}],["的偏导数为",{"5":{"5":1}}],["的总影响是通过所有中间变量",{"5":{"5":1}}],["的平面相交得到的一元函数曲线的切线斜率",{"5":{"5":1}}],["的在线阅读版本",{"5":{"4":2}}],["关于",{"5":{"5":1}}],["关于变量",{"5":{"5":1}}],["关于为什么深度神经网络能够有效优化",{"5":{"5":2}}],["穿行",{"5":{"5":2}}],["允许梯度下降在不同局部最小值之间",{"5":{"5":2}}],["此外",{"5":{"5":2,"6":2,"7":2}}],["深度学习框架也提供了高效的函数来计算矩阵的特征值分解和奇异值分解",{"5":{"7":2}}],["深度学习框架会在不显式复制数据的情况下",{"5":{"7":2}}],["深度网络往往收敛到具有相似损失值的局部最小值",{"5":{"5":2}}],["深度神经网络的损失函数通常是非凸的",{"5":{"5":2}}],["研究发现",{"5":{"5":2}}],["研究表明",{"5":{"5":2}}],["局部最小值的等价性是大规模深度学习中的一个有趣现象",{"5":{"5":2}}],["鞍点比局部最小值更为常见",{"5":{"5":2}}],["鞍点逃离是大规模非凸优化中的一个重要问题",{"5":{"5":2}}],["鞍点和平坦区域",{"5":{"5":2}}],["性质",{"5":{"5":2}}],["伪凸",{"5":{"5":2}}],["包含所有可以通过原点的k维平面上的点",{"5":{"7":2}}],["包含了关于所有自变量的二阶偏导数信息",{"5":{"5":2}}],["包含了函数关于所有自变量的偏导数信息",{"5":{"5":2}}],["包括矩阵乘法",{"5":{"7":2}}],["包括结合律",{"5":{"7":2}}],["包括gram",{"5":{"7":2}}],["包括参数估计和假设检验两大类方法",{"5":{"6":2}}],["包括损失函数的",{"5":{"5":2}}],["然而",{"5":{"5":2,"6":4}}],["然后检验这些差异的均值是否显著不为零",{"5":{"6":2}}],["然后使用累加的梯度进行一次参数更新",{"5":{"7":2}}],["然后使用这些估计的分布来表征原始统计量的不确定性",{"5":{"6":2}}],["然后使用链式法则组合这些梯度",{"5":{"5":2}}],["然后通过最小化近似分布与真实后验分布之间的kl散度来找到最好的近似",{"5":{"6":2}}],["然后再进行衰减",{"5":{"5":2}}],["然后再衰减",{"5":{"5":2}}],["然后应用链式法则累加梯度",{"5":{"5":2}}],["然后逐层向后传递",{"5":{"5":2}}],["然后根据梯度方向更新参数",{"5":{"5":2}}],["限制梯度的范数以防止梯度爆炸",{"5":{"5":2}}],["避免在随机初始化阶段受到过大梯度的干扰",{"5":{"5":2}}],["但奇异值总是非负的",{"5":{"7":2}}],["但又有所不同",{"5":{"7":2}}],["但又足够高以捕获丰富的语义信息",{"5":{"7":2}}],["但由于广播可能在语法上合法而在语义上错误",{"5":{"7":2}}],["但矩阵求逆的数学思想",{"5":{"7":2}}],["但不满足交换律",{"5":{"7":2}}],["但改进幅度可能很小以至于在实际应用中可以忽略",{"5":{"6":2}}],["但在深度学习中使用较少",{"5":{"7":2}}],["但在深度学习中的直接应用较少",{"5":{"6":2}}],["但在处理序列数据的某些场景下",{"5":{"7":2}}],["但在语言模型中更常用的是分类分布",{"5":{"6":2}}],["但仍需要保持某些计算的精度",{"5":{"5":2}}],["但最近的研究表明",{"5":{"5":2}}],["以保持数值稳定性",{"5":{"7":2}}],["以匹配较大的张量",{"5":{"7":2}}],["以transformer模型为例",{"5":{"7":2}}],["以捕获更复杂的多模态特性",{"5":{"6":2}}],["以确保信号在各层之间平稳传播",{"5":{"6":2}}],["以及评估词嵌入中捕获的语义信息量",{"5":{"6":2}}],["以及评估模型对不同类型输入的响应一致性",{"5":{"6":2}}],["以及在变分自编码器中定义潜在空间的先验分布",{"5":{"6":2}}],["以及在概率图模型中作为共轭先验使用",{"5":{"6":2}}],["以及模型输出的不确定性估计",{"5":{"6":2}}],["以及某些正则化技术的理论基础",{"5":{"6":2}}],["以及归一性",{"5":{"6":2}}],["以及局部最小值通常具有相似的损失值等",{"5":{"5":2}}],["以计算换内存",{"5":{"5":2}}],["以此类推直到输入层",{"5":{"5":2}}],["数学上",{"5":{"7":6}}],["数学基础",{"4":{"0":1,"1":1,"2":1,"5":1,"6":1,"7":1},"5":{"4":15}}],["数十亿甚至万亿级",{"5":{"5":2}}],["尽管adam在实践中表现优异",{"5":{"5":2}}],["efficient",{"5":{"7":2}}],["effect",{"5":{"6":2}}],["entropy",{"5":{"6":8}}],["expectation",{"5":{"6":4}}],["exponential",{"5":{"5":2,"6":2}}],["estimation",{"5":{"5":2,"6":2}}],["并将计算速度提高近一倍",{"5":{"7":2}}],["并有效缓解梯度消失问题",{"5":{"7":2}}],["并相应地调整学习率",{"5":{"5":2}}],["并且计算成本更低",{"5":{"5":2}}],["自注意力机制需要计算查询向量与键向量的相似度",{"5":{"7":2}}],["自助法可用于估计困惑度",{"5":{"6":2}}],["自助法",{"5":{"6":2}}],["自适应学习率方法为每个参数单独调整学习率",{"5":{"5":2}}],["自动微分结合了数值计算的效率和符号计算的精确性",{"5":{"5":2}}],["自动微分将函数分解为基本操作的序列",{"5":{"5":2}}],["自动微分",{"5":{"5":2}}],["network",{"5":{"7":2}}],["nesterov动量是动量方法的一个变体",{"5":{"5":2}}],["nuclear",{"5":{"7":2}}],["null",{"5":{"7":2}}],["numpy",{"5":{"7":2}}],["numbers",{"5":{"6":2}}],["nat",{"5":{"6":2}}],["norm",{"5":{"7":6}}],["normalization",{"5":{"6":2,"7":4}}],["normal",{"5":{"6":2}}],["nofollow",{"5":{"4":3}}],["noopener",{"5":{"4":3}}],["​附近",{"5":{"5":1}}],["​​",{"5":{"5":1,"7":2}}],["​",{"5":{"5":8,"6":12,"7":3}}],["动量更新的公式为",{"5":{"5":2}}],["动量方法和噪声梯度有助于算法逃离鞍点",{"5":{"5":2}}],["动量方法可以看作是对梯度进行指数移动平均",{"5":{"5":2}}],["动量方法",{"5":{"5":2}}],["micro",{"5":{"7":2}}],["mirsky定理",{"5":{"7":2}}],["mobilenet中的深度可分离卷积可以用kronecker积来表示",{"5":{"7":2}}],["moment",{"5":{"5":2,"6":4}}],["momentum",{"5":{"5":2}}],["mle是相合的",{"5":{"6":2}}],["mle是训练的标准方法",{"5":{"6":2}}],["mle",{"5":{"6":2}}],["msg",{"5":{"6":4}}],["mutual",{"5":{"6":2}}],["multinomial",{"5":{"6":2}}],["mgf",{"5":{"6":2}}],["maximum",{"5":{"6":2}}],["mahalanobis",{"5":{"6":2}}],["mass",{"5":{"6":2}}],["manimce",{"5":{"5":4}}],["matrix",{"5":{"5":2,"7":2}}],["目标函数值与最优值的差距为",{"5":{"5":2}}],["可以看作是查询张量和键张量的收缩",{"5":{"7":1}}],["可以用一个矩阵",{"5":{"7":1}}],["可以用于表示高阶交互关系",{"5":{"7":2}}],["可以将批次进一步划分为更小的微批次",{"5":{"7":2}}],["可以一次性完成所有样本的计算",{"5":{"7":2}}],["可以显著减少不必要的内存分配和数据拷贝",{"5":{"7":2}}],["可以显著减少参数量和计算量",{"5":{"7":2}}],["可以在保持模型性能的同时显著减少参数量和计算开销",{"5":{"7":2}}],["可以理解为一维数组中的有序数集",{"5":{"7":2}}],["可以得到p值",{"5":{"6":2}}],["可以得到各阶矩",{"5":{"6":2}}],["可能导致训练过程不稳定或收敛较慢",{"5":{"6":2}}],["可微函数是凸函数的充要条件是其梯度单调不减",{"5":{"5":2}}],["可达数十亿",{"5":{"5":2}}],["有效批大小",{"5":{"7":2}}],["有效自由度",{"5":{"7":2}}],["有限",{"5":{"6":3}}],["有助于逃离平坦区域",{"5":{"5":2}}],["有多种理论解释",{"5":{"5":2}}],["有",{"5":{"5":2,"6":5}}],["凸性的充要条件是hessian矩阵半正定",{"5":{"5":2}}],["凸函数是一类特殊的函数",{"5":{"5":2}}],["凸优化是研究凸函数在凸集上最小化问题的数学分支",{"5":{"5":2}}],["常用于矩阵补全",{"5":{"7":2}}],["常用的矩阵范数包括frobenius范数",{"5":{"7":2}}],["常用的效应量包括cohen",{"5":{"6":2}}],["常见的张量分解方法包括",{"5":{"7":2}}],["常见的学习率调度方法包括",{"5":{"5":2}}],["常数因子的偏导数等于该因子乘以函数的偏导数",{"5":{"5":2}}],["常数的偏导数为零",{"5":{"5":2}}],["lu分解将矩阵分解为下三角矩阵",{"5":{"7":1}}],["lu分解将矩阵分解为下三角矩阵和上三角矩阵的乘积",{"5":{"7":1}}],["lu分解的主要用途是高效求解线性方程组",{"5":{"7":2}}],["low",{"5":{"7":2}}],["logits",{"5":{"6":4}}],["lstm和gru通过门控机制设计",{"5":{"7":2}}],["llm",{"5":{"7":4}}],["large",{"5":{"6":2}}],["law",{"5":{"6":2}}],["label",{"5":{"6":2}}],["limit",{"5":{"6":2}}],["likelihood",{"5":{"6":2}}],["link",{"5":{"4":3}}],["length",{"5":{"7":2}}],["leibler",{"5":{"6":2}}],["learning",{"5":{"5":2}}],["学习率调整和梯度裁剪中都有应用",{"5":{"7":2}}],["学习率调度",{"5":{"5":2}}],["学习率预热",{"5":{"5":2}}],["学习率按余弦曲线从初始值逐渐减小到零",{"5":{"5":2}}],["学习率按指数函数递减",{"5":{"5":2}}],["学习率太大可能导致跳过最优解甚至发散",{"5":{"5":2}}],["学习率太小会导致收敛缓慢",{"5":{"5":2}}],["学习率是一个关键的超参数",{"5":{"5":2}}],["拉普拉斯近似用于构建后验分布的高斯代理",{"5":{"5":2}}],["拉普拉斯近似利用hessian矩阵在峰值附近对概率分布进行高斯近似",{"5":{"5":2}}],["通过仔细规划张量的内存布局和运算顺序",{"5":{"7":2}}],["通过将大型权重张量分解为张量网络的形式",{"5":{"7":2}}],["通过将大型权重张量分解为若干低秩张量",{"5":{"7":2}}],["通过向量化操作可以将其转化为线性方程",{"5":{"7":2}}],["通过惩罚大权重来防止过拟合",{"5":{"7":2}}],["通过gram",{"5":{"7":2}}],["通过分析权重矩阵的奇异值分布",{"5":{"7":2}}],["通过保留最大的k个特征值对应的特征向量",{"5":{"7":2}}],["通过保持",{"5":{"5":1}}],["通过保持的单调性来保证收敛性",{"5":{"5":1}}],["通过逆运算解线性方程组",{"5":{"7":2}}],["通过张量运算",{"5":{"7":2}}],["通过对矩母函数求导并在",{"5":{"6":1}}],["通过对矩母函数求导并在处求值",{"5":{"6":1}}],["通过计算真实排列的统计量在置换分布中的位置",{"5":{"6":2}}],["通过边际似然",{"5":{"6":2}}],["通过编码器学习将输入映射到该分布的参数",{"5":{"6":2}}],["通过引入一个校正因子来修复adam在训练早期的方差问题",{"5":{"5":2}}],["通过hessian矩阵可以判断驻点的性质",{"5":{"5":2}}],["通常在数千量级",{"5":{"7":1}}],["通常不等于",{"5":{"7":1}}],["通常使用默认值",{"5":{"5":1}}],["通常能提供更好的收敛性质",{"5":{"5":2}}],["通常是正定的",{"5":{"5":2}}],["与随机高斯初始化或xavier初始化不同",{"5":{"7":2}}],["与向量的大小无关",{"5":{"7":2}}],["与pmf不同",{"5":{"6":2}}],["与目标",{"5":{"5":1}}],["与凸优化问题不同",{"5":{"5":2}}],["与完整的hessian相比",{"5":{"5":2}}],["与数值微分",{"5":{"5":2}}],["牛顿矩阵省略了二阶导数项",{"5":{"5":2}}],["牛顿矩阵为",{"5":{"5":2}}],["牛顿矩阵是hessian的一个常用近似",{"5":{"5":2}}],["高斯过程的一个优势是它提供了预测的不确定性估计",{"5":{"6":2}}],["高斯过程回归在超参数优化",{"5":{"6":2}}],["高斯过程是函数的先验分布",{"5":{"6":2}}],["高斯输出分布用于回归任务",{"5":{"6":2}}],["高斯分布通常作为潜在空间的先验分布",{"5":{"6":2}}],["高斯分布可用于建模响应的不确定性",{"5":{"6":2}}],["高斯分布扮演着多种重要角色",{"5":{"6":2}}],["高斯分布具有最大的熵",{"5":{"6":2}}],["高斯分布的共轭先验还是高斯分布",{"5":{"6":2}}],["高斯分布的熵",{"5":{"6":2}}],["高斯分布的偏度为0",{"5":{"6":2}}],["高斯分布的众数",{"5":{"6":2}}],["高斯分布的期望为",{"5":{"6":2}}],["高斯分布在卷积",{"5":{"6":2}}],["高斯分布之所以如此重要",{"5":{"6":2}}],["高斯分布",{"5":{"6":2}}],["高斯分布与多元高斯",{"2":{"6":1},"5":{"6":1}}],["高斯",{"5":{"5":6}}],["kullback",{"5":{"6":2}}],["kurtosis",{"5":{"6":2}}],["kl散度可以理解为使用分布",{"5":{"6":1}}],["kl散度可以理解为使用分布来编码来自分布的样本时所需要的额外信息量",{"5":{"6":1}}],["kl散度在不同场景下的不同形式反映了其灵活性",{"5":{"6":2}}],["kl散度用于衡量学生模型和教师模型输出分布之间的差异",{"5":{"6":2}}],["kl散度用于限制策略更新的幅度",{"5":{"6":2}}],["kl散度有多种重要应用",{"5":{"6":2}}],["kl散度的定义为",{"5":{"6":2}}],["kl散度不对称",{"5":{"6":2}}],["kl散度是非负的",{"5":{"6":2}}],["kl散度定义为",{"5":{"6":2}}],["kl散度衡量两个概率分布之间的",{"5":{"6":2}}],["kl散度",{"5":{"6":2}}],["kl散度与交叉熵",{"2":{"6":1},"5":{"6":1}}],["k",{"5":{"5":3}}],["kronecker积可以帮助我们将大矩阵运算分解为多个小矩阵运算的组合",{"5":{"7":2}}],["kronecker积的主要应用包括权重共享",{"5":{"7":2}}],["kronecker积具有许多有用的代数性质",{"5":{"7":2}}],["kronecker积是两个矩阵之间的特殊二元运算",{"5":{"7":2}}],["kronecker积与向量化",{"2":{"7":1},"5":{"7":1}}],["kronecker",{"5":{"5":2}}],["从张量的角度来看",{"5":{"7":2}}],["从张量网络的角度来看",{"5":{"7":2}}],["从矩阵乘法的并行计算到张量运算的维度变换",{"5":{"7":2}}],["从transformer架构的注意力机制到词嵌入的向量表示",{"5":{"7":2}}],["从中心极限定理的角度看",{"5":{"6":2}}],["从kl散度的角度看",{"5":{"6":2}}],["从优化的角度看",{"5":{"6":2}}],["从公式可以看出",{"5":{"6":2}}],["从数学上看",{"5":{"6":2}}],["从应用上看",{"5":{"6":2}}],["从理论上看",{"5":{"6":2}}],["从梯度计算的期望到模型不确定性的估计",{"5":{"6":2}}],["从损失函数的定义到模型性能的评价",{"5":{"6":2}}],["从很小的值线性增长到目标学习率",{"5":{"5":2}}],["从而实现高效的存储和计算",{"5":{"7":2}}],["从而在多个计算设备上分布式执行",{"5":{"7":2}}],["从而在反向传播时为梯度提供了一条恒等映射路径",{"5":{"7":2}}],["从而约束了网络函数的lipschitz常数",{"5":{"7":2}}],["从而缓解了梯度消失问题",{"5":{"7":2}}],["从而使得机器学习算法能够在这些向量上进行运算和优化",{"5":{"7":2}}],["从而加速收敛并减少震荡",{"5":{"5":2}}],["从而大幅降低计算和存储成本",{"5":{"5":2}}],["从几何角度来看",{"5":{"5":2,"7":2}}],["而数据并行",{"5":{"7":2}}],["而奇异值分解适用于任意矩阵",{"5":{"7":2}}],["而零空间则包含了可能被",{"5":{"7":2}}],["而是取连续的实数值",{"5":{"7":2}}],["而正规方程的求解等价于计算矩阵的伪逆",{"5":{"7":2}}],["而且并非所有方阵都有逆矩阵",{"5":{"7":2}}],["而反向",{"5":{"6":1}}],["而反向则鼓励找到一个主要的模式并紧密地拟合它",{"5":{"6":1}}],["而强大数定律保证样本均值几乎必然收敛于期望",{"5":{"6":2}}],["而不是高斯消元的",{"5":{"7":1}}],["而不是最小化全局kl散度",{"5":{"6":2}}],["而不是点估计",{"5":{"6":2}}],["而不显式地计算和存储完整的hessian矩阵",{"5":{"5":2}}],["而真实分布是",{"5":{"6":2}}],["而连续空间中的嵌入表示和隐藏状态则是连续的",{"5":{"6":2}}],["而",{"5":{"5":1,"7":2}}],["而​",{"5":{"5":1}}],["陡峭",{"5":{"5":2}}],["多重比较谬误",{"5":{"6":2}}],["多重比较问题是在进行多个假设检验时需要考虑的重要问题",{"5":{"6":2}}],["多项分布的共轭先验是dirichlet分布",{"5":{"6":2}}],["多元高斯分布具有许多重要的性质",{"5":{"6":2}}],["多元高斯分布的pdf为",{"5":{"6":2}}],["多元高斯分布是一维高斯分布向多维空间的推广",{"5":{"6":2}}],["多元函数梯度与hessian",{"2":{"5":1},"5":{"5":1}}],["多数特征值很小",{"5":{"5":2}}],["平坦",{"5":{"5":2,"6":2}}],["少数几个特征值很大",{"5":{"5":2}}],["凹凸程度",{"5":{"5":2}}],["函数",{"5":{"5":2}}],["函数是凸函数的充要条件是对于任意和任意",{"5":{"5":1}}],["函数的二阶泰勒展开为",{"5":{"5":1}}],["函数值随某一特定变量变化的速率",{"5":{"5":2}}],["影响局部优化的收敛速度",{"5":{"5":2}}],["所有特征值大于零",{"5":{"5":2}}],["首先计算",{"5":{"5":1}}],["首先计算​",{"5":{"5":1}}],["首先",{"5":{"5":4,"6":2,"7":10}}],["上为零",{"5":{"5":2}}],["上取得最小值",{"5":{"5":2}}],["上取得最大值",{"5":{"5":2}}],["负的最大值",{"5":{"5":2}}],["负梯度方向则是函数下降最快的方向",{"5":{"5":2}}],["极大值或鞍点",{"5":{"5":2}}],["即对所有向量成立",{"5":{"7":1}}],["即通常不等于",{"5":{"7":1}}],["即在真实类别上赋予",{"5":{"6":1}}],["即在真实类别上赋予的概率",{"5":{"6":1}}],["即给定前文的条件下预测下一个词的对数概率之和",{"5":{"6":2}}],["即的支持集必须包含的支持集",{"5":{"6":1}}],["即一般情况下",{"5":{"6":2}}],["即最大的不确定性",{"5":{"6":2}}],["即存在",{"5":{"5":1}}],["即存在使得",{"5":{"5":1}}],["即经过",{"5":{"5":1}}],["即经过次迭代后",{"5":{"5":1}}],["即使所有原假设都为真",{"5":{"6":2}}],["即使很大的差异也可能不显著",{"5":{"6":2}}],["即使是微小的差异也可能是统计显著的",{"5":{"6":2}}],["即使这些最小值在参数空间中相距甚远",{"5":{"5":2}}],["即使不能保证达到全局最优",{"5":{"5":2}}],["即未中心化的方差",{"5":{"5":2}}],["即动量",{"5":{"5":2}}],["即误差按几何级数衰减",{"5":{"5":2}}],["即​",{"5":{"5":1}}],["即",{"5":{"5":5,"6":3,"7":10}}],["pipeline",{"5":{"7":2}}],["partial",{"5":{"7":2}}],["parallelism",{"5":{"7":6}}],["parameter",{"5":{"7":2}}],["parafac",{"5":{"7":2}}],["peft",{"5":{"7":2}}],["permutation",{"5":{"6":2}}],["perplexity",{"5":{"6":2}}],["pca",{"5":{"7":6}}],["power",{"5":{"7":2}}],["point",{"5":{"5":2}}],["pytorch",{"5":{"7":2}}],["p值受样本量影响很大",{"5":{"6":2}}],["p值",{"5":{"6":2}}],["processing",{"5":{"7":2}}],["propagation",{"5":{"6":2}}],["probability",{"5":{"6":4}}],["ppo",{"5":{"6":2}}],["pdf同样满足非负性",{"5":{"6":1}}],["pdf同样满足非负性和归一性",{"5":{"6":1}}],["pdf在单点的取值不代表概率",{"5":{"6":2}}],["pdf",{"5":{"6":2}}],["pmf必须满足两个基本性质",{"5":{"6":2}}],["pmf",{"5":{"6":2}}],["png",{"5":{"5":12,"6":12,"7":8}}],["驻点",{"5":{"5":2}}],["反幂迭代",{"5":{"7":2}}],["反映了增长的速率",{"5":{"5":2}}],["反向传播算法沿着计算图反向遍历",{"5":{"5":2}}],["反向传播从输出层开始",{"5":{"5":2}}],["范数​",{"5":{"7":1}}],["范数分别是列范数和行范数",{"5":{"7":2}}],["范数和∞",{"5":{"7":2}}],["范数",{"5":{"5":2,"7":5}}],["其列向量",{"5":{"7":2}}],["其维度分别代表批量大小",{"5":{"7":2}}],["其协方差矩阵",{"5":{"6":1}}],["其协方差矩阵是一个的对称半正定矩阵",{"5":{"6":1}}],["其他为0",{"5":{"6":2}}],["其pdf为",{"5":{"6":2}}],["其pmf记为",{"5":{"6":2}}],["其期望定义为",{"5":{"6":2}}],["其元素表示生成词汇表中每个词的概率",{"5":{"6":2}}],["其上方的图构成一个凸集",{"5":{"5":2}}],["其次",{"5":{"5":4,"6":2,"7":10}}],["其长度",{"5":{"5":2}}],["其中查询向量与键向量在特征维度上进行点积运算",{"5":{"7":2}}],["其中第",{"5":{"6":1}}],["其中第个元素为",{"5":{"6":1}}],["其中条件均值为",{"5":{"6":2}}],["其中表示完全正相关",{"5":{"6":1}}],["其中且",{"5":{"6":1}}],["其中",{"5":{"5":9,"6":15,"7":15}}],["其中是序列长度",{"5":{"7":1}}],["其中是批大小",{"5":{"7":1}}],["其中是批量大小",{"5":{"7":1}}],["其中是注意力头的数量",{"5":{"7":1}}],["其中是单位矩阵",{"5":{"7":1}}],["其中是标准正态分布的cdf",{"5":{"6":1}}],["其中是似然函数",{"5":{"6":1}}],["其中是未知参数",{"5":{"6":1}}],["其中是输入神经元的数量",{"5":{"6":1}}],["其中是均值向量",{"5":{"6":1}}],["其中是均值参数",{"5":{"6":1}}],["其中是率参数",{"5":{"6":1}}],["其中是成功概率",{"5":{"6":1}}],["其中是衰减系数",{"5":{"5":1}}],["其中是逐元素累积的梯度平方",{"5":{"5":1}}],["其中是速度向量",{"5":{"5":1}}],["其中是第步的参数",{"5":{"5":1}}],["其中是残差函数的雅可比矩阵",{"5":{"5":1}}],["其中是梯度与方向之间的夹角",{"5":{"5":1}}],["其中是外层函数关于中间变量的导数",{"5":{"5":1}}],["其中节点表示变量或操作",{"5":{"5":2}}],["其中​",{"5":{"5":1,"6":1}}],["其中和是激活函数",{"5":{"5":1}}],["其中每个词被映射为空间中的一个点",{"5":{"7":2}}],["其中每个元素",{"5":{"7":2}}],["其中每个",{"5":{"5":2}}],["使梯度能够绕过复杂的非线性变换直接传播到较浅层",{"5":{"7":2}}],["使其均值接近0",{"5":{"6":2}}],["使学生模型能够学习教师模型的知识",{"5":{"6":2}}],["使得等效的变换矩阵具有接近1的特征值",{"5":{"7":2}}],["使得模型能够生成多样化的回复",{"5":{"6":2}}],["使得模型在训练数据上的损失函数值最小化",{"5":{"5":2}}],["使得不同特征方向捕捉不同的信息",{"5":{"7":2}}],["使得不同变量对之间的相关性可以直接比较",{"5":{"6":2}}],["使得不同的参数化方式可以产生相同的函数",{"5":{"5":2}}],["使得生成的文本既流畅又符合语义逻辑",{"5":{"6":2}}],["使得",{"5":{"5":1,"7":3}}],["使得算法能够适应非平稳的目标函数",{"5":{"5":2}}],["使得更新方向更加平滑",{"5":{"5":2}}],["使得预测具有不确定性估计",{"5":{"5":2}}],["使得整体损失函数朝着下降的方向移动",{"5":{"5":2}}],["使用",{"5":{"5":1}}],["使用和",{"5":{"5":1}}],["使用指数移动平均来累积梯度平方",{"5":{"5":2}}],["使用代数规则显式推导导数公式",{"5":{"5":2}}],["使用有限差分近似导数",{"5":{"5":2}}],["每一层的输入和输出都需要被保存用于反向传播",{"5":{"7":2}}],["每种分解都有其特定的应用场景和数值性质",{"5":{"7":2}}],["每种操作都有预定义的梯度计算规则",{"5":{"5":2}}],["每次只使用一小批样本来估计梯度",{"5":{"6":2}}],["每经过若干个epoch将学习率降低一个因子",{"5":{"5":2}}],["每个微批次独立计算",{"5":{"7":2}}],["每个边代表张量的一个维度",{"5":{"7":2}}],["每个节点是一个张量",{"5":{"7":2}}],["每个秩一矩阵对应一个主成分方向",{"5":{"7":2}}],["每个索引对应张量的一个维度",{"5":{"7":2}}],["每个词被映射为一个高维实数向量",{"5":{"7":2}}],["每个中间变量的贡献是它对",{"5":{"5":1}}],["每个中间变量的贡献是它对的偏导数与它对的偏导数的乘积之和",{"5":{"5":1}}],["每个参数分量根据其在梯度中的值进行调整",{"5":{"5":2}}],["和注意力维度",{"5":{"7":1}}],["和上三角矩阵",{"5":{"7":2}}],["和标量",{"5":{"7":1}}],["和分层张量分解",{"5":{"7":2}}],["和分配律",{"5":{"7":2}}],["和连接节点的边",{"5":{"7":2}}],["和低秩矩阵恢复问题",{"5":{"7":2}}],["和对角矩阵",{"5":{"7":2}}],["和防止对抗攻击都很重要",{"5":{"7":2}}],["和transformer中",{"5":{"7":2}}],["和qr算法来计算特征值和特征向量",{"5":{"7":2}}],["和张量收缩",{"5":{"7":2}}],["和矩阵",{"5":{"7":2}}],["和随机变量",{"5":{"6":1}}],["和归一性",{"5":{"6":1}}],["和归一化操作",{"5":{"5":2}}],["和相关系数",{"5":{"6":2}}],["和备择假设",{"5":{"6":2}}],["和渐近有效的",{"5":{"6":2}}],["和交叉熵",{"5":{"6":2}}],["和向量",{"5":{"6":2}}],["和中位数都等于均值",{"5":{"6":2}}],["和线性无关",{"5":{"6":1}}],["和倾向于反向变化",{"5":{"6":1}}],["和倾向于同向变化",{"5":{"6":1}}],["和任意",{"5":{"5":1}}],["和",{"5":{"5":6,"6":10,"7":12}}],["和符号微分",{"5":{"5":2}}],["和的偏导数等于偏导数的和",{"5":{"5":2}}],["如何将矩阵运算分解为可以在不同设备上独立执行的子运算",{"5":{"7":2}}],["如bf16",{"5":{"7":2}}],["如bonferroni校正或false",{"5":{"6":2}}],["如线性注意力",{"5":{"7":2}}],["如t",{"5":{"7":2}}],["如pytorch和tensorflow中",{"5":{"7":2}}],["如用户满意度",{"5":{"6":2}}],["如两个模型或两种策略",{"5":{"6":2}}],["如0",{"5":{"6":2}}],["如",{"5":{"6":4,"7":2}}],["如随机初始化",{"5":{"6":2}}],["如基于贝叶斯方法的超参数优化",{"5":{"6":2}}],["如高斯分布",{"5":{"6":2}}],["如结合vae的表示学习",{"5":{"6":2}}],["如分析神经网络输出的分布特性",{"5":{"6":2}}],["如判断一个词是否属于某个类别",{"5":{"6":2}}],["如词语的嵌入向量",{"5":{"6":2}}],["如抛硬币的结果",{"5":{"6":2}}],["如神经元排列的不变性",{"5":{"5":2}}],["如relu激活函数",{"5":{"7":1}}],["如relu激活函数就是典型的逐元素运算",{"5":{"7":1}}],["如relu",{"5":{"5":2}}],["如果满足",{"5":{"7":1}}],["如果存在非零向量",{"5":{"7":1}}],["如果存在非零向量和标量满足",{"5":{"7":1}}],["如果使用张量运算而非循环",{"5":{"7":2}}],["如果进行行交换",{"5":{"7":2}}],["如果主元都不为零",{"5":{"7":2}}],["如果奇异值分布比较均匀",{"5":{"7":2}}],["如果只有少数几个奇异值很大",{"5":{"7":2}}],["如果只改变这个参数而保持其他参数不变",{"5":{"5":2}}],["如果有特征值的实部为正",{"5":{"7":2}}],["如果所有特征值的实部都为负",{"5":{"7":2}}],["如果对应维度相等",{"5":{"7":2}}],["如果同时检验多个假设",{"5":{"6":2}}],["如果我们使用相同的测试集评估两个模型",{"5":{"6":2}}],["如果p值小于预先设定的显著性水平",{"5":{"6":2}}],["如果函数",{"5":{"5":1}}],["如果函数的二阶偏导数连续",{"5":{"5":1}}],["如果hessian半正定或半负定",{"5":{"5":2}}],["如果hessian既有正特征值又有负特征值",{"5":{"5":2}}],["如果hessian负定",{"5":{"5":2}}],["如果hessian正定",{"5":{"5":2}}],["如果",{"5":{"5":2,"7":2}}],["元素级函数",{"5":{"5":2}}],["加法",{"5":{"5":2,"7":2}}],["计算量约为lu分解的一半",{"5":{"7":2}}],["计算复杂度与内存效率",{"2":{"7":1},"5":{"7":1}}],["计算每次的统计量估计",{"5":{"6":2}}],["计算分类任务中的正确预测数量等",{"5":{"6":2}}],["计算其输出关于输入的梯度",{"5":{"5":2}}],["计算图是表示数学表达式的有向无环图",{"5":{"5":2}}],["计算梯度",{"5":{"5":2}}],["雅可比矩阵",{"5":{"5":2}}],["雅可比矩阵包含所有一阶偏导数",{"5":{"5":2}}],["为",{"5":{"6":2}}],["为大语言模型提供了处理不确定性的理论基础",{"5":{"6":2}}],["为了校正偏差",{"5":{"5":2}}],["为了高效地实现这些计算",{"5":{"5":2}}],["为理解和优化大语言模型提供了核心的数学工具",{"5":{"5":2}}],[">在大语言模型的训练中",{"5":{"6":1}}],[">期望",{"5":{"6":1}}],[">",{"5":{"5":1,"6":1,"7":2}}],[">k",{"5":{"5":1}}],[">矩阵形式的链式法则在深度学习中尤为重要",{"5":{"5":1}}],[">第1章",{"5":{"4":3}}],["sum",{"5":{"7":2}}],["schmidt正交化",{"5":{"7":2}}],["schmidt正交化可以移除表示中的冗余成分",{"5":{"7":2}}],["schmidt正交化过程是构建正交基的标准算法",{"5":{"7":2}}],["scheduling",{"5":{"5":2}}],["svd",{"5":{"7":2}}],["spectral",{"5":{"7":8}}],["space",{"5":{"7":4}}],["system",{"5":{"7":2}}],["sne和umap",{"5":{"7":2}}],["sequence",{"5":{"7":2}}],["self",{"5":{"4":3,"5":3,"6":3,"7":2}}],["s",{"5":{"6":2}}],["singular",{"5":{"7":2}}],["size",{"5":{"6":2,"7":2}}],["sigmoid",{"5":{"5":2}}],["smoothing",{"5":{"6":2}}],["skewness",{"5":{"6":2}}],["standard",{"5":{"6":2}}],["step",{"5":{"5":2}}],["sgd倾向于收敛到平坦的局部最小值",{"5":{"5":2}}],["sgd的随机性本身就提供了一种隐式的噪声源",{"5":{"5":2}}],["softmax输出为",{"5":{"6":1}}],["softmax输出为​",{"5":{"6":1}}],["softmax函数将模型的原始输出",{"5":{"6":2}}],["softmax",{"5":{"5":2}}],["src=",{"5":{"5":3,"6":3,"7":2}}],["dynamical",{"5":{"7":2}}],["d",{"5":{"6":4}}],["dropout噪声等",{"5":{"6":2}}],["dropout",{"5":{"6":2}}],["dropout可以被解释为对神经网络进行高斯近似",{"5":{"6":2}}],["drawio",{"5":{"5":4,"6":12,"7":8}}],["decomposition",{"5":{"7":4}}],["decay",{"5":{"5":4,"7":2}}],["deviation",{"5":{"6":2}}],["density",{"5":{"6":2}}],["derivative",{"5":{"5":2}}],["dimension",{"5":{"7":2}}],["discovery",{"5":{"6":2}}],["distance",{"5":{"6":2}}],["dist",{"5":{"6":8}}],["distribution",{"5":{"6":16}}],["divergence",{"5":{"6":2}}],["directional",{"5":{"5":2}}],["differentiation",{"5":{"5":2}}],["data",{"5":{"4":3,"7":2}}],["a",{"5":{"6":6}}],["amsgrad是adam的一个修改版本",{"5":{"5":2}}],["adaptation",{"5":{"7":2}}],["adaptive",{"5":{"5":2}}],["adam的超参数",{"5":{"5":1}}],["adam的超参数和通常使用默认值",{"5":{"5":1}}],["adam维护两个指数移动平均",{"5":{"5":2}}],["adam",{"5":{"5":4}}],["adagrad特别适合处理稀疏特征和非均匀分布的梯度",{"5":{"5":2}}],["adagrad为每个参数维护一个累积梯度平方和",{"5":{"5":2}}],["annealing",{"5":{"5":2}}],["approximate",{"5":{"5":2}}],["automatic",{"5":{"5":2}}],["alt=",{"5":{"5":3,"6":3,"7":2}}],["a>",{"5":{"4":3}}],["损失函数包含重构项和kl正则项",{"5":{"6":2}}],["损失函数",{"5":{"5":1}}],["损失函数的等值面在高维空间中可能比直觉上预期的更加连通",{"5":{"5":2}}],["损失函数的hessian通常具有极值的特征值分布",{"5":{"5":2}}],["损失函数衡量输出与目标之间的差异",{"5":{"5":1}}],["损失函数会以多快的速度变化",{"5":{"5":2}}],["对任意",{"5":{"6":1}}],["对任意常数",{"5":{"6":1}}],["对任意常数和随机变量",{"5":{"6":1}}],["对常数",{"5":{"6":1}}],["对常数有",{"5":{"6":1}}],["对所有向量",{"5":{"7":1}}],["对所有",{"5":{"6":1}}],["对数似然可以分解为序列中每个位置的条件对数概率之和",{"5":{"6":2}}],["对数似然函数为",{"5":{"6":2}}],["对",{"5":{"5":1}}],["对应",{"5":{"5":4}}],["对称性大大简化了hessian矩阵的分析和计算",{"5":{"5":2}}],["对的总影响是通过所有中间变量传导的",{"5":{"5":1}}],["对于序列长度",{"5":{"7":1}}],["对于序列长度和注意力维度",{"5":{"7":1}}],["对于设计和实现高效的大规模训练系统至关重要",{"5":{"7":2}}],["对于深层网络和长序列",{"5":{"7":2}}],["对于深入理解语言模型的工作原理和改进模型设计都有重要意义",{"5":{"6":2}}],["对于大语言模型中的全连接层",{"5":{"7":2}}],["对于大型矩阵",{"5":{"7":2}}],["对于一个批次",{"5":{"7":2}}],["对于矩阵",{"5":{"7":1}}],["对于矩阵方程",{"5":{"7":2}}],["对于矩阵和",{"5":{"7":1}}],["对于对称矩阵",{"5":{"7":2}}],["对于方阵",{"5":{"7":2}}],["对于任何整数",{"5":{"7":2}}],["对于任意矩阵",{"5":{"7":2}}],["对于实对称矩阵",{"5":{"7":2}}],["对于",{"5":{"6":1}}],["对于词汇表大小为",{"5":{"6":1}}],["对于词汇表大小为的语言模型",{"5":{"6":1}}],["对于每个测试样本",{"5":{"6":2}}],["对于每个位置",{"5":{"6":2}}],["对于每个操作节点",{"5":{"5":2}}],["对于两个分布",{"5":{"6":1}}],["对于两个分布和",{"5":{"6":1}}],["对于两个离散概率分布",{"5":{"6":1}}],["对于两个离散概率分布和",{"5":{"6":1}}],["对于两个随机变量",{"5":{"6":1}}],["对于两个随机变量和",{"5":{"6":1}}],["对于确定性分布",{"5":{"6":2}}],["对于维随机向量",{"5":{"6":1}}],["对于连续随机变量",{"5":{"6":4}}],["对于离散随机变量及其概率分布",{"5":{"6":1}}],["对于离散随机变量",{"5":{"6":5}}],["对于乘积",{"5":{"5":1}}],["对于乘积​",{"5":{"5":1}}],["对于不同的随机初始化",{"5":{"5":2}}],["对于强凸函数",{"5":{"5":2}}],["对于具有l",{"5":{"5":2}}],["对于二次可微函数",{"5":{"5":2}}],["对于残差形式的问题",{"5":{"5":2}}],["对于函数",{"5":{"5":4}}],["对于多元函数的复合",{"5":{"5":2}}],["设单个样本的特征表示是形状为",{"5":{"7":1}}],["设单个样本的特征表示是形状为的二维张量",{"5":{"7":1}}],["设有两个",{"5":{"7":1}}],["设有两个维向量",{"5":{"7":1}}],["设有一个三阶张量",{"5":{"7":2}}],["设连续随机变量",{"5":{"6":1}}],["设连续随机变量的pdf为",{"5":{"6":1}}],["设计优化算法和理解泛化性质时非常有用",{"5":{"6":2}}],["设参数",{"5":{"6":2}}],["设我们有观测数据",{"5":{"6":2}}],["设是独立同分布的随机变量",{"5":{"6":2}}],["设是第个类别的logit",{"5":{"6":1}}],["设是一个维随机向量",{"5":{"6":1}}],["设是一个单位向量",{"5":{"5":1}}],["设是一个可微函数",{"5":{"5":1}}],["设是一个多元函数",{"5":{"5":1}}],["设随机变量在个类别上服从类别分布",{"5":{"6":1}}],["设随机变量",{"5":{"6":3}}],["设",{"5":{"5":7,"6":10}}],["情况稍微复杂一些",{"5":{"5":2}}],["则参数量从",{"5":{"7":1}}],["则参数量从减少到",{"5":{"7":1}}],["则复制整个模型在多个数据批次上并行训练",{"5":{"7":2}}],["则一个批次的表示就是形状为",{"5":{"7":2}}],["则是一个的列向量",{"5":{"7":1}}],["则plu分解总是存在",{"5":{"7":2}}],["则lu分解存在",{"5":{"7":2}}],["则称",{"5":{"7":1}}],["则称为标准正交基",{"5":{"7":2}}],["则称它们是正交的",{"5":{"7":2}}],["则称是的特征向量",{"5":{"7":1}}],["则模型利用了更多的参数自由度",{"5":{"7":2}}],["则系统是不稳定的",{"5":{"7":2}}],["则系统是稳定的",{"5":{"7":2}}],["则会在较小张量的左侧自动补",{"5":{"7":2}}],["则该维度可以广播",{"5":{"7":2}}],["则该点是鞍点",{"5":{"5":2}}],["则该点是局部极大值",{"5":{"5":2}}],["则该点是局部极小值",{"5":{"5":2}}],["则其元素可以表示为",{"5":{"7":1}}],["则其元素可以表示为​",{"5":{"7":1}}],["则其和为",{"5":{"7":1}}],["则其和为向量加法满足交换律和结合律",{"5":{"7":1}}],["则其pdf为",{"5":{"6":2}}],["则其pdf为当",{"5":{"6":2}}],["则标准化和",{"5":{"6":1}}],["则标准化和的分布趋向于标准正态分布",{"5":{"6":1}}],["则样本均值",{"5":{"6":1}}],["则样本均值依概率收敛于对任意成立",{"5":{"6":1}}],["则鼓励",{"5":{"6":1}}],["则交叉熵损失相对于",{"5":{"6":1}}],["则交叉熵损失相对于的梯度为​",{"5":{"6":1}}],["则给定",{"5":{"6":1}}],["则给定时的条件分布仍然是高斯分布",{"5":{"6":1}}],["则任意分量",{"5":{"6":1}}],["则任意分量服从一维高斯分布",{"5":{"6":1}}],["则拒绝原假设",{"5":{"6":2}}],["则对任意矩阵",{"5":{"6":2}}],["则记为",{"5":{"6":2}}],["则梯度的分布趋向于高斯分布",{"5":{"6":2}}],["则梯度",{"5":{"5":1}}],["则梯度是一个与形状相同的三阶张量",{"5":{"5":1}}],["则需要进一步分析",{"5":{"5":2}}],["则hessian矩阵是对称的",{"5":{"5":2}}],["则沿的方向导数为",{"5":{"5":1}}],["则在点处的梯度定义为",{"5":{"5":1}}],["则关于的偏导数为",{"5":{"5":1}}],["则关于变量在点处的偏导数定义为",{"5":{"5":1}}],["则",{"5":{"5":6,"6":10,"7":1}}],["在批处理情况下变成",{"5":{"7":1}}],["在第二和第三维度上的收缩产生一个",{"5":{"7":1}}],["在深度维度上划分层",{"5":{"7":2}}],["在深度学习优化中",{"5":{"5":2}}],["在深度学习的反向传播算法中",{"5":{"5":2}}],["在深度学习中有着广泛的应用",{"5":{"7":2}}],["在深度学习中",{"5":{"5":4,"6":4,"7":6}}],["在反向传播时重新计算被省略的激活值来节省内存",{"5":{"7":2}}],["在反向传播时重新计算其余激活值",{"5":{"5":2}}],["在前向传播过程中",{"5":{"7":2}}],["在pytorch中",{"5":{"7":2}}],["在物理学和机器学习中都有重要应用",{"5":{"7":2}}],["在模型的宽度维度上划分参数",{"5":{"7":2}}],["在模型并行化中",{"5":{"7":2}}],["在模型压缩和低秩近似中",{"5":{"7":2}}],["在模型压缩方面",{"5":{"7":2}}],["在分析梯度流和稳定性时",{"5":{"7":2}}],["在分析神经网络梯度的分布时",{"5":{"6":2}}],["在推荐系统和缺失数据插补中",{"5":{"7":2}}],["在神经网络的稳定性分析和lipschitz约束中",{"5":{"7":2}}],["在神经网络中",{"5":{"7":2}}],["在对比学习中",{"5":{"7":2}}],["在对话系统中",{"5":{"6":2}}],["在特征提取和表示学习中经常被使用",{"5":{"7":2}}],["在参数高效微调",{"5":{"7":2}}],["在frobenius范数和谱范数意义下",{"5":{"7":2}}],["在循环神经网络",{"5":{"7":2}}],["在主成分分析",{"5":{"7":2}}],["在变换下只发生缩放而不改变方向",{"5":{"7":2}}],["在变分推断中",{"5":{"6":4}}],["在变分自编码器和生成模型中",{"5":{"6":2}}],["在词嵌入研究中",{"5":{"7":2}}],["在注意力机制中",{"5":{"7":2}}],["在多头注意力中",{"5":{"7":2}}],["在多类分类中为one",{"5":{"6":2}}],["在优化算法和正则化方法中有着重要的应用",{"5":{"7":2}}],["在实际实现中",{"5":{"7":2}}],["在实际应用中",{"5":{"6":2}}],["在实践中",{"5":{"7":2}}],["在实现反向传播时",{"5":{"7":2}}],["在现代深度学习框架中",{"5":{"7":2}}],["在线性代数的学习路径中",{"5":{"7":2}}],["在观测数据",{"5":{"6":1}}],["在观测数据后",{"5":{"6":1}}],["在",{"5":{"6":1}}],["在原假设下所有排列是等可能的",{"5":{"6":2}}],["在比较语言模型时",{"5":{"6":2}}],["在比较语言模型性能时",{"5":{"6":2}}],["在机器学习训练中",{"5":{"6":2}}],["在机器学习特别是大语言模型中有着广泛应用",{"5":{"6":2}}],["在正则化中",{"5":{"7":2}}],["在正则化技术方面",{"5":{"6":2}}],["在正则条件下",{"5":{"6":2}}],["在生成模型中",{"5":{"6":2}}],["在其他类别上均匀分配",{"5":{"6":1}}],["在其他类别上均匀分配的概率",{"5":{"6":1}}],["在其存在的范围内",{"5":{"6":2}}],["在知识蒸馏中",{"5":{"6":2}}],["在强化学习与语言模型的结合中",{"5":{"6":2}}],["在vae中",{"5":{"6":2}}],["在高斯过程回归和二次优化中有广泛应用",{"5":{"7":2}}],["在高斯过程和贝叶斯优化中",{"5":{"6":2}}],["在高维空间中",{"5":{"5":2}}],["在输出建模方面",{"5":{"6":2}}],["在权重初始化方面",{"5":{"6":2}}],["在统计学和机器学习中有着核心地位",{"5":{"6":2}}],["在某些正则化方法中",{"5":{"7":2}}],["在某些深度学习应用中",{"5":{"6":2}}],["在某些情况下",{"5":{"5":2}}],["在语言模型评估中",{"5":{"6":2}}],["在语言模型训练中",{"5":{"6":2}}],["在语言模型的评估中",{"5":{"6":2}}],["在语言模型的某些应用中",{"5":{"6":2}}],["在语言模型研究中",{"5":{"6":2}}],["在语言模型中",{"5":{"6":12}}],["在自然语言处理中",{"5":{"6":2}}],["在函数",{"5":{"5":1}}],["在函数的等值面上",{"5":{"5":1}}],["在点",{"5":{"5":3}}],["在点​附近",{"5":{"5":1}}],["在适当的噪声水平下",{"5":{"5":2}}],["在训练大语言模型中尤为重要",{"5":{"5":2}}],["在训练初期逐渐增加学习率",{"5":{"5":2}}],["在凸优化问题中",{"5":{"5":2}}],["在后期精细调优",{"5":{"5":2}}],["在贝叶斯深度学习和变分推断中",{"5":{"5":2}}],["在最小二乘问题中特别有用",{"5":{"5":2}}],["在与梯度垂直的方向",{"5":{"5":2}}],["在与梯度相反的方向",{"5":{"5":2}}],["在大语言模型",{"5":{"7":2}}],["在大语言模型评估中",{"5":{"6":2}}],["在大语言模型的架构设计中",{"5":{"7":2}}],["在大语言模型的实践中",{"5":{"7":2}}],["在大语言模型的理论基础中占据着不可替代的核心地位",{"5":{"7":2}}],["在大语言模型的训练中",{"5":{"6":1}}],["在大语言模型的在线评估中",{"5":{"6":2}}],["在大语言模型的开发和研究中",{"5":{"6":2}}],["在大语言模型的语境下",{"5":{"5":2,"7":2}}],["在大语言模型中",{"5":{"5":2,"6":14,"7":18}}],["在计算图中",{"5":{"5":2}}],["它对应于从",{"5":{"7":1}}],["它对应于从到的线性变换中最大的拉伸因子",{"5":{"7":1}}],["它使用半精度浮点数",{"5":{"7":2}}],["它允许同时处理多个样本以提高计算效率",{"5":{"7":2}}],["它通过在前向传播时只保存部分层的激活值",{"5":{"7":2}}],["它通过将权重矩阵除以其谱范数来约束网络函数的lipschitz常数",{"5":{"7":2}}],["它通过匹配矩",{"5":{"6":2}}],["它可以看作是矩阵展开为向量后的l2范数",{"5":{"7":2}}],["它可以显著简化代码",{"5":{"7":2}}],["它只旋转或反射空间而不改变向量的长度",{"5":{"7":2}}],["它用于权重归一化以稳定训练过程",{"5":{"7":2}}],["它描述了输入中的冗余信息",{"5":{"7":2}}],["它描述了矩阵所能表示的所有输出",{"5":{"7":2}}],["它描述了当其他变量保持不变时",{"5":{"5":2}}],["它与随机变量",{"5":{"6":1}}],["它与随机变量具有相同的量纲",{"5":{"6":1}}],["它不仅告诉我们差异是否显著",{"5":{"6":2}}],["它指出大量独立同分布随机变量之和",{"5":{"6":2}}],["它们的kronecker积定义为",{"5":{"7":2}}],["它们的均值",{"5":{"6":2}}],["它们揭示了线性变换的本质特征",{"5":{"7":2}}],["它们描述了大量随机变量之和",{"5":{"6":2}}],["它们用于衡量两个概率分布之间的差异",{"5":{"6":2}}],["它们刻画了随机变量的集中趋势",{"5":{"6":2}}],["它考虑了各变量之间的相关性",{"5":{"6":2}}],["它总是存在的",{"5":{"6":2}}],["它唯一确定了随机变量的概率分布",{"5":{"6":2}}],["它将一个",{"5":{"7":1}}],["它将一个的矩阵与一个的矩阵组合成一个的大矩阵",{"5":{"7":1}}],["它将矩阵分解为下三角矩阵与其转置的乘积",{"5":{"7":2}}],["它将权重矩阵的谱范数归一化为1",{"5":{"7":2}}],["它将标量",{"5":{"7":2}}],["它将某一子层的输入与该子层的输出直接相加",{"5":{"7":2}}],["它将后验分布的推断转化为优化问题",{"5":{"6":2}}],["它将参数视为随机变量并使用贝叶斯公式进行推断",{"5":{"6":2}}],["它将硬标签",{"5":{"6":2}}],["它将随机试验的结果数值化",{"5":{"6":2}}],["它将hessian矩阵近似为两个小矩阵的kronecker积",{"5":{"5":2}}],["它结合了动量方法和rmsprop的优点",{"5":{"5":2}}],["它在更新之前先对梯度进行校正",{"5":{"5":2}}],["它累积历史梯度来平滑参数更新",{"5":{"5":2}}],["它决定了每次更新的幅度",{"5":{"5":2}}],["它利用中心极限定理的原理来稳定训练过程",{"5":{"6":2}}],["它利用负梯度方向作为搜索方向来迭代地最小化目标函数",{"5":{"5":2}}],["它利用链式法则自动计算复杂函数的导数",{"5":{"5":2}}],["它告诉我们如何计算复合函数的导数",{"5":{"5":2}}],["是注意力头的数量",{"5":{"7":1}}],["是批大小",{"5":{"7":1}}],["是批量大小",{"5":{"7":1}}],["是批处理的一个变体",{"5":{"7":2}}],["是单位矩阵",{"5":{"7":1}}],["是训练超大规模语言模型的必要技术",{"5":{"7":2}}],["是训练神经网络的关键技术基础",{"5":{"5":2}}],["是的矩阵",{"5":{"7":1}}],["是的特征值的平方根",{"5":{"7":1}}],["是将矩阵转换为列向量的操作",{"5":{"7":2}}],["是将任意矩阵分解为三个矩阵乘积的强大工具",{"5":{"7":2}}],["是对应的特征值",{"5":{"7":2}}],["是所有被矩阵映射到零向量的输入向量构成的空间",{"5":{"7":2}}],["是矩阵所有奇异值的和",{"5":{"7":2}}],["是矩阵所有列向量的线性组合构成的空间",{"5":{"7":2}}],["是矩阵的最大奇异值",{"5":{"7":4}}],["是矩阵概念在更高维度的延伸",{"5":{"7":2}}],["是隐藏维度",{"5":{"7":2}}],["是序列长度",{"5":{"7":3}}],["是线性代数中最重要的研究对象之一",{"5":{"7":2}}],["是独立同分布的随机变量",{"5":{"6":2}}],["是似然函数",{"5":{"6":1}}],["是未知参数",{"5":{"6":1}}],["是模型预测分布时",{"5":{"6":1}}],["是真实分布而",{"5":{"6":1}}],["是真实词元",{"5":{"6":2}}],["是均值向量",{"5":{"6":1}}],["是均值参数",{"5":{"6":1}}],["是均值为0",{"5":{"6":2}}],["是率参数",{"5":{"6":1}}],["是成功概率",{"5":{"6":1}}],["是概率论中最深刻的定理之一",{"5":{"6":2}}],["是概率论中最重要的连续分布",{"5":{"6":2}}],["是另一种近似推断方法",{"5":{"6":2}}],["是边际似然",{"5":{"6":2}}],["是常数",{"5":{"6":2}}],["是给定一个随机变量时另一个随机变量的熵",{"5":{"6":2}}],["是两个随机变量的联合分布的熵",{"5":{"6":2}}],["是定义损失函数和分析模型行为的重要工具",{"5":{"6":2}}],["是信息论中的核心概念",{"5":{"6":2}}],["是协方差矩阵的逆矩阵",{"5":{"6":2}}],["是协方差矩阵的行列式",{"5":{"6":2}}],["是协方差矩阵",{"5":{"6":2}}],["是标准正态分布的cdf",{"5":{"6":1}}],["是标准差",{"5":{"6":2}}],["是标准化的协方差",{"5":{"6":2}}],["是方差参数",{"5":{"6":2}}],["是",{"5":{"6":3,"7":3}}],["是随机变量幂次期望的统称",{"5":{"6":2}}],["是随机变量取值的加权平均",{"5":{"6":2}}],["是伯努利分布向多值情况的推广",{"5":{"6":2}}],["是n次独立伯努利试验中成功次数的分布",{"5":{"6":2}}],["是最基本也是最重要的参数估计方法",{"5":{"6":2}}],["是最简单的连续分布",{"5":{"6":2}}],["是最简单的离散分布",{"5":{"6":2}}],["是最流行的自适应优化算法",{"5":{"5":2}}],["是描述离散随机变量概率分布的函数",{"5":{"6":2}}],["是掌握语言模型概率本质的必要前提",{"5":{"6":2}}],["是衰减系数",{"5":{"5":1}}],["是逐元素累积的梯度平方",{"5":{"5":1}}],["是速度向量",{"5":{"5":1}}],["是凸函数的充要条件是对于任意",{"5":{"5":1}}],["是第",{"5":{"5":1,"6":1}}],["是残差函数的雅可比矩阵",{"5":{"5":1}}],["是梯度与方向",{"5":{"5":1}}],["是梯度为零向量的点",{"5":{"5":2}}],["是激活函数",{"5":{"5":1}}],["是外层函数关于中间变量的导数",{"5":{"5":1}}],["是深度学习训练和推理的基本范式",{"5":{"7":2}}],["是深度学习中最重要的技术之一",{"5":{"6":2}}],["是深度学习优化中最重要的进展之一",{"5":{"5":2}}],["是深入掌握深度学习优化算法的必要基础",{"5":{"5":2}}],["是动量系数",{"5":{"5":2}}],["是加速梯度下降的重要技术",{"5":{"5":2}}],["是在训练过程中动态调整学习率的策略",{"5":{"5":2}}],["是损失函数在当前参数处的梯度",{"5":{"5":2}}],["是学习率",{"5":{"5":2}}],["是一种重要的权重归一化技术",{"5":{"7":2}}],["是一种常用的内存优化技术",{"5":{"7":2}}],["是一种常用的权重归一化技术",{"5":{"7":2}}],["是一种常用的正则化技术",{"5":{"6":2}}],["是一种非参数方法",{"5":{"6":2}}],["是一种通过重采样来估计统计量方差的非参数方法",{"5":{"6":2}}],["是一种用于深度学习的hessian近似方法",{"5":{"5":2}}],["是一个单位向量",{"5":{"5":1}}],["是一个可微函数",{"5":{"5":1}}],["是一个",{"5":{"5":1,"6":2,"7":2}}],["是一个与",{"5":{"5":1}}],["是一个多元函数",{"5":{"5":1}}],["是一个的矩阵",{"5":{"5":1,"7":1}}],["是理解牛顿法等二阶优化方法的基础",{"5":{"5":2}}],["是现代深度学习框架的核心功能",{"5":{"5":2}}],["是输出",{"5":{"5":2}}],["是输入神经元的数量",{"5":{"6":1}}],["是输入",{"5":{"5":2}}],["是偏置向量",{"5":{"5":2}}],["是权重矩阵",{"5":{"5":2}}],["是内层函数关于自变量的导数",{"5":{"5":2}}],["是微积分中最重要的法则之一",{"5":{"5":2}}],["r为矩阵的秩",{"5":{"7":2}}],["rnn",{"5":{"7":2}}],["reshape",{"5":{"7":2}}],["relation",{"5":{"6":8}}],["rel=",{"5":{"4":3}}],["rectified",{"5":{"5":2}}],["rank",{"5":{"7":2}}],["radam",{"5":{"5":2}}],["rate控制",{"5":{"6":2}}],["rate",{"5":{"5":2}}],["rmsprop是adagrad的改进版本",{"5":{"5":2}}],["rule",{"5":{"5":2}}],["checkpointing",{"5":{"7":2}}],["cholesky分解的数值稳定性好",{"5":{"7":2}}],["cholesky分解是lu分解的特例",{"5":{"7":2}}],["chain",{"5":{"5":2}}],["candecomp",{"5":{"7":2}}],["categorical",{"5":{"6":2}}],["cp分解可以将一个",{"5":{"7":1}}],["cp分解可以将一个的三阶张量表示为个秩一张量的和",{"5":{"7":1}}],["cp",{"5":{"7":4}}],["central",{"5":{"6":2}}],["cross",{"5":{"6":2}}],["critical",{"5":{"5":2}}],["clt",{"5":{"6":2}}],["clipping",{"5":{"5":2}}],["class=",{"5":{"4":3}}],["column",{"5":{"7":2}}],["contraction",{"5":{"7":2}}],["conditional",{"5":{"6":2}}],["completion",{"5":{"7":2}}],["component",{"5":{"7":2}}],["combinedscene",{"5":{"5":4}}],["coefficient",{"5":{"6":2}}],["correlation",{"5":{"6":2}}],["covariance",{"5":{"6":2}}],["cosine",{"5":{"5":2}}],["curvature",{"5":{"5":2}}],["复合函数的偏导数法则与单变量情况类似",{"5":{"5":2}}],["我们得到形状为",{"5":{"7":1}}],["我们得到形状为的张量",{"5":{"7":1}}],["我们先计算若干个小批次的梯度",{"5":{"7":2}}],["我们关心如何用少量的奇异值最好地近似原始矩阵",{"5":{"7":2}}],["我们在优化目标中加入正交性惩罚项",{"5":{"7":2}}],["我们可以通过梯度累积来模拟更大的有效批大小",{"5":{"7":2}}],["我们可以了解模型使用了多少",{"5":{"7":2}}],["我们可以实现降维同时最小化信息损失",{"5":{"7":2}}],["我们可以构建置信区间来量化估计的不确定性",{"5":{"6":2}}],["我们使用迭代算法如幂迭代",{"5":{"7":2}}],["我们使用雅可比矩阵",{"5":{"5":2}}],["我们很少直接计算矩阵的逆",{"5":{"7":2}}],["我们希望用简单的近似分布",{"5":{"6":1}}],["我们希望用简单的近似分布来逼近复杂的后验分布",{"5":{"6":1}}],["我们计算两个模型的性能差异",{"5":{"6":2}}],["我们根据样本数据计算检验统计量",{"5":{"6":2}}],["我们需要首先明确几个基本定义",{"5":{"7":2}}],["我们需要平衡验证集大小和计算成本",{"5":{"6":2}}],["我们需要找到一组模型参数",{"5":{"5":2}}],["我们通常使用随机梯度下降或其变体",{"5":{"6":2}}],["我们同时处理这两类随机变量",{"5":{"6":2}}],["我们会以最快的速度离开当前的等值面",{"5":{"5":2}}],["我们实际上是在问",{"5":{"5":2}}],["梯度检查点",{"5":{"7":2}}],["梯度检查点技术通过在前向传播时只保存部分激活值",{"5":{"5":2}}],["梯度累积在数学上等价于使用更大的批次进行训练",{"5":{"7":2}}],["梯度累积是另一种与批处理相关的技术",{"5":{"7":2}}],["梯度估计的方差较大",{"5":{"6":2}}],["梯度估计的方差减小",{"5":{"6":2}}],["梯度的方向更加稳定",{"5":{"6":2}}],["梯度的性质对于理解优化过程至关重要",{"5":{"5":2}}],["梯度裁剪就是将梯度向量的模长限制在某个阈值之内",{"5":{"7":2}}],["梯度裁剪",{"5":{"5":2}}],["梯度下降以线性速度收敛",{"5":{"5":2}}],["梯度下降在凸优化中的收敛性有完善的理论保证",{"5":{"5":2}}],["梯度下降是最基本也最广泛使用的优化算法",{"5":{"5":2}}],["梯度下降与凸优化基础",{"2":{"5":1},"5":{"5":1}}],["梯度向量与等值面垂直",{"5":{"5":2}}],["梯度向量指导着参数的更新方向",{"5":{"5":2}}],["梯度向量指向函数增长最快的方向",{"5":{"5":2}}],["梯度与等值面",{"5":{"5":2}}],["梯度是多元函数最陡上升方向的向量",{"5":{"5":2}}],["梯度和hessian等概念",{"5":{"5":2}}],["链式法则给出了多个随机变量的联合熵分解",{"5":{"6":2}}],["链式法则的应用更加直观和系统",{"5":{"5":2}}],["链式法则的应用更加系统化",{"5":{"5":2}}],["链式法则的基本形式可以这样表述",{"5":{"5":2}}],["链式法则使得我们能够计算损失函数相对于任意深层参数的梯度",{"5":{"5":2}}],["链式法则将在下文详细讨论",{"5":{"5":2}}],["链式法则",{"5":{"5":4}}],["这就是为什么transformer在处理长序列时面临计算挑战",{"5":{"7":2}}],["这就是梯度下降算法的理论基础",{"5":{"5":2}}],["这有助于理解其参数效率",{"5":{"7":2}}],["这涉及到部分和",{"5":{"7":2}}],["这样",{"5":{"7":4}}],["这对训练生成对抗网络",{"5":{"7":2}}],["这对于主动学习和探索",{"5":{"6":2}}],["这对于训练深层网络和循环神经网络尤为重要",{"5":{"5":2}}],["这在理论上可以防止初始化时的梯度消失或爆炸问题",{"5":{"7":2}}],["这在将展平后的向量恢复为嵌入序列时非常有用",{"5":{"7":2}}],["这在近端策略优化",{"5":{"6":2}}],["这本质上就是计算查询矩阵与转置后的键矩阵的乘积",{"5":{"7":2}}],["这也是大语言模型能够实现高效训练和推理的关键技术基础",{"5":{"7":2}}],["这表明在给定方差的所有连续分布中",{"5":{"6":2}}],["这从参数命名就可以看出",{"5":{"6":2}}],["这使得求解正交矩阵的逆矩阵变得极其简单",{"5":{"7":2}}],["这使得贝叶斯推断在计算上更加方便",{"5":{"6":2}}],["这使得高斯分布成为最易于处理的分布之一",{"5":{"6":2}}],["这使得在保持二阶信息的同时实现了可扩展性",{"5":{"5":2}}],["这一性质被用于分析梯度分布",{"5":{"6":2}}],["这一性质在变分推断和高斯过程回归中非常重要",{"5":{"6":2}}],["这一性质在反向传播算法的推导中至关重要",{"5":{"6":2}}],["这一小批样本的梯度是整体数据梯度的一个良好近似",{"5":{"6":2}}],["这一过程称为反向传播",{"5":{"5":2}}],["这可能是因为神经网络具有大量的对称性",{"5":{"5":2}}],["这被认为有助于模型在早期阶段找到一个更好的参数区域",{"5":{"5":2}}],["这解决了adagrad学习率单调递减的问题",{"5":{"5":2}}],["这意味着计算一个",{"5":{"7":1}}],["这意味着计算一个权重矩阵与向量的乘积需要次标量乘法和加法",{"5":{"7":1}}],["这意味着正交变换是一种保距变换",{"5":{"7":2}}],["这意味着每个维度都不是二元的或稀疏的",{"5":{"7":2}}],["这意味着函数图上任意两点的连线位于函数图的上方",{"5":{"5":2}}],["这意味着如果我们沿着梯度方向移动",{"5":{"5":2}}],["这些运算的吞吐量通常是全精度运算的数倍",{"5":{"7":2}}],["这些运算在大语言模型中有着直接的应用场景",{"5":{"7":2}}],["这些范数在稀疏优化和线性规划中有重要应用",{"5":{"7":2}}],["这些方向称为主成分",{"5":{"7":2}}],["这些矩阵乘法的计算效率直接影响模型的整体性能",{"5":{"7":2}}],["这些定理为机器学习中许多方法的合理性提供了理论依据",{"5":{"6":2}}],["这些性质为mle在大样本场景下的应用提供了理论保障",{"5":{"6":2}}],["这些高阶矩在统计分析中有一定应用",{"5":{"6":2}}],["这些统计量在机器学习的理论和实践中都有广泛应用",{"5":{"6":2}}],["这些最小值通常具有更好的泛化能力",{"5":{"5":2}}],["这些变体反映了深度学习优化研究的持续进展",{"5":{"5":2}}],["这些调度策略的目标是在训练初期快速收敛",{"5":{"5":2}}],["这些点可能是极小值",{"5":{"5":2}}],["这种技术在训练大语言模型时尤为重要",{"5":{"7":2}}],["这种转化在推导反向传播公式和优化算法时非常有用",{"5":{"7":2}}],["这种约束有助于提高模型的稳定性和泛化能力",{"5":{"7":2}}],["这种分析对于诊断模型过拟合和理解模型容量都很有价值",{"5":{"7":2}}],["这种方法的数学基础正是低秩近似理论",{"5":{"7":2}}],["这种方法在模型压缩和少样本学习中有一定的应用价值",{"5":{"5":2}}],["这种压缩是显著的",{"5":{"7":2}}],["这种密集表示使得模型能够在连续空间中学习平滑的语义关系",{"5":{"7":2}}],["这种几何表示使得我们可以利用空间中的几何关系来理解和操作语义信息",{"5":{"7":2}}],["这种运算在模型的权重初始化",{"5":{"7":2}}],["这种近似在分析神经网络的学习动态",{"5":{"6":2}}],["这种选择影响着生成样本的多样性和质量",{"5":{"6":2}}],["这种结构影响着优化算法的行为",{"5":{"5":2}}],["这个运算被高度优化",{"5":{"7":2}}],["这个近似将原始矩阵分解为k个秩一矩阵的和",{"5":{"7":2}}],["这个近似在局部区域非常准确",{"5":{"5":2}}],["这个向量的每一个维度都编码了某种语义或语法特征",{"5":{"7":2}}],["这个估计越可靠",{"5":{"6":2}}],["这个简洁的梯度形式是交叉熵损失被广泛采用的重要原因之一",{"5":{"6":2}}],["这个公式在计算方差时更加实用",{"5":{"6":2}}],["这个公式清楚地表明",{"5":{"5":2}}],["这个公式表明",{"5":{"5":2}}],["这个信息对于确定参数更新的方向和幅度至关重要",{"5":{"5":2}}],["这个优化过程涉及对损失函数求导",{"5":{"5":2}}],["这是一个关于",{"5":{"7":1}}],["这是一个关于的多项式方程",{"5":{"7":1}}],["这是一个重要的性质",{"5":{"6":2}}],["这是所谓的",{"5":{"6":2}}],["这是多数实际应用中的常见假设",{"5":{"5":2}}],["这是",{"5":{"4":2}}],["大小",{"5":{"7":2}}],["大语言模型中的输入通常是一个三维张量",{"5":{"7":2}}],["大语言模型中的所有参数最终都以矩阵或张量的形式存储和计算",{"5":{"7":2}}],["大语言模型的某些扩展",{"5":{"6":2}}],["大语言模型的优化面临独特的挑战",{"5":{"5":2}}],["大语言模型的训练本质上是一个优化问题",{"5":{"5":2}}],["大数定律和中心极限定理也指导着模型评估策略",{"5":{"6":2}}],["大数定律和中心极限定理是概率论中最重要的极限定理",{"5":{"6":2}}],["大数定律解释了为什么使用更大的批量",{"5":{"6":2}}],["大数定律",{"5":{"6":2}}],["大数定律与中心极限定理",{"2":{"6":1},"5":{"6":1}}],["大模型中的数学",{"0":{"4":1},"5":{"4":3}}],["<br><img",{"5":{"5":1}}],["<br><a",{"5":{"4":2}}],["<br>",{"5":{"5":1,"6":2,"7":1}}],["<img",{"5":{"5":2,"6":3,"7":2}}],["<a",{"5":{"4":1}}],["hierarchical",{"5":{"7":2}}],["hidden",{"5":{"7":2}}],["householder变换和givens旋转",{"5":{"7":2}}],["hot编码",{"5":{"6":6}}],["he初始化使用均值为0",{"5":{"6":2}}],["hessian的特征值表示函数在不同特征方向上的曲率",{"5":{"5":2}}],["hessian矩阵的奇异值分解揭示了函数的局部曲率结构",{"5":{"5":2}}],["hessian矩阵决定了泰勒展开的二次项",{"5":{"5":2}}],["hessian矩阵在优化中的作用主要体现在以下几个方面",{"5":{"5":2}}],["hessian矩阵定义为",{"5":{"5":2}}],["hessian矩阵是多元函数的二阶导数矩阵",{"5":{"5":2}}],["href=",{"5":{"4":6}}],["html",{"4":{"0":1,"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1},"5":{"4":6}}],["第四",{"5":{"5":2,"7":2}}],["第三",{"5":{"5":4,"7":8}}],["第一章",{"2":{"4":1},"5":{"4":1}}],["第1章",{"4":{"0":1,"1":1,"2":1,"5":1,"6":1,"7":1},"5":{"4":12}}],["3",{"0":{"5":1},"2":{"5":4,"6":1,"7":1},"4":{"2":1,"5":1},"5":{"4":5,"5":5,"6":1,"7":1}}],["2",{"0":{"6":1},"2":{"5":1,"6":8,"7":1},"4":{"1":1,"6":1},"5":{"4":5,"5":1,"6":9,"7":1}}],["13",{"2":{"7":1},"5":{"7":1}}],["12",{"2":{"7":1},"5":{"7":1}}],["11",{"2":{"7":1},"5":{"7":1}}],["10",{"2":{"7":1},"5":{"7":1}}],["1到6",{"5":{"6":2}}],["19",{"5":{"5":4}}],["1",{"0":{"5":1,"6":1,"7":2},"2":{"5":4,"6":8,"7":27},"4":{"0":2,"1":1,"2":1,"5":1,"6":1,"7":2},"5":{"4":20,"5":9,"6":9,"7":35}}]],"serializationVersion":2}