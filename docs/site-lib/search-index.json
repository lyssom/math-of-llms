{"documentCount":12,"nextId":28,"documentIds":{"5":"第2章-前馈网络数学/2.1-神经元的数学模型.html","7":"第1章-数学基础/1.3-微积分与优化基础.html","8":"第1章-数学基础/1.2-概率论与统计.html","9":"第1章-数学基础/1.1-线性代数与张量运算.html","13":"第2章-前馈网络数学/2.3-前向传播的数学本质.html","15":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html","21":"大模型中的数学.html","22":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html","23":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html","24":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html","25":"第2章-前馈网络数学/2.4-反向传播梯度推导.html","27":"index.html"},"fieldIds":{"title":0,"aliases":1,"headers":2,"tags":3,"path":4,"content":5},"fieldLength":{"5":[3,1,14,1,6,542],"7":[3,1,6,1,6,500],"8":[3,1,15,1,6,891],"9":[2,1,26,1,5,871],"13":[3,1,15,1,6,611],"15":[2,1,7,1,5,295],"21":[1,1,17,1,2,64],"22":[2,1,35,1,5,900],"23":[3,1,14,1,6,410],"24":[3,1,28,1,6,858],"25":[3,1,19,1,6,513],"27":[1,1,17,1,2,64]},"averageFieldLength":[2.4166666666666665,1,17.749999999999996,1,5.083333333333333,543.2500000000001],"storedFields":{"5":{"title":"2.1 神经元的数学模型","aliases":[],"headers":["2.1.1 从生物神经元到数学抽象","2.1.2 感知机与仿射变换","2.1.3 激活函数的数学角色","2.1.4 神经元的几何解释","2.1.5 概率视角下的神经元模型","2.1.6 神经元模型的矩阵表示","2.1.7 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.1-神经元的数学模型.html"},"7":{"title":"1.3 微积分与优化基础","aliases":[],"headers":["1.3.1 偏导数与链式法则","1.3.2 多元函数梯度与Hessian","1.3.3 梯度下降与凸优化基础"],"tags":[],"path":"第1章-数学基础/1.3-微积分与优化基础.html"},"8":{"title":"1.2 概率论与统计","aliases":[],"headers":["1.2.1 随机变量与概率分布","1.2.2 期望、方差与协方差","1.2.3 高斯分布与多元高斯","1.2.4 KL散度与交叉熵","1.2.5 统计推断与参数估计","1.2.6 大数定律与中心极限定理","1.2.7 假设检验与模型评估"],"tags":[],"path":"第1章-数学基础/1.2-概率论与统计.html"},"9":{"title":"1.1 线性代数与张量运算","aliases":[],"headers":["1.1.1 线性代数的核心地位","1.1.2 基本运算与运算规则","1.1.3 张量的表示与运算","1.1.4 特征空间的几何直觉","1.1.5 特征值与特征向量","1.1.6 奇异值与低秩近似","1.1.7 正交性与正交矩阵","1.1.8 矩阵范数与度量","1.1.9 矩阵分解的深化理解","1.1.10 Kronecker积与向量化","1.1.11 张量网络与高维运算","1.1.12 批处理与并行计算的张量视图","1.1.13 计算复杂度与内存效率"],"tags":[],"path":"第1章-数学基础/1.1-线性代数与张量运算.html"},"13":{"title":"2.3 前向传播的数学本质","aliases":[],"headers":["2.3.1 引言：从计算到函数逼近","2.3.2 复合函数视角下的前向传播","2.3.3 信息流动的几何描述","2.3.4 计算图的表示与实现","2.3.5 前向传播的计算复杂度","2.3.6 前向传播的并行性","2.3.7 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.3-前向传播的数学本质.html"},"15":{"title":"2.2 神经网络的矩阵形式","aliases":[],"headers":["2.2.1 引言：从神经元到层","2.2.2 单层网络的矩阵表示","2.2.3 多层网络的矩阵形式"],"tags":[],"path":"第2章-前馈网络数学/2.2-神经网络的矩阵形式.html"},"21":{"title":"大模型中的数学","aliases":[],"headers":["Wir müssen wissen, wir werden wissen --David Hilbert","我们必须知道，我们终将知道 --大卫·希尔伯特","第一章 线性代数基础","第二章 前馈网络数学","第三章 激活函数与非线性数学"],"tags":[],"path":"大模型中的数学.html"},"22":{"title":"3.3 梯度饱和与梯度爆炸的数学根源","aliases":[],"headers":["3.3.1 梯度饱和的数学机制","饱和现象的数学定义","Sigmoid函数的饱和区域分析","Tanh函数的饱和特性","饱和效应的累积放大机制","3.3.2 梯度爆炸的数学机制","梯度爆炸的现象描述","简单RNN的梯度分析","Transformer中的梯度爆炸机制","残差连接的数学作用","梯度爆炸的阈值与裁剪策略","3.3.3 梯度流与信息保持","有效秩与信息传递","奇异值与梯度范数分析","初始化与梯度流","与注意力机制的梯度流","3.3.4 归一化技术与梯度稳定","批归一化的数学原理","层归一化的数学原理","层归一化与Transformer的配合","RMSNorm与计算优化","归一化技术的梯度稳定机制","3.3.5 激活函数设计的数学原则","避免梯度饱和区域","保持梯度流稳定性","提供足够的非线性表达能力","计算效率","与下游任务的契合","与注意力机制的设计协同","3.3.6本节小结"],"tags":[],"path":"第3章-激活函数与非线性数学/3.3-梯度饱和与梯度爆炸的数学根源.html"},"23":{"title":"3.2 导数推导与梯度特性","aliases":[],"headers":["3.2.1 Sigmoid函数的导数与性质","Sigmoid函数的数学定义","导数推导与自化性质","高阶导数与泰勒展开","几何解释与梯度特性","3.2.2 Tanh函数的导数与性质","Tanh函数的数学定义与性质","与Sigmoid的数学关系","导数推导与性质分析","3.2.3 ReLU函数族的导数与性质","ReLU函数的定义与导数"],"tags":[],"path":"第3章-激活函数与非线性数学/3.2-导数推导与梯度特性.html"},"24":{"title":"3.1 激活函数的数学角色","aliases":[],"headers":["3.1.1 非线性的数学必要性","线性映射的数学结构","线性模型表达能力的根本局限","非线性变换打破表达瓶颈","3.1.2 激活函数作为函数逼近的基石","通用逼近定理的数学表述","局部基函数与函数重构","深度与宽度的数学权衡","3.1.3 概率视角下的激活函数","Sigmoid函数与伯努利分布","Softmax函数与多项式分布","Gumbel-Softmax与离散分布采样","3.1.4 信息论视角与激活函数","激活函数的信息压缩作用","激活函数与互信息最大化","激活函数的信息几何视角","3.1.5 激活函数与Transformer架构的数学协同","前馈网络中的激活函数选择","注意力机制与激活函数的协同","激活函数与位置编码的配合","3.1.6 本节小结"],"tags":[],"path":"第3章-激活函数与非线性数学/3.1-激活函数的数学角色.html"},"25":{"title":"2.4 反向传播梯度推导","aliases":[],"headers":["2.4.1 引言：为什么需要反向传播","2.4.2 链式法则的矩阵形式","2.4.3 误差信号的传播","2.4.4 参数梯度的完整推导","2.4.5 批量处理的梯度计算","2.4.6 计算复杂度分析","2.4.7 反向传播与自动微分的关系","2.4.8 数值稳定性与实现细节","2.4.9 本节小结"],"tags":[],"path":"第2章-前馈网络数学/2.4-反向传播梯度推导.html"},"27":{"title":"大模型中的数学","aliases":[],"headers":["Wir müssen wissen, wir werden wissen --David Hilbert","我们必须知道，我们终将知道 --大卫·希尔伯特","第一章 线性代数基础","第二章 前馈网络数学","第三章 激活函数与非线性数学"],"tags":[],"path":"index.html"}},"dirtCount":16,"index":[["|f",{"5":{"15":1}}],["|",{"5":{"15":3,"23":4}}],["|w|",{"5":{"23":2}}],["+",{"5":{"15":20,"23":26}}],["zj",{"5":{"15":2}}],["z",{"5":{"15":11,"23":81}}],["z^",{"5":{"15":4}}],["zig",{"5":{"23":2}}],["zag",{"5":{"23":2}}],["zk",{"5":{"23":1}}],["qr分解将矩阵分解为正交矩阵",{"5":{"9":1}}],["qr分解将矩阵分解为正交矩阵和上三角矩阵的乘积",{"5":{"9":1}}],["qr分解在线性最小二乘问题中有重要应用",{"5":{"9":2}}],["qr分解有多种计算方法",{"5":{"9":2}}],["qr分解",{"5":{"9":2}}],["qk点积的尺度问题",{"5":{"22":2}}],["quad",{"5":{"15":4,"23":6}}],["query和key向量的点积期望方差为",{"5":{"22":2}}],["query",{"5":{"24":2}}],["^t",{"5":{"15":6,"23":4}}],["^2",{"5":{"15":2,"23":8}}],["^",{"5":{"15":30}}],["^c",{"5":{"23":10}}],["=",{"5":{"15":38,"23":98}}],["y",{"5":{"15":4,"23":24}}],["young",{"5":{"9":2}}],["yk",{"5":{"23":1}}],["yi",{"5":{"23":1}}],["$$",{"5":{"15":2}}],["原因在于",{"5":{"15":2}}],["原假设通常是我们想要拒绝的假设",{"5":{"8":2}}],["原始梯度信息仍然可以部分保留",{"5":{"22":2}}],["原则3",{"5":{"22":10}}],["原点保持不动",{"5":{"24":2}}],["卷积层",{"5":{"15":2}}],["卷积运算和批处理操作",{"5":{"9":2}}],["现代深度学习框架的优化器",{"5":{"15":2}}],["现代神经网络通常由以下类型的层组成",{"5":{"15":2}}],["现代gpu和tpu等硬件加速器擅长并行计算",{"5":{"13":2}}],["现代gpu的并行架构非常适合这种批处理计算",{"5":{"9":2}}],["现代gpu架构专门优化了张量运算",{"5":{"9":2}}],["现代transformer",{"5":{"24":2}}],["右上",{"5":{"15":2}}],["右下",{"5":{"15":2}}],["左下",{"5":{"15":2}}],["左上",{"5":{"15":2}}],["形状相同的三阶张量",{"5":{"7":1}}],["形式在数学上非常优美",{"5":{"23":2}}],["形成多层网络结构",{"5":{"15":2}}],["形成更复杂的token交互模式",{"5":{"24":2}}],["部分",{"5":{"13":1,"22":2}}],["寄存器",{"5":{"13":1}}],["流多处理器",{"5":{"13":1}}],["流水线并行",{"5":{"9":2}}],["量级",{"5":{"13":1}}],["适用场景",{"5":{"13":1,"25":1}}],["适用于rnn",{"5":{"22":1}}],["适用于transformer",{"5":{"22":1}}],["适当标准化后",{"5":{"8":2}}],["适当的权重初始化",{"5":{"25":1}}],["存储中间值",{"5":{"13":1,"25":1}}],["存在",{"5":{"13":1}}],["存在介于和之间",{"5":{"13":1}}],["存在正交矩阵",{"5":{"9":2}}],["存在大量的局部最小值",{"5":{"7":2}}],["存在一个单隐藏层神经网络",{"5":{"24":2}}],["额外",{"5":{"13":1,"25":1}}],["划分为两个区域",{"5":{"13":1}}],["划分为两个子向量",{"5":{"8":1}}],["之间",{"5":{"13":1}}],["之间的夹角",{"5":{"7":1}}],["之间的差异",{"5":{"7":1}}],["之间的数学协同关系",{"5":{"24":2}}],["之间的乘积交互",{"5":{"24":1}}],["之间的互信息定义为",{"5":{"24":1}}],["之间的依赖程度",{"5":{"24":1}}],["介于",{"5":{"13":1}}],["逼近目标函数",{"5":{"13":1}}],["下一节我们将讨论损失函数与优化目标",{"5":{"13":2}}],["下一个词的条件概率分布的期望特性",{"5":{"8":2}}],["下一个词的选择遵循某种概率分布",{"5":{"8":2}}],["下一层的输入分布会发生偏移",{"5":{"23":2}}],["同一batch内",{"5":{"13":2}}],["同时理解评估结果的统计显著性",{"5":{"8":2}}],["同时",{"5":{"5":2,"24":2}}],["同时实现了通道间的归一化",{"5":{"22":2}}],["同时归一化稳定了分支路径的激活分布",{"5":{"22":2}}],["同时引入有益的非线性",{"5":{"22":2}}],["同时保留了输入的相对顺序信息",{"5":{"23":2}}],["同时使得注意力机制能够学习非线性的token交互模式",{"5":{"24":2}}],["同时对每个维度施加了非线性变换",{"5":{"24":2}}],["同时最小化",{"5":{"24":2}}],["同时相邻位置之间具有平滑的插值性质",{"5":{"24":2}}],["同时通过非线性组合创造了位置",{"5":{"24":2}}],["同上",{"5":{"22":1}}],["总计约",{"5":{"13":1}}],["总计约​次flops",{"5":{"13":1}}],["总flops为各层flops之和",{"5":{"13":2}}],["浮点运算数",{"5":{"13":2}}],["浮点运算数flops",{"5":{"13":2}}],["两层网络的异或解",{"5":{"15":2}}],["两种",{"5":{"13":2}}],["两个神经元的组合可以解决异或问题",{"5":{"15":2}}],["两个向量",{"5":{"9":1}}],["两个向量和如果满足",{"5":{"9":1}}],["两个三阶张量",{"5":{"9":1}}],["两个三阶张量和在第二和第三维度上的收缩产生一个的矩阵",{"5":{"9":1}}],["两个矩阵",{"5":{"9":1}}],["两个矩阵和的乘积",{"5":{"9":1}}],["两个二阶张量",{"5":{"9":2}}],["两个维度相同的向量可以逐分量相加",{"5":{"9":2}}],["两个模型没有差异",{"5":{"8":2}}],["两个概率分布之间的",{"5":{"24":2}}],["两个随机变量和之间的互信息定义为",{"5":{"24":1}}],["两个随机变量",{"5":{"24":1}}],["两者的输出再通过某种方式组合",{"5":{"15":2}}],["两者的组合使得网络能够学习复杂的非线性决策边界",{"5":{"13":2}}],["两者都是函数值与其",{"5":{"23":2}}],["两者都存在梯度饱和问题",{"5":{"23":2}}],["两者都涉及概率加权的信号传递",{"5":{"24":2}}],["拓扑排序保证每个节点在其所有前驱节点之后",{"5":{"13":2}}],["遍历",{"5":{"13":2}}],["例2",{"5":{"13":4}}],["例如将一个",{"5":{"9":1}}],["例如将一个的二维张量重塑为的三维张量",{"5":{"9":1}}],["例如",{"5":{"8":4,"9":20,"15":2,"24":1}}],["例如在sigmoid函数的中间区域",{"5":{"22":2}}],["例如​",{"5":{"24":1}}],["高级语义特征",{"5":{"13":2}}],["高斯过程的一个优势是它提供了预测的不确定性估计",{"5":{"8":2}}],["高斯过程回归在超参数优化",{"5":{"8":2}}],["高斯过程是函数的先验分布",{"5":{"8":2}}],["高斯输出分布用于回归任务",{"5":{"8":2}}],["高斯分布通常作为潜在空间的先验分布",{"5":{"8":2}}],["高斯分布可用于建模响应的不确定性",{"5":{"8":2}}],["高斯分布扮演着多种重要角色",{"5":{"8":2}}],["高斯分布具有最大的熵",{"5":{"8":2}}],["高斯分布的共轭先验还是高斯分布",{"5":{"8":2}}],["高斯分布的熵",{"5":{"8":2}}],["高斯分布的偏度为0",{"5":{"8":2}}],["高斯分布的众数",{"5":{"8":2}}],["高斯分布的期望为",{"5":{"8":2}}],["高斯分布在卷积",{"5":{"8":2}}],["高斯分布之所以如此重要",{"5":{"8":2}}],["高斯分布",{"5":{"8":2}}],["高斯分布与多元高斯",{"2":{"8":1},"5":{"8":1}}],["高斯",{"5":{"7":6}}],["高阶导数与泰勒展开",{"2":{"23":1},"5":{"23":1}}],["高阶导数的复杂结构反映了sigmoid非线性变换的深层特性",{"5":{"23":2}}],["高频成分编码细粒度的位置关系",{"5":{"24":2}}],["高频",{"5":{"24":2}}],["靠近输出",{"5":{"13":2}}],["靠近输入",{"5":{"13":2}}],["较高层",{"5":{"13":2}}],["较低层",{"5":{"13":2}}],["较小时",{"5":{"22":3}}],["较小的确保稳定性但可能限制学习速度",{"5":{"22":1}}],["较小的值表示激活函数具有更好的抗饱和性能",{"5":{"22":1}}],["较小的值表示激活函数的导数接近1",{"5":{"22":1}}],["较小的",{"5":{"22":3}}],["较小的奇异值对应于",{"5":{"24":2}}],["较大时",{"5":{"22":1}}],["较大的允许快速学习但可能容忍不稳定",{"5":{"22":1}}],["较大的",{"5":{"22":1}}],["较大的奇异值对应于",{"5":{"24":2}}],["纹理",{"5":{"13":2}}],["低级特征",{"5":{"13":2}}],["低秩更新是一种核心技术",{"5":{"9":2}}],["低秩近似技术有着广泛的应用",{"5":{"9":2}}],["低秩近似是奇异值分解最重要的应用之一",{"5":{"9":2}}],["低频成分编码粗粒度的位置关系",{"5":{"24":2}}],["低频",{"5":{"24":2}}],["操作",{"5":{"13":2,"24":2}}],["切割",{"5":{"13":2}}],["先将输入向量旋转到",{"5":{"13":2}}],["先通过pca进行初步降维",{"5":{"9":2}}],["先通过前向传播保存必要的中间值",{"5":{"25":2}}],["执行另一个旋转",{"5":{"13":2}}],["执行缩放",{"5":{"13":2}}],["执行旋转",{"5":{"13":2}}],["执行完整的前向传播",{"5":{"25":2}}],["网络输出",{"5":{"13":1}}],["网络输出是输入的线性函数",{"5":{"13":1}}],["网络通过前向传播计算输出",{"5":{"13":2}}],["网络架构设计之间的深层联系",{"5":{"22":2}}],["网络的学习目标",{"5":{"13":2}}],["网络的表达能力始终局限于输入的线性变换",{"5":{"24":2}}],["网络的原始输出",{"5":{"24":2}}],["网络包含一个隐藏层",{"5":{"24":2}}],["网络现在是分段线性或非线性的",{"5":{"24":1}}],["网络现在是",{"5":{"24":1}}],["网络参数​",{"5":{"25":1}}],["网络参数",{"5":{"25":1}}],["依此类推",{"5":{"13":2}}],["依概率收敛于",{"5":{"8":1}}],["递推地",{"5":{"13":2}}],["按列分割",{"5":{"13":1}}],["按列",{"5":{"13":1}}],["按列优先的顺序排列矩阵元素",{"5":{"9":2}}],["按后向顺序计算可以保证每次计算时所需的依赖值已经就绪",{"5":{"13":2}}],["按拓扑排序遍历可以保证计算每个节点时",{"5":{"13":2}}],["按从输入到输出的顺序依次计算各节点的值",{"5":{"13":2}}],["按奇异值拉伸或压缩各方向",{"5":{"13":2}}],["按元素应用",{"5":{"13":2}}],["按范数裁剪与按值裁剪",{"5":{"22":2}}],["按范数裁剪保持梯度的方向",{"5":{"22":2}}],["按范数裁剪保持了梯度的方向信息",{"5":{"22":2}}],["按范数裁剪更为常用",{"5":{"22":2}}],["按范数裁剪",{"5":{"25":2}}],["按降序排列",{"5":{"22":2}}],["按值裁剪将每个梯度元素裁剪到区间",{"5":{"22":1}}],["按值裁剪将每个梯度元素裁剪到",{"5":{"22":1}}],["按值裁剪",{"5":{"25":2}}],["展现了深度表示学习的数学基础",{"5":{"13":2}}],["空间复杂度",{"5":{"13":1,"25":1}}],["空间按",{"5":{"13":1}}],["空间变换等多个数学视角",{"5":{"13":2}}],["空间中的每个点代表一个数据样本在该空间中的表示",{"5":{"9":2}}],["训练动态和泛化性能奠定理论基础",{"5":{"13":2}}],["训练过程就是使用随机梯度下降等优化算法来最大化这个目标函数",{"5":{"8":2}}],["训练不稳定甚至发散",{"5":{"22":2}}],["训练曲线剧烈振荡无法收敛",{"5":{"22":2}}],["训练越不稳定",{"5":{"22":2}}],["训练迭代的总复杂度",{"5":{"25":2}}],["训练停滞",{"5":{"25":1}}],["信息经过",{"5":{"13":1}}],["信息经过次非线性变换后到达输出层",{"5":{"13":1}}],["信息流动的几何描述",{"2":{"13":1},"5":{"13":1}}],["信息论的基本概念始于熵",{"5":{"8":2}}],["信息论视角与激活函数",{"2":{"24":1},"5":{"24":1}}],["信息论提供了量化信息",{"5":{"24":2}}],["信息瓶颈理论认为",{"5":{"24":2}}],["信息几何是研究概率分布空间几何性质的理论框架",{"5":{"24":2}}],["引言",{"2":{"13":1,"15":1,"25":1},"5":{"13":1,"15":1,"25":1}}],["引入了可学习的参数和连续可微的激活函数",{"5":{"5":2}}],["引入了一定的噪声",{"5":{"22":2}}],["前一层的输出作为后一层的输入",{"5":{"15":2}}],["前向模式",{"5":{"13":1,"25":2}}],["前向模式计算",{"5":{"13":1}}],["前向模式计算​",{"5":{"13":1}}],["前向模式的微分计算",{"5":{"13":2}}],["前向模式与反向模式自动微分的比较",{"5":{"13":1}}],["前向模式与反向模式自动微分的比较理解前向传播的计算复杂度对于评估网络效率",{"5":{"13":1}}],["前向模式与反向模式的比较定义2",{"5":{"25":1}}],["前向模式与反向模式的比较",{"5":{"25":1}}],["前向模式自动微分虽然计算效率较低",{"5":{"13":2}}],["前向模式自动微分",{"5":{"25":1}}],["前向传播具有良好的并行性",{"5":{"13":2}}],["前向传播实现了从输入空间到输出空间的几何变换",{"5":{"13":2}}],["前向传播在数学上是一个复合函数的逐层求值过程",{"5":{"13":2}}],["前向传播对应于计算图的拓扑排序",{"5":{"13":2}}],["前向传播不仅是一个代数计算过程",{"5":{"13":2}}],["前向传播最本质的数学描述是复合函数",{"5":{"13":2}}],["前向传播是这些数学描述在实际运行时的具体执行过程",{"5":{"13":2}}],["前向传播本质上是一个复合函数的求值过程",{"5":{"13":2}}],["前向传播",{"5":{"13":2,"25":3}}],["前向传播的本质是复合函数的求值",{"5":{"13":2}}],["前向传播的矩阵运算形式天然具有良好的并行性",{"5":{"13":2}}],["前向传播的核心数学问题可以概括为",{"5":{"13":2}}],["前向传播的意义远不止于",{"5":{"13":2}}],["前向传播的并行性层次",{"5":{"13":1}}],["前向传播的并行性层次本节从多个数学视角深入分析了前向传播的本质",{"5":{"13":1}}],["前向传播的并行性",{"2":{"13":1},"5":{"13":1}}],["前向传播的计算复杂度",{"2":{"13":1},"5":{"13":1}}],["前向传播的数学本质<",{"5":{"14":1,"21":1,"27":1}}],["前向传播的数学本质",{"0":{"13":1},"4":{"11":1,"13":1},"5":{"13":1,"14":4,"21":4,"27":4}}],["前向传播只是完成了",{"5":{"25":2}}],["前向传播保存的中间值",{"5":{"25":2}}],["前向传播与反向传播的flops比较推论2",{"5":{"25":1}}],["前向传播与反向传播的flops比较",{"5":{"25":1}}],["前向和反向方差稳定",{"5":{"22":1}}],["前馈网络",{"5":{"13":2,"24":4}}],["前馈网络数学",{"2":{"6":1,"14":1,"21":1,"27":1},"4":{"3":1,"5":1,"10":1,"11":1,"13":1,"15":1,"16":1,"25":1},"5":{"6":3,"14":7,"21":9,"27":9}}],["前馈网络中的激活函数决定了信息如何被非线性变换",{"5":{"22":2}}],["前馈网络中的激活函数选择",{"2":{"24":1},"5":{"24":1}}],["前馈扩展维度4d=16384",{"5":{"13":2}}],["前馈神经网络中的全连接层更是由多个权重矩阵堆叠而成",{"5":{"9":2}}],["前层的参数几乎无法接收到关于损失函数的信息",{"5":{"22":2}}],["前层参数保持随机初始化状态",{"5":{"22":2}}],["次加法",{"5":{"13":1}}],["次乘法和",{"5":{"13":2}}],["次非线性变换后到达输出层",{"5":{"13":1}}],["次标量乘法和加法",{"5":{"9":1}}],["次迭代后",{"5":{"7":1}}],["降低到",{"5":{"9":1}}],["测量两个向量方向的相似程度",{"5":{"9":1}}],["测量两个向量之间的直线距离",{"5":{"9":1}}],["欧几里得距离",{"5":{"9":1}}],["欧几里得距离测量两个向量之间的直线距离",{"5":{"9":1}}],["视为被重复",{"5":{"9":1}}],["激活节点",{"5":{"13":2}}],["激活函数为阈值函数",{"5":{"15":6}}],["激活函数负责非线性切割",{"5":{"13":2}}],["激活函数的几何作用",{"5":{"13":2}}],["激活函数的设计需要满足以下几个条件",{"5":{"5":2}}],["激活函数的数学角色",{"0":{"24":1},"2":{"5":1},"4":{"17":1,"24":1},"5":{"5":1,"21":4,"24":1,"27":4}}],["激活函数的数学角色<",{"5":{"21":1,"27":1}}],["激活函数的饱和可以从两个层面理解",{"5":{"22":2}}],["激活函数的计算成本直接影响训练和推理速度",{"5":{"22":2}}],["激活函数的研究仍在继续",{"5":{"22":2}}],["激活函数的导数不应该过大",{"5":{"22":2}}],["激活函数的导数性质是理解神经网络训练动力学的核心数学基础",{"5":{"23":2}}],["激活函数的导数结构直接决定了梯度如何在不同层之间传播",{"5":{"23":2}}],["激活函数的信息压缩作用",{"2":{"24":1},"5":{"24":1}}],["激活函数的信息几何视角",{"2":{"24":1},"5":{"24":1}}],["激活函数的引入打破了这种线性局限",{"5":{"24":2}}],["激活函数的引入彻底改变了这一局面",{"5":{"24":2}}],["激活函数的输出值较高",{"5":{"24":2}}],["激活函数的概率解释与损失函数的信息论基础共同构成了分类任务训练的数学框架",{"5":{"24":2}}],["激活函数的非线性变换正是实现这种信息筛选的机制",{"5":{"24":2}}],["激活函数的非线性变换增加了编码的丰富性",{"5":{"24":2}}],["激活函数的选择与注意力机制的设计紧密相关",{"5":{"22":2}}],["激活函数的选择影响这个下界的紧致程度",{"5":{"24":2}}],["激活函数",{"5":{"5":3,"13":3,"22":1,"24":5,"25":1}}],["激活函数设计的数学原则",{"2":{"22":1},"5":{"22":1}}],["激活函数导数的范数应该接近1",{"5":{"22":2}}],["激活函数需要能够表示复杂的非线性映射",{"5":{"22":2}}],["激活函数作为函数逼近的基石",{"2":{"24":1},"5":{"24":1}}],["激活函数与非线性数学",{"2":{"21":1,"27":1},"4":{"17":1,"18":1,"19":1,"22":1,"23":1,"24":1},"5":{"21":7,"27":7}}],["激活函数与互信息最大化",{"2":{"24":1},"5":{"24":1}}],["激活函数与transformer架构的数学协同",{"2":{"24":1},"5":{"24":1}}],["激活函数与位置编码的配合",{"2":{"24":1},"5":{"24":1}}],["激活函数是神经网络能够拟合复杂非线性函数的核心数学组件",{"5":{"24":2}}],["激活函数将神经元的净输入映射到输出",{"5":{"5":1}}],["激活函数将线性变换的输出通过一个非线性函数进行变换",{"5":{"24":2}}],["激活函数在几何上对仿射变换的结果执行了逐元素的非线性",{"5":{"13":1}}],["激活函数在这一逼近能力中扮演着核心角色",{"5":{"24":2}}],["激活函数在这个过程中扮演关键角色",{"5":{"24":2}}],["激活函数在这个几何框架中扮演着重要角色",{"5":{"24":2}}],["激活函数在transformer架构中的应用体现了深度学习组件之间数学协同的设计哲学",{"5":{"24":2}}],["激活函数构成的函数族在适当条件下可以满足这些要求",{"5":{"24":1}}],["激活函数类型与逼近效率",{"5":{"24":2}}],["激活函数可以被理解为对输入信息的某种非线性编码和变换",{"5":{"24":2}}],["激活函数可以视为一种信息瓶颈",{"5":{"24":2}}],["激活函数通过引入非线性",{"5":{"5":2}}],["激活函数通过引入非线性打破了这一限制",{"5":{"5":2}}],["激活函数通过影响概率分布的形状",{"5":{"24":2}}],["激活函数对这种频率分解信息的处理决定了位置信息如何在网络中传递",{"5":{"24":2}}],["激活分布的稳定化",{"5":{"22":2}}],["激活区",{"5":{"24":2}}],["激活值的内存消耗可能超过参数本身",{"5":{"9":2}}],["激活值溢出",{"5":{"25":1}}],["内存效率不仅与参数数量有关",{"5":{"9":2}}],["又不受单批次大小的限制",{"5":{"9":2}}],["又称为张量点积或广义矩阵乘法",{"5":{"9":2}}],["既保证了一次更新使用的样本数量",{"5":{"9":2}}],["微批处理",{"5":{"9":2}}],["微积分是研究变化率和累积效应的数学分支",{"5":{"7":2}}],["微积分与优化基础<",{"5":{"6":1,"14":1,"21":1,"27":1}}],["微积分与优化基础",{"0":{"7":1},"4":{"2":1,"7":1},"5":{"6":4,"7":1,"14":4,"21":4,"27":4}}],["微分关系导致了导数的简洁表示",{"5":{"23":2}}],["×",{"5":{"9":2}}],["序列长度",{"5":{"9":2}}],["序列长度和嵌入维度",{"5":{"9":2}}],["被分解为多个低阶张量的组合",{"5":{"9":2}}],["被誉为",{"5":{"9":2}}],["查询",{"5":{"9":2}}],["查询矩阵与键矩阵的乘积产生注意力分数",{"5":{"9":2}}],["混合精度利用了现代gpu对低精度运算的专门优化",{"5":{"9":2}}],["混合精度训练可以将内存消耗减半",{"5":{"9":2}}],["混合精度训练是另一种重要的效率优化技术",{"5":{"9":2}}],["混合精度训练使用半精度浮点数进行计算以节省内存和加速",{"5":{"7":2}}],["混合精度训练中的梯度缩放",{"5":{"25":2}}],["混合精度训练",{"5":{"25":3}}],["混合积性质",{"5":{"9":2}}],["混合",{"5":{"22":2}}],["主轴方向",{"5":{"13":2}}],["主成分分析和谱聚类中都有应用",{"5":{"9":2}}],["主题模型的推断和某些贝叶斯神经网络方法",{"5":{"8":2}}],["主要操作",{"5":{"25":1}}],["才能进行这种分解",{"5":{"9":2}}],["带置换矩阵",{"5":{"9":2}}],["核范数正则化可以鼓励解的低秩性质",{"5":{"9":2}}],["核范数是frobenius范数和谱范数之间的折中",{"5":{"9":2}}],["核范数",{"5":{"9":2}}],["核范数和1",{"5":{"9":2}}],["∞",{"5":{"9":2}}],["齐次坐标形式的单层网络",{"5":{"15":2}}],["齐次坐标被广泛用于处理平移",{"5":{"5":2}}],["齐次坐标表示在理论上具有优雅性",{"5":{"15":2}}],["齐次坐标表示的优势在于它将仿射变换转化为单一的矩阵乘法运算",{"5":{"5":2}}],["齐次坐标表示",{"5":{"5":2}}],["齐次性和三角不等式等性质",{"5":{"9":2}}],["齐次性",{"5":{"24":2}}],["傅里叶变换和正交基扩展是信号处理和自然语言处理中的重要工具",{"5":{"9":2}}],["傅里叶变换和微分方程等运算下保持封闭形式",{"5":{"8":2}}],["说明模型的表达能力主要集中在少数方向上",{"5":{"9":2}}],["典型的低秩适配方法如lora",{"5":{"9":2}}],["典型的操作包括矩阵乘法",{"5":{"7":2}}],["用秩不超过k的矩阵对",{"5":{"9":1}}],["用秩不超过k的矩阵对的最佳近似",{"5":{"9":1}}],["用于处理序列数据",{"5":{"15":2}}],["用于处理超大序列或有限gpu内存的情况",{"5":{"9":2}}],["用于比较两个版本",{"5":{"8":2}}],["用于根据样本数据判断关于总体的假设是否成立",{"5":{"8":2}}],["用于建模连续的随机过程",{"5":{"8":2}}],["用于将模型的logits输出转换为概率分布",{"5":{"5":2}}],["用于模拟独立随机变量序列的最大值分布",{"5":{"24":2}}],["用logits表示",{"5":{"24":2}}],["另外",{"5":{"9":2}}],["奇异值为",{"5":{"13":1}}],["奇异值为对角线上特征值的平方根",{"5":{"13":1}}],["奇异值分解揭示了线性变换的几何本质",{"5":{"13":2}}],["奇异值分解定理",{"5":{"13":2}}],["奇异值分解可以用于理解模型的表达能力",{"5":{"9":2}}],["奇异值分解之所以强大",{"5":{"9":2}}],["奇异值分解与特征值分解有密切关系",{"5":{"9":2}}],["奇异值分解",{"5":{"9":2}}],["奇异值与低秩近似",{"2":{"9":1},"5":{"9":1}}],["奇异值与梯度范数分析",{"2":{"22":1},"5":{"22":1}}],["奇异值的数量",{"5":{"22":2}}],["奇异值大于1的方向会导致梯度放大",{"5":{"22":2}}],["奇异值小于1的方向会导致梯度衰减",{"5":{"22":2}}],["奇异值决定了梯度在对应方向上的缩放因子",{"5":{"22":1}}],["奇异值和权重初始化的数学原理",{"5":{"22":2}}],["奇异值",{"5":{"22":1}}],["谱归一化",{"5":{"9":4}}],["谱范数最重要",{"5":{"9":2}}],["谱范数是核心概念",{"5":{"9":2}}],["谱范数在深度学习中有重要应用",{"5":{"9":2}}],["谱范数",{"5":{"9":6}}],["谱半径与特征值的关系",{"5":{"22":2}}],["谱半径是特征值模的最大值",{"5":{"22":2}}],["谱半径等于1是临界状态",{"5":{"22":2}}],["谱半径等于最大奇异值",{"5":{"22":2}}],["谱半径可能大于或小于最大奇异值",{"5":{"22":2}}],["类别分布",{"5":{"8":2}}],["类别为1的条件概率",{"5":{"5":2}}],["类似的稳定性分析有助于理解和解决梯度消失或梯度爆炸问题",{"5":{"9":2}}],["类似地",{"5":{"22":2,"23":2}}],["类的概率估计",{"5":{"24":1}}],["给定输入",{"5":{"13":1,"24":1}}],["给定输入​",{"5":{"13":1}}],["给定输入和其上下文",{"5":{"24":1}}],["给定网络结构和输入",{"5":{"13":2}}],["给定一个方阵",{"5":{"9":2}}],["给定前文的条件下",{"5":{"8":2}}],["遗忘",{"5":{"9":2}}],["零空间",{"5":{"9":2}}],["零阶张量",{"5":{"9":2}}],["零中心输出减少了梯度偏移问题",{"5":{"23":2}}],["零中心输出使得各层的输入分布更加稳定",{"5":{"23":2}}],["零中心版本",{"5":{"23":2}}],["子空间是特征空间中的重要结构",{"5":{"9":2}}],["剪切则是一种保持体积但改变形状的变换",{"5":{"9":2}}],["投影将向量映射到低维子空间",{"5":{"9":2}}],["投影和剪切等",{"5":{"9":2}}],["缩放和平移",{"5":{"13":2}}],["缩放和剪切等基本变换的组合",{"5":{"13":2}}],["缩放和反射",{"5":{"24":2}}],["缩放的倍数就是特征值",{"5":{"9":2}}],["缩放改变向量的模长",{"5":{"9":2}}],["缩放",{"5":{"9":2}}],["缩放等仿射变换",{"5":{"5":2}}],["缩放点积注意力",{"5":{"24":2}}],["余弦相似度",{"5":{"9":1}}],["余弦相似度是最常用的相似度度量",{"5":{"9":2}}],["余弦相似度测量两个向量方向的相似程度",{"5":{"9":1}}],["余弦退火",{"5":{"7":2}}],["过高的维度则可能导致过拟合和计算效率的下降",{"5":{"9":2}}],["过低的维度可能导致表达能力的损失",{"5":{"9":2}}],["过于简单的激活函数可能限制网络的表达能力",{"5":{"22":2}}],["过于简单的激活函数",{"5":{"22":2}}],["过于复杂的激活函数可能难以训练或过拟合",{"5":{"22":2}}],["嵌入维度",{"5":{"9":2}}],["嵌入空间的维度通常远小于词表大小",{"5":{"9":2}}],["嵌入空间通常是密集的",{"5":{"9":2}}],["嵌入层将词汇映射为向量",{"5":{"9":2}}],["语句含义",{"5":{"13":2}}],["语义不同的词则距离较远",{"5":{"9":2}}],["语义相近的词在空间中距离较近",{"5":{"9":2}}],["语义关系",{"5":{"24":2}}],["语音",{"5":{"24":2}}],["语言模型预测下一个词的概率分布为",{"5":{"8":2}}],["语言模型的训练目标是最大化训练语料库的对数似然",{"5":{"8":2}}],["语言模型的训练目标就是学习这个条件概率分布",{"5":{"8":2}}],["语言模型的输出层正是类别分布的一个典型应用",{"5":{"8":2}}],["语言",{"5":{"24":2}}],["逐渐增加直到找到稳定的最大允许值",{"5":{"22":2}}],["逐步揭示非线性变换的必要性和数学价值",{"5":{"24":2}}],["逐步推广到现代深度学习中常用的连续激活神经元",{"5":{"5":2}}],["逐步推导出各层参数的梯度计算公式",{"5":{"25":2}}],["逐步推导出完整的梯度计算公式",{"5":{"25":2}}],["逐元素运算对张量中的每个独立元素应用相同的函数",{"5":{"9":2}}],["逐元素非线性变换",{"5":{"24":2}}],["逐元素应用于向量",{"5":{"24":1}}],["逐元素乘法约为​",{"5":{"25":1}}],["逐元素乘法约为",{"5":{"25":1}}],["逐元素乘法",{"5":{"25":1}}],["逐层计算误差信号和参数梯度",{"5":{"25":2}}],["逐层传播到输入层",{"5":{"25":1}}],["广播规则通常是从最后一个维度开始逐维比较",{"5":{"9":2}}],["广播是张量逐元素运算中的一种自动对齐机制",{"5":{"9":2}}],["广播",{"5":{"9":2}}],["便于进行残差连接和层归一化操作",{"5":{"9":2}}],["便于解释和比较",{"5":{"8":2}}],["经过注意力计算后的输出张量会与输入张量形状相同",{"5":{"9":2}}],["经过线性投影并拆分注意力头后",{"5":{"9":2}}],["经过线性变换",{"5":{"5":1}}],["经过线性变换后",{"5":{"5":1}}],["经过层后",{"5":{"22":1,"23":1}}],["经过",{"5":{"22":1,"23":1}}],["经过sigmoid变换后",{"5":{"24":2}}],["经过激活函数的输出为",{"5":{"24":2}}],["输入节点",{"5":{"13":2}}],["输入节点表示网络的输入数据",{"5":{"13":2}}],["输入张量的形状通常是",{"5":{"9":2}}],["输入到激活函数的线性组合可能直接落在饱和区域",{"5":{"22":1}}],["输入到激活函数的线性组合",{"5":{"22":1}}],["输入梯度和输出梯度满足",{"5":{"22":1}}],["输入梯度",{"5":{"22":1}}],["输入的无限精度被截断为有限范围的输出",{"5":{"24":2}}],["输入数据依次通过每一层的仿射变换和非线性激活",{"5":{"13":2}}],["输入数据通过多层非线性变换",{"5":{"25":2}}],["输入",{"5":{"25":2}}],["输出向量为",{"5":{"15":1}}],["输出向量为​",{"5":{"15":1}}],["输出节点表示网络的最终输出",{"5":{"13":2}}],["输出",{"5":{"9":2,"25":2}}],["输出为",{"5":{"13":1,"15":2}}],["输出为​",{"5":{"13":1}}],["输出为0",{"5":{"5":2}}],["输出为1",{"5":{"5":6}}],["输出饱和和梯度饱和",{"5":{"22":2}}],["输出饱和关注的是激活函数输出值进入平坦区域的物理现象",{"5":{"22":2}}],["输出饱和",{"5":{"22":2}}],["输出饱和是激活函数值域边界的行为",{"5":{"22":2}}],["输出均值为0",{"5":{"23":2}}],["输出总是正的",{"5":{"23":2}}],["输出层的预测分布是一个",{"5":{"8":1}}],["输出层的预测分布是一个维的概率向量",{"5":{"8":1}}],["输出层有个神经元",{"5":{"24":1}}],["输出层有",{"5":{"24":1}}],["输出层误差信号",{"5":{"25":2}}],["输出层误差信号与预测误差成正比",{"5":{"25":1}}],["输出层误差信号与预测误差",{"5":{"25":1}}],["输出层误差",{"5":{"25":2}}],["输出层误差为",{"5":{"25":2}}],["输出层误差简化为预测误差的平均值",{"5":{"25":2}}],["输出层",{"5":{"25":2}}],["输出映射",{"5":{"24":2}}],["输出值均为正且和为1",{"5":{"5":2}}],["输出值较低",{"5":{"24":2}}],["输出分布关于输入的fisher信息矩阵正好是softmax雅可比矩阵与自身转置的乘积",{"5":{"24":1}}],["输出分布",{"5":{"24":1}}],["记作",{"5":{"9":1}}],["记作或",{"5":{"9":1}}],["记为或",{"5":{"9":1}}],["记为​",{"5":{"8":1}}],["记为",{"5":{"8":7,"9":1,"15":2}}],["就可以实现异或逻辑",{"5":{"15":2}}],["就可以实现对大型语言模型的有效微调",{"5":{"9":2}}],["就是典型的逐元素运算",{"5":{"9":1}}],["就是保留前k个最大的奇异值及其对应的奇异向量",{"5":{"9":2}}],["就依赖于这种思想",{"5":{"9":2}}],["就将梯度向量乘以一个缩放因子使其范数等于阈值",{"5":{"9":2}}],["就近似服从正态分布",{"5":{"8":2}}],["就具有通用逼近能力",{"5":{"24":2}}],["向量的乘积需要",{"5":{"9":1}}],["向量加法满足交换律和结合律",{"5":{"9":1}}],["向量加法是逐分量",{"5":{"9":2}}],["向量乘积",{"5":{"9":2}}],["向量化操作与kronecker积结合可以简化矩阵方程的表示",{"5":{"9":2}}],["向量化",{"5":{"9":2}}],["向量距离和相似度是度量特征空间中样本关系的基本工具",{"5":{"9":2}}],["向量",{"5":{"9":2}}],["向量是线性空间中最基本的元素",{"5":{"9":2}}],["向量链式法则",{"5":{"25":2}}],["转置运算满足",{"5":{"9":1}}],["转置运算满足和",{"5":{"9":1}}],["转置性质",{"5":{"9":2}}],["转置",{"5":{"9":2}}],["转置操作也经常用于梯度的反向传播计算",{"5":{"9":2}}],["转置和求逆等",{"5":{"9":2}}],["转换为标准高斯随机变量",{"5":{"8":1}}],["转换为有效的类别概率分布",{"5":{"8":2}}],["转换为后验概率的过程",{"5":{"24":2}}],["转换为显式",{"5":{"24":2}}],["转换为概率",{"5":{"24":1}}],["几乎所有的计算都在张量之上进行",{"5":{"9":2}}],["几何上",{"5":{"7":2}}],["几何解释与梯度特性",{"2":{"23":1},"5":{"23":1}}],["行和",{"5":{"13":1}}],["行仅依赖于",{"5":{"13":1}}],["行列式非零",{"5":{"9":2}}],["行数和列数",{"5":{"9":2}}],["行为",{"5":{"5":2,"23":2}}],["行为样本",{"5":{"25":1}}],["值的交互",{"5":{"9":2}}],["值投影都是通过矩阵乘法实现的",{"5":{"9":2}}],["值得注意的是",{"5":{"5":2}}],["值表示激活函数具有更好的抗饱和性能",{"5":{"22":1}}],["值表示激活函数的导数接近1",{"5":{"22":1}}],["值域为",{"5":{"23":4}}],["值即可",{"5":{"23":1}}],["键和值张量",{"5":{"9":2}}],["键",{"5":{"9":4}}],["批处理相当于将多个独立的矩阵运算堆叠成一个大的张量运算",{"5":{"9":2}}],["批处理的数学优势来自于矩阵运算的并行性",{"5":{"9":2}}],["批处理是在现有维度之外增加一个新的批处理维度",{"5":{"9":2}}],["批处理",{"5":{"9":2}}],["批处理与并行计算的张量视图",{"2":{"9":1},"5":{"9":1}}],["批量归一化对每一层的输入进行标准化",{"5":{"8":2}}],["批量归一化",{"5":{"8":2}}],["批量处理还提供了隐式的正则化效果",{"5":{"5":2}}],["批量处理",{"5":{"5":2}}],["批量处理的梯度计算",{"2":{"25":1},"5":{"25":1}}],["批量前向传播",{"5":{"25":2}}],["批量误差信号",{"5":{"25":2}}],["批量损失通常定义为各样本损失的平均值",{"5":{"25":2}}],["批量损失对净输入的梯度为",{"5":{"25":2}}],["批量权重梯度",{"5":{"25":2}}],["批量情况下",{"5":{"25":4}}],["批量偏置梯度",{"5":{"25":2}}],["批量标签​",{"5":{"25":1}}],["批量标签",{"5":{"25":1}}],["批归一化",{"5":{"22":7,"25":1}}],["批归一化的数学原理",{"2":{"22":1},"5":{"22":1}}],["批归一化的核心思想是规范化每层的输入分布",{"5":{"22":2}}],["批归一化的梯度计算",{"5":{"22":2}}],["批归一化的反向传播需要计算四个梯度",{"5":{"22":2}}],["批归一化定义为",{"5":{"22":2}}],["批归一化将激活值的分布规范化到均值为0",{"5":{"22":2}}],["批归一化通常应用于通道维度",{"5":{"22":2}}],["批归一化对每个通道独立计算统计量",{"5":{"22":2}}],["批归一化在卷积网络中的应用",{"5":{"22":2}}],["批归一化在这些场景中会遇到统计量估计不稳定的问题",{"5":{"22":2}}],["张量可重排为",{"5":{"9":1}}],["张量可重排为的张量",{"5":{"9":1}}],["张量并行",{"5":{"9":2}}],["张量运算的高效实现是现代深度学习框架的核心能力之一",{"5":{"9":2}}],["张量运算的基本操作包括逐元素运算",{"5":{"9":2}}],["张量网络在模型压缩和高效推理中有着重要的应用价值",{"5":{"9":2}}],["张量网络的思维方式有助于理解复杂的计算结构",{"5":{"9":2}}],["张量网络的核心思想是用多个小张量的乘积之和来表示一个巨大的高维张量",{"5":{"9":2}}],["张量网络是用低维张量通过网络结构表示高维张量的方法",{"5":{"9":2}}],["张量网络与高维运算",{"2":{"9":1},"5":{"9":1}}],["张量分解在模型压缩",{"5":{"9":2}}],["张量分解是处理高维张量的重要技术",{"5":{"9":2}}],["张量收缩指定两个张量的若干维度进行配对相乘并求和",{"5":{"9":2}}],["张量收缩是矩阵乘法在高维张量上的推广",{"5":{"9":2}}],["张量的维度通常具有明确的语义含义",{"5":{"9":2}}],["张量的表示与运算",{"2":{"9":1},"5":{"9":1}}],["张量是多维数组在数学上的抽象表示",{"5":{"9":2}}],["张量是多维数组的自然推广",{"5":{"9":2}}],["张量是最基本的数据结构",{"5":{"9":2}}],["找到一个主要的模式并紧密地拟合它",{"5":{"8":1}}],["覆盖",{"5":{"8":1}}],["鼓励权重矩阵的列向量相互正交",{"5":{"9":2}}],["鼓励",{"5":{"8":1}}],["及其概率分布",{"5":{"8":1}}],["处求值",{"5":{"8":1}}],["处的梯度定义为",{"5":{"7":1}}],["处的偏导数定义为",{"5":{"7":1}}],["处于激活区域",{"5":{"22":2}}],["处于深度饱和状态",{"5":{"22":2}}],["处于充分激活状态",{"5":{"22":2}}],["处",{"5":{"23":3}}],["处取得",{"5":{"23":2}}],["处有拐点",{"5":{"23":1}}],["处有",{"5":{"23":1}}],["处理编码向量",{"5":{"24":2}}],["倾向于反向变化",{"5":{"8":1}}],["倾向于同向变化",{"5":{"8":1}}],["独立",{"5":{"8":1}}],["独立于样本量",{"5":{"8":2}}],["独立时互信息为0",{"5":{"8":1}}],["独立时互信息为零",{"5":{"24":1}}],["维度的选择是一个重要的超参数",{"5":{"9":2}}],["维随机向量",{"5":{"8":2}}],["维的概率向量",{"5":{"8":1}}],["维向量",{"5":{"5":1,"9":1}}],["维向量映射到概率单纯形",{"5":{"24":1}}],["取特定值",{"5":{"8":1}}],["任何高斯随机变量都可以通过标准化变换",{"5":{"8":1}}],["任何高斯随机变量都可以通过标准化变换转换为标准高斯随机变量",{"5":{"8":1}}],["任何局部最优解都是全局最优解",{"5":{"7":2}}],["任何超平面",{"5":{"5":1}}],["任何超平面都只能将平面划分为两个半平面",{"5":{"5":1}}],["任务完成率等指标",{"5":{"8":2}}],["任务契合",{"5":{"22":2}}],["任意矩阵",{"5":{"13":1}}],["任意矩阵可以分解为",{"5":{"13":1}}],["任意非常数有界连续函数",{"5":{"24":2}}],["置换检验在比较两个模型在特定测试集上的性能时很有用",{"5":{"8":2}}],["置换检验通过随机打乱标签来生成置换样本",{"5":{"8":2}}],["置换检验",{"5":{"8":2}}],["置信区间比p值提供更多信息",{"5":{"8":2}}],["需要大量gpu并行计算才能在合理时间内完成",{"5":{"13":2}}],["需要",{"5":{"9":1,"22":1}}],["需要使用适当的多重比较校正方法",{"5":{"8":2}}],["需要注意的是",{"5":{"8":2}}],["需要仔细分析",{"5":{"22":2}}],["需要检测梯度爆炸并采取应对措施",{"5":{"22":2}}],["需要​",{"5":{"22":1}}],["需要在数学特性和计算效率之间进行权衡",{"5":{"22":2}}],["需要一种中间表示来桥接理论模型和计算程序",{"5":{"13":2}}],["需要一条非线性的决策边界",{"5":{"24":2}}],["需要特别注意数值稳定性问题",{"5":{"25":2}}],["至少有一个被拒绝的概率也会大大增加",{"5":{"8":2}}],["还与激活值的存储需求有关",{"5":{"9":2}}],["还给出了差异大小的可能范围",{"5":{"8":2}}],["还可以从概率论的角度进行分析",{"5":{"5":2}}],["报告效应量可以帮助读者理解改进的实际意义",{"5":{"8":2}}],["样本级并行",{"5":{"13":3}}],["样本量很小时",{"5":{"8":2}}],["样本量很大时",{"5":{"8":2}}],["样本均值趋向于随机变量的期望",{"5":{"8":2}}],["样本可以表示为确定函数",{"5":{"24":1}}],["样本",{"5":{"24":1}}],["重塑操作改变张量的形状但不改变其包含的数据元素",{"5":{"9":2}}],["重塑",{"5":{"9":2}}],["重要",{"5":{"8":2}}],["重复多次",{"5":{"8":2}}],["实现模型并行",{"5":{"13":2}}],["实数",{"5":{"9":2}}],["实际的神经网络由大量神经元组成",{"5":{"15":2}}],["实际上是一个巨大的查找矩阵",{"5":{"9":2}}],["实际显著",{"5":{"8":2}}],["实际参与传播的神经元数量约为​​",{"5":{"22":1}}],["实际参与传播的神经元数量约为",{"5":{"22":1}}],["实践表明深度学习模型通常能够找到性能不错的解",{"5":{"7":2}}],["实践中常用的策略是从较小的值",{"5":{"22":2}}],["实践中通常需要仔细的学习率调度",{"5":{"22":2}}],["实验表明",{"5":{"22":2}}],["统计显著",{"5":{"8":2}}],["统计推断的思想贯穿于模型训练",{"5":{"8":2}}],["统计推断是利用样本数据对总体特征进行推断的过程",{"5":{"8":2}}],["统计推断与参数估计",{"2":{"8":1},"5":{"8":1}}],["统计量来源",{"5":{"22":1}}],["配对t检验可以判断两个模型性能的差异是否显著",{"5":{"8":2}}],["配对t检验是比较两个相关样本均值的常用方法",{"5":{"8":2}}],["备择假设是我们想要支持的假设",{"5":{"8":2}}],["验证改进措施的有效性以及检测数据中的系统性偏差",{"5":{"8":2}}],["验证集越大",{"5":{"8":2}}],["该层定义的映射",{"5":{"13":1}}],["该层定义的映射​为",{"5":{"13":1}}],["该指标是总体性能的一个估计",{"5":{"8":2}}],["该批量内样本的均值和方差是整体数据均值和方差的良好估计",{"5":{"8":2}}],["该位置的交叉熵损失为",{"5":{"8":2}}],["该表示保留了位置信息",{"5":{"24":2}}],["该隐藏层有个神经元",{"5":{"24":1}}],["该隐藏层有",{"5":{"24":1}}],["随着层数增加",{"5":{"13":2}}],["随着批量大小的增加",{"5":{"8":2}}],["随着阶数增加",{"5":{"23":2}}],["随机采样策略的设计",{"5":{"8":2}}],["随机变量分为离散随机变量和连续随机变量",{"5":{"8":2}}],["随机变量是样本空间到实数集的映射",{"5":{"8":2}}],["随机变量与概率分布",{"2":{"8":1},"5":{"8":1}}],["随机梯度下降的隐式正则化效应",{"5":{"7":2}}],["随",{"5":{"22":2}}],["弱大数定律保证样本均值以概率收敛于期望",{"5":{"8":2}}],["指出",{"5":{"8":2}}],["指数分布的期望为",{"5":{"8":1}}],["指数分布的期望为​",{"5":{"8":1}}],["指数分布在语言模型中的应用包括描述某些随机过程的到达时间间隔",{"5":{"8":2}}],["指数分布",{"5":{"8":2}}],["指数衰减",{"5":{"7":2,"22":1}}],["指数增长",{"5":{"22":1}}],["帮助我们理解模型性能评估的可靠性",{"5":{"8":2}}],["来自矩阵乘法",{"5":{"13":1}}],["来逼近复杂的后验分布",{"5":{"8":1}}],["来编码来自分布",{"5":{"8":1}}],["来控制总体错误率",{"5":{"8":2}}],["来近似后验分布",{"5":{"8":2}}],["来近似",{"5":{"8":2}}],["来组织偏导数",{"5":{"7":2}}],["来稳定训练",{"5":{"22":2}}],["来保持激活的稳定性",{"5":{"22":1}}],["来估计",{"5":{"24":1}}],["共轭先验是使后验分布与先验分布同分布族的先验选择",{"5":{"8":2}}],["共轭梯度法利用hessian的信息来加速收敛",{"5":{"7":2}}],["贝叶斯方法可用于模型选择",{"5":{"8":2}}],["贝叶斯估计使用后验分布进行预测",{"5":{"8":2}}],["贝叶斯估计是另一种参数估计方法",{"5":{"8":2}}],["贝叶斯估计和期望传播等统计推断方法",{"5":{"8":2}}],["罗下界",{"5":{"8":2}}],["罗森布拉特",{"5":{"5":2}}],["费希尔信息矩阵逆为方差的正态分布",{"5":{"8":2}}],["服从一维高斯分布",{"5":{"8":1}}],["服从多元高斯分布",{"5":{"8":1}}],["服从以真实参数为均值",{"5":{"8":2}}],["服从伯努利分布",{"5":{"5":1}}],["渐近正态的",{"5":{"8":2}}],["收敛于真实参数值",{"5":{"8":2}}],["似然函数定义为",{"5":{"8":2}}],["于某个概率分布",{"5":{"8":2}}],["提高泛化能力",{"5":{"8":2}}],["提出的感知机",{"5":{"5":2}}],["提供足够的非线性表达能力",{"2":{"22":1},"5":{"22":1}}],["提供了自适应的激活特性",{"5":{"22":1}}],["提供了从",{"5":{"24":1}}],["替换为软标签",{"5":{"8":2}}],["operations",{"5":{"13":2}}],["one",{"5":{"8":2}}],["order",{"5":{"13":2}}],["orthant",{"5":{"13":2}}],["or",{"5":{"5":4}}],["odot",{"5":{"23":2}}],["odds",{"5":{"24":2}}],["交叉熵损失相对于模型输出",{"5":{"8":2}}],["交叉熵损失的优势在于它直接与对数似然相关",{"5":{"8":2}}],["交叉熵损失是最常用的损失函数",{"5":{"8":2}}],["交叉熵损失具有清晰的概率解释",{"5":{"24":2}}],["交叉熵损失",{"5":{"24":2,"25":4}}],["交叉熵可以分解为熵与kl散度之和",{"5":{"8":2}}],["交叉熵定义为",{"5":{"8":2}}],["交叉熵与kl散度密切相关",{"5":{"8":2}}],["确保比较的公平性",{"5":{"8":2}}],["确保新策略不会偏离旧策略太远",{"5":{"8":2}}],["确保了梯度可以直接通过恒等映射传递",{"5":{"22":2}}],["确保了主路径",{"5":{"22":2}}],["确保位置信息在深层网络中稳定传递",{"5":{"22":2}}],["确保稳定性但可能限制学习速度",{"5":{"22":1}}],["确保参数更新不会过大",{"5":{"25":2}}],["确定在原假设下观察到该统计量或更极端情况的概率",{"5":{"8":2}}],["确定性高",{"5":{"8":2}}],["严格来说不是真正的距离",{"5":{"8":2}}],["严格凸函数至多有一个全局最小值点",{"5":{"7":2}}],["严格凸函数是凸性更强的形式",{"5":{"7":2}}],["距离",{"5":{"8":2,"24":2}}],["互信息可用于分析不同位置或不同层之间的信息流动",{"5":{"8":2}}],["互信息始终非负",{"5":{"8":2}}],["互信息",{"5":{"8":2,"24":2}}],["互信息是非负的",{"5":{"24":2}}],["互信息的下界估计",{"5":{"24":2}}],["互信息衡量的是知道后关于的信息量",{"5":{"24":1}}],["互信息衡量的是知道",{"5":{"24":1}}],["j​",{"5":{"15":1}}],["j​var",{"5":{"15":1}}],["ji",{"5":{"15":8}}],["j",{"5":{"15":7,"23":39}}],["j=1",{"5":{"15":2,"23":6}}],["joint",{"5":{"8":2}}],["jacobian",{"5":{"7":2,"13":2}}],["联合熵",{"5":{"8":2}}],["某些特定方向",{"5":{"9":2}}],["某些语言模型变体使用高斯混合模型来表示输出分布",{"5":{"8":2}}],["某个",{"5":{"8":1}}],["某个的概率为1",{"5":{"8":1}}],["尖锐",{"5":{"8":2}}],["分割到多个计算设备上",{"5":{"13":2}}],["分片",{"5":{"13":2}}],["分解则将张量表示为一个核心张量与多个因子矩阵在各模上的乘积",{"5":{"9":2}}],["分解将一个张量表示为若干个秩一张量的和",{"5":{"9":2}}],["分解",{"5":{"9":4}}],["分布用pdf表示",{"5":{"8":2}}],["分布越",{"5":{"8":4}}],["分布式训练",{"5":{"22":2}}],["分别为第",{"5":{"15":1}}],["分别是",{"5":{"22":1}}],["分别是输入和输出的维度",{"5":{"22":1}}],["分别编码粗粒度和细粒度的位置关系",{"5":{"24":2}}],["分别编码",{"5":{"24":1}}],["分析不同层之间隐藏状态的相关性",{"5":{"8":2}}],["分析其几何意义和梯度特性",{"5":{"23":2}}],["分段线性",{"5":{"24":3}}],["直到",{"5":{"13":2}}],["直接由定义2",{"5":{"15":2}}],["直接由前向传播的计算顺序可得",{"5":{"13":2}}],["直接求解特征方程通常是不切实际的",{"5":{"9":2}}],["直接计算",{"5":{"5":2}}],["直接对复合函数求导会导致计算量爆炸",{"5":{"25":2}}],["直接加到净输入上",{"5":{"25":1}}],["直观上",{"5":{"8":2}}],["直线映射为直线",{"5":{"24":2}}],["熵为0",{"5":{"8":2}}],["熵越小",{"5":{"8":2}}],["熵越大",{"5":{"8":2}}],["熵衡量了随机变量不确定性的大小",{"5":{"8":2}}],["熵定义为",{"5":{"8":2}}],["熵和互信息的数学框架",{"5":{"24":2}}],["也是理解反向传播算法的必要前提",{"5":{"15":2}}],["也是",{"5":{"13":1}}],["也是为什么许多研究致力于开发高效的注意力变体",{"5":{"9":2}}],["也是证明神经网络泛化性质的关键工具",{"5":{"9":2}}],["也是参数更新的直接输入",{"5":{"25":2}}],["也决定了它在网络中的具体作用方式",{"5":{"9":2}}],["也指导着我们理解和解释模型训练过程中的各种现象",{"5":{"8":2}}],["也采用了类似的思想",{"5":{"8":2}}],["也称为隐藏表示或隐藏激活",{"5":{"15":2}}],["也称为正态分布",{"5":{"8":2}}],["也称为多项分布",{"5":{"8":2}}],["也称为逻辑函数",{"5":{"23":2}}],["也影响梯度的传播特性",{"5":{"22":2}}],["也非常小",{"5":{"22":1}}],["也为未来新型激活函数的发展提供了指导",{"5":{"22":2}}],["也为理解transformer架构中非线性组件的设计提供了数学基础",{"5":{"24":2}}],["也为设计新的激活函数和理解训练动态提供了理论指导",{"5":{"24":2}}],["也为分析和改进现有优化算法提供了理论工具",{"5":{"23":2}}],["也为分析神经网络的信息流和表示学习提供了数学工具",{"5":{"24":2}}],["也使得sigmoid在某些特定场景下仍有应用价值",{"5":{"23":2}}],["最后旋转回原坐标系",{"5":{"13":2}}],["最后",{"5":{"22":2,"24":2,"25":2}}],["最后通过解码器重建原始输入",{"5":{"8":2}}],["最后通过对比损失",{"5":{"24":2}}],["最后用这个分布对value进行加权平均",{"5":{"24":2}}],["最后将梯度用于参数更新",{"5":{"25":2}}],["最终映射到输出空间",{"5":{"13":4}}],["最终达到较好的收敛效果",{"5":{"7":2}}],["最终产生网络输出的过程",{"5":{"13":2}}],["最终产生网络输出",{"5":{"25":2}}],["最陡点",{"5":{"23":2}}],["最大似然估计具有一些重要的渐近性质",{"5":{"8":2}}],["最大似然估计就是找到使对数似然最大的参数值",{"5":{"8":2}}],["最大似然估计",{"5":{"8":2}}],["最大值在处取得",{"5":{"23":2}}],["最大值在",{"5":{"23":2}}],["最小化交叉熵等价于最大化数据的对数似然",{"5":{"8":2}}],["最小化infonce损失等价于最大化互信息的下界",{"5":{"24":2}}],["再用非线性方法进行二维或三维嵌入",{"5":{"9":2}}],["再通过重参数化技巧进行采样",{"5":{"8":2}}],["再进行平移变换",{"5":{"5":2}}],["再加上参数更新的少量计算",{"5":{"25":2}}],["利用代数规则求导",{"5":{"13":2}}],["利用有限差分近似",{"5":{"13":2}}],["利用微分中值定理",{"5":{"13":2}}],["利用权衡很有价值",{"5":{"8":2}}],["利用​",{"5":{"23":1}}],["利用平方差公式",{"5":{"23":2}}],["利用",{"5":{"23":1}}],["利普希茨连续梯度的凸函数",{"5":{"7":2}}],["噪声的参数通过变分推断学习得到",{"5":{"8":2}}],["变分推断被用于vae的训练",{"5":{"8":2}}],["变分推断假设后验分布可以用某个简单的分布族",{"5":{"8":2}}],["变分推断是一类近似贝叶斯推断的方法",{"5":{"8":2}}],["变分dropout",{"5":{"8":2}}],["变为",{"5":{"5":1}}],["变换到输出空间的过程",{"5":{"24":2}}],["等于矩阵",{"5":{"9":1}}],["等于矩阵的最大特征值的平方根",{"5":{"9":1}}],["等现代深度学习框架均支持广播机制",{"5":{"9":2}}],["等",{"5":{"9":2}}],["等效的高斯dropout在权重上引入方差为",{"5":{"8":1}}],["等效的高斯dropout在权重上引入方差为的高斯噪声",{"5":{"8":1}}],["等算法中非常重要",{"5":{"8":2}}],["等部分组成",{"5":{"5":2}}],["具有记忆功能",{"5":{"15":2}}],["具有完整特征向量基",{"5":{"9":2}}],["具有旋转不变性",{"5":{"9":2}}],["具有重要的几何意义和计算优势",{"5":{"9":2}}],["具有相同的量纲",{"5":{"8":1}}],["具有优美的理论性质和高效的算法",{"5":{"7":2}}],["具体的",{"5":{"9":2}}],["具体做法是如果梯度的范数超过阈值",{"5":{"9":2}}],["具体选择取决于变分推断的目标",{"5":{"8":2}}],["具体来说",{"5":{"8":4,"9":6}}],["具体而言",{"5":{"8":4,"22":2,"23":2,"24":2}}],["x^2​",{"5":{"15":2}}],["xi",{"5":{"15":4}}],["x",{"5":{"15":14,"23":72}}],["xorscene",{"5":{"5":4}}],["xor",{"5":{"5":2}}],["xavier初始化和he初始化都假设权重服从某种高斯分布",{"5":{"8":2}}],["xavier初始化和he初始化正是为了避免这个问题而设计的",{"5":{"22":2}}],["xavier初始化",{"5":{"22":2}}],["xavier初始化将权重初始化为",{"5":{"22":2}}],["xavier初始化将权重方差设置为​",{"5":{"22":1}}],["xavier初始化将权重方差设置为",{"5":{"22":1}}],["xavier初始化的方差推导",{"5":{"22":2}}],["xavier初始化取两者的几何平均​",{"5":{"22":1}}],["xavier初始化取两者的几何平均",{"5":{"22":1}}],["xavier方差",{"5":{"22":1}}],["卡尔曼滤波和贝叶斯线性回归中有着核心应用",{"5":{"8":2}}],["注意力层",{"5":{"15":2}}],["注意力计算",{"5":{"13":2}}],["注意力头数为32",{"5":{"13":2}}],["注意力机制的计算复杂度与序列长度的平方成正比",{"5":{"9":2}}],["注意力机制的计算过程就包含了多个矩阵乘法操作",{"5":{"9":2}}],["注意力机制的批处理实现是理解这一概念的良好例子",{"5":{"9":2}}],["注意力机制的梯度流可以视为一种信息瓶颈",{"5":{"22":2}}],["注意力机制与激活函数的协同",{"2":{"24":1},"5":{"24":1}}],["注意力机制",{"5":{"24":4}}],["注意力机制中的查询",{"5":{"9":2}}],["注意力机制中的softmax和ffn中的gelu形成了",{"5":{"24":2}}],["注意力权重矩阵",{"5":{"9":1,"22":2}}],["注意力权重矩阵可以看作是查询张量和键张量的收缩",{"5":{"9":1}}],["注意力权重矩阵是非负行随机矩阵",{"5":{"22":1}}],["注意力权重矩阵的谱性质决定了梯度如何在不同位置之间传播",{"5":{"22":2}}],["注意力权重矩阵的谱性质决定了梯度如何被",{"5":{"22":1}}],["注意力权重与值向量的乘积同样需要",{"5":{"9":2}}],["注意力权重与值矩阵的乘积产生加权输出",{"5":{"9":2}}],["注意力权重",{"5":{"8":2}}],["注意力权重本身是一种概率分布",{"5":{"22":2}}],["注意力权重的谱结构",{"5":{"22":2}}],["注意力权重的计算公式为",{"5":{"24":2}}],["注意力权重的计算将query",{"5":{"24":1}}],["注意力权重的计算",{"5":{"24":1}}],["注意力分数",{"5":{"22":2}}],["注意力分数的计算",{"5":{"9":2}}],["注意力分数的计算需要的计算量",{"5":{"9":1}}],["注意力分数的计算在批处理情况下变成的三维张量运算",{"5":{"9":1}}],["注意力分数的计算可以看作是一个张量收缩操作",{"5":{"9":2}}],["注意力分数的方差会很大",{"5":{"22":2}}],["注意力输出为",{"5":{"22":2}}],["注意力输出需要经过前馈网络处理",{"5":{"22":2}}],["注意条件协方差不依赖于观测值",{"5":{"8":2}}],["注意矩阵乘法的方向",{"5":{"25":2}}],["均为可微函数",{"5":{"13":1}}],["均匀分布可用于权重初始化",{"5":{"8":2}}],["均匀分布的期望为",{"5":{"8":2}}],["均匀分布",{"5":{"8":2}}],["均值和对数方差",{"5":{"8":2}}],["均值和协方差相应地分块为",{"5":{"8":2}}],["均值信息可能包含重要的信号",{"5":{"22":2}}],["均值为0",{"5":{"22":2}}],["均方误差损失",{"5":{"25":2}}],["条件协方差为",{"5":{"8":1}}],["条件协方差为​",{"5":{"8":1}}],["条件熵",{"5":{"8":2}}],["条件分布在高斯过程回归",{"5":{"8":2}}],["条件分布是多元高斯分布最优美也最实用的性质之一",{"5":{"8":2}}],["条件期望",{"5":{"8":1}}],["条件期望用于描述在给定上下文的条件下",{"5":{"8":2}}],["条件期望的一个重要性质是全期望公式",{"5":{"8":2}}],["条件期望是的函数",{"5":{"8":1}}],["条件期望是在给定某些信息的条件下对随机变量的期望",{"5":{"8":2}}],["条件数与梯度稳定性",{"5":{"22":2}}],["条件数越大",{"5":{"22":2}}],["边",{"5":{"13":2}}],["边界是由数据决定的",{"5":{"13":2}}],["边缘分布仍然是高斯分布",{"5":{"8":2}}],["边表示依赖关系",{"5":{"7":2}}],["称为奇异值",{"5":{"9":2,"13":2}}],["称为欠拟合",{"5":{"9":2}}],["称为马氏距离",{"5":{"8":2}}],["称为精度矩阵",{"5":{"8":2}}],["称为净输入",{"5":{"5":2}}],["称为logit",{"5":{"24":2}}],["称为",{"5":{"25":2}}],["必须是对称半正定的",{"5":{"8":2}}],["稍后详述",{"5":{"8":2}}],["相一致",{"5":{"13":2}}],["相乘",{"5":{"9":2}}],["相比之下",{"5":{"22":2}}],["相关系数可用于分析不同词嵌入维度之间的冗余程度",{"5":{"8":2}}],["相关系数消除了量纲的影响",{"5":{"8":2}}],["相关系数的取值范围为",{"5":{"8":2}}],["相关系数",{"5":{"8":2}}],["相关",{"5":{"22":1}}],["相对于标准正态分布",{"5":{"8":2}}],["相对误差小于",{"5":{"25":2}}],["完全相同",{"5":{"15":1}}],["完全对称",{"5":{"8":2}}],["完全抑制负信息",{"5":{"24":2}}],["完全无法捕获和之间的乘积交互",{"5":{"24":1}}],["完全无法捕获",{"5":{"24":1}}],["完成了从输入空间到输出空间的非线性变换",{"5":{"5":2}}],["完成了从线性预测到概率预测的关键一步",{"5":{"24":2}}],["源于多个方面的原因",{"5":{"8":2}}],["且满足分解式",{"5":{"13":2}}],["且特征向量矩阵是正交的",{"5":{"9":2}}],["且当且仅当",{"5":{"8":1}}],["且当且仅当时kl散度为0",{"5":{"8":1}}],["且",{"5":{"8":1,"13":2,"23":1,"24":2}}],["且同样唯一确定概率分布",{"5":{"8":2}}],["且最优解集是凸集",{"5":{"7":2}}],["峰度为3",{"5":{"8":2}}],["峰度",{"5":{"8":2}}],["矩母函数",{"5":{"8":2}}],["矩",{"5":{"8":2}}],["矩阵是半正定对称矩阵",{"5":{"13":1}}],["矩阵是二维数组",{"5":{"9":2}}],["矩阵和张量运算的计算复杂度和内存效率是核心关注点",{"5":{"9":2}}],["矩阵",{"5":{"9":2,"13":1,"22":4}}],["矩阵分解还包括lu分解",{"5":{"9":2}}],["矩阵分解的深化理解",{"2":{"9":1},"5":{"9":1}}],["矩阵范数的选择取决于具体问题的需求",{"5":{"9":2}}],["矩阵范数将矩阵映射到非负实数",{"5":{"9":2}}],["矩阵范数是衡量矩阵",{"5":{"9":2}}],["矩阵范数与度量",{"2":{"9":1},"5":{"9":1}}],["矩阵的求逆是找到一个矩阵",{"5":{"9":1}}],["矩阵的求逆是找到一个矩阵使得",{"5":{"9":1}}],["矩阵的转置是将矩阵的行和列互换",{"5":{"9":2}}],["矩阵的形状",{"5":{"9":2}}],["矩阵的谱半径决定了系统的稳定性",{"5":{"22":1}}],["矩阵的条件数定义为",{"5":{"22":1}}],["矩阵乘法tile",{"5":{"13":1}}],["矩阵乘法节点",{"5":{"13":2}}],["矩阵乘法是连接两个张量的最基本操作",{"5":{"9":2}}],["矩阵乘法是最重要也是计算量最大的运算",{"5":{"9":2}}],["矩阵乘法满足结合律",{"5":{"9":2}}],["矩阵乘法",{"5":{"9":2,"25":3}}],["矩阵乘法退化为逐元素乘法",{"5":{"25":2}}],["矩阵乘法的计算复杂度是",{"5":{"9":2}}],["矩阵乘法的flops约为",{"5":{"25":1}}],["矩阵形式的链式法则在深度学习中尤为重要",{"5":{"7":1}}],["矩阵运算级并行",{"5":{"13":1}}],["矩阵运算级和模型级等多个层次进行并行优化",{"5":{"13":2}}],["矩阵运算一次性处理了所有",{"5":{"5":1}}],["矩阵运算一次性处理了所有个样本",{"5":{"5":1}}],["矩阵运算与批量处理的等价性",{"5":{"5":2}}],["矩阵谱半径与稳定性",{"5":{"22":2}}],["协方差矩阵在大语言模型中的应用包括",{"5":{"8":2}}],["协方差矩阵的对角线元素是各随机变量的方差",{"5":{"8":2}}],["协方差矩阵是多维随机变量的二阶中心矩描述",{"5":{"8":2}}],["协方差可以展开为",{"5":{"8":2}}],["协方差定义为",{"5":{"8":2}}],["协方差",{"5":{"8":2}}],["根据链式法则",{"5":{"13":1}}],["根据链式法则​",{"5":{"13":1}}],["根据矩阵微积分的链式法则",{"5":{"13":2}}],["根据层映射的定义",{"5":{"13":2}}],["根据eckart",{"5":{"9":2}}],["根据中心极限定理",{"5":{"8":2}}],["根据大数定律",{"5":{"8":4}}],["根据期望的线性性质",{"5":{"8":2}}],["根据取值方式的不同",{"5":{"8":2}}],["根据误差信号的定义和链式法则",{"5":{"25":2}}],["根据定理2",{"5":{"25":2}}],["vectorization",{"5":{"9":2}}],["var",{"5":{"15":13,"23":2}}],["variational",{"5":{"8":2}}],["variance",{"5":{"8":2}}],["value",{"5":{"9":2,"13":2}}],["value的投影计算是线性的",{"5":{"24":2}}],["v0",{"5":{"5":8,"7":4}}],["vs",{"5":{"23":2}}],["v",{"5":{"23":12}}],["v^t",{"5":{"23":4}}],["v^2",{"5":{"23":2}}],["位置",{"5":{"8":2}}],["位置编码",{"5":{"22":2,"24":8}}],["位置编码可以被理解为对",{"5":{"24":2}}],["位置编码与激活函数的结合决定了位置信息如何在网络中传递",{"5":{"24":2}}],["位置编码的数学设计",{"5":{"24":2}}],["位置编码的频率分解意味着不同维度携带不同尺度的位置信息",{"5":{"24":2}}],["位置信息被缩放到合适的范围",{"5":{"22":2}}],["位置信息",{"5":{"24":2}}],["位置信息和token信息通过不同的权重矩阵投影后相加",{"5":{"24":2}}],["位置感知的特征表示",{"5":{"24":2}}],["都离不开这些基本概念",{"5":{"8":2}}],["都只能将平面划分为两个半平面",{"5":{"5":1}}],["都无法达到良好的分类效果",{"5":{"24":2}}],["都可以作为激活函数",{"5":{"24":2}}],["离散程度和相互关系",{"5":{"8":2}}],["离散随机变量只能取有限或可数无穷个值",{"5":{"8":2}}],["描述分布的尾部厚度",{"5":{"8":2}}],["描述分布的对称性",{"5":{"8":2}}],["描述词嵌入向量的分布特性",{"5":{"8":2}}],["描述了随机变量的",{"5":{"8":2}}],["描述独立事件发生的时间间隔",{"5":{"8":2}}],["描述",{"5":{"8":2,"13":2,"24":2}}],["描述单次二元试验的成功概率",{"5":{"8":2}}],["二字的含义所在",{"5":{"15":2}}],["二次型",{"5":{"8":2}}],["二阶张量",{"5":{"9":2}}],["二阶张量是矩阵",{"5":{"9":2}}],["二阶中心矩是方差",{"5":{"8":2}}],["二阶矩估计",{"5":{"7":2}}],["二阶导数的几何意义表示sigmoid曲线的曲率变化",{"5":{"23":2}}],["二阶导数为正",{"5":{"23":2}}],["二阶导数为负",{"5":{"23":2}}],["二项分布的共轭先验是beta分布",{"5":{"8":2}}],["二项分布的期望为",{"5":{"8":2}}],["二项分布在语言模型中的应用包括描述一批样本中被mask的词的数量",{"5":{"8":2}}],["二项分布",{"5":{"8":2}}],["二分类任务",{"5":{"25":2}}],["一层中的",{"5":{"15":1}}],["一层中的个神经元实际上定义了个超平面",{"5":{"15":1}}],["一组向量如果两两正交且都是单位向量",{"5":{"9":2}}],["一个仿射变换",{"5":{"13":1}}],["一个仿射变换可以理解为",{"5":{"13":1}}],["一个",{"5":{"9":1}}],["一个张量网络由若干个节点",{"5":{"9":2}}],["一个线性变换",{"5":{"9":1}}],["一个线性变换可以用一个矩阵",{"5":{"9":1}}],["一个线性",{"5":{"9":2}}],["一个k维子空间是特征空间的一个k维线性子集",{"5":{"9":2}}],["一个n维特征空间可以理解为一个n维欧几里得空间",{"5":{"9":2}}],["一个阶张量有个索引",{"5":{"9":1}}],["一个300维的词向量可以看作是一个从自然语言到连续向量空间的映射",{"5":{"9":2}}],["一个很大的模型可能在某些指标上有统计显著的改进",{"5":{"8":2}}],["一个词是否被mask掉",{"5":{"8":2}}],["一个映射是线性的",{"5":{"24":1}}],["一个映射",{"5":{"24":1}}],["一维高斯分布的概率密度函数为",{"5":{"8":2}}],["一阶张量",{"5":{"9":2}}],["一阶张量是向量",{"5":{"9":2}}],["一阶原点矩是期望",{"5":{"8":2}}],["一阶矩估计",{"5":{"7":2}}],["一般情况下",{"5":{"8":2}}],["一次乘法或一次加法计为一次flops",{"5":{"13":2}}],["一次前向传播加上一次反向传播",{"5":{"25":2}}],["伯努利分布可用于描述二元随机事件",{"5":{"8":2}}],["伯努利分布的期望为",{"5":{"8":2}}],["伯努利分布",{"5":{"8":2}}],["伯努利分布与sigmoid激活",{"5":{"5":2}}],["词嵌入的降维可视化",{"5":{"9":2}}],["词嵌入空间具有一些重要的几何性质",{"5":{"9":2}}],["词嵌入空间就是一个典型的高维特征空间",{"5":{"9":2}}],["词向量就是典型的向量表示",{"5":{"9":2}}],["词汇表上的概率分布就是典型的pmf",{"5":{"8":2}}],["词汇表中的词索引等",{"5":{"8":2}}],["词索引是离散的",{"5":{"8":2}}],["连续随机变量由概率密度函数",{"5":{"8":2}}],["连续随机变量可以取任意实数值或实数区间内的值",{"5":{"8":2}}],["连续可微",{"5":{"5":2}}],["掷骰子的点数",{"5":{"8":2}}],["正如我们在4",{"5":{"15":2}}],["正则化",{"5":{"9":2}}],["正向",{"5":{"8":1}}],["正向鼓励覆盖的所有模式",{"5":{"8":1}}],["正确答案",{"5":{"8":2}}],["正面或反面",{"5":{"8":2}}],["正交变换仍然是有价值的分析工具",{"5":{"9":2}}],["正交约束在优化中被用于防止权重矩阵的秩退化",{"5":{"9":2}}],["正交初始化将权重矩阵初始化为正交矩阵",{"5":{"9":2}}],["正交初始化是一种常用的权重初始化方法",{"5":{"9":2}}],["正交矩阵是满足",{"5":{"9":1}}],["正交矩阵是满足的方阵",{"5":{"9":1}}],["正交矩阵的行列式的绝对值为1",{"5":{"9":2}}],["正交矩阵的逆矩阵等于其转置矩阵",{"5":{"9":2}}],["正交矩阵保持向量的范数不变",{"5":{"9":2}}],["正交矩阵有几个重要的性质",{"5":{"9":2}}],["正交性有着多方面的应用",{"5":{"9":2}}],["正交性是线性代数中一个核心概念",{"5":{"9":2}}],["正交性与正交矩阵",{"2":{"9":1},"5":{"9":1}}],["正交",{"5":{"7":2}}],["正是因为它对任何矩阵都成立",{"5":{"9":2}}],["正是这种简单性带来了卓越的计算效率和优异的性能",{"5":{"23":2}}],["正是这些保持的性质限制了线性映射的表达能力",{"5":{"24":2}}],["正是非线性激活函数赋予了网络这种强大的表达能力",{"5":{"24":2}}],["正是衡量两个概率分布",{"5":{"24":1}}],["正弦余弦编码的频率分解特性意味着不同频率的成分携带不同尺度的位置信息",{"5":{"24":2}}],["正弦和余弦函数的取值范围是",{"5":{"24":2}}],["文本生成本质上是一个随机过程",{"5":{"8":2}}],["方差接近1",{"5":{"8":2}}],["方差有限",{"5":{"8":1}}],["方差达到克拉美",{"5":{"8":2}}],["方差用于分析梯度的波动",{"5":{"8":2}}],["方差的性质包括",{"5":{"8":2}}],["方差的平方根称为标准差",{"5":{"8":2}}],["方差可以展开为",{"5":{"8":2}}],["方差",{"5":{"8":3}}],["方差和协方差是概率论中最重要的一阶和二阶统计量",{"5":{"8":2}}],["方差为的高斯分布",{"5":{"8":1}}],["方差为​",{"5":{"8":2}}],["方差为",{"5":{"8":9}}],["方差为1的特殊高斯分布",{"5":{"8":2}}],["方差为1的标准正态分布附近",{"5":{"22":2}}],["方差为1",{"5":{"22":2}}],["方差与协方差",{"2":{"8":1},"5":{"8":1}}],["方向",{"5":{"7":4}}],["方向导数在梯度方向",{"5":{"7":2}}],["方向导数",{"5":{"7":2}}],["方向和幅度来构造任意复杂的曲面",{"5":{"24":2}}],["期望有限",{"5":{"8":2}}],["期望传播提供了有用的近似工具",{"5":{"8":2}}],["期望传播在某些情况下比变分推断更准确",{"5":{"8":2}}],["期望传播",{"5":{"8":2}}],["期望具有线性性质",{"5":{"8":2}}],["期望定义为",{"5":{"8":2}}],["期望",{"2":{"8":1},"5":{"8":6}}],["步的参数",{"5":{"7":1}}],["步长",{"5":{"7":2}}],["步骤",{"5":{"25":2}}],["沿",{"5":{"7":1}}],["沿计算图的前向方向累积计算导数",{"5":{"25":2}}],["沿计算图的反向方向累积计算导数",{"5":{"25":2}}],["沿计算图的边反向传播梯度",{"5":{"25":2}}],["衡量差异的实际大小",{"5":{"8":2}}],["衡量两个随机变量之间的信息共享程度",{"5":{"8":2}}],["衡量两个随机变量之间的线性相关程度",{"5":{"8":2}}],["衡量随机变量取值的离散程度",{"5":{"8":2}}],["衡量输出",{"5":{"7":1}}],["衡量函数沿特定方向的变化率",{"5":{"7":2}}],["传导的",{"5":{"7":1}}],["传统的带动量sgd可能优于adam",{"5":{"7":2}}],["商的偏导数",{"5":{"7":1}}],["商的偏导数​​",{"5":{"7":1}}],["关于",{"5":{"7":1}}],["关于变量",{"5":{"7":1}}],["关于为什么深度神经网络能够有效优化",{"5":{"7":2}}],["关于第一层权重",{"5":{"22":1}}],["关于输入",{"5":{"24":1}}],["穿行",{"5":{"7":2}}],["研究发现",{"5":{"7":2}}],["研究表明",{"5":{"7":2,"22":2,"24":2}}],["研究的是梯度在深度网络中传播时的行为",{"5":{"22":2}}],["鞍点比局部最小值更为常见",{"5":{"7":2}}],["鞍点逃离是大规模非凸优化中的一个重要问题",{"5":{"7":2}}],["鞍点和平坦区域",{"5":{"7":2}}],["性质",{"5":{"7":2}}],["伪凸",{"5":{"7":2}}],["非对角线元素是变量之间的协方差",{"5":{"8":2}}],["非负性",{"5":{"8":3}}],["非负性对所有成立",{"5":{"8":1}}],["非凸性意味着梯度下降可能收敛到局部最小值而非全局最小值",{"5":{"7":2}}],["非凸优化是深度学习面临的现实挑战",{"5":{"7":2}}],["非线性",{"5":{"5":2,"24":5}}],["非线性表达能力",{"5":{"22":2}}],["非线性的数学必要性",{"2":{"24":1},"5":{"24":1}}],["非线性变换打破表达瓶颈",{"2":{"24":1},"5":{"24":1}}],["非线性激活使得网络能够学习越来越复杂的特征组合",{"5":{"13":2}}],["非线性激活函数的",{"5":{"24":2}}],["非常高效",{"5":{"22":2}}],["非常小",{"5":{"22":1}}],["非常接近",{"5":{"23":2}}],["非常数",{"5":{"24":2}}],["非平凡",{"5":{"24":2}}],["非单调性",{"5":{"24":2}}],["非饱和但有界",{"5":{"24":2}}],["避免在随机初始化阶段受到过大梯度的干扰",{"5":{"7":2}}],["避免数值溢出或下溢",{"5":{"5":2}}],["避免梯度饱和区域",{"2":{"22":1},"5":{"22":1}}],["避免某些路径成为",{"5":{"22":2}}],["避免了某些位置主导信息流的问题",{"5":{"22":2}}],["避免饱和",{"5":{"22":2}}],["避免存在大范围的饱和区域",{"5":{"22":2}}],["模型的不同层",{"5":{"13":1}}],["模型级并行",{"5":{"13":1}}],["模型并行",{"5":{"13":2}}],["模型并行化将大模型分布在多个计算设备上",{"5":{"9":2}}],["模型能够高效地并行处理大量数据",{"5":{"9":2}}],["模型a优于模型b",{"5":{"8":2}}],["模型为词汇表中的每个词分配一个概率",{"5":{"8":2}}],["模型参数量巨大",{"5":{"7":2}}],["模型克服了这些局限性",{"5":{"5":2}}],["模型将退化为简单的加权平均",{"5":{"24":2}}],["尽管整体上是非线性的",{"5":{"13":2}}],["尽管adam在实践中表现优异",{"5":{"7":2}}],["尽管单个神经元结构极其简单",{"5":{"5":2}}],["更会错失利用矩阵运算简化计算的机会",{"5":{"15":2}}],["更是一个几何变换过程",{"5":{"13":2}}],["更能为分析网络的表达能力",{"5":{"13":2}}],["更复杂的网络结构如树状张量网络",{"5":{"9":2}}],["更新规则为",{"5":{"7":2}}],["更重要的是",{"5":{"5":2}}],["更接近置换或选择矩阵",{"5":{"22":2}}],["更强",{"5":{"23":2}}],["更有价值",{"5":{"24":2}}],["更语义化的特征",{"5":{"24":2}}],["更抽象",{"5":{"24":1}}],["表4",{"5":{"13":2,"25":6}}],["表2",{"5":{"13":2}}],["表示​",{"5":{"13":1}}],["表示数据",{"5":{"13":2}}],["表示数学运算",{"5":{"13":2}}],["表示空间的层次结构",{"5":{"13":2}}],["表示完全正相关",{"5":{"8":1}}],["表示完全负相关",{"5":{"8":2}}],["表示",{"5":{"8":1,"9":2,"13":1}}],["表示线性无关",{"5":{"8":2}}],["表示生成该词的可能性",{"5":{"8":2}}],["表示取特定值的概率",{"5":{"8":1}}],["表示逐元素乘法",{"5":{"7":2,"25":1}}],["表示在sigmoid曲线上各点的斜率",{"5":{"23":2}}],["表示sigmoid曲线在各点的斜率",{"5":{"23":2}}],["表示的形式",{"5":{"23":1}}],["表示样本属于第",{"5":{"24":1}}],["表达式",{"5":{"5":1}}],["表达式是线性变换与平移的组合",{"5":{"5":1}}],["表达能力更强",{"5":{"24":2}}],["表达能力更强的激活函数可能提供更紧的下界",{"5":{"24":2}}],["表明它是输入",{"5":{"24":1}}],["自注意力机制需要计算查询向量与键向量的相似度",{"5":{"9":2}}],["自助法可用于估计困惑度",{"5":{"8":2}}],["自助法",{"5":{"8":2}}],["自动微分分为前向模式",{"5":{"13":2}}],["自动微分利用链式法则和计算图的结构",{"5":{"13":2}}],["自动微分结合了数值计算的效率和符号计算的精确性",{"5":{"7":2}}],["自动微分将函数分解为基本操作的序列",{"5":{"7":2}}],["自动微分",{"5":{"7":2,"13":4}}],["自动微分的分类",{"5":{"25":2}}],["自动微分主要分为两种模式",{"5":{"25":2}}],["自回归模型",{"5":{"22":2}}],["自适应学习率方法为每个参数单独调整学习率",{"5":{"7":2}}],["自适应激活函数",{"5":{"22":2}}],["自然",{"5":{"22":1,"25":2}}],["自化",{"5":{"23":4}}],["自身表示导数",{"5":{"23":1}}],["动量更新的公式为",{"5":{"7":2}}],["动量方法和噪声梯度有助于算法逃离鞍点",{"5":{"7":2}}],["动量方法可以看作是对梯度进行指数移动平均",{"5":{"7":2}}],["动量方法",{"5":{"7":2}}],["目标函数值与最优值的差距为",{"5":{"7":2}}],["凸性的充要条件是hessian矩阵半正定",{"5":{"7":2}}],["凸函数是一类特殊的函数",{"5":{"7":2}}],["凸优化是研究凸函数在凸集上最小化问题的数学分支",{"5":{"7":2}}],["e^",{"5":{"15":4}}],["e^x",{"5":{"23":4}}],["edge",{"5":{"13":2}}],["efficient",{"5":{"9":2}}],["effect",{"5":{"8":2}}],["expectation",{"5":{"8":4}}],["exponential",{"5":{"7":2,"8":2}}],["exp",{"5":{"23":56}}],["estimation",{"5":{"7":2,"8":2}}],["ell",{"5":{"15":26}}],["ell=1",{"5":{"15":10}}],["elu",{"5":{"22":2,"23":6}}],["entropy",{"5":{"5":2,"8":8}}],["end",{"5":{"23":20}}],["e",{"5":{"23":8}}],["erf",{"5":{"23":2}}],["error",{"5":{"25":2}}],["阶张量有",{"5":{"9":1}}],["阶中心矩定义为",{"5":{"8":2}}],["阶原点矩定义为",{"5":{"8":2}}],["阶梯衰减",{"5":{"7":2}}],["阶跃激活函数的数学定义为",{"5":{"5":2}}],["拉普拉斯近似用于构建后验分布的高斯代理",{"5":{"7":2}}],["拉普拉斯近似利用hessian矩阵在峰值附近对概率分布进行高斯近似",{"5":{"7":2}}],["通常在数千量级",{"5":{"9":1}}],["通常不等于",{"5":{"9":1}}],["通常使用默认值",{"5":{"7":1}}],["通常能提供更好的收敛性质",{"5":{"7":2}}],["通常是正定的",{"5":{"7":2}}],["通常设置为1或5",{"5":{"22":2}}],["通常保持较高秩",{"5":{"22":2}}],["通过注意力机制聚合信息",{"5":{"15":2}}],["通过卷积核提取局部特征",{"5":{"15":2}}],["通过增加网络的深度",{"5":{"15":2}}],["通过构建计算图",{"5":{"13":2}}],["通过仔细规划张量的内存布局和运算顺序",{"5":{"9":2}}],["通过将大型权重张量分解为张量网络的形式",{"5":{"9":2}}],["通过将大型权重张量分解为若干低秩张量",{"5":{"9":2}}],["通过惩罚大权重来防止过拟合",{"5":{"9":2}}],["通过gram",{"5":{"9":2}}],["通过分析权重矩阵的奇异值分布",{"5":{"9":2}}],["通过保留最大的k个特征值对应的特征向量",{"5":{"9":2}}],["通过保持",{"5":{"7":1}}],["通过保持的单调性来保证收敛性",{"5":{"7":1}}],["通过逆运算解线性方程组",{"5":{"9":2}}],["通过张量运算",{"5":{"9":2}}],["通过对矩母函数求导并在",{"5":{"8":1}}],["通过对矩母函数求导并在处求值",{"5":{"8":1}}],["通过计算真实排列的统计量在置换分布中的位置",{"5":{"8":2}}],["通过边际似然",{"5":{"8":2}}],["通过引入一个校正因子来修复adam在训练早期的方差问题",{"5":{"7":2}}],["通过引入非线性变换",{"5":{"5":2}}],["通过hessian矩阵可以判断驻点的性质",{"5":{"7":2}}],["通过一次矩阵运算同时处理多个样本",{"5":{"5":2}}],["通过选择适当的权重和偏置",{"5":{"5":2}}],["通过数学归纳法证明",{"5":{"5":2}}],["通过严格的数学推导和几何直觉",{"5":{"22":2}}],["通过规范化每层的输入分布",{"5":{"22":2}}],["通过可学习的参数",{"5":{"22":1}}],["通过这个关系可以清晰地看到tanh如何从sigmoid演化而来",{"5":{"23":2}}],["通过矩阵乘法的结合律和分配律",{"5":{"24":2}}],["通过向量化操作可以将其转化为线性方程",{"5":{"9":2}}],["通过向logits添加gumbel噪声并进行softmax变换",{"5":{"24":2}}],["通过序列顺序隐含",{"5":{"24":2}}],["通过编码器学习将输入映射到该分布的参数",{"5":{"8":2}}],["通过编码向量表示",{"5":{"24":2}}],["通过",{"5":{"24":2}}],["通过理解误差信号的传播机制",{"5":{"25":2}}],["通过在隐藏层引入非线性激活函数",{"5":{"5":2}}],["通过在反向传播时放大损失值来间接放大梯度",{"5":{"25":2}}],["通过定义搜索空间和评估指标",{"5":{"22":2}}],["通过定义误差信号",{"5":{"25":2}}],["通过批量处理",{"5":{"25":2}}],["通过公式逐层传播到输入层",{"5":{"25":1}}],["通过公式",{"5":{"25":1}}],["通道维度",{"5":{"22":1}}],["通用逼近器",{"5":{"24":2}}],["通用逼近定理",{"5":{"24":2}}],["通用逼近定理的数学表述",{"2":{"24":1},"5":{"24":1}}],["通用逼近定理的核心数学洞见在于",{"5":{"24":2}}],["通用逼近定理指出",{"5":{"24":2}}],["通用逼近定理表明",{"5":{"24":2}}],["牛顿矩阵省略了二阶导数项",{"5":{"7":2}}],["牛顿矩阵为",{"5":{"7":2}}],["牛顿矩阵是hessian的一个常用近似",{"5":{"7":2}}],["kullback",{"5":{"8":2}}],["kurtosis",{"5":{"8":2}}],["kl散度可以理解为使用分布",{"5":{"8":1}}],["kl散度可以理解为使用分布来编码来自分布的样本时所需要的额外信息量",{"5":{"8":1}}],["kl散度在不同场景下的不同形式反映了其灵活性",{"5":{"8":2}}],["kl散度用于衡量学生模型和教师模型输出分布之间的差异",{"5":{"8":2}}],["kl散度用于限制策略更新的幅度",{"5":{"8":2}}],["kl散度有多种重要应用",{"5":{"8":2}}],["kl散度的定义为",{"5":{"8":2}}],["kl散度不对称",{"5":{"8":2}}],["kl散度是非负的",{"5":{"8":2}}],["kl散度定义为",{"5":{"8":2}}],["kl散度衡量两个概率分布之间的",{"5":{"8":2}}],["kl散度",{"5":{"8":2,"24":2}}],["kl散度与交叉熵",{"2":{"8":1},"5":{"8":1}}],["k",{"5":{"7":3,"15":6,"23":76}}],["kronecker积可以帮助我们将大矩阵运算分解为多个小矩阵运算的组合",{"5":{"9":2}}],["kronecker积的主要应用包括权重共享",{"5":{"9":2}}],["kronecker积具有许多有用的代数性质",{"5":{"9":2}}],["kronecker积是两个矩阵之间的特殊二元运算",{"5":{"9":2}}],["kronecker积与向量化",{"2":{"9":1},"5":{"9":1}}],["kronecker",{"5":{"7":2}}],["k​",{"5":{"23":2}}],["key",{"5":{"24":2}}],["key相似度分数转换为概率分布",{"5":{"24":2}}],["显著提升了深层模型的可训练性",{"5":{"9":2}}],["显著提高了训练效率",{"5":{"5":2}}],["显著性检验的解读需要谨慎",{"5":{"8":2}}],["显著",{"5":{"22":2}}],["显式计算hessian是不现实的",{"5":{"7":2}}],["可能对权重和偏置维护不同的优化状态",{"5":{"15":2}}],["可能产生问题",{"5":{"22":2}}],["可能产生范数增长",{"5":{"22":2}}],["可能放大或缩小梯度",{"5":{"22":2}}],["可能导致",{"5":{"22":2}}],["可能导致训练过程不稳定或收敛较慢",{"5":{"8":2}}],["可能导致训练不稳定",{"5":{"23":2}}],["可能捕获更复杂的模式",{"5":{"22":2}}],["可能带来性能的提升",{"5":{"22":2}}],["可能直接落在饱和区域",{"5":{"22":1}}],["可能具有不同的范数尺度",{"5":{"22":1}}],["可能是曲线",{"5":{"24":2}}],["可验证为正交矩阵",{"5":{"13":2}}],["可以分解为",{"5":{"13":1}}],["可以分解为旋转",{"5":{"13":1}}],["可以从样本级",{"5":{"13":2}}],["可以将权重矩阵",{"5":{"13":1}}],["可以将权重矩阵按列",{"5":{"13":1}}],["可以将批次进一步划分为更小的微批次",{"5":{"9":2}}],["可以将多个样本的梯度计算合并为矩阵运算",{"5":{"25":2}}],["可以并行执行",{"5":{"13":2}}],["可以完全并行执行",{"5":{"13":2}}],["可以充分利用硬件的并行计算能力",{"5":{"13":2}}],["可以正交对角化为",{"5":{"13":2}}],["可以用一个矩阵",{"5":{"9":1}}],["可以用于表示高阶交互关系",{"5":{"9":2}}],["可以一次性完成所有样本的计算",{"5":{"9":2}}],["可以显著减少不必要的内存分配和数据拷贝",{"5":{"9":2}}],["可以显著减少参数量和计算量",{"5":{"9":2}}],["可以理解为",{"5":{"13":2}}],["可以理解为寻找能够最好地逼近目标映射的复合函数形式",{"5":{"13":2}}],["可以理解为一维数组中的有序数集",{"5":{"9":2}}],["可以理解为将输入向量",{"5":{"5":1}}],["可以得到p值",{"5":{"8":2}}],["可以得到各阶矩",{"5":{"8":2}}],["可以定义为各层雅可比矩阵范数的乘积",{"5":{"22":2}}],["可以在保持模型性能的同时显著减少参数量和计算开销",{"5":{"9":2}}],["可以在任何情况下使用",{"5":{"22":2}}],["可以系统地组织前向传播的计算步骤",{"5":{"13":2}}],["可以系统地探索激活函数的参数空间",{"5":{"22":2}}],["可以看作是查询张量和键张量的收缩",{"5":{"9":1}}],["可以看出",{"5":{"22":1}}],["可以推导出sigmoid的三阶导数",{"5":{"23":2}}],["可以被视为一个",{"5":{"24":2}}],["可以精确地计算任意复杂函数的导数",{"5":{"13":2}}],["可以精确重构任意复杂的连续函数模式",{"5":{"24":2}}],["可以形成复杂的决策边界",{"5":{"24":2}}],["可以通过反向传播自动学习",{"5":{"22":1}}],["可以通过调整每个曲面的位置",{"5":{"24":2}}],["可以通过采样来估计",{"5":{"24":1}}],["可以通过采样",{"5":{"24":1}}],["可以学习从低级局部模式到高级抽象语义的层次化表示",{"5":{"24":2}}],["可以取任意值",{"5":{"24":2}}],["可以揭示其信息处理能力的本质",{"5":{"24":2}}],["可以表示为确定函数",{"5":{"24":1}}],["可达数十亿",{"5":{"7":2}}],["可加性",{"5":{"24":2}}],["可微函数是凸函数的充要条件是其梯度单调不减",{"5":{"7":2}}],["可微",{"5":{"24":2}}],["可得",{"5":{"25":2}}],["而数据并行",{"5":{"9":2}}],["而奇异值分解适用于任意矩阵",{"5":{"9":2}}],["而零空间则包含了可能被",{"5":{"9":2}}],["而正规方程的求解等价于计算矩阵的伪逆",{"5":{"9":2}}],["而且并非所有方阵都有逆矩阵",{"5":{"9":2}}],["而反向",{"5":{"8":1}}],["而反向则鼓励找到一个主要的模式并紧密地拟合它",{"5":{"8":1}}],["而强大数定律保证样本均值几乎必然收敛于期望",{"5":{"8":2}}],["而不是高斯消元的",{"5":{"9":1}}],["而不是最小化全局kl散度",{"5":{"8":2}}],["而不是点估计",{"5":{"8":2}}],["而不显式地计算和存储完整的hessian矩阵",{"5":{"7":2}}],["而真实分布是",{"5":{"8":2}}],["而连续空间中的嵌入表示和隐藏状态则是连续的",{"5":{"8":2}}],["而",{"5":{"7":1,"9":2}}],["而​",{"5":{"7":1}}],["而梯度饱和则关注导数趋近于零导致的梯度消失问题",{"5":{"22":2}}],["而梯度饱和是激活函数导数的行为",{"5":{"22":2}}],["而非预先指定的",{"5":{"13":2}}],["而非批维度",{"5":{"22":2}}],["而非对角元素也非常小",{"5":{"22":1}}],["而非对角元素",{"5":{"22":1}}],["而当注意力分布趋于one",{"5":{"22":2}}],["而在极端值",{"5":{"23":2}}],["而深度网络仅需多项式规模",{"5":{"24":2}}],["而各种信息量",{"5":{"24":2}}],["而这个映射的几何性质决定了神经网络的学习动态和表达能力",{"5":{"24":2}}],["而是取连续的实数值",{"5":{"9":2}}],["而是由fisher信息加权的距离",{"5":{"24":2}}],["而是经过深思熟虑的数学设计",{"5":{"24":2}}],["陡峭",{"5":{"7":2,"24":2}}],["平坦",{"5":{"7":2,"8":2,"24":2}}],["平移变换",{"5":{"5":1}}],["平移变换则将整个超平面平移",{"5":{"5":1}}],["平移不变性",{"5":{"5":2}}],["平均",{"5":{"22":2}}],["平行线保持平行",{"5":{"24":2}}],["凹凸程度",{"5":{"7":2}}],["函数",{"5":{"7":2}}],["函数是凸函数的充要条件是对于任意和任意",{"5":{"7":1}}],["函数的二阶泰勒展开为",{"5":{"7":1}}],["函数值随某一特定变量变化的速率",{"5":{"7":2}}],["函数逼近基石",{"5":{"24":2}}],["影响局部优化的收敛速度",{"5":{"7":2}}],["所界定",{"5":{"5":1}}],["所揭示的深刻数学原理",{"5":{"24":2}}],["所有特征值大于零",{"5":{"7":2}}],["所有特征值位于单位圆内",{"5":{"22":2}}],["所有指数项趋向于1",{"5":{"24":2}}],["所有参数的梯度",{"5":{"25":2}}],["负的最大值",{"5":{"7":2}}],["负梯度方向则是函数下降最快的方向",{"5":{"7":2}}],["上表现为线性函数",{"5":{"13":2}}],["上为零",{"5":{"7":2}}],["上取得最小值",{"5":{"7":2}}],["上取得最大值",{"5":{"7":2}}],["上述矩阵运算等价于对每个样本单独进行神经元计算后的结果堆叠",{"5":{"5":2}}],["上述表达式可以简化为",{"5":{"24":2}}],["上",{"5":{"24":1}}],["驻点",{"5":{"7":2}}],["进一步将dropout解释为贝叶斯推断",{"5":{"8":2}}],["进行大部分计算",{"5":{"9":2}}],["进行梯度估计时",{"5":{"8":2}}],["进行非线性变换",{"5":{"5":1}}],["进行线性投影",{"5":{"5":1}}],["进行高度并行化的矩阵运算",{"5":{"5":2}}],["进行",{"5":{"22":1}}],["进入函数值更高的区域",{"5":{"7":2}}],["进入深度饱和区域",{"5":{"22":2}}],["进而影响模型的收敛速度",{"5":{"23":2}}],["或行",{"5":{"13":2}}],["或行向量",{"5":{"9":2}}],["或等价地",{"5":{"13":2}}],["或等值线",{"5":{"7":2}}],["或其中一个维度为",{"5":{"9":2}}],["或",{"5":{"8":3,"9":2,"13":2,"22":3,"24":4,"25":3}}],["或和",{"5":{"8":2}}],["或平均值",{"5":{"8":2}}],["或证据",{"5":{"8":2}}],["或奈特",{"5":{"8":2}}],["或称单位高斯分布",{"5":{"8":2}}],["或经过dropout操作后某个神经元是否被激活等",{"5":{"8":2}}],["或回归",{"5":{"5":2}}],["或之后",{"5":{"22":2}}],["或均匀分布",{"5":{"8":2}}],["或均方根为1",{"5":{"22":2}}],["或均方根",{"5":{"22":2}}],["或过小",{"5":{"22":2}}],["或更低精度",{"5":{"9":2}}],["或更宽",{"5":{"22":1}}],["或层次化结构",{"5":{"24":2}}],["或对于分类任务",{"5":{"25":1}}],["反向模式",{"5":{"13":1,"25":2}}],["反向模式的核心是链式法则",{"5":{"13":1}}],["反向模式的核心是链式法则​",{"5":{"13":1}}],["反向模式的微分计算",{"5":{"13":2}}],["反向模式自动微分是深度学习中反向传播算法的理论基础",{"5":{"13":2}}],["反向模式自动微分从输出节点开始",{"5":{"25":2}}],["反向模式自动微分",{"5":{"25":1}}],["反向传播时对权重和偏置的梯度更新可能使用不同的学习率或正则化系数",{"5":{"15":2}}],["反向传播从输出层开始",{"5":{"7":2}}],["反向传播梯度推导",{"0":{"25":1},"4":{"16":1,"25":1},"5":{"21":4,"25":1,"27":4}}],["反向传播梯度推导<",{"5":{"21":1,"27":1}}],["反向传播的核心",{"5":{"13":1,"25":1}}],["反向传播的核心数学工具是链式法则",{"5":{"25":2}}],["反向传播的核心是链式法则的高效应用",{"5":{"25":2}}],["反向传播的梯度满足",{"5":{"22":2}}],["反向传播的梯度满足某种形式的尺度约束",{"5":{"22":2}}],["反向传播的梯度为​",{"5":{"22":1}}],["反向传播的梯度为",{"5":{"22":1}}],["反向传播的flops",{"5":{"25":2}}],["反向传播的主要计算包括两部分",{"5":{"25":2}}],["反向传播的总flops约为前向传播的两倍",{"5":{"25":2}}],["反向传播的计算复杂度约为前向传播的两倍",{"5":{"25":2}}],["反向传播后",{"5":{"22":1}}],["反向传播与自动微分的关系",{"2":{"25":1},"5":{"25":1}}],["反向传播",{"5":{"25":3}}],["反向传播本质上是链式法则在复合函数梯度计算中的高效应用",{"5":{"25":2}}],["反向传播通过巧妙地利用链式法则和动态规划思想",{"5":{"25":2}}],["反向传播算法沿着计算图反向遍历",{"5":{"7":2}}],["反向传播算法",{"5":{"25":2}}],["反向传播算法可以描述为",{"5":{"25":2}}],["反向传播算法对应于反向模式自动微分在神经网络中的应用",{"5":{"25":2}}],["反向传播循环",{"5":{"25":2}}],["反向传播和参数更新",{"5":{"25":2}}],["反向传播是自动微分",{"5":{"25":2}}],["反向传播是反向模式自动微分",{"5":{"25":2}}],["反向传播正是沿此图的反向边计算各节点对损失的梯度",{"5":{"25":2}}],["反幂迭代",{"5":{"9":2}}],["反映了增长的速率",{"5":{"7":2}}],["反之亦然",{"5":{"22":2,"24":2}}],["使梯度能够绕过复杂的非线性变换直接传播到较浅层",{"5":{"9":2}}],["使其均值接近0",{"5":{"8":2}}],["使学生模型能够学习教师模型的知识",{"5":{"8":2}}],["使用齐次坐标",{"5":{"15":2}}],["使用两层神经网络可以解决异或问题",{"5":{"15":2}}],["使用莱布尼茨记号表示为",{"5":{"13":2}}],["使用",{"5":{"7":1,"23":1,"24":2}}],["使用和",{"5":{"7":1}}],["使用指数移动平均来累积梯度平方",{"5":{"7":2}}],["使用代数规则显式推导导数公式",{"5":{"7":2}}],["使用有限差分近似导数",{"5":{"7":2}}],["使用链式法则求导",{"5":{"23":2}}],["使用自身表示导数",{"5":{"23":1}}],["使用乘积法则对求导",{"5":{"23":1}}],["使用乘积法则对",{"5":{"23":1}}],["使用商的求导法则对求导",{"5":{"23":1}}],["使用商的求导法则对",{"5":{"23":1}}],["使用sigmoid函数作为激活函数",{"5":{"24":2}}],["使用交叉熵或均方误差损失时",{"5":{"25":2}}],["使用推论2",{"5":{"25":2}}],["使得等效的变换矩阵具有接近1的特征值",{"5":{"9":2}}],["使得模型能够生成多样化的回复",{"5":{"8":2}}],["使得模型在训练数据上的损失函数值最小化",{"5":{"7":2}}],["使得不同特征方向捕捉不同的信息",{"5":{"9":2}}],["使得不同变量对之间的相关性可以直接比较",{"5":{"8":2}}],["使得不同的参数化方式可以产生相同的函数",{"5":{"7":2}}],["使得不同层之间的信息传递更加可控",{"5":{"24":2}}],["使得不同位置具有可区分的编码",{"5":{"24":2}}],["使得生成的文本既流畅又符合语义逻辑",{"5":{"8":2}}],["使得",{"5":{"7":1,"9":3,"13":2,"22":1,"24":3}}],["使得算法能够适应非平稳的目标函数",{"5":{"7":2}}],["使得更新方向更加平滑",{"5":{"7":2}}],["使得预测具有不确定性估计",{"5":{"7":2}}],["使得整体损失函数朝着下降的方向移动",{"5":{"7":2}}],["使得神经元能够从数据中自动学习最优的连接权重",{"5":{"5":2}}],["使得神经网络能够拟合任意复杂的非线性函数",{"5":{"5":2}}],["使得神经网络能够表示任意复杂的非线性映射关系",{"5":{"24":2}}],["使得我们可以使用线性代数的标准工具进行分析",{"5":{"5":2}}],["使得sigmoid和tanh激活后的输出方差保持稳定",{"5":{"22":2}}],["使得输出梯度更加平衡",{"5":{"22":2}}],["使得参数估计可以在无界的实数空间中进行",{"5":{"23":2}}],["使得它在实践中通常比sigmoid表现更好",{"5":{"23":2}}],["使得注意力模式可以逐层组合",{"5":{"24":2}}],["使得网络具有稀疏表示的能力",{"5":{"24":2}}],["使得和能够捕获输入之间的复杂依赖关系",{"5":{"24":1}}],["使得函数族能够在输入空间中形成",{"5":{"24":1}}],["使得函数族",{"5":{"24":1}}],["使得训练深层网络成为可能",{"5":{"25":2}}],["使模型完全崩溃",{"5":{"22":2}}],["使激活函数能够适应不同的数据分布和任务需求",{"5":{"22":2}}],["与前向传播相反",{"5":{"13":1,"25":1}}],["与前向传播相同",{"5":{"13":1,"25":1}}],["与随机高斯初始化或xavier初始化不同",{"5":{"9":2}}],["与向量的大小无关",{"5":{"9":2}}],["与pmf不同",{"5":{"8":2}}],["与目标",{"5":{"7":1}}],["与凸优化问题不同",{"5":{"7":2}}],["与完整的hessian相比",{"5":{"7":2}}],["与数值微分",{"5":{"7":2,"13":2}}],["与麦肯罗皮层神经元使用阶跃函数不同",{"5":{"5":2}}],["与下游任务的契合",{"2":{"22":1},"5":{"22":1}}],["与注意力机制的梯度流",{"2":{"22":1},"5":{"22":1}}],["与注意力机制的设计协同",{"2":{"22":1},"5":{"22":1}}],["与权重矩阵区别对待",{"5":{"15":2}}],["与权重初始化的关系",{"5":{"22":2}}],["与批归一化不同",{"5":{"22":2}}],["与批归一化的比较",{"5":{"22":2}}],["与层归一化相比",{"5":{"22":2}}],["与sigmoid相比",{"5":{"22":2,"23":4}}],["与sigmoid导数的比较",{"5":{"23":2}}],["与sigmoid导数在形式上非常相似",{"5":{"23":1}}],["与sigmoid导数",{"5":{"23":1}}],["与sigmoid的数学关系",{"2":{"23":1},"5":{"23":1}}],["与sigmoid的梯度比较",{"5":{"23":2}}],["与梯度饱和相反",{"5":{"22":2}}],["与梯度消失问题的联系",{"5":{"23":2}}],["与transformer架构的联系",{"5":{"24":2}}],["与第五章注意力的联系",{"5":{"24":2}}],["与第四章损失函数的联系",{"5":{"24":2}}],["与位置编码的联合设计",{"5":{"22":2}}],["与位置编码的信息论联系",{"5":{"24":2}}],["与损失函数的联系",{"5":{"24":2}}],["与其他行无关",{"5":{"13":2}}],["与其通过标准正态累积分布函数",{"5":{"24":1}}],["与输入向量的外积",{"5":{"25":2}}],["元素级函数",{"5":{"7":2}}],["加法节点",{"5":{"13":2}}],["加法",{"5":{"7":2,"9":2,"13":2}}],["加权和至少为1",{"5":{"5":2}}],["加权和为",{"5":{"5":6}}],["加权求和信息整合与阈值触发机制",{"5":{"5":2}}],["加权求和结果",{"5":{"23":2}}],["加速了训练收敛",{"5":{"23":2}}],["计算各层的输出",{"5":{"15":2}}],["计算顺序",{"5":{"13":1}}],["计算​后合并结果",{"5":{"13":1}}],["计算",{"5":{"13":3,"25":5}}],["计算图的前向遍历",{"5":{"13":2}}],["计算图的表示与实现",{"2":{"13":1},"5":{"13":1}}],["计算图是连接数学理论和工程实现的桥梁",{"5":{"13":2}}],["计算图是有向无环图",{"5":{"13":2}}],["计算图是一个有向无环图",{"5":{"13":2}}],["计算图是表示数学表达式的有向无环图",{"5":{"7":2}}],["计算图是前向传播构建的",{"5":{"25":2}}],["计算图",{"5":{"13":4}}],["计算每次的统计量估计",{"5":{"8":2}}],["计算分类任务中的正确预测数量等",{"5":{"8":2}}],["计算其输出关于输入的梯度",{"5":{"7":2}}],["计算梯度",{"5":{"7":2,"25":2}}],["计算效率",{"2":{"22":1},"5":{"22":3}}],["计算矩阵的谱半径通常需要数值方法",{"5":{"22":2}}],["计算量约为lu分解的一半",{"5":{"9":2}}],["计算量减少",{"5":{"22":2}}],["计算成本较高",{"5":{"22":2}}],["计算复杂度与内存效率",{"2":{"9":1},"5":{"9":1}}],["计算复杂度分析",{"2":{"25":1},"5":{"25":1}}],["计算权重梯度",{"5":{"25":2}}],["计算偏置梯度",{"5":{"25":2}}],["计算偏置梯度约为​",{"5":{"25":1}}],["计算偏置梯度约为",{"5":{"25":1}}],["计算前一层误差",{"5":{"25":2}}],["计算的是query和key之间的相似度矩阵",{"5":{"24":2}}],["计算的flops约为​",{"5":{"25":1}}],["计算类型",{"5":{"25":1}}],["计算方向",{"5":{"25":1}}],["雅可比矩阵向量积",{"5":{"13":1,"25":1}}],["雅可比矩阵",{"5":{"7":2,"13":2,"22":1,"25":1}}],["雅可比矩阵包含所有一阶偏导数",{"5":{"7":2}}],["雅可比矩阵的对角元素非常小",{"5":{"22":1}}],["雅可比矩阵​是对角矩阵",{"5":{"25":1}}],["情况稍微复杂一些",{"5":{"7":2}}],["复合函数的雅可比矩阵",{"5":{"13":2}}],["复合函数的梯度",{"5":{"13":2}}],["复合函数的偏导数法则与单变量情况类似",{"5":{"7":2}}],["复合函数",{"5":{"13":2}}],["复合函数视角下的前向传播",{"2":{"13":1},"5":{"13":1}}],["复合后的结果仍然是线性变换",{"5":{"5":2}}],["常用于矩阵补全",{"5":{"9":2}}],["常用的矩阵范数包括frobenius范数",{"5":{"9":2}}],["常用的效应量包括cohen",{"5":{"8":2}}],["常用的激活函数如sigmoid和tanh被选择是因为它们具有良好的数学性质",{"5":{"24":2}}],["常用激活函数",{"5":{"5":2}}],["常数因子的偏导数等于该因子乘以函数的偏导数",{"5":{"7":2}}],["常数的偏导数为零",{"5":{"7":2}}],["常见的张量分解方法包括",{"5":{"9":2}}],["常见的学习率调度方法包括",{"5":{"7":2}}],["常见的裁剪方式包括",{"5":{"25":2}}],["常见的数值稳定性问题与解决方案本节系统阐述了反向传播算法的数学理论基础",{"5":{"25":1}}],["常见的数值稳定性问题与解决方案",{"5":{"25":1}}],["常见输出层误差信号",{"5":{"25":2}}],["梯度累积在数学上等价于使用更大的批次进行训练",{"5":{"9":2}}],["梯度累积是另一种与批处理相关的技术",{"5":{"9":2}}],["梯度累积效应",{"5":{"22":2}}],["梯度估计的方差较大",{"5":{"8":2}}],["梯度估计的方差减小",{"5":{"8":2}}],["梯度的方向更加稳定",{"5":{"8":2}}],["梯度的性质对于理解优化过程至关重要",{"5":{"7":2}}],["梯度的范数满足",{"5":{"22":2}}],["梯度的累积效应可能导致不稳定",{"5":{"22":2}}],["梯度的大小也更加稳定",{"5":{"22":2}}],["梯度裁剪就是将梯度向量的模长限制在某个阈值之内",{"5":{"9":2}}],["梯度裁剪",{"5":{"7":2,"25":7}}],["梯度裁剪是最常用的策略",{"5":{"22":2}}],["梯度裁剪有两种主要形式",{"5":{"22":2}}],["梯度裁剪的边界效应",{"5":{"25":2}}],["梯度裁剪将梯度范数限制在范围内",{"5":{"25":1}}],["梯度裁剪将梯度范数限制在",{"5":{"25":1}}],["梯度下降以线性速度收敛",{"5":{"7":2}}],["梯度下降在凸优化中的收敛性有完善的理论保证",{"5":{"7":2}}],["梯度下降是最基本也最广泛使用的优化算法",{"5":{"7":2}}],["梯度下降与凸优化基础",{"2":{"7":1},"5":{"7":1}}],["梯度向量与等值面垂直",{"5":{"7":2}}],["梯度向量指导着参数的更新方向",{"5":{"7":2}}],["梯度向量指向函数增长最快的方向",{"5":{"7":2}}],["梯度向量就是误差信号本身",{"5":{"25":2}}],["梯度是多元函数最陡上升方向的向量",{"5":{"7":2}}],["梯度和hessian等概念",{"5":{"7":2}}],["梯度饱和的数学机制",{"2":{"22":1},"5":{"22":1}}],["梯度饱和与梯度爆炸的数学根源",{"0":{"22":1},"4":{"19":1,"22":1},"5":{"21":4,"22":1,"27":4}}],["梯度饱和与梯度爆炸的数学根源<",{"5":{"21":1,"27":1}}],["梯度饱和与梯度爆炸是深度学习训练中的两大核心挑战",{"5":{"22":2}}],["梯度饱和是深度学习训练中的核心挑战之一",{"5":{"22":2}}],["梯度饱和",{"5":{"22":2,"23":2}}],["梯度饱和可以在输出尚未饱和时发生",{"5":{"22":2}}],["梯度饱和意味着信息在反向传播过程中丢失",{"5":{"22":2}}],["梯度从输出层向输入层传播",{"5":{"22":2}}],["梯度从输出层向输入层传播时",{"5":{"23":2}}],["梯度衰减为原来的",{"5":{"22":2,"23":2}}],["梯度衰减为",{"5":{"22":2,"23":2}}],["梯度爆炸的数学机制",{"2":{"22":1},"5":{"22":1}}],["梯度爆炸的现象描述",{"2":{"22":1},"5":{"22":1}}],["梯度爆炸的阈值与裁剪策略",{"2":{"22":1},"5":{"22":1}}],["梯度爆炸的表现包括",{"5":{"22":2}}],["梯度爆炸指的是在反向传播过程中梯度值变得非常大",{"5":{"22":2}}],["梯度爆炸在循环神经网络和transformer中尤为突出",{"5":{"22":2}}],["梯度爆炸",{"5":{"22":2,"25":1}}],["梯度爆炸可能通过多个渠道发生",{"5":{"22":2}}],["梯度需要沿时间步反向传播",{"5":{"22":2}}],["梯度至少可以通过恒等路径",{"5":{"22":2}}],["梯度高速公路",{"5":{"22":2}}],["梯度流与信息保持",{"2":{"22":1},"5":{"22":1}}],["梯度流",{"5":{"22":2}}],["梯度流的信息瓶颈",{"5":{"22":2}}],["梯度流稳定",{"5":{"22":2}}],["梯度流更加稳定",{"5":{"22":2}}],["梯度流通过注意力权重矩阵进行",{"5":{"22":1}}],["梯度流通过注意力权重矩阵",{"5":{"22":1}}],["梯度范数约为",{"5":{"22":2}}],["梯度范数与奇异值",{"5":{"22":2}}],["梯度范数可能放大",{"5":{"22":2}}],["梯度范数可能衰减",{"5":{"22":2}}],["梯度范数边界",{"5":{"22":2}}],["梯度范数的可能变化范围越大",{"5":{"22":2}}],["梯度主要集中在少数方向",{"5":{"22":2}}],["梯度会均匀分布到所有位置",{"5":{"22":2}}],["梯度尺度的归一化",{"5":{"22":2}}],["梯度尺度归一化",{"5":{"22":2}}],["梯度也不会因为激活值的尺度变化而放大或缩小",{"5":{"22":2}}],["梯度也被限制在这个流形的切空间中",{"5":{"22":2}}],["梯度稳定性分析",{"5":{"22":2}}],["梯度关于的雅可比矩阵的范数满足",{"5":{"22":1}}],["梯度关于",{"5":{"22":1}}],["梯度仍然可以通过路径传递到前一层",{"5":{"22":1}}],["梯度仍然可以通过",{"5":{"22":1}}],["梯度几乎为零",{"5":{"23":2}}],["梯度消失",{"5":{"23":2,"25":1}}],["梯度消失或梯度爆炸等问题",{"5":{"25":2}}],["梯度与等值面",{"5":{"7":2}}],["梯度与雅可比矩阵",{"5":{"25":2}}],["梯度",{"5":{"25":2}}],["梯度检查点",{"5":{"9":2}}],["梯度检查点技术通过在前向传播时只保存部分激活值",{"5":{"7":2}}],["梯度检查",{"5":{"25":4}}],["梯度可以稳定传播",{"5":{"22":2}}],["梯度可以正可以负",{"5":{"23":2}}],["梯度可能因精度限制而下溢",{"5":{"25":2}}],["梯度缩放",{"5":{"25":3}}],["梯度缩放等技术都是保证大规模神经网络训练稳定性的重要手段",{"5":{"25":2}}],["梯度计算",{"5":{"25":1}}],["梯度值接近0",{"5":{"25":1}}],["梯度值溢出",{"5":{"25":1}}],["梯度值在fp16下为0",{"5":{"25":1}}],["链式法则是分析这一复合过程的核心数学工具",{"5":{"13":2}}],["链式法则在反向传播算法中扮演着核心角色",{"5":{"13":2}}],["链式法则给出了多个随机变量的联合熵分解",{"5":{"8":2}}],["链式法则的应用更加直观和系统",{"5":{"7":2}}],["链式法则的应用更加系统化",{"5":{"7":2}}],["链式法则的基本形式可以这样表述",{"5":{"7":2}}],["链式法则的矩阵形式",{"2":{"25":1},"5":{"25":1}}],["链式法则使得我们能够计算损失函数相对于任意深层参数的梯度",{"5":{"7":2}}],["链式法则将在下文详细讨论",{"5":{"7":2}}],["链式法则",{"5":{"7":4,"13":4}}],["偏度",{"5":{"8":2}}],["偏导数可以理解为多元函数图像与包含坐标轴",{"5":{"7":1}}],["偏导数可以理解为多元函数图像与包含坐标轴的平面相交得到的一元函数曲线的切线斜率",{"5":{"7":1}}],["偏导数的计算遵循与单变量导数相同的规则",{"5":{"7":2}}],["偏导数是多元函数对单个变量的导数",{"5":{"7":2}}],["偏导数与链式法则",{"2":{"7":1},"5":{"7":1}}],["偏置向量通常有特殊的初始化策略",{"5":{"15":2}}],["偏置向量为",{"5":{"5":2}}],["偏置",{"5":{"13":1,"15":6}}],["偏置加法",{"5":{"13":2}}],["偏置的全连接层",{"5":{"13":1}}],["偏置节点",{"5":{"13":2}}],["偏置项",{"5":{"5":1,"25":1}}],["偏置项提供灵活的阈值调节",{"5":{"5":2}}],["偏置项则提供了灵活的阈值调节能力",{"5":{"5":1}}],["偏置项直接加到净输入上",{"5":{"25":1}}],["偏置梯度",{"5":{"25":2}}],["偏置梯度为",{"5":{"25":2}}],["希尔伯特",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["w^2",{"5":{"15":2}}],["w",{"5":{"15":22,"23":6}}],["werden",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["weight",{"5":{"9":2}}],["weierstrass逼近定理",{"5":{"24":2}}],["weierstrass定理指出",{"5":{"24":2}}],["weierstrass定理推导出通用逼近能力",{"5":{"24":2}}],["wise",{"5":{"9":2}}],["wissen",{"2":{"6":2,"14":2,"21":2,"27":2},"5":{"6":2,"14":2,"21":2,"27":2}}],["wir",{"2":{"6":2,"14":2,"21":2,"27":2},"5":{"6":2,"14":2,"21":2,"27":2}}],["williams",{"5":{"25":2}}],["warp",{"5":{"13":1}}],["warmup策略在训练初期逐渐增加学习率",{"5":{"7":2}}],["warmup",{"5":{"7":4}}],["warren",{"5":{"5":2}}],["walter",{"5":{"5":2}}],["大小",{"5":{"9":2}}],["大语言模型中的输入通常是一个三维张量",{"5":{"9":2}}],["大语言模型中的所有参数最终都以矩阵或张量的形式存储和计算",{"5":{"9":2}}],["大语言模型的某些扩展",{"5":{"8":2}}],["大语言模型的优化面临独特的挑战",{"5":{"7":2}}],["大语言模型的训练本质上是一个优化问题",{"5":{"7":2}}],["大数定律和中心极限定理也指导着模型评估策略",{"5":{"8":2}}],["大数定律和中心极限定理是概率论中最重要的极限定理",{"5":{"8":2}}],["大数定律解释了为什么使用更大的批量",{"5":{"8":2}}],["大数定律",{"5":{"8":2}}],["大数定律与中心极限定理",{"2":{"8":1},"5":{"8":1}}],["大卫",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"25":2,"27":1}}],["大模型中的数学",{"0":{"6":1,"14":1,"21":1,"27":1},"4":{"20":1,"21":1},"5":{"6":3,"14":1,"21":1,"27":1}}],["大大提高了计算效率",{"5":{"5":2}}],["大量这样的局部探测器通过线性组合",{"5":{"24":2}}],["列索引",{"5":{"15":1}}],["列索引对应输入特征的索引",{"5":{"15":1}}],["列和",{"5":{"13":1}}],["列仅依赖于",{"5":{"13":1}}],["列空间",{"5":{"9":2}}],["列",{"5":{"5":1}}],["若两个张量的维度数不同",{"5":{"9":2}}],["若服从多元高斯分布",{"5":{"8":1}}],["若和独立",{"5":{"8":1}}],["若",{"5":{"5":1,"8":6,"25":6}}],["若​",{"5":{"5":1}}],["若目标函数具有组合",{"5":{"24":1}}],["若目标函数具有",{"5":{"24":1}}],["个元素",{"5":{"15":1}}],["个元素为",{"5":{"8":1}}],["个输入特征与第",{"5":{"15":1}}],["个输出神经元之间的连接强度",{"5":{"15":1}}],["个输出",{"5":{"13":1}}],["个超平面",{"5":{"15":1}}],["个神经元实际上定义了",{"5":{"15":1}}],["个神经元的层",{"5":{"5":1}}],["个神经元的输出归一化为概率分布",{"5":{"5":1}}],["个神经元",{"5":{"24":2}}],["个独立的子计算",{"5":{"13":1}}],["个象限",{"5":{"13":1}}],["个坐标轴分割成",{"5":{"13":1}}],["个秩一张量的和",{"5":{"9":1}}],["个参数",{"5":{"9":1}}],["个参数而不是",{"5":{"9":1}}],["个索引",{"5":{"9":1}}],["个类别的logit",{"5":{"8":1}}],["个类别上服从类别分布",{"5":{"8":1}}],["个类别之一",{"5":{"5":1}}],["个样本的计算可以分解为",{"5":{"13":1}}],["个样本的计算可以分解为个独立的子计算",{"5":{"13":1}}],["个样本的第",{"5":{"13":1}}],["个样本",{"5":{"5":2,"13":2}}],["个头的输出拼接为",{"5":{"22":1}}],["个权重加",{"5":{"24":2}}],["个偏置",{"5":{"24":2}}],["个分量",{"5":{"25":2}}],["映射到新的特征空间",{"5":{"5":1}}],["映射到输出",{"5":{"5":1}}],["属于类别1",{"5":{"5":1}}],["属于类别0",{"5":{"5":1}}],["点",{"5":{"5":2}}],["点和属于类别1",{"5":{"5":1}}],["点和属于类别0",{"5":{"5":1}}],["05",{"5":{"8":2}}],["0",{"5":{"5":6,"23":42}}],["044715x^3",{"5":{"23":2}}],["层前馈网络的计算可以表示为一系列函数的复合",{"5":{"15":2}}],["层前馈网络定义的函数",{"5":{"13":1}}],["层前馈网络定义的函数​是各层映射的复合",{"5":{"13":1}}],["层前馈网络",{"5":{"25":1}}],["层非线性变换",{"5":{"13":1}}],["层内的输出神经元",{"5":{"13":1}}],["层内并行",{"5":{"13":2}}],["层全连接网络",{"5":{"13":1,"25":1}}],["层",{"5":{"13":2}}],["层映射",{"5":{"13":2}}],["层的激活函数",{"5":{"15":1}}],["层的权重矩阵和偏置向量",{"5":{"15":1}}],["层的概念在神经网络架构设计中具有核心地位",{"5":{"15":2}}],["层的输出",{"5":{"13":1}}],["层的输出为",{"5":{"5":1}}],["层的输入为",{"5":{"13":1,"15":1}}],["层的输入",{"5":{"22":1}}],["层的前馈神经网络",{"5":{"13":1,"15":1}}],["层的前馈网络",{"5":{"5":1,"22":1,"23":1}}],["层的表示向量",{"5":{"24":1}}],["层的计算为",{"5":{"22":1}}],["层的计算定义为",{"5":{"25":1}}],["层的误差信号",{"5":{"25":1}}],["层的误差信号矩阵为",{"5":{"25":1}}],["层网络中",{"5":{"13":1}}],["层网络的等效仿射变换形式",{"5":{"5":1}}],["层网络",{"5":{"5":1}}],["层操作",{"5":{"5":2}}],["层归一化与transformer的配合",{"2":{"22":1},"5":{"22":1}}],["层归一化",{"5":{"22":7}}],["层归一化定义为",{"5":{"22":2}}],["层归一化的数学原理",{"2":{"22":1},"5":{"22":1}}],["层归一化的关键优势是它不依赖于batch",{"5":{"22":2}}],["层归一化的计算特性",{"5":{"22":2}}],["层归一化在特征维度上进行归一化",{"5":{"22":2}}],["层归一化在每个样本内部计算统计量",{"5":{"22":2}}],["层归一化通常部署在注意力层和前馈层之间",{"5":{"22":2}}],["层归一化稳定了注意力计算和前馈计算的输入分布",{"5":{"22":2}}],["层归一化位于残差分支的输入端",{"5":{"22":2}}],["层归一化位于残差分支的输出端",{"5":{"22":2}}],["层归一化是标准",{"5":{"22":1}}],["层后",{"5":{"22":1,"23":1}}],["层次化特征学习",{"5":{"24":2}}],["层误差信号",{"5":{"25":1}}],["层激活函数的导数",{"5":{"25":1}}],["层权重的梯度为",{"5":{"25":2}}],["层偏置的梯度为",{"5":{"25":2}}],["本质上是对输入",{"5":{"13":1}}],["本身是线性的",{"5":{"13":1}}],["本身是线性变换",{"5":{"5":1}}],["本身",{"5":{"13":2}}],["本节将从复合函数",{"5":{"13":2}}],["本节将从数学建模的角度",{"5":{"5":2}}],["本节将从数学的角度深入分析这两种现象的根源",{"5":{"22":2}}],["本节将从数学的角度分析梯度流的各种机制",{"5":{"22":2}}],["本节将从数学的角度系统性地推导各类激活函数的导数公式",{"5":{"23":2}}],["本节将从数学的角度系统性地分析激活函数的理论基础",{"5":{"24":2}}],["本节将从数学推导的角度",{"5":{"25":2}}],["本节将从多个角度深入分析梯度饱和的根源",{"5":{"22":2}}],["本节将从函数逼近论的角度",{"5":{"24":2}}],["本节将系统阐述如何将神经网络表示为矩阵运算的形式",{"5":{"15":2}}],["本节将系统分析梯度爆炸的数学根源",{"5":{"22":2}}],["本节将系统分析各类归一化技术的数学原理及其对梯度稳定性的影响",{"5":{"22":2}}],["本节将系统分析激活函数与transformer其他组件",{"5":{"24":2}}],["本节将系统性地分析主要激活函数的概率解释",{"5":{"24":2}}],["本节的核心内容可以概括为以下几点",{"5":{"13":2,"25":2}}],["本节的内容可以概括为以下几个核心要点",{"5":{"5":2}}],["本节小结",{"2":{"5":1,"13":1,"24":1,"25":1},"5":{"5":1,"13":1,"24":1,"25":1}}],["本节从数学的角度系统性地分析了梯度饱和与梯度爆炸的根源",{"5":{"22":2}}],["本节从多个数学视角深入分析了前向传播的本质",{"5":{"13":1}}],["本节从多个数学视角系统性地分析了激活函数的角色",{"5":{"24":2}}],["本节系统地阐述了神经元的数学模型",{"5":{"5":2}}],["本节系统阐述了反向传播算法的数学理论基础",{"5":{"25":1}}],["扩展为齐次坐标",{"5":{"5":1}}],["决定了它所代表的线性变换的类型",{"5":{"9":2}}],["决定了不同特征之间的线性组合方式",{"5":{"5":1}}],["决定了系统的稳定性",{"5":{"22":1}}],["决定了梯度在对应方向上的缩放因子",{"5":{"22":1}}],["决策边界",{"5":{"5":4,"24":2}}],["承担着特征提取和信息整合的功能",{"5":{"5":1}}],["后者就是编码器分布与标准高斯先验之间的kl散度",{"5":{"8":2}}],["后",{"5":{"5":1,"8":1}}],["后关于",{"5":{"24":1}}],["到",{"5":{"5":1,"9":1,"15":1,"24":2}}],["到对应的输入维度",{"5":{"25":2}}],["时间复杂度",{"5":{"13":1,"25":1}}],["时可归入任一模式",{"5":{"13":1}}],["时kl散度为0",{"5":{"8":1}}],["时取严格小于号",{"5":{"7":1}}],["时",{"5":{"5":8,"8":7,"9":1,"13":1,"15":4,"22":13,"23":14,"24":2}}],["时变化最慢",{"5":{"23":2}}],["掌握本节的数学基础后",{"5":{"5":2,"25":2}}],["理解了仿射变换与激活函数的组合如何实现从输入到输出的非线性映射",{"5":{"15":2}}],["理解前向传播的计算复杂度对于评估网络效率",{"5":{"13":1}}],["理解前向传播的数学本质",{"5":{"13":4}}],["理解这些并行策略的数学基础",{"5":{"9":2}}],["理解这些原因对于理解现代大语言模型的架构设计至关重要",{"5":{"24":2}}],["理解这个几何结构对于设计有效的优化算法至关重要",{"5":{"24":2}}],["理解这一几何本质对于分析神经网络的表达能力和特征学习机制至关重要",{"5":{"13":2}}],["理解这一几何本质对于分析神经网络的能力和局限性至关重要",{"5":{"5":2}}],["理解这一关系有助于我们从更高的视角理解梯度的计算原理",{"5":{"25":2}}],["理解不同运算的资源需求对于优化模型设计和硬件利用至关重要",{"5":{"9":2}}],["理解张量运算的数学本质有助于编写高效的代码和诊断性能瓶颈",{"5":{"9":2}}],["理解它们的数学性质和计算特性对于优化模型性能和调试模型行为至关重要",{"5":{"9":2}}],["理解最大似然估计",{"5":{"8":2}}],["理解随机变量和概率分布的概念",{"5":{"8":2}}],["理解偏导数",{"5":{"7":2}}],["理解矩阵运算与逐样本计算的一致性",{"5":{"5":2}}],["理解仿射变换的几何本质",{"5":{"5":2}}],["理解梯度饱和的数学机制对于设计有效的激活函数和训练策略至关重要",{"5":{"22":2}}],["理解梯度流对于设计深层网络架构和训练策略至关重要",{"5":{"22":2}}],["理解反向传播的计算复杂度对于评估训练效率和设计高效算法至关重要",{"5":{"25":2}}],["理论上可以拟合任意连续函数",{"5":{"5":2}}],["理论简洁",{"5":{"22":2}}],["理想的激活函数应该在整个输入范围内都有非零导数",{"5":{"22":2}}],["理想的层次化学习应该满足",{"5":{"24":2}}],["赋予神经网络万能逼近能力",{"5":{"5":2}}],["包括矩阵乘法",{"5":{"9":2}}],["包括结合律",{"5":{"9":2}}],["包括gram",{"5":{"9":2}}],["包括参数估计和假设检验两大类方法",{"5":{"8":2}}],["包括损失函数的",{"5":{"7":2}}],["包括逻辑与",{"5":{"5":2}}],["包括梯度如何衰减",{"5":{"22":2}}],["包括在线学习和自回归模型",{"5":{"22":2}}],["包括rnn的时间反向传播问题和transformer中的梯度挑战",{"5":{"22":2}}],["包括有效秩",{"5":{"22":2}}],["包括leaky",{"5":{"23":2}}],["包含注意力机制和前馈网络",{"5":{"13":2}}],["包含了关于所有自变量的二阶偏导数信息",{"5":{"7":2}}],["包含了函数关于所有自变量的偏导数信息",{"5":{"7":2}}],["包含",{"5":{"5":1}}],["包含仿射变换和非线性激活两个组成部分",{"5":{"5":2}}],["包含所有可以通过原点的k维平面上的点",{"5":{"9":2}}],["包含所有奇异值",{"5":{"22":1}}],["包含所有常数函数",{"5":{"24":2}}],["包含前向传播",{"5":{"25":2}}],["并各自产生输出",{"5":{"15":1}}],["并各自产生输出​",{"5":{"15":1}}],["并行粒度",{"5":{"13":1}}],["并行层次",{"5":{"13":1}}],["并为反向传播的梯度计算提供数据结构支持",{"5":{"13":2}}],["并为理解网络的表达能力和计算特性提供了理论工具",{"5":{"13":2}}],["并按与前向传播相反的顺序传播这些梯度",{"5":{"13":2}}],["并按与前向传播相同的顺序计算",{"5":{"13":2}}],["并将计算速度提高近一倍",{"5":{"9":2}}],["并相应地调整学习率",{"5":{"7":2}}],["并且计算成本更低",{"5":{"7":2}}],["并从几何和概率两个角度分析了神经元的行为",{"5":{"5":2}}],["并建立与后续章节的联系",{"5":{"5":2}}],["并深入探讨激活函数导数对神经网络训练的影响",{"5":{"23":2}}],["并分析反向传播的计算复杂度",{"5":{"25":2}}],["并与反向传播计算的梯度进行比较",{"5":{"25":2}}],["并有效缓解梯度消失问题",{"5":{"9":2}}],["并有能力实现自己的深度学习框架或深入理解现有框架的底层机制",{"5":{"25":2}}],["并化简",{"5":{"25":1}}],["结果是将向量的每个分量都乘以该标量",{"5":{"9":2}}],["结果等价于逐样本计算后按行堆叠",{"5":{"5":2}}],["结论成立",{"5":{"5":2}}],["结论显然成立",{"5":{"5":2}}],["结构意味着激活值在0",{"5":{"23":2}}],["结构",{"5":{"24":2}}],["深入剖析前向传播的内在机制",{"5":{"13":2}}],["深入分析单层神经元的数学结构",{"5":{"5":2}}],["深入分析激活函数的数学角色",{"5":{"24":2}}],["深度",{"5":{"15":2}}],["深度学习应用",{"5":{"13":1,"25":1}}],["深度学习框架也提供了高效的函数来计算矩阵的特征值分解和奇异值分解",{"5":{"9":2}}],["深度学习框架会在不显式复制数据的情况下",{"5":{"9":2}}],["深度学习框架",{"5":{"5":2}}],["深度网络的表示空间具有层次化的结构",{"5":{"13":2}}],["深度网络的特征空间层次",{"5":{"13":2}}],["深度网络往往收敛到具有相似损失值的局部最小值",{"5":{"7":2}}],["深度网络通过逐层的非线性变换",{"5":{"24":2}}],["深度网络能够表示的函数空间远大于同等参数量的浅层网络",{"5":{"24":2}}],["深度网络用指数级更少的参数可以达到与浅层网络相同的表达能力",{"5":{"24":2}}],["深度与宽度的数学权衡",{"2":{"24":1},"5":{"24":1}}],["深度优先",{"5":{"24":2}}],["深度指数优势",{"5":{"24":2}}],["深度为的网络可以等效于宽度为指数级别的浅层网络",{"5":{"24":1}}],["深度为",{"5":{"24":1}}],["深度神经网络的损失函数通常是非凸的",{"5":{"7":2}}],["深度神经网络的训练涉及大量的梯度计算和参数更新",{"5":{"25":2}}],["深层网络因此陷入",{"5":{"22":2}}],["特性",{"5":{"13":1,"22":1,"25":1}}],["特别适用于样本量不大或分布未知的情况",{"5":{"8":2}}],["特别是当目标分布是高度非高斯的时候",{"5":{"8":2}}],["特别是gpu",{"5":{"5":2}}],["特别是第五章注意力机制",{"5":{"24":2}}],["特征层次假说",{"5":{"13":2}}],["特征向量",{"5":{"9":2}}],["特征向量表示这些曲率对应的方向",{"5":{"7":2}}],["特征值的平方根",{"5":{"13":1}}],["特征值的计算需要求解特征方程",{"5":{"9":2}}],["特征值是实数",{"5":{"9":2}}],["特征值可以是负数",{"5":{"9":2}}],["特征值分解在动力系统分析",{"5":{"9":2}}],["特征值分解总是可行的",{"5":{"9":2}}],["特征值分解将方阵分解为特征向量矩阵和特征值对角矩阵的乘积",{"5":{"9":2}}],["特征值分解和cholesky分解等多种方法",{"5":{"9":2}}],["特征值分解只适用于方阵且要求矩阵可对角化",{"5":{"9":2}}],["特征值分解用于提取数据中方差最大的方向",{"5":{"9":2}}],["特征值与网络稳定性密切相关",{"5":{"9":2}}],["特征值与特征向量",{"2":{"9":1},"5":{"9":1}}],["特征值和特征向量有着多方面的应用",{"5":{"9":2}}],["特征值和特征向量是线性代数中最深刻也最有应用价值的概念之一",{"5":{"9":2}}],["特征空间",{"5":{"13":2}}],["特征空间是机器学习和深度学习中描述数据表示的核心概念",{"5":{"9":2}}],["特征空间的几何直觉",{"2":{"9":1},"5":{"9":1}}],["特征空间的维度",{"5":{"5":1}}],["特征空间的维度由神经元的数量决定",{"5":{"5":1}}],["特征函数提供了有用的分析工具",{"5":{"8":2}}],["特征函数是矩母函数的复数形式",{"5":{"8":2}}],["特征维度",{"5":{"22":1}}],["特定任务激活函数",{"5":{"22":2}}],["是理解其他层类型的基础",{"5":{"15":2}}],["是理解牛顿法等二阶优化方法的基础",{"5":{"7":2}}],["是半正定对称矩阵",{"5":{"13":1}}],["是正交矩阵",{"5":{"13":1}}],["是由网络参数定义的函数映射",{"5":{"13":1}}],["是衡量计算复杂度的标准指标",{"5":{"13":2}}],["是这种中间表示的标准形式",{"5":{"13":2}}],["是对角矩阵",{"5":{"13":2}}],["是对应的特征值",{"5":{"9":2}}],["是神经网络信息流动的基本方式",{"5":{"13":2}}],["是注意力头的数量",{"5":{"9":1}}],["是批大小",{"5":{"9":1}}],["是批处理的一个变体",{"5":{"9":2}}],["是批方差",{"5":{"22":2}}],["是批均值",{"5":{"22":1}}],["是批量大小",{"5":{"9":1}}],["是批量预测矩阵",{"5":{"25":1}}],["是批量隐藏激活矩阵",{"5":{"25":1}}],["是单位矩阵",{"5":{"9":1}}],["是训练超大规模语言模型的必要技术",{"5":{"9":2}}],["是训练神经网络的关键技术基础",{"5":{"7":2}}],["是训练这些模型的主要挑战之一",{"5":{"22":2}}],["是的伪逆",{"5":{"13":1}}],["是的矩阵",{"5":{"9":1}}],["是的特征值的平方根",{"5":{"9":1}}],["是将矩阵转换为列向量的操作",{"5":{"9":2}}],["是将任意矩阵分解为三个矩阵乘积的强大工具",{"5":{"9":2}}],["是矩阵所有奇异值的和",{"5":{"9":2}}],["是矩阵所有列向量的线性组合构成的空间",{"5":{"9":2}}],["是矩阵的最大奇异值",{"5":{"9":4}}],["是矩阵概念在更高维度的延伸",{"5":{"9":2}}],["是序列长度",{"5":{"9":3}}],["是线性代数中最重要的研究对象之一",{"5":{"9":2}}],["是线性变换与平移的组合",{"5":{"5":1}}],["是线性的",{"5":{"24":1}}],["是似然函数",{"5":{"8":1}}],["是未知参数",{"5":{"8":1}}],["是模型预测分布时",{"5":{"8":1}}],["是真实分布而",{"5":{"8":1}}],["是真实词元",{"5":{"8":2}}],["是真实标签",{"5":{"25":2}}],["是均值向量",{"5":{"8":1}}],["是均值参数",{"5":{"8":1}}],["是均值为0",{"5":{"8":2}}],["是均值",{"5":{"22":1}}],["是率参数",{"5":{"8":1}}],["是成功概率",{"5":{"8":1}}],["是边际似然",{"5":{"8":2}}],["是常数",{"5":{"8":2}}],["是给定一个随机变量时另一个随机变量的熵",{"5":{"8":2}}],["是定义损失函数和分析模型行为的重要工具",{"5":{"8":2}}],["是信息论中的核心概念",{"5":{"8":2}}],["是协方差矩阵的逆矩阵",{"5":{"8":2}}],["是协方差矩阵的行列式",{"5":{"8":2}}],["是协方差矩阵",{"5":{"8":2}}],["是标准正态分布的cdf",{"5":{"8":1}}],["是标准差",{"5":{"8":2}}],["是标准化的协方差",{"5":{"8":2}}],["是伯努利分布向多值情况的推广",{"5":{"8":2}}],["是n次独立伯努利试验中成功次数的分布",{"5":{"8":2}}],["是最基本也是最重要的参数估计方法",{"5":{"8":2}}],["是最简单的连续分布",{"5":{"8":2}}],["是最简单的离散分布",{"5":{"8":2}}],["是最流行的自适应优化算法",{"5":{"7":2}}],["是最早提出且应用广泛的归一化技术",{"5":{"22":2}}],["是描述离散随机变量概率分布的函数",{"5":{"8":2}}],["是掌握语言模型概率本质的必要前提",{"5":{"8":2}}],["是衰减系数",{"5":{"7":1}}],["是速度向量",{"5":{"7":1}}],["是凸函数的充要条件是对于任意",{"5":{"7":1}}],["是第",{"5":{"7":1,"8":1,"24":1}}],["是残差函数的雅可比矩阵",{"5":{"7":1}}],["是梯度与方向",{"5":{"7":1}}],["是梯度为零向量的点",{"5":{"7":2}}],["是激活函数",{"5":{"7":1,"24":1}}],["是外层函数关于中间变量的导数",{"5":{"7":1}}],["是动量系数",{"5":{"7":2}}],["是加速梯度下降的重要技术",{"5":{"7":2}}],["是在训练过程中动态调整学习率的策略",{"5":{"7":2}}],["是损失函数在当前参数处的梯度",{"5":{"7":2}}],["是学习率",{"5":{"7":2}}],["是现代深度学习框架的核心功能",{"5":{"7":2}}],["是一个单位向量",{"5":{"7":1}}],["是一个可微函数",{"5":{"7":1}}],["是一个",{"5":{"7":1,"8":2,"9":2}}],["是一个与",{"5":{"7":1}}],["是一个多元函数",{"5":{"7":1}}],["是一个的矩阵",{"5":{"7":1,"9":1}}],["是一种利用计算图高效计算导数的技术",{"5":{"13":2}}],["是一种重要的权重归一化技术",{"5":{"9":2}}],["是一种常用的内存优化技术",{"5":{"9":2}}],["是一种常用的权重归一化技术",{"5":{"9":2}}],["是一种常用的正则化技术",{"5":{"8":2}}],["是一种非参数方法",{"5":{"8":2}}],["是一种用于深度学习的hessian近似方法",{"5":{"7":2}}],["是一种二值神经元模型",{"5":{"5":2}}],["是一种通过重采样来估计统计量方差的非参数方法",{"5":{"8":2}}],["是一种通过调整权重来稳定训练的方法",{"5":{"22":2}}],["是一种验证反向传播实现正确性的数值方法",{"5":{"25":2}}],["是一种防止梯度爆炸的技术",{"5":{"25":2}}],["是输出",{"5":{"7":2}}],["是输出矩阵",{"5":{"5":2}}],["是输出层的偏置向量",{"5":{"24":2}}],["是输出权重",{"5":{"24":2}}],["是输入神经元的数量",{"5":{"8":1}}],["是输入",{"5":{"7":2,"13":1}}],["是输入分布的支持域",{"5":{"22":1}}],["是输入层到隐藏层的权重矩阵",{"5":{"24":1}}],["是输入权重向量",{"5":{"24":1}}],["是权重矩阵",{"5":{"7":2}}],["是内层函数关于自变量的导数",{"5":{"7":2}}],["是微积分中最重要的法则之一",{"5":{"7":2}}],["是深度学习训练和推理的基本范式",{"5":{"9":2}}],["是深度学习训练的标准范式",{"5":{"5":2}}],["是深度学习优化中最重要的进展之一",{"5":{"7":2}}],["是深度学习中最重要的技术之一",{"5":{"8":2}}],["是深度学习中最核心的算法之一",{"5":{"25":2}}],["是深入掌握深度学习优化算法的必要基础",{"5":{"7":2}}],["是",{"5":{"5":1,"8":3,"9":3,"13":1,"22":3,"23":4,"24":1}}],["是全1向量",{"5":{"5":2}}],["是仿射变换算子",{"5":{"5":1}}],["是净输入矩阵",{"5":{"5":2}}],["是防止除零的小常数",{"5":{"22":2}}],["是方差参数",{"5":{"8":2}}],["是方差",{"5":{"22":2}}],["是层归一化的一种简化变体",{"5":{"22":2}}],["是饱和阈值",{"5":{"22":2}}],["是分析梯度爆炸的经典模型",{"5":{"22":2}}],["是分析梯度饱和的经典案例",{"5":{"22":1}}],["是裁剪阈值",{"5":{"22":1}}],["是隐藏维度",{"5":{"9":2}}],["是隐藏状态",{"5":{"22":1}}],["是隐藏状态向量",{"5":{"22":1}}],["是隐藏层到输出层的权重矩阵",{"5":{"24":2}}],["是可学习的缩放和偏移参数",{"5":{"22":1}}],["是可学习的逐元素缩放和偏移参数",{"5":{"22":1}}],["是所有被矩阵映射到零向量的输入向量构成的空间",{"5":{"9":2}}],["是所谓的",{"5":{"23":2}}],["是另一种近似推断方法",{"5":{"8":2}}],["是另一个经典的激活函数",{"5":{"23":2}}],["是目前深度学习中最广泛使用的激活函数",{"5":{"23":2}}],["是双曲正割函数",{"5":{"23":1}}],["是相应的偏置向量",{"5":{"24":2}}],["是组合偏置向量",{"5":{"24":2}}],["是平移向量",{"5":{"24":2}}],["是因为它与注意力的概率输出特性相契合",{"5":{"22":2}}],["是因为在适当条件下",{"5":{"24":2}}],["是紧集",{"5":{"24":2}}],["是偏置向量",{"5":{"7":2}}],["是偏置",{"5":{"24":2}}],["是概率论中最深刻的定理之一",{"5":{"8":2}}],["是概率论中最重要的连续分布",{"5":{"8":2}}],["是概率单纯形中的点",{"5":{"24":1}}],["是概率单纯形",{"5":{"24":1}}],["是独立同分布的随机变量",{"5":{"8":2}}],["是独立同分布的均匀随机变量",{"5":{"24":2}}],["是温度参数",{"5":{"24":2}}],["是transformer中缓解梯度问题的重要技术",{"5":{"22":2}}],["是transformer中使用的标准归一化技术",{"5":{"22":2}}],["是token嵌入矩阵",{"5":{"24":2}}],["是两个随机变量的联合分布的熵",{"5":{"8":2}}],["是两个权重矩阵的乘积",{"5":{"24":1}}],["是变换矩阵",{"5":{"24":1}}],["是非负行随机矩阵",{"5":{"22":1}}],["是非线性激活函数",{"5":{"24":1}}],["是非线性函数",{"5":{"24":1}}],["是任意非常数的有界连续函数",{"5":{"24":1}}],["是gumbel噪声",{"5":{"24":1}}],["是随机变量幂次期望的统称",{"5":{"8":2}}],["是随机变量取值的加权平均",{"5":{"8":2}}],["是随机噪声源",{"5":{"24":1}}],["是负样本的数量",{"5":{"24":1}}],["是位置编码矩阵",{"5":{"24":1}}],["是网络的原始输出",{"5":{"24":1}}],["是网络预测",{"5":{"25":1}}],["是逐元素累积的梯度平方",{"5":{"7":1}}],["是逐元素函数",{"5":{"25":1}}],["是行向量还是列向量需要注意维度匹配",{"5":{"25":1}}],["是缩放后损失的梯度",{"5":{"25":2}}],["是缩放因子",{"5":{"25":1}}],["每头维度",{"5":{"13":2}}],["每种分解都有其特定的应用场景和数值性质",{"5":{"9":2}}],["每种操作都有预定义的梯度计算规则",{"5":{"7":2}}],["每经过若干个epoch将学习率降低一个因子",{"5":{"7":2}}],["每个输出神经元与所有输入神经元相连",{"5":{"15":2}}],["每个输出元素需要",{"5":{"13":1}}],["每个输出元素需要次乘法和​次加法",{"5":{"13":1}}],["每个子矩阵",{"5":{"13":1}}],["每个子矩阵​可以分配给不同的设备",{"5":{"13":1}}],["每个微批次独立计算",{"5":{"9":2}}],["每个边代表张量的一个维度",{"5":{"9":2}}],["每个节点是一个张量",{"5":{"9":2}}],["每个秩一矩阵对应一个主成分方向",{"5":{"9":2}}],["每个索引对应张量的一个维度",{"5":{"9":2}}],["每个词被映射为一个高维实数向量",{"5":{"9":2}}],["每个中间变量的贡献是它对",{"5":{"7":1}}],["每个中间变量的贡献是它对的偏导数与它对的偏导数的乘积之和",{"5":{"7":1}}],["每个参数分量根据其在梯度中的值进行调整",{"5":{"7":2}}],["每个样本为",{"5":{"5":1}}],["每个样本为维向量",{"5":{"5":1}}],["每个头的梯度可能具有不同的范数尺度",{"5":{"22":1}}],["每个头的梯度",{"5":{"22":1}}],["每个神经元",{"5":{"24":2}}],["每个神经元本质上是在输入空间中划分一个",{"5":{"24":2}}],["每个注意力层学习token之间的交互模式",{"5":{"24":2}}],["每个激活函数在输入空间中定义了一个",{"5":{"24":1}}],["每个激活函数",{"5":{"24":1}}],["每个sigmoid神经元在输入空间中形成一个",{"5":{"24":1}}],["每个sigmoid神经元",{"5":{"24":1}}],["每层的激活输出为",{"5":{"22":2,"23":2}}],["每一层的每个神经元与前一层的所有神经元相连",{"5":{"15":2}}],["每一层的输出可以视为输入数据在该层定义的",{"5":{"13":2}}],["每一层的输入和输出都需要被保存用于反向传播",{"5":{"9":2}}],["每一层的输入分布更加稳定",{"5":{"22":2}}],["每一层网络执行了一次从输入空间到输出空间的变换",{"5":{"13":2}}],["每一层网络定义了一个从其输入空间到输出空间的映射",{"5":{"13":2}}],["每一层仅进行仿射变换而不应用激活函数",{"5":{"5":2}}],["每一层通过权重矩阵的转置将误差",{"5":{"25":2}}],["每一次非线性变换都扩展了网络能够表示的函数空间",{"5":{"24":2}}],["每次只使用一小批样本来估计梯度",{"5":{"8":2}}],["每次训练迭代",{"5":{"25":2}}],["如adam",{"5":{"15":2}}],["如初始化为零",{"5":{"15":2}}],["如jacobian矩阵计算",{"5":{"13":2}}],["如张量",{"5":{"13":2}}],["如矩阵乘法",{"5":{"13":2}}],["如物体类别",{"5":{"13":2}}],["如边缘",{"5":{"13":2}}],["如何高效地计算这个复合函数的输出",{"5":{"13":2}}],["如何将矩阵运算分解为可以在不同设备上独立执行的子运算",{"5":{"9":2}}],["如bf16",{"5":{"9":2}}],["如bonferroni校正或false",{"5":{"8":2}}],["如线性注意力",{"5":{"9":2}}],["如t",{"5":{"9":2}}],["如用户满意度",{"5":{"8":2}}],["如两个模型或两种策略",{"5":{"8":2}}],["如0",{"5":{"8":2,"22":2}}],["如",{"5":{"8":4,"9":2}}],["如随机初始化",{"5":{"8":2}}],["如基于贝叶斯方法的超参数优化",{"5":{"8":2}}],["如高斯分布",{"5":{"8":2}}],["如结合vae的表示学习",{"5":{"8":2}}],["如分析神经网络输出的分布特性",{"5":{"8":2}}],["如判断一个词是否属于某个类别",{"5":{"8":2}}],["如抛硬币的结果",{"5":{"8":2}}],["如神经元排列的不变性",{"5":{"7":2}}],["如果我们对每个神经元单独进行数学描述和计算",{"5":{"15":2}}],["如果我们使用相同的测试集评估两个模型",{"5":{"8":2}}],["如果满足",{"5":{"9":1}}],["如果使用张量运算而非循环",{"5":{"9":2}}],["如果进行行交换",{"5":{"9":2}}],["如果主元都不为零",{"5":{"9":2}}],["如果奇异值分布比较均匀",{"5":{"9":2}}],["如果只有少数几个奇异值很大",{"5":{"9":2}}],["如果只改变这个参数而保持其他参数不变",{"5":{"7":2}}],["如果有特征值的实部为正",{"5":{"9":2}}],["如果所有特征值的实部都为负",{"5":{"9":2}}],["如果对应维度相等",{"5":{"9":2}}],["如果同时检验多个假设",{"5":{"8":2}}],["如果p值小于预先设定的显著性水平",{"5":{"8":2}}],["如果函数",{"5":{"7":1}}],["如果函数的二阶偏导数连续",{"5":{"7":1}}],["如果hessian半正定或半负定",{"5":{"7":2}}],["如果hessian既有正特征值又有负特征值",{"5":{"7":2}}],["如果hessian负定",{"5":{"7":2}}],["如果hessian正定",{"5":{"7":2}}],["如果",{"5":{"7":2,"9":2,"22":18,"23":1}}],["如果存在非零向量",{"5":{"9":1}}],["如果存在非零向量和标量满足",{"5":{"9":1}}],["如果存在输入区域使得趋近于其渐近值",{"5":{"22":1}}],["如果存在输入区域使得",{"5":{"22":3,"23":2}}],["如果每一层的激活都处于饱和状态",{"5":{"22":2,"23":2}}],["如果初始权重过大",{"5":{"22":2}}],["如果很大而没有进行适当的缩放",{"5":{"22":1}}],["如果的各元素独立同分布且方差为",{"5":{"22":1}}],["如果的非主特征值很小",{"5":{"22":1}}],["如果接近均匀混合",{"5":{"22":1}}],["如果输入梯度的某些维度较大",{"5":{"22":2}}],["如果神经元的输入",{"5":{"23":2}}],["如果为正",{"5":{"23":1}}],["如果一组函数满足以下条件",{"5":{"24":2}}],["如果激活函数的导数接近零",{"5":{"25":2}}],["如果两种方法计算的梯度足够接近",{"5":{"25":2}}],["如果梯度矩阵的范数随层数增加而指数级增长",{"5":{"22":2}}],["如果梯度范数超过阈值",{"5":{"25":2}}],["如pytorch和tensorflow中",{"5":{"9":2}}],["如pytorch",{"5":{"5":2}}],["如异或问题",{"5":{"5":2}}],["如warmup",{"5":{"22":2}}],["如relu激活函数",{"5":{"9":1}}],["如relu激活函数就是典型的逐元素运算",{"5":{"9":1}}],["如relu",{"5":{"7":2,"22":2}}],["如relu或gelu",{"5":{"24":2}}],["如rope",{"5":{"22":2}}],["如长文本建模",{"5":{"22":2}}],["如凸优化问题和解析解的存在性",{"5":{"24":2}}],["如逻辑回归",{"5":{"24":2}}],["如图像",{"5":{"24":2}}],["如词语的嵌入向量",{"5":{"8":2}}],["如词汇关系",{"5":{"24":2}}],["如gpt",{"5":{"5":2,"24":2}}],["如gelu",{"5":{"24":4}}],["如变分自编码器的离散潜在变量",{"5":{"24":2}}],["如sigmoid",{"5":{"24":2}}],["如同一序列中相邻的token",{"5":{"24":2}}],["如infonce",{"5":{"24":2}}],["如fisher信息",{"5":{"24":2}}],["如正弦余弦编码的频率分解",{"5":{"24":2}}],["如正弦余弦编码",{"5":{"24":2}}],["故也是的线性函数",{"5":{"13":1}}],["故",{"5":{"5":2,"13":5,"25":7}}],["故的维度为​",{"5":{"25":1}}],["单层网络的矩阵表示",{"2":{"15":1},"5":{"15":1}}],["单层网络的计算图",{"5":{"13":2}}],["单层transformer的flops约为",{"5":{"13":2}}],["单层前向传播的flops",{"5":{"13":2}}],["单层神经网络可以逼近任意连续函数到任意精度",{"5":{"24":2}}],["单层flops",{"5":{"25":1}}],["单位为比特",{"5":{"8":2}}],["单调性",{"5":{"5":2}}],["单个感知机只能解决线性可分的问题",{"5":{"15":2}}],["单个神经元只形成了一个超平面决策边界",{"5":{"15":2}}],["单个神经元的能力是有限的",{"5":{"15":2}}],["单个神经元实现了一个超平面分类器",{"5":{"5":2}}],["单个神经元无法解决异或问题",{"5":{"5":2}}],["单个神经元无法找到正确的分类边界",{"5":{"5":2}}],["单个神经元",{"5":{"5":2}}],["单个神经元执行的是一个超平面分类",{"5":{"5":2}}],["单个神经元可以模拟任意单变量布尔函数",{"5":{"5":2}}],["单个麦肯罗皮层神经元可以实现所有的基本逻辑运算",{"5":{"5":2}}],["单个样本的注意力计算涉及形状为",{"5":{"9":1}}],["单个样本的注意力计算涉及形状为的查询",{"5":{"9":1}}],["单个样本",{"5":{"22":1}}],["单个噪声样本不太可能同时激活多个稀疏神经元",{"5":{"24":2}}],["单次前向传播的flops约为",{"5":{"13":1}}],["单次前向传播的flops约为量级",{"5":{"13":1}}],["单次前向传播的flops为",{"5":{"13":2}}],["单次参数更新就可能将权重推向无穷大或负无穷",{"5":{"22":2}}],["单次反向传播的浮点运算数为",{"5":{"25":2}}],["单隐藏层神经网络就可以以任意精度逼近任意连续函数",{"5":{"24":2}}],["有效批大小",{"5":{"9":2}}],["有效自由度",{"5":{"9":2}}],["有效秩决定了网络能够捕获的信息维度",{"5":{"22":2}}],["有效秩",{"5":{"22":2}}],["有效秩定义为",{"5":{"22":2}}],["有效秩衡量了矩阵中",{"5":{"22":2}}],["有效秩与信息传递",{"2":{"22":1},"5":{"22":1}}],["有效秩与梯度衰减",{"5":{"22":2}}],["有限",{"5":{"8":3,"13":1,"25":1}}],["有助于逃离平坦区域",{"5":{"7":2}}],["有多种理论解释",{"5":{"7":2}}],["有",{"5":{"5":2,"7":2,"8":5,"13":4,"25":2}}],["有更强的梯度信号",{"5":{"23":2}}],["有界振荡性",{"5":{"24":2}}],["有界",{"5":{"24":2}}],["有利于梯度下降的收敛",{"5":{"24":2}}],["有损编码",{"5":{"24":1}}],["对角线上",{"5":{"13":1}}],["对任意",{"5":{"8":1}}],["对任意常数和随机变量",{"5":{"8":1}}],["对任意常数",{"5":{"5":2,"8":1}}],["对常数",{"5":{"8":1}}],["对常数有",{"5":{"8":1}}],["对所有向量",{"5":{"9":1}}],["对所有",{"5":{"8":1,"24":1}}],["对所有成立",{"5":{"24":1}}],["对所有层",{"5":{"25":1}}],["对",{"5":{"7":1,"13":2,"25":3}}],["对应输入特征的索引",{"5":{"15":1}}],["对应输出神经元的索引",{"5":{"15":1}}],["对应",{"5":{"7":4}}],["对应硬路由和弱混合行为",{"5":{"22":2}}],["对应hardmax",{"5":{"24":2}}],["对称性大大简化了hessian矩阵的分析和计算",{"5":{"7":2}}],["对的总影响是通过所有中间变量传导的",{"5":{"7":1}}],["对于输入",{"5":{"13":1}}],["对于输入​",{"5":{"13":1}}],["对于输入向量",{"5":{"24":2}}],["对于输出向量的第",{"5":{"15":1}}],["对于输出向量的第个元素",{"5":{"15":1}}],["对于输出层",{"5":{"25":2}}],["对于异或等线性不可分的模式无能为力",{"5":{"15":2}}],["对于超大规模模型",{"5":{"13":2}}],["对于超平面",{"5":{"5":2}}],["对于批量输入",{"5":{"13":2,"25":2}}],["对于gpt",{"5":{"13":2}}],["对于复合函数",{"5":{"13":2}}],["对于固定的符号模式",{"5":{"13":4}}],["对于映射",{"5":{"13":2}}],["对于序列长度",{"5":{"9":1}}],["对于序列长度和注意力维度",{"5":{"9":1}}],["对于设计和实现高效的大规模训练系统至关重要",{"5":{"9":2}}],["对于深入理解语言模型的工作原理和改进模型设计都有重要意义",{"5":{"8":2}}],["对于深层网络和长序列",{"5":{"9":2}}],["对于深层网络",{"5":{"22":2}}],["对于大语言模型中的全连接层",{"5":{"9":2}}],["对于大型矩阵",{"5":{"9":2}}],["对于一个批次",{"5":{"9":2}}],["对于矩阵",{"5":{"9":1}}],["对于矩阵方程",{"5":{"9":2}}],["对于矩阵和",{"5":{"9":1}}],["对于对称矩阵",{"5":{"9":2,"22":2}}],["对于方阵",{"5":{"9":2}}],["对于任意输入",{"5":{"13":2}}],["对于任意矩阵",{"5":{"9":2}}],["对于任意",{"5":{"22":2}}],["对于任意连续函数和任意",{"5":{"24":1}}],["对于任意连续函数",{"5":{"24":1}}],["对于任何整数",{"5":{"9":2}}],["对于实对称矩阵",{"5":{"9":2}}],["对于",{"5":{"8":1,"13":1,"22":2,"25":1}}],["对于词汇表大小为",{"5":{"8":1}}],["对于词汇表大小为的语言模型",{"5":{"8":1}}],["对于两个分布",{"5":{"8":1}}],["对于两个分布和",{"5":{"8":1}}],["对于两个离散概率分布",{"5":{"8":1}}],["对于两个离散概率分布和",{"5":{"8":1}}],["对于两个随机变量",{"5":{"8":1}}],["对于两个随机变量和",{"5":{"8":1}}],["对于确定性分布",{"5":{"8":2}}],["对于维随机向量",{"5":{"8":1}}],["对于连续随机变量",{"5":{"8":4}}],["对于离散随机变量及其概率分布",{"5":{"8":1}}],["对于离散随机变量",{"5":{"8":5}}],["对于乘积",{"5":{"7":1}}],["对于乘积​",{"5":{"7":1}}],["对于强凸函数",{"5":{"7":2}}],["对于具有l",{"5":{"7":2}}],["对于二次可微函数",{"5":{"7":2}}],["对于二分类问题",{"5":{"5":2}}],["对于函数",{"5":{"7":4}}],["对于多元函数的复合",{"5":{"7":2}}],["对于多分类问题",{"5":{"5":2}}],["对于理解深度学习框架的工作原理至关重要",{"5":{"5":2}}],["对于性质",{"5":{"5":6}}],["对于逻辑非运算",{"5":{"5":2}}],["对于逻辑或运算",{"5":{"5":2}}],["对于逻辑与运算",{"5":{"5":2}}],["对于激活函数",{"5":{"22":4,"23":2}}],["对于tanh",{"5":{"22":2,"23":2}}],["对于线性不可分的数据集",{"5":{"5":2}}],["对于线性变换",{"5":{"22":2}}],["对于线性层",{"5":{"22":2}}],["对于线性系统​",{"5":{"22":1}}],["对于线性系统",{"5":{"22":1}}],["对于非对称矩阵",{"5":{"22":2}}],["对于残差形式的问题",{"5":{"7":2}}],["对于残差连接",{"5":{"22":2}}],["对于网络中的一条前向路径",{"5":{"22":2}}],["对于relu激活函数",{"5":{"22":2}}],["对于层全连接网络",{"5":{"13":1,"25":1}}],["对于层归一化",{"5":{"22":2}}],["对于分析神经网络的能力边界和训练动态至关重要",{"5":{"5":2}}],["对于分类任务",{"5":{"22":2,"25":1}}],["对于语言模型",{"5":{"22":2}}],["对于视觉任务",{"5":{"22":2}}],["对于这种复杂的分布",{"5":{"24":2}}],["对于相同的逼近精度",{"5":{"24":2}}],["对于某些函数类",{"5":{"24":2}}],["对于sigmoid函数",{"5":{"22":2}}],["对于sigmoid",{"5":{"22":2,"23":2}}],["对于sigmoid激活函数",{"5":{"22":2}}],["对于sigmoid和tanh激活函数",{"5":{"22":2}}],["对于softmax激活函数",{"5":{"24":2}}],["对于向量值函数",{"5":{"25":2}}],["对于不同的随机初始化",{"5":{"7":2}}],["对于不同的输出层配置",{"5":{"25":2}}],["对于隐藏层",{"5":{"25":2}}],["对于常见的分类和回归任务",{"5":{"25":2}}],["对于常见的均方误差和交叉熵损失",{"5":{"25":2}}],["对于每个测试样本",{"5":{"8":2}}],["对于每个位置",{"5":{"8":2}}],["对于每个操作节点",{"5":{"7":2}}],["对于每层",{"5":{"25":2}}],["对于神经网络",{"5":{"25":2}}],["对于按范数裁剪",{"5":{"25":2}}],["对每个通道内的所有空间位置和batch应用相同的归一化参数",{"5":{"22":2}}],["对加法和乘法封闭",{"5":{"24":2}}],["对数似然可以分解为序列中每个位置的条件对数概率之和",{"5":{"8":2}}],["对数似然函数为",{"5":{"8":2}}],["对数似然为",{"5":{"24":2}}],["对数几率",{"5":{"24":2}}],["对位置编码的处理会保留或抑制不同频率的成分",{"5":{"24":2}}],["对注意力输出进行非线性变换",{"5":{"24":2}}],["对梯度向量逐元素限制在范围内",{"5":{"25":1}}],["对梯度向量逐元素限制在",{"5":{"25":1}}],["96",{"5":{"15":2}}],["9",{"2":{"9":1,"25":1},"5":{"5":2,"9":1,"13":4,"22":2,"24":2,"25":3}}],["逻辑回归模型正是使用sigmoid神经元进行二分类",{"5":{"5":2}}],["逻辑或",{"5":{"5":2}}],["中有其独特优势",{"5":{"13":2}}],["中",{"5":{"9":6,"22":2}}],["中心极限定理提供了重要的洞见",{"5":{"8":2}}],["中心极限定理的重要性在于它表明正态分布在自然界中无处不在",{"5":{"8":2}}],["中心极限定理",{"5":{"8":2}}],["中心极限定理表明大量独立随机变量之和趋向于高斯分布",{"5":{"8":2}}],["中心",{"5":{"8":2}}],["中任意点",{"5":{"5":1}}],["中任意点变为",{"5":{"5":1}}],["中得到了充分的应用",{"5":{"5":2}}],["中的表示",{"5":{"13":2}}],["中的生成机制相契合",{"5":{"24":2}}],["中的点",{"5":{"24":1}}],["中最常用的输出激活函数",{"5":{"5":2}}],["中最大值的指数增长占主导",{"5":{"24":2}}],["中间变量",{"5":{"13":1}}],["中间变量的伴随值为",{"5":{"13":1}}],["中间值过大或过小",{"5":{"25":1}}],["满足",{"5":{"9":2,"22":1,"23":2}}],["满足非负性",{"5":{"9":2}}],["满足和",{"5":{"9":1}}],["满足大规模训练的需求",{"5":{"5":2}}],["满足以下两个条件",{"5":{"24":2}}],["满足概率的基本要求",{"5":{"5":2}}],["满足概率分布的所有公理",{"5":{"24":1}}],["dynamical",{"5":{"9":2}}],["d",{"5":{"8":4,"23":4}}],["dropout噪声等",{"5":{"8":2}}],["dropout",{"5":{"8":2}}],["dropout可以被解释为对神经网络进行高斯近似",{"5":{"8":2}}],["drawio",{"5":{"7":4,"8":12,"9":8,"13":8,"25":4}}],["dimension",{"5":{"9":2}}],["discovery",{"5":{"8":2}}],["distance",{"5":{"8":2}}],["dist",{"5":{"8":8}}],["distribution",{"5":{"5":2,"8":16}}],["divergence",{"5":{"8":2}}],["directional",{"5":{"7":2}}],["differentiation",{"5":{"7":2,"13":2,"25":2}}],["diag",{"5":{"23":8}}],["dag",{"5":{"13":2,"25":2}}],["data",{"5":{"6":4,"9":2,"14":6,"21":10,"27":10}}],["david",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"25":2,"27":1}}],["deviation",{"5":{"8":2}}],["dense",{"5":{"15":2}}],["density",{"5":{"8":2}}],["dendrites",{"5":{"5":2}}],["decomposition",{"5":{"9":4}}],["decay",{"5":{"7":4,"9":2}}],["decision",{"5":{"5":2}}],["derivative",{"5":{"7":2,"23":2}}],["descent",{"5":{"25":2}}],["dx",{"5":{"23":4}}],["dot",{"5":{"24":2}}],["标量",{"5":{"13":2}}],["标量乘法是将一个向量与一个标量",{"5":{"9":2}}],["标量乘法",{"5":{"9":2}}],["标量链式法则",{"5":{"25":2}}],["标量对矩阵的链式法则",{"5":{"25":2}}],["标量损失函数的梯度",{"5":{"25":1}}],["标准化均值差异",{"5":{"8":2}}],["标准vae假设潜在变量服从标准高斯分布",{"5":{"8":2}}],["标准高斯分布",{"5":{"8":2}}],["标准梯度下降以",{"5":{"7":1}}],["标准梯度下降以的速度收敛",{"5":{"7":1}}],["标准优化算法的内存开销巨大",{"5":{"7":2}}],["标签平滑相当于在训练时使用真实分布与均匀分布的混合作为目标分布",{"5":{"8":2}}],["标签平滑可以防止模型对训练数据过度自信",{"5":{"8":2}}],["标签平滑",{"5":{"8":2}}],["标签",{"5":{"5":1}}],["标签服从伯努利分布",{"5":{"5":1}}],["标志着计算神经科学和人工智能的诞生",{"5":{"5":2}}],["8",{"2":{"9":1,"25":1},"5":{"5":2,"9":1,"13":4,"22":2,"24":2,"25":3}}],["多节点",{"5":{"13":1}}],["多gpu",{"5":{"13":1}}],["多重比较谬误",{"5":{"8":2}}],["多重比较问题是在进行多个假设检验时需要考虑的重要问题",{"5":{"8":2}}],["多元高斯分布具有许多重要的性质",{"5":{"8":2}}],["多元高斯分布的pdf为",{"5":{"8":2}}],["多元高斯分布是一维高斯分布向多维空间的推广",{"5":{"8":2}}],["多元函数梯度与hessian",{"2":{"7":1},"5":{"7":1}}],["多数特征值很小",{"5":{"7":2}}],["多个层的组合则构成了这些映射的复合",{"5":{"13":2}}],["多个样本的信息被平均",{"5":{"5":2}}],["多个神经元的组合形成了一个丰富的特征基",{"5":{"5":2}}],["多个头的梯度会复合",{"5":{"22":2}}],["多个这样的超平面组合",{"5":{"24":2}}],["多层神经网络",{"5":{"15":2}}],["多层神经网络可以学习复杂的非线性决策边界",{"5":{"5":2}}],["多层网络的复合函数表示",{"5":{"15":2}}],["多层网络的表达能力超越了单层网络",{"5":{"15":2}}],["多层网络的矩阵形式",{"2":{"15":1},"5":{"15":1}}],["多层网络的flops",{"5":{"13":2}}],["多层感知机",{"5":{"5":2}}],["多层线性网络的恒等性",{"5":{"5":2}}],["多层堆叠使得网络能够表示极其复杂的函数",{"5":{"24":2}}],["多头注意力的梯度复合",{"5":{"22":2}}],["多模态学习",{"5":{"22":2}}],["多项分布的共轭先验是dirichlet分布",{"5":{"8":2}}],["多项式分布的似然函数为",{"5":{"24":2}}],["多分类任务",{"5":{"25":2}}],["多分类情况的证明类似",{"5":{"25":2}}],["不仅会使得表达式繁琐冗长",{"5":{"15":2}}],["不仅有助于我们把握神经网络的工作原理",{"5":{"13":2}}],["不相关时等号才成立",{"5":{"8":1}}],["不等于",{"5":{"8":2}}],["不确定性量化和小样本学习",{"5":{"8":2}}],["不确定性高",{"5":{"8":2}}],["不同列之间的计算不相互依赖",{"5":{"13":2}}],["不同输出神经元",{"5":{"13":2}}],["不同样本之间的计算是相互独立的",{"5":{"13":2}}],["不同的权重矩阵定义了不同的投影方向",{"5":{"13":2}}],["不同的神经元学习不同的特征表示",{"5":{"5":2}}],["不同的下游任务可能需要不同特性的激活函数",{"5":{"22":2}}],["不同",{"5":{"7":2,"13":2}}],["不同初始化策略的比较",{"5":{"22":2}}],["不同注意力分布对应截然不同的谱结构",{"5":{"22":2}}],["不同设备处理不同样本",{"5":{"22":2}}],["不同层的注意力可能关注不同类型的关系",{"5":{"24":2}}],["不改变其超平面性质",{"5":{"5":2}}],["不依赖于其他样本",{"5":{"22":2}}],["不能使用未来信息",{"5":{"22":2}}],["不受层归一化的影响",{"5":{"22":2}}],["不应被归一化消除",{"5":{"22":2}}],["不存在饱和问题",{"5":{"22":2}}],["不会过度放大或抑制梯度",{"5":{"22":2}}],["不是欧几里得距离",{"5":{"24":2}}],["不再能够简化为的形式",{"5":{"24":1}}],["不再能够简化为",{"5":{"24":1}}],["不当使用容易引入隐蔽且难以察觉的",{"5":{"9":2}}],["不当的实现可能导致数值溢出",{"5":{"25":2}}],["<a",{"5":{"6":1,"14":1,"21":1,"27":1}}],["<br><img",{"5":{"7":1}}],["<br><a",{"5":{"6":3,"14":5,"21":9,"27":9}}],["<br>",{"5":{"5":1,"7":1,"8":2,"9":1,"13":1}}],["<img",{"5":{"5":2,"7":2,"8":3,"9":2,"13":2,"25":1}}],["<",{"5":{"23":2}}],["然后检验这些差异的均值是否显著不为零",{"5":{"8":2}}],["然后使用累加的梯度进行一次参数更新",{"5":{"9":2}}],["然后使用这些估计的分布来表征原始统计量的不确定性",{"5":{"8":2}}],["然后使用链式法则组合这些梯度",{"5":{"7":2}}],["然后使用激活函数",{"5":{"24":2}}],["然后通过最小化近似分布与真实后验分布之间的kl散度来找到最好的近似",{"5":{"8":2}}],["然后通过激活函数",{"5":{"5":1}}],["然后通过激活函数进行非线性变换",{"5":{"5":1}}],["然后通过激活函数的导数进行缩放",{"5":{"25":2}}],["然后通过平移",{"5":{"24":2}}],["然后再进行衰减",{"5":{"7":2}}],["然后再衰减",{"5":{"7":2}}],["然后应用链式法则累加梯度",{"5":{"7":2}}],["然后逐层向后传递",{"5":{"7":2}}],["然后根据梯度方向更新参数",{"5":{"7":2}}],["然后",{"5":{"22":2}}],["然后将输出平移和缩移到区间",{"5":{"23":1}}],["然后将输出平移和缩移到",{"5":{"23":1}}],["然后在各主轴方向上进行缩放和平移",{"5":{"13":2}}],["然后在下一层再次进行线性变换",{"5":{"24":2}}],["然后经过gelu激活",{"5":{"24":2}}],["然后从输出层开始",{"5":{"25":2}}],["然而在实际实现中",{"5":{"15":2}}],["然而",{"5":{"5":2,"7":2,"8":4,"13":2,"15":2,"22":6,"23":4,"24":6,"25":2}}],["首先计算",{"5":{"7":1}}],["首先计算​",{"5":{"7":1}}],["首先",{"5":{"7":4,"8":2,"9":10,"22":6,"23":2,"24":2}}],["首先通过仿射变换",{"5":{"5":1}}],["首先通过仿射变换进行线性投影",{"5":{"5":1}}],["首次提出了人工神经元的数学模型",{"5":{"5":2}}],[">从前向传播的数学描述到实际代码实现",{"5":{"13":1}}],[">在大语言模型的训练中",{"5":{"8":1}}],[">期望",{"5":{"8":1}}],[">k",{"5":{"7":1}}],[">矩阵形式的链式法则在深度学习中尤为重要",{"5":{"7":1}}],[">2",{"5":{"6":1,"14":3,"21":4,"27":4}}],[">1",{"5":{"6":3,"14":3,"21":3,"27":3}}],[">数学上",{"5":{"5":1}}],[">",{"5":{"5":1,"7":1,"8":1,"9":2,"13":1,"23":5,"25":1}}],[">3",{"5":{"21":3,"27":3}}],["mid",{"5":{"15":2}}],["micro",{"5":{"9":2}}],["mirsky定理",{"5":{"9":2}}],["mini",{"5":{"22":1,"25":2}}],["min",{"5":{"23":2}}],["mode",{"5":{"13":4,"25":4}}],["mobilenet中的深度可分离卷积可以用kronecker积来表示",{"5":{"9":2}}],["moment",{"5":{"7":2,"8":4}}],["momentum",{"5":{"7":2}}],["mle是相合的",{"5":{"8":2}}],["mle是训练的标准方法",{"5":{"8":2}}],["mle",{"5":{"8":2}}],["msg",{"5":{"8":4,"13":4}}],["multi",{"5":{"15":2}}],["multinomial",{"5":{"8":2}}],["mutual",{"5":{"8":2}}],["mgf",{"5":{"8":2}}],["matrix",{"5":{"7":2,"9":2,"13":2}}],["mathbf",{"5":{"15":44}}],["mathbb",{"5":{"23":4}}],["mahalanobis",{"5":{"8":2}}],["mass",{"5":{"8":2}}],["manimce",{"5":{"5":8,"7":4}}],["maximum",{"5":{"8":2}}],["max",{"5":{"23":6}}],["müssen",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["mcculloch",{"5":{"5":4}}],["mean",{"5":{"22":2}}],["gpt规模模型的flops估算",{"5":{"13":2}}],["gpu共享内存",{"5":{"13":1}}],["gpu线程",{"5":{"13":1}}],["gpu线程块",{"5":{"13":1}}],["gpu可以同时计算批次中所有样本的注意力分数",{"5":{"9":2}}],["gpu的并行计算能力得到充分利用",{"5":{"5":2}}],["gan",{"5":{"9":2}}],["gauss",{"5":{"8":4}}],["gaussian",{"5":{"8":2}}],["generating",{"5":{"8":2}}],["gelu激活函数",{"5":{"5":2}}],["gelu通过引入高斯分布相关的非线性",{"5":{"22":2}}],["gelu涉及误差函数的计算",{"5":{"22":2}}],["gelu的设计考虑了这一点",{"5":{"22":2}}],["gelu的平滑性和概率解释使其成为自然的选择",{"5":{"22":2}}],["gelu的平滑特性有助于保持相邻位置信息的连续性",{"5":{"24":4}}],["gelu的导数",{"5":{"24":2}}],["gelu的导数不会过于接近零",{"5":{"24":2}}],["gelu的非线性变换将这种位置感知的嵌入转换为新的表示",{"5":{"24":2}}],["gelu的定义表明它是输入与其通过标准正态累积分布函数权重的乘积",{"5":{"24":1}}],["gelu的定义",{"5":{"24":1}}],["gelu被选择作为transformer的激活函数",{"5":{"22":2}}],["gelu可以平滑地处理这些概率信息",{"5":{"22":2}}],["gelu等变体通过不同的策略处理负区域",{"5":{"22":2}}],["gelu等",{"5":{"23":2}}],["gelu等激活函数在理论上都具有同等的表达能力",{"5":{"24":2}}],["gelu",{"5":{"23":6}}],["gelu提供token内部的特征非线性",{"5":{"24":2}}],["geq",{"5":{"23":4}}],["geoffrey",{"5":{"25":2}}],["gram",{"5":{"9":2}}],["gradient",{"5":{"7":2,"9":2,"22":2,"25":8}}],["grad",{"5":{"7":4}}],["graph",{"5":{"5":8,"7":9,"8":9,"9":6,"13":12,"25":3}}],["gt",{"5":{"23":5}}],["g^t",{"5":{"23":2}}],["gumbel",{"2":{"24":1},"5":{"24":15}}],["gumbel分布是极值分布的一种",{"5":{"24":2}}],["因为前向和后向替换的复杂度是",{"5":{"9":1}}],["因为前向和后向替换的复杂度是而不是高斯消元的",{"5":{"9":1}}],["因为梯度是线性的",{"5":{"9":2}}],["因为长序列可能消耗大量内存",{"5":{"9":2}}],["因为正规方程",{"5":{"9":1}}],["因为正规方程的解等价于求解",{"5":{"9":1}}],["因为正交矩阵的奇异值全部为1",{"5":{"9":2}}],["因为在数值上计算高阶多项式的根非常困难",{"5":{"9":2}}],["因为它可以将不同样本的计算分配到不同的处理单元上同时执行",{"5":{"9":2}}],["因为它将复杂的矩阵运算转化为标准的矩阵",{"5":{"9":2}}],["因为它决定了信息放大或衰减的上界",{"5":{"9":2}}],["因为它能够捕捉语义方向的相似性而不受词频等因素的影响",{"5":{"9":2}}],["因为它不满足对称性和三角不等式",{"5":{"8":2}}],["因为它保留了梯度的主要方向信息",{"5":{"22":2}}],["因为它的数学形式最为清晰",{"5":{"15":2}}],["因为它的稀疏激活特性和线性区域的恒定导数有利于深层网络的训练",{"5":{"24":2}}],["因为数值稳定性较差且计算成本高昂",{"5":{"9":2}}],["因为真实分布固定",{"5":{"8":2}}],["因为我们可以将期望操作与梯度操作交换次序",{"5":{"8":2}}],["因为异或问题的正例点位于对角位置",{"5":{"5":2}}],["因为",{"5":{"22":3,"25":2}}],["因为或",{"5":{"22":1}}],["因为层归一化位于关键路径上",{"5":{"22":2}}],["因为语言",{"5":{"24":2}}],["因此得名",{"5":{"15":2}}],["因此单个层的计算量是相当可观的",{"5":{"9":2}}],["因此现代gpu都针对矩阵运算进行了专门的硬件优化",{"5":{"9":2}}],["因此深入掌握线性代数的基本概念和运算规则是理解模型工作原理的必要前提",{"5":{"9":2}}],["因此标准化操作是合理的",{"5":{"8":2}}],["因此最小化交叉熵等价于最小化kl散度",{"5":{"8":2}}],["因此共轭梯度法和拟牛顿法等方法在处理大规模优化问题时具有优势",{"5":{"7":2}}],["因此",{"5":{"5":6,"13":8,"15":2,"23":6,"24":8,"25":10}}],["因此输出饱和往往伴随梯度饱和",{"5":{"22":2}}],["因此可以使用更大的学习率进行训练",{"5":{"22":2}}],["因此需要使用更大的权重方差来保持激活的稳定性",{"5":{"22":1}}],["因此需要使用更大的权重方差",{"5":{"22":1}}],["因此在实践中表现出更强的表示能力和泛化性能",{"5":{"24":2}}],["问题",{"5":{"5":2,"22":4,"23":2,"24":2,"25":1}}],["问题设定",{"5":{"25":2}}],["l",{"5":{"15":30,"23":14}}],["l层前馈网络",{"5":{"15":2}}],["lu分解将矩阵分解为下三角矩阵",{"5":{"9":1}}],["lu分解将矩阵分解为下三角矩阵和上三角矩阵的乘积",{"5":{"9":1}}],["lu分解的主要用途是高效求解线性方程组",{"5":{"9":2}}],["lstm和gru通过门控机制设计",{"5":{"9":2}}],["layer或fully",{"5":{"15":2}}],["layer",{"5":{"15":12,"22":4}}],["large",{"5":{"8":2}}],["law",{"5":{"8":2}}],["label",{"5":{"8":2}}],["limit",{"5":{"8":2}}],["likelihood",{"5":{"8":2}}],["link",{"5":{"6":4,"14":6,"21":10,"27":10}}],["linearinseparablescene",{"5":{"5":4}}],["linearly",{"5":{"5":2}}],["linear",{"5":{"23":2}}],["left",{"5":{"15":8,"23":22}}],["left|",{"5":{"23":4}}],["length",{"5":{"9":2}}],["leibler",{"5":{"8":2}}],["learn",{"5":{"13":4}}],["learning",{"5":{"7":2}}],["leaky",{"5":{"22":2}}],["leakyrelu",{"5":{"23":4}}],["leq",{"5":{"23":8}}],["low",{"5":{"9":2}}],["loss",{"5":{"5":2}}],["logistic",{"5":{"5":2}}],["logical",{"5":{"5":2}}],["logits",{"5":{"8":4,"24":6}}],["logits空间",{"5":{"23":2}}],["logits与概率的变换",{"5":{"24":2}}],["logits可以理解为",{"5":{"24":2}}],["logit函数",{"5":{"23":2}}],["log",{"5":{"24":2}}],["ln",{"5":{"22":12}}],["ln结构更有利于训练非常深的网络",{"5":{"22":2}}],["ln结构中",{"5":{"22":2}}],["ln的数学优势",{"5":{"22":2}}],["ln的挑战",{"5":{"22":2}}],["lt",{"5":{"23":2}}],["ldots",{"5":{"23":2}}],["llm",{"5":{"9":4}}],["llama",{"5":{"24":2}}],["b",{"5":{"15":10}}],["bug",{"5":{"9":2}}],["broadcasting",{"5":{"9":2}}],["b测试可以用于评估模型对用户交互的实际影响",{"5":{"8":2}}],["b测试的核心是随机分组和控制变量",{"5":{"8":2}}],["b测试是互联网公司和研究机构常用的实验方法",{"5":{"8":2}}],["bootstrap",{"5":{"8":2}}],["boundary",{"5":{"5":2}}],["bottleneck",{"5":{"24":2}}],["beta先验和dirichlet先验可用于建模词汇概率的不确定性",{"5":{"8":2}}],["bernoulli",{"5":{"5":2,"8":2}}],["bert等",{"5":{"5":2}}],["bert",{"5":{"24":2}}],["begin",{"5":{"23":20}}],["batch内的样本",{"5":{"13":1}}],["batch",{"5":{"5":2,"8":4,"9":6,"22":5,"25":2}}],["batch中的激活值为",{"5":{"22":2}}],["batch的统计量而非全局统计量",{"5":{"22":2}}],["batch依赖",{"5":{"22":1}}],["backprop",{"5":{"7":4}}],["backpropagation",{"5":{"25":2}}],["backward",{"5":{"25":4}}],["bit",{"5":{"8":2}}],["binomial",{"5":{"8":2}}],["bias",{"5":{"5":2,"9":2}}],["biological",{"5":{"5":2}}],["bptt",{"5":{"22":2}}],["bn",{"5":{"22":2}}],["考虑一个两层网络",{"5":{"15":2}}],["考虑一个简化的transformer层",{"5":{"13":2}}],["考虑一个简单的两层神经网络",{"5":{"7":2}}],["考虑一个简单的例子",{"5":{"24":2}}],["考虑一个简单的互信息估计问题",{"5":{"24":2}}],["考虑一个层的前馈网络",{"5":{"22":1,"23":1}}],["考虑一个",{"5":{"22":1,"23":1}}],["考虑一个没有激活函数的浅层神经网络",{"5":{"24":2}}],["考虑一个二维空间中的分类问题",{"5":{"24":2}}],["考虑全连接层的计算",{"5":{"13":2}}],["考虑单个输出元素",{"5":{"13":1}}],["考虑单个输出元素​",{"5":{"13":1}}],["考虑单层全连接网络",{"5":{"13":2}}],["考虑函数",{"5":{"13":2}}],["考虑将",{"5":{"8":1}}],["考虑将划分为两个子向量和​",{"5":{"8":1}}],["考虑比值",{"5":{"5":2}}],["考虑二分类问题",{"5":{"5":2}}],["考虑异或",{"5":{"5":2}}],["考虑使用sigmoid激活函数的神经元",{"5":{"5":2}}],["考虑超平面",{"5":{"5":2}}],["考虑三层网络的简单情况",{"5":{"22":2}}],["考虑了relu激活一半神经元输出为零的特性",{"5":{"22":2}}],["考虑简化情况",{"5":{"22":2}}],["考虑残差网络的反向传播",{"5":{"22":2}}],["考虑线性层",{"5":{"22":2}}],["考虑因素",{"5":{"22":1}}],["考虑权重归一化后",{"5":{"22":2}}],["考虑权重和偏置",{"5":{"24":2}}],["考虑的第个分量​和的第个分量​",{"5":{"25":1}}],["考虑批量中每个样本的权重梯度",{"5":{"25":2}}],["考虑",{"5":{"25":1}}],["考虑梯度矩阵的元素​",{"5":{"25":1}}],["考虑梯度矩阵的元素",{"5":{"25":1}}],["以使",{"5":{"13":1}}],["以relu激活函数为例",{"5":{"13":2}}],["以保持数值稳定性",{"5":{"9":2}}],["以匹配较大的张量",{"5":{"9":2}}],["以transformer模型为例",{"5":{"9":2}}],["以捕获更复杂的多模态特性",{"5":{"8":2}}],["以计算换内存",{"5":{"7":2}}],["以此类推直到输入层",{"5":{"7":2}}],["以及评估词嵌入中捕获的语义信息量",{"5":{"8":2}}],["以及评估模型对不同类型输入的响应一致性",{"5":{"8":2}}],["以及在变分自编码器中定义潜在空间的先验分布",{"5":{"8":2}}],["以及在概率图模型中作为共轭先验使用",{"5":{"8":2}}],["以及模型输出的不确定性估计",{"5":{"8":2}}],["以及某些正则化技术的理论基础",{"5":{"8":2}}],["以及归一性",{"5":{"8":2}}],["以及局部最小值通常具有相似的损失值等",{"5":{"7":2}}],["以及整个神经网络的矩阵表示形式",{"5":{"5":2}}],["以及计算效率",{"5":{"5":2}}],["以及实际实现中的数值稳定性问题",{"5":{"25":2}}],["以确保信号在各层之间平稳传播",{"5":{"8":2}}],["以确保梯度可以稳定地反向传播",{"5":{"22":2}}],["以下是深度学习中常用的激活函数及其数学定义",{"5":{"5":2}}],["以下是一些有前景的未来方向",{"5":{"22":2}}],["以下",{"5":{"22":1}}],["以概率",{"5":{"24":2}}],["以类别为基准",{"5":{"24":1}}],["以类别",{"5":{"24":1}}],["以二分类为例",{"5":{"25":2}}],["以处理批量输入",{"5":{"25":2}}],["数十亿甚至万亿级",{"5":{"7":2}}],["数学上",{"5":{"5":1,"9":6}}],["数学基础",{"4":{"0":1,"1":1,"2":1,"7":1,"8":1,"9":1},"5":{"6":6,"14":6,"21":6,"27":6}}],["数学评估指标",{"5":{"22":2}}],["数据分布为两个交织的螺旋形状",{"5":{"24":2}}],["数值稳定性",{"5":{"5":2}}],["数值稳定性与实现细节",{"2":{"25":1},"5":{"25":1}}],["数值稳定性是反向传播实现中的关键考虑",{"5":{"25":2}}],["数值稳定的softmax实现",{"5":{"25":1}}],["right",{"5":{"15":8,"23":22}}],["right|",{"5":{"23":4}}],["r为矩阵的秩",{"5":{"9":2}}],["rnn",{"5":{"9":2,"22":2}}],["rnn的隐藏状态更新为",{"5":{"22":2}}],["rnn的梯度范数增长",{"5":{"22":2}}],["rank",{"5":{"9":2}}],["radam",{"5":{"7":2}}],["rate控制",{"5":{"8":2}}],["rate",{"5":{"7":2}}],["recurrent",{"5":{"15":2}}],["rectified",{"5":{"7":2,"23":2}}],["reverse",{"5":{"13":2,"25":2}}],["relation",{"5":{"8":8}}],["rel=",{"5":{"6":4,"14":6,"21":10,"27":10}}],["relu对每个坐标独立操作",{"5":{"13":2}}],["relu网络表现为一个线性函数",{"5":{"13":2}}],["relu激活函数将",{"5":{"13":1}}],["relu激活函数将空间按个坐标轴分割成个象限",{"5":{"13":1}}],["relu激活的几何效果",{"5":{"13":2}}],["relu激活",{"5":{"25":1}}],["relu",{"5":{"5":2,"22":3,"23":6,"24":2}}],["relu通过分段线性的设计实现了这一点",{"5":{"22":2}}],["relu的负区域完全抑制了信号",{"5":{"22":2}}],["relu的计算仅涉及比较和乘法",{"5":{"22":2}}],["relu的数学形式极其简单",{"5":{"23":2}}],["relu的信息选择性",{"5":{"24":2}}],["relu的稀疏激活特性可能更有价值",{"5":{"22":2}}],["relu的稀疏激活特性",{"5":{"24":2}}],["relu函数族的导数与性质",{"2":{"23":1},"5":{"23":1}}],["relu函数的定义与导数",{"2":{"23":1},"5":{"23":1}}],["relu函数",{"5":{"23":2,"24":2}}],["relu函数定义为",{"5":{"23":2}}],["relu及其变体",{"5":{"23":2}}],["relu虽然数学形式简单",{"5":{"24":2}}],["relu可以被解释为一种",{"5":{"24":2}}],["regression",{"5":{"5":2}}],["reshape",{"5":{"9":2}}],["residual",{"5":{"22":2}}],["rmsprop是adagrad的改进版本",{"5":{"7":2}}],["rmsnorm与计算优化",{"2":{"22":1},"5":{"22":1}}],["rmsnorm",{"5":{"22":6}}],["rmsnorm定义为",{"5":{"22":2}}],["rmsnorm的数学动机是",{"5":{"22":2}}],["rmsnorm的数学性质",{"5":{"22":2}}],["rmsnorm具有以下优势",{"5":{"22":2}}],["rmsnorm不需要计算均值",{"5":{"22":2}}],["rmsnorm在保持性能的同时减少了计算量",{"5":{"22":2}}],["rms范数是更简单的统计量",{"5":{"22":2}}],["rosenblatt",{"5":{"5":2}}],["root",{"5":{"22":2}}],["ronald",{"5":{"25":2}}],["rule",{"5":{"7":2}}],["rumelhart",{"5":{"25":2}}],["修正线性单元",{"5":{"5":2,"23":2}}],["假设一个",{"5":{"9":1}}],["假设一个的权重矩阵被近似为​",{"5":{"9":1}}],["假设预训练模型的权重更新可以表示为低秩矩阵",{"5":{"9":2}}],["假设检验的基本框架包括原假设",{"5":{"8":2}}],["假设检验被用于比较不同模型的性能",{"5":{"8":2}}],["假设检验是统计推断的重要工具",{"5":{"8":2}}],["假设检验与模型评估",{"2":{"8":1},"5":{"8":1}}],["假设网络中有大量独立的噪声源",{"5":{"8":2}}],["假设这些数据独立同分布",{"5":{"8":2}}],["假设每层的参数可以独立地近似其hessian块",{"5":{"7":2}}],["假设给定输入",{"5":{"5":1}}],["假设给定输入时",{"5":{"5":1}}],["假设对于",{"5":{"5":1}}],["假设对于层网络",{"5":{"5":1}}],["假设输入为",{"5":{"5":2}}],["即对所有向量成立",{"5":{"9":1}}],["即通常不等于",{"5":{"9":1}}],["即在真实类别上赋予",{"5":{"8":1}}],["即在真实类别上赋予的概率",{"5":{"8":1}}],["即给定前文的条件下预测下一个词的对数概率之和",{"5":{"8":2}}],["即的支持集必须包含的支持集",{"5":{"8":1}}],["即一般情况下",{"5":{"8":2}}],["即最大的不确定性",{"5":{"8":2}}],["即存在",{"5":{"7":1}}],["即存在使得",{"5":{"7":1}}],["即存在常数使得",{"5":{"22":1}}],["即存在常数",{"5":{"22":1}}],["即经过",{"5":{"7":1}}],["即经过次迭代后",{"5":{"7":1}}],["即使所有原假设都为真",{"5":{"8":2}}],["即使是微小的差异也可能是统计显著的",{"5":{"8":2}}],["即使这些最小值在参数空间中相距甚远",{"5":{"7":2}}],["即使不能保证达到全局最优",{"5":{"7":2}}],["即使很大的差异也可能不显著",{"5":{"8":2}}],["即使很小",{"5":{"22":1}}],["即使的梯度完全消失",{"5":{"22":1}}],["即使输入梯度很小",{"5":{"22":2}}],["即使",{"5":{"22":2}}],["即使网络包含多个隐藏层",{"5":{"24":2}}],["即未中心化的方差",{"5":{"7":2}}],["即动量",{"5":{"7":2}}],["即​",{"5":{"7":1,"13":1}}],["即",{"5":{"5":4,"7":5,"8":3,"9":10,"13":5,"15":2,"22":2,"23":2,"24":4,"25":2}}],["即进入显著饱和区域",{"5":{"22":2}}],["即具有实质贡献的维度数",{"5":{"22":2}}],["即当时​",{"5":{"22":1}}],["即当",{"5":{"22":1}}],["即交叉熵损失的曲率正好由softmax分布的fisher信息度量给出",{"5":{"24":2}}],["即损失函数对该层净输入的梯度",{"5":{"25":2}}],["即为",{"5":{"25":2}}],["即误差按几何级数衰减",{"5":{"7":2}}],["即误差信号矩阵在批量维度上的平均值",{"5":{"25":2}}],["打破了神经网络的线性瓶颈",{"5":{"5":2}}],["f",{"5":{"15":1,"23":3}}],["ffn",{"5":{"15":2,"24":2}}],["f^",{"5":{"15":6}}],["flops",{"5":{"15":4}}],["floating",{"5":{"13":2}}],["flow",{"5":{"9":4,"22":2}}],["fp32",{"5":{"9":2}}],["fp16",{"5":{"9":2}}],["frac",{"5":{"15":2,"23":72}}],["frank",{"5":{"5":2}}],["frobenius范数因其可导性好而最常用",{"5":{"9":2}}],["frobenius范数常用于权重衰减",{"5":{"9":2}}],["frobenius范数是最直观的矩阵范数",{"5":{"9":2}}],["fac基于层间独立性假设",{"5":{"7":2}}],["factored",{"5":{"7":2}}],["fac",{"5":{"7":2}}],["function",{"5":{"5":2,"8":6,"13":4}}],["forward",{"5":{"13":4,"15":2,"24":2,"25":2}}],["forall",{"5":{"23":2}}],["fine",{"5":{"9":2}}],["fisher信息度量",{"5":{"24":2}}],["fisher信息矩阵定义为",{"5":{"24":2}}],["fisher信息矩阵定义了概率流形上的黎曼度量",{"5":{"24":2}}],["feed",{"5":{"24":2}}],["无论权重如何调整",{"5":{"15":2}}],["无论是否满秩",{"5":{"9":2}}],["无论是方阵还是矩形阵",{"5":{"9":2}}],["无论原始随机变量的分布是什么",{"5":{"8":2}}],["无论堆叠多少层",{"5":{"5":2}}],["无论堆叠多少层神经元",{"5":{"24":2}}],["无论训练多长时间",{"5":{"24":2}}],["无论训练数据有多少",{"5":{"24":2}}],["无法拟合复杂的非线性函数",{"5":{"5":2}}],["无法将类别0的点放在一侧而将类别1的点放在另一侧",{"5":{"5":2}}],["无法通过数据自动学习",{"5":{"5":2}}],["无法捕获token之间的复杂依赖关系",{"5":{"24":2}}],["无激活函数",{"5":{"22":2}}],["无输入",{"5":{"22":2}}],["无衰减地传播",{"5":{"22":2}}],["旋转保持向量的模长不变",{"5":{"9":2}}],["旋转",{"5":{"5":2}}],["将到的等式依次代入",{"5":{"15":1}}],["将矩阵乘法展开为元素形式",{"5":{"15":2}}],["将",{"5":{"13":1,"15":1,"24":1}}],["将按列分割",{"5":{"13":1}}],["将结果旋转到输出空间",{"5":{"13":2}}],["将形状较小的张量在某些维度上",{"5":{"9":1}}],["将形状较小的张量在某些维度上视为被重复",{"5":{"9":1}}],["将它们累加起来",{"5":{"9":2}}],["将离散的符号表示转化为连续的数值表示",{"5":{"9":2}}],["将神经元的净输入",{"5":{"5":1}}],["将在第三章进行详细的数学分析",{"5":{"5":2}}],["将权重矩阵和偏置合并为扩展权重矩阵",{"5":{"5":2,"15":2}}],["将位置信息嵌入到token嵌入中",{"5":{"22":2}}],["将sigmoid函数重写为",{"5":{"23":2}}],["将空间分为",{"5":{"24":2}}],["将代入对数几率的表达式",{"5":{"24":1}}],["将这些概念应用于激活函数分析",{"5":{"24":2}}],["将前一层的表示转换为新的表示",{"5":{"24":1}}],["将实数域映射到",{"5":{"24":1}}],["将实数轴压缩到有限的区间或",{"5":{"24":1}}],["将实数轴压缩到有限的区间",{"5":{"24":1}}],["将query",{"5":{"24":1}}],["将梯度计算的时间复杂度从指数级降低到线性级",{"5":{"25":2}}],["将所有元素写为矩阵形式",{"5":{"25":2}}],["将输入扩展为齐次坐标",{"5":{"15":2}}],["将输入空间按超平面",{"5":{"13":1}}],["将输入空间按超平面划分为两个区域",{"5":{"13":1}}],["将输入空间划分为两个区域",{"5":{"5":2}}],["将输入向量旋转到奇异向量方向",{"5":{"13":2}}],["将输入向量",{"5":{"5":1}}],["将输入向量扩展为齐次坐标",{"5":{"5":1}}],["将输入的尺度归一化",{"5":{"23":2}}],["将输出层的损失信息传递到输入层",{"5":{"25":2}}],["将求和写为矩阵形式",{"5":{"25":2}}],["将各层求和",{"5":{"25":2}}],["cholesky分解的数值稳定性好",{"5":{"9":2}}],["cholesky分解是lu分解的特例",{"5":{"9":2}}],["chain",{"5":{"7":2}}],["checkpointing",{"5":{"9":2}}],["checking",{"5":{"25":2}}],["cp分解可以将一个",{"5":{"9":1}}],["cp分解可以将一个的三阶张量表示为个秩一张量的和",{"5":{"9":1}}],["cp",{"5":{"9":4}}],["central",{"5":{"8":2}}],["candecomp",{"5":{"9":2}}],["categorical",{"5":{"8":2}}],["calculus",{"5":{"5":2}}],["cases",{"5":{"23":24}}],["clt",{"5":{"8":2}}],["clipping",{"5":{"7":2,"25":2}}],["class=",{"5":{"6":4,"14":6,"21":10,"27":10}}],["curvature",{"5":{"7":2}}],["column",{"5":{"9":2}}],["convolutional",{"5":{"15":2}}],["contraction",{"5":{"9":2}}],["conditional",{"5":{"8":2}}],["connected",{"5":{"15":2}}],["connection",{"5":{"22":2}}],["comp",{"5":{"13":4}}],["computational",{"5":{"13":2}}],["component",{"5":{"9":2}}],["composite",{"5":{"13":2}}],["compositional",{"5":{"24":2}}],["completion",{"5":{"9":2}}],["combinedscene",{"5":{"7":4}}],["coefficient",{"5":{"8":2}}],["correlation",{"5":{"8":2}}],["cosine",{"5":{"7":2}}],["coordinates",{"5":{"5":2,"15":2}}],["covariance",{"5":{"8":2}}],["covariate",{"5":{"22":2}}],["critical",{"5":{"7":2}}],["cross",{"5":{"5":2,"8":2}}],["cdots",{"5":{"15":4}}],["cdot",{"5":{"23":14}}],["c",{"5":{"23":2}}],["h",{"5":{"15":6,"23":33}}],["hierarchical",{"5":{"9":2}}],["hidden",{"5":{"9":2}}],["hilbert",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["hinton",{"5":{"25":2}}],["householder变换和givens旋转",{"5":{"9":2}}],["hot编码",{"5":{"8":6}}],["hot分布",{"5":{"22":2}}],["hot分布时",{"5":{"22":2}}],["hot时",{"5":{"22":2}}],["hot采样",{"5":{"24":4}}],["homogeneous",{"5":{"5":2,"15":2}}],["homogeneity",{"5":{"24":2}}],["hessian的特征值表示函数在不同特征方向上的曲率",{"5":{"7":2}}],["hessian矩阵的奇异值分解揭示了函数的局部曲率结构",{"5":{"7":2}}],["hessian矩阵决定了泰勒展开的二次项",{"5":{"7":2}}],["hessian矩阵在优化中的作用主要体现在以下几个方面",{"5":{"7":2}}],["hessian矩阵定义为",{"5":{"7":2}}],["hessian矩阵是多元函数的二阶导数矩阵",{"5":{"7":2}}],["he初始化使用均值为0",{"5":{"8":2}}],["he初始化",{"5":{"22":2}}],["he初始化将权重初始化为",{"5":{"22":2}}],["he初始化将权重方差设置为​",{"5":{"22":1}}],["he初始化将权重方差设置为",{"5":{"22":1}}],["he初始化的推导考虑了relu的",{"5":{"22":2}}],["he方差",{"5":{"22":1}}],["href=",{"5":{"6":8,"14":12,"21":20,"27":20}}],["html",{"4":{"0":1,"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1},"5":{"6":8,"14":12,"21":20,"27":20}}],["hat",{"5":{"15":2,"23":108}}],["hadamard积",{"5":{"25":2}}],["权重",{"5":{"13":2,"15":6}}],["权重节点",{"5":{"13":2}}],["权重矩阵与",{"5":{"9":1}}],["权重矩阵的元素​表示第个输入特征与第个输出神经元之间的连接强度",{"5":{"15":1}}],["权重矩阵的元素含义",{"5":{"15":2}}],["权重矩阵的低秩近似可以显著减少参数量和计算量",{"5":{"9":2}}],["权重矩阵的列空间决定了该层能够表示的特征空间的范围",{"5":{"9":2}}],["权重矩阵无处不在",{"5":{"9":2}}],["权重矩阵",{"5":{"5":1,"15":1}}],["权重矩阵决定了不同特征之间的线性组合方式",{"5":{"5":1}}],["权重和阈值需要人工设定",{"5":{"5":2}}],["权重初始化的尺度选择",{"5":{"8":2}}],["权重初始化的一个核心目标是确保各层的前向激活和反向梯度都具有稳定的方差",{"5":{"22":2}}],["权重的乘积",{"5":{"24":1}}],["权重梯度",{"5":{"25":2}}],["权重梯度为",{"5":{"25":2}}],["仍为超平面",{"5":{"5":2}}],["线性回归的闭式解就是通过求解正规方程得到的",{"5":{"9":2}}],["线性无关",{"5":{"8":1}}],["线性变换",{"5":{"13":1}}],["线性变换可以分解为旋转",{"5":{"13":1}}],["线性变换对特征空间的作用包括旋转",{"5":{"9":2}}],["线性变换是理解特征空间中数据变换的关键概念",{"5":{"9":2}}],["线性变换保持高斯性",{"5":{"8":2}}],["线性变换后的点集满足",{"5":{"5":2}}],["线性变换的奇异值分解",{"5":{"13":2}}],["线性变换的复合仍然是线性变换",{"5":{"24":2}}],["线性代数的瑞士军刀",{"5":{"9":2}}],["线性代数的基本运算包括向量加法",{"5":{"9":2}}],["线性代数的核心地位",{"2":{"9":1},"5":{"9":1}}],["线性代数为理解和构建深度学习模型提供了坚实的数学框架",{"5":{"9":2}}],["线性代数作为现代数学的重要分支",{"5":{"9":2}}],["线性代数基础",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["线性代数与张量运算<",{"5":{"6":1,"14":1,"21":1,"27":1}}],["线性代数与张量运算",{"0":{"9":1},"4":{"0":1,"9":1},"5":{"6":4,"9":1,"14":4,"21":4,"27":4}}],["线性映射的数学结构",{"2":{"24":1},"5":{"24":1}}],["线性映射",{"5":{"24":2}}],["线性映射可以用矩阵乘法完全描述",{"5":{"24":2}}],["线性映射描述了输入空间通过旋转",{"5":{"24":2}}],["线性模型表达能力的根本局限",{"2":{"24":1},"5":{"24":1}}],["线性模型虽然具有优美的数学性质",{"5":{"24":2}}],["线性模型的学习能力可以用函数空间的维度来刻画",{"5":{"24":2}}],["线性模型的函数空间是所有仿射函数的集合",{"5":{"24":2}}],["线性模型必然产生系统性误差",{"5":{"24":2}}],["线性模型只能学习形如的函数",{"5":{"24":1}}],["线性模型只能学习形如",{"5":{"24":1}}],["线性分类器",{"5":{"24":2}}],["线性",{"5":{"24":6}}],["则各层的计算定义为",{"5":{"15":2}}],["则全连接层可以表示为",{"5":{"15":2}}],["则对的雅可比矩阵为",{"5":{"13":1}}],["则对的导数为",{"5":{"13":1}}],["则对的梯度为",{"5":{"25":1}}],["则对任意矩阵",{"5":{"8":2}}],["则对应的概率为",{"5":{"24":2}}],["则参数量从",{"5":{"9":1}}],["则参数量从减少到",{"5":{"9":1}}],["则复制整个模型在多个数据批次上并行训练",{"5":{"9":2}}],["则一个批次的表示就是形状为",{"5":{"9":2}}],["则是一个的列向量",{"5":{"9":1}}],["则plu分解总是存在",{"5":{"9":2}}],["则lu分解存在",{"5":{"9":2}}],["则称",{"5":{"9":1}}],["则称为标准正交基",{"5":{"9":2}}],["则称它们是正交的",{"5":{"9":2}}],["则称是的特征向量",{"5":{"9":1}}],["则称该网络存在梯度爆炸问题",{"5":{"22":2}}],["则模型利用了更多的参数自由度",{"5":{"9":2}}],["则系统是不稳定的",{"5":{"9":2}}],["则系统是稳定的",{"5":{"9":2}}],["则会在较小张量的左侧自动补",{"5":{"9":2}}],["则其元素可以表示为",{"5":{"9":1}}],["则其元素可以表示为​",{"5":{"9":1}}],["则其和为",{"5":{"9":1}}],["则其和为向量加法满足交换律和结合律",{"5":{"9":1}}],["则其pdf为",{"5":{"8":2}}],["则其pdf为当",{"5":{"8":2}}],["则标准化和",{"5":{"8":1}}],["则标准化和的分布趋向于标准正态分布",{"5":{"8":1}}],["则样本均值",{"5":{"8":1}}],["则样本均值依概率收敛于对任意成立",{"5":{"8":1}}],["则鼓励",{"5":{"8":1}}],["则交叉熵损失相对于",{"5":{"8":1}}],["则交叉熵损失相对于的梯度为​",{"5":{"8":1}}],["则给定",{"5":{"8":1}}],["则给定时的条件分布仍然是高斯分布",{"5":{"8":1}}],["则任意分量",{"5":{"8":1}}],["则任意分量服从一维高斯分布",{"5":{"8":1}}],["则拒绝原假设",{"5":{"8":2}}],["则记为",{"5":{"8":2}}],["则梯度的分布趋向于高斯分布",{"5":{"8":2}}],["则梯度",{"5":{"7":1}}],["则梯度是一个与形状相同的三阶张量",{"5":{"7":1}}],["则梯度会按指数级衰减",{"5":{"23":2}}],["则需要进一步分析",{"5":{"7":2}}],["则该维度可以广播",{"5":{"9":2}}],["则该点是鞍点",{"5":{"7":2}}],["则该点是局部极大值",{"5":{"7":2}}],["则该点是局部极小值",{"5":{"7":2}}],["则该层的输出为",{"5":{"5":2}}],["则hessian矩阵是对称的",{"5":{"7":2}}],["则关于的偏导数为",{"5":{"7":1}}],["则关于变量在点处的偏导数定义为",{"5":{"7":1}}],["则第",{"5":{"5":1}}],["则第层的输出为",{"5":{"5":1}}],["则提供了灵活的阈值调节能力",{"5":{"5":1}}],["则将整个超平面平移",{"5":{"5":1}}],["则整个网络等价于一个单一的仿射变换",{"5":{"5":2}}],["则仿射变换可以统一表示为",{"5":{"5":2}}],["则",{"5":{"5":8,"7":6,"8":10,"9":1,"13":8,"22":12,"23":3,"25":9}}],["则当且仅当两个输入均为1时",{"5":{"5":2}}],["则在点处的梯度定义为",{"5":{"7":1}}],["则在该区域输入的饱和称为输出饱和",{"5":{"22":2}}],["则在该区域输入的微小变化不会引起输出的显著变化",{"5":{"22":2,"23":2}}],["则每层的梯度传递因子约为",{"5":{"22":2}}],["则​",{"5":{"22":1}}],["则沿的方向导数为",{"5":{"7":1}}],["则沿时间步反向传播后",{"5":{"22":1}}],["则沿时间步",{"5":{"22":1}}],["则随指数增长",{"5":{"22":1}}],["则随指数衰减",{"5":{"22":1}}],["则总损失",{"5":{"22":2}}],["则的梯度为​",{"5":{"13":1}}],["则的特征值接近1",{"5":{"22":1}}],["则的各元素方差为",{"5":{"22":1}}],["则和",{"5":{"22":1}}],["则反向传播的梯度变换为​",{"5":{"22":1}}],["则反向传播的梯度变换为",{"5":{"22":1}}],["则只能为正或零",{"5":{"23":1}}],["则网络的前向传播可以表示为两个连续的线性变换",{"5":{"24":2}}],["则输出向量为",{"5":{"24":2}}],["则它们的线性组合可以逼近任意连续函数",{"5":{"24":2}}],["则深度神经网络在表示效率上可能显著优于浅层网络",{"5":{"24":2}}],["则层次化特征学习可以表示为",{"5":{"24":2}}],["则位置感知的嵌入表示为",{"5":{"24":2}}],["则可以理解为在​空间中的表示",{"5":{"13":1}}],["则可以认为反向传播实现正确",{"5":{"25":2}}],["则按比例缩放",{"5":{"25":2}}],["彭罗斯广义逆",{"5":{"5":2}}],["从神经元到层",{"2":{"15":1},"5":{"15":1}}],["从前向传播的数学描述到实际代码实现",{"5":{"13":1}}],["从计算到函数逼近",{"2":{"13":1},"5":{"13":1}}],["从张量的角度来看",{"5":{"9":2}}],["从张量网络的角度来看",{"5":{"9":2}}],["从矩阵乘法的并行计算到张量运算的维度变换",{"5":{"9":2}}],["从transformer架构的注意力机制到词嵌入的向量表示",{"5":{"9":2}}],["从中心极限定理的角度看",{"5":{"8":2}}],["从kl散度的角度看",{"5":{"8":2}}],["从优化的角度看",{"5":{"8":2}}],["从公式可以看出",{"5":{"8":2}}],["从数学角度来看",{"5":{"13":2,"25":2}}],["从数学角度看",{"5":{"5":2,"24":2}}],["从数学上看",{"5":{"8":2}}],["从数学上分析",{"5":{"24":4}}],["从应用上看",{"5":{"8":2}}],["从理论上看",{"5":{"8":2}}],["从损失函数的定义到模型性能的评价",{"5":{"8":2}}],["从很小的值线性增长到目标学习率",{"5":{"7":2}}],["从而捕获更抽象的语义信息",{"5":{"13":2}}],["从而实现高效的存储和计算",{"5":{"9":2}}],["从而在多个计算设备上分布式执行",{"5":{"9":2}}],["从而在反向传播时为梯度提供了一条恒等映射路径",{"5":{"9":2}}],["从而约束了网络函数的lipschitz常数",{"5":{"9":2}}],["从而缓解了梯度消失问题",{"5":{"9":2}}],["从而使得机器学习算法能够在这些向量上进行运算和优化",{"5":{"9":2}}],["从而加速收敛并减少震荡",{"5":{"7":2}}],["从而大幅降低计算和存储成本",{"5":{"7":2}}],["从而解决异或等线性不可分问题",{"5":{"5":2}}],["从而通过stone",{"5":{"24":2}}],["从而逼近任意形状的函数曲面",{"5":{"24":2}}],["从而更准确地估计和最大化互信息",{"5":{"24":2}}],["从而影响模型对位置模式的捕获能力",{"5":{"24":2}}],["从而开启了深度学习的现代时代",{"5":{"25":2}}],["从而避免下溢",{"5":{"25":2}}],["从原始输入空间",{"5":{"5":1}}],["从",{"5":{"5":1}}],["从几何角度来看",{"5":{"7":2,"9":2}}],["从几何角度看",{"5":{"5":2,"13":2,"15":2,"23":2}}],["从几何角度理解",{"5":{"22":2,"24":4}}],["从几何角度分析",{"5":{"23":2}}],["从几何视角来看",{"5":{"5":2}}],["从几何上看",{"5":{"23":2}}],["从麦肯罗皮层神经元的二值模型出发",{"5":{"5":2}}],["从特征学习的角度来看",{"5":{"5":2}}],["从到的仿射变换定义为",{"5":{"5":1}}],["从生物神经元到数学抽象",{"2":{"5":1},"5":{"5":1}}],["从导数公式可以看出",{"5":{"22":1}}],["从导数公式",{"5":{"22":1}}],["从极限的角度分析sigmoid函数的边界行为",{"5":{"23":2}}],["从某种意义上",{"5":{"23":2}}],["从变换的角度",{"5":{"23":2}}],["从简单的曲线到高维空间中的复杂流形",{"5":{"24":2}}],["从直观上看",{"5":{"24":2}}],["从概率角度看",{"5":{"5":2}}],["从概率论的角度",{"5":{"24":4}}],["从概率论角度",{"5":{"24":2}}],["从贝叶斯推断的角度",{"5":{"24":2}}],["从多项式分布的角度",{"5":{"24":2}}],["从信息论角度",{"5":{"22":2,"24":8}}],["从信息论的角度",{"5":{"24":4}}],["从梯度计算的期望到模型不确定性的估计",{"5":{"8":2}}],["从梯度角度分析",{"5":{"24":2}}],["从线性映射的局限性出发",{"5":{"24":2}}],["从函数逼近论的角度",{"5":{"24":2}}],["从链式法则的矩阵形式出发",{"5":{"25":2}}],["仿射变换负责旋转",{"5":{"13":2}}],["仿射变换本质上是对输入的线性投影",{"5":{"13":1}}],["仿射变换本身是线性变换",{"5":{"5":1}}],["仿射变换在几何上等价于线性变换",{"5":{"13":1}}],["仿射变换在几何上等价于先进行线性变换",{"5":{"5":1}}],["仿射变换进行线性特征组合",{"5":{"5":2}}],["仿射变换承担着特征提取和信息整合的功能",{"5":{"5":1}}],["仿射变换将其映射为另一个超平面",{"5":{"5":2}}],["仿射变换的几何意义",{"5":{"5":2,"13":2}}],["仿射变换退化为线性变换",{"5":{"5":2}}],["仿射变换",{"5":{"5":5,"13":2}}],["仿射变换是线性变换的推广",{"5":{"5":2}}],["但可以从信息论和函数逼近的角度理解",{"5":{"13":2}}],["但网络在局部区域",{"5":{"13":2}}],["但深入理解前向传播的数学本质",{"5":{"13":2}}],["但奇异值总是非负的",{"5":{"9":2}}],["但又有所不同",{"5":{"9":2}}],["但又足够高以捕获丰富的语义信息",{"5":{"9":2}}],["但矩阵求逆的数学思想",{"5":{"9":2}}],["但不满足交换律",{"5":{"9":2}}],["但不要求保持原点不变",{"5":{"5":2}}],["但不同激活函数的逼近效率是不同的",{"5":{"24":2}}],["但改进幅度可能很小以至于在实际应用中可以忽略",{"5":{"8":2}}],["但在某些场景",{"5":{"13":2}}],["但在深度学习中使用较少",{"5":{"9":2}}],["但在深度学习中的直接应用较少",{"5":{"8":2}}],["但在处理序列数据的某些场景下",{"5":{"9":2}}],["但在语言模型中更常用的是分类分布",{"5":{"8":2}}],["但仍需要保持某些计算的精度",{"5":{"7":2}}],["但最近的研究表明",{"5":{"7":2}}],["但由于广播可能在语法上合法而在语义上错误",{"5":{"9":2}}],["但由于其输出范围是",{"5":{"22":2}}],["但由于tanh的导数峰值是1而sigmoid只有0",{"5":{"23":2}}],["但其核心洞见对连续激活函数同样适用",{"5":{"15":2}}],["但其饱和程度较轻",{"5":{"22":2}}],["但其表达能力受到根本性的限制",{"5":{"24":2}}],["但其分段线性的性质使得它在大规模网络中表现优异",{"5":{"24":2}}],["但引入了新的梯度挑战",{"5":{"22":2}}],["但当这种极端分布与后续的线性变换结合时",{"5":{"22":2}}],["但按值裁剪可能改变梯度的方向",{"5":{"22":2}}],["但表达能力可能受限",{"5":{"22":2}}],["但现代硬件和库优化使得其计算开销在可接受范围内",{"5":{"22":2}}],["但范数被限制在以下",{"5":{"22":1}}],["但范数被限制在",{"5":{"22":1}}],["但它已经具备了一定的逻辑推理能力",{"5":{"5":2}}],["但它在理解激活函数导数性质方面具有重要的教学价值",{"5":{"23":2}}],["但它们都保持着用",{"5":{"23":2}}],["但近年来relu的流行表明",{"5":{"24":2}}],["但softmax激活函数引入了关键的非线性",{"5":{"24":2}}],["但softmax操作本质上是另一种形式的非线性激活",{"5":{"24":2}}],["但并没有告诉我们如何调整参数以改善输出",{"5":{"25":2}}],["但这是实现高效梯度计算的必要代价",{"5":{"25":2}}],["times",{"5":{"15":14}}],["tr",{"5":{"15":2}}],["tree",{"5":{"9":2}}],["transpose",{"5":{"9":2}}],["transformer",{"5":{"9":2,"22":6}}],["transformer架构虽然解决了rnn的长期依赖问题",{"5":{"22":2}}],["transformer中的注意力机制可以被看作一个三阶张量",{"5":{"9":2}}],["transformer中的梯度爆炸机制",{"2":{"22":1},"5":{"22":1}}],["transformer中的激活函数选择不是随意的",{"5":{"24":2}}],["transformer中的前馈网络",{"5":{"24":2}}],["transformer中",{"5":{"22":4}}],["transformer采用了深度网络的架构设计",{"5":{"24":2}}],["transformer的自注意力机制本身不包含显式的激活函数",{"5":{"24":2}}],["transformation",{"5":{"5":2}}],["tuning",{"5":{"9":2}}],["tucker",{"5":{"9":6}}],["text",{"5":{"15":28,"23":58}}],["tensor",{"5":{"9":4}}],["tensorflow",{"5":{"5":2}}],["test",{"5":{"8":2}}],["target=",{"5":{"5":2,"6":4,"7":3,"8":3,"9":2,"13":2,"14":6,"21":10,"25":1,"27":10}}],["tanh",{"5":{"5":2,"22":1,"23":2,"24":4}}],["tanh的导数为",{"5":{"22":2}}],["tanh的导数",{"5":{"23":2}}],["tanh的导数峰值是1而非0",{"5":{"22":2}}],["tanh的导数峰值更大",{"5":{"23":2}}],["tanh的输出以零为中心",{"5":{"22":2}}],["tanh的输出以0为中心",{"5":{"23":2}}],["tanh的零中心性质",{"5":{"23":2}}],["tanh的零中心性质缓解了这个问题",{"5":{"23":2}}],["tanh的零中心输出消除了这个问题",{"5":{"23":2}}],["tanh的梯度信号更强",{"5":{"22":2}}],["tanh的梯度",{"5":{"23":2}}],["tanh的饱和区域",{"5":{"22":2}}],["tanh的饱和问题依然严重",{"5":{"23":2}}],["tanh有两个显著差异",{"5":{"22":2}}],["tanh函数的饱和特性",{"2":{"22":1},"5":{"22":1}}],["tanh函数的饱和特性与sigmoid类似",{"5":{"22":1}}],["tanh函数的梯度饱和区域为",{"5":{"22":2}}],["tanh函数的数学定义与性质",{"2":{"23":1},"5":{"23":1}}],["tanh函数的导数与性质",{"2":{"23":1},"5":{"23":1}}],["tanh函数的导数为",{"5":{"23":1}}],["tanh函数",{"5":{"22":1,"23":7}}],["tanh函数是奇函数",{"5":{"23":2}}],["tanh函数可以表示为sigmoid函数的缩放和平移",{"5":{"23":2}}],["tanh与sigmoid有密切的数学关系",{"5":{"23":2}}],["tanh与sigmoid的饱和比较",{"5":{"22":2}}],["tanh与sigmoid的关系",{"5":{"23":2}}],["tanh可以视为sigmoid的缩放和平移版本",{"5":{"23":2}}],["tanh和sigmoid存在直接的数学关系",{"5":{"23":2}}],["tanh本质上是sigmoid的",{"5":{"23":2}}],["tanh以0为中心",{"5":{"23":2}}],["tanh先对输入进行2倍缩放",{"5":{"23":2}}],["tanh导数",{"5":{"23":4}}],["tanh导数的取值范围",{"5":{"23":2}}],["tanh导数的取值范围是",{"5":{"23":2}}],["tanh导数同样会迅速衰减至",{"5":{"23":2}}],["tanh在激活值接近零的区域",{"5":{"23":2}}],["tanh输出区间",{"5":{"23":1}}],["tanh输出",{"5":{"23":1}}],["theorem",{"5":{"8":2}}],["the",{"5":{"5":2}}],["topological",{"5":{"13":2}}],["to",{"5":{"23":2}}],["token的交互特征",{"5":{"24":2}}],["在此处按元素应用于向量",{"5":{"15":2}}],["在全连接层中",{"5":{"15":2}}],["在本节中",{"5":{"15":2}}],["在上一节中",{"5":{"15":2}}],["在同一层内",{"5":{"13":2}}],["在反向模式自动微分中",{"5":{"13":2}}],["在反向传播时重新计算被省略的激活值来节省内存",{"5":{"9":2}}],["在反向传播时重新计算其余激活值",{"5":{"7":2}}],["在反向传播中",{"5":{"22":8,"23":6,"24":2,"25":2}}],["在反向传播中需要​",{"5":{"22":1}}],["在反向传播中需要",{"5":{"22":1}}],["在反向传播算法中",{"5":{"23":2}}],["在前向模式自动微分中",{"5":{"13":2}}],["在前向传播过程中",{"5":{"9":2}}],["在层网络中",{"5":{"13":1}}],["在第2",{"5":{"13":2}}],["在第二和第三维度上的收缩产生一个",{"5":{"9":1}}],["在第四章中",{"5":{"24":2}}],["在2",{"5":{"13":2,"25":2}}],["在批处理情况下变成",{"5":{"9":1}}],["在物理学和机器学习中都有重要应用",{"5":{"9":2}}],["在模型的宽度维度上划分参数",{"5":{"9":2}}],["在模型并行化中",{"5":{"9":2}}],["在模型压缩和低秩近似中",{"5":{"9":2}}],["在模型压缩方面",{"5":{"9":2}}],["在分析神经网络梯度的分布时",{"5":{"8":2}}],["在分析梯度流和稳定性时",{"5":{"9":2}}],["在分析梯度饱和之前",{"5":{"22":2}}],["在推荐系统和缺失数据插补中",{"5":{"9":2}}],["在对比学习中",{"5":{"9":2}}],["在对话系统中",{"5":{"8":2}}],["在特征提取和表示学习中经常被使用",{"5":{"9":2}}],["在frobenius范数和谱范数意义下",{"5":{"9":2}}],["在循环神经网络",{"5":{"9":2}}],["在主成分分析",{"5":{"9":2}}],["在变换下只发生缩放而不改变方向",{"5":{"9":2}}],["在变分推断中",{"5":{"8":4}}],["在变分自编码器和生成模型中",{"5":{"8":2}}],["在神经网络的稳定性分析和lipschitz约束中",{"5":{"9":2}}],["在神经网络的语境下",{"5":{"5":2}}],["在神经网络中",{"5":{"9":2,"24":2}}],["在词嵌入研究中",{"5":{"9":2}}],["在注意力机制中",{"5":{"9":2}}],["在多头注意力中",{"5":{"9":2,"22":2}}],["在多类分类中为one",{"5":{"8":2}}],["在多项式分布中",{"5":{"24":2}}],["在多层神经网络中",{"5":{"25":2}}],["在优化算法和正则化方法中有着重要的应用",{"5":{"9":2}}],["在实际应用中",{"5":{"5":2,"8":2,"22":2}}],["在实际训练中",{"5":{"22":2,"25":2}}],["在实际计算中也具有重要的应用价值",{"5":{"23":2}}],["在实际实现中",{"5":{"9":2}}],["在实际实现反向传播时",{"5":{"25":2}}],["在实践中",{"5":{"9":2,"22":4,"24":2}}],["在实践中使用作为近似",{"5":{"22":1}}],["在实践中使用",{"5":{"22":1}}],["在实现反向传播时",{"5":{"9":2}}],["在现代深度学习框架中",{"5":{"9":2}}],["在观测数据",{"5":{"8":1}}],["在观测数据后",{"5":{"8":1}}],["在",{"5":{"8":1,"13":2,"22":2,"23":8,"25":2}}],["在原假设下所有排列是等可能的",{"5":{"8":2}}],["在比较语言模型时",{"5":{"8":2}}],["在比较语言模型性能时",{"5":{"8":2}}],["在机器学习训练中",{"5":{"8":2}}],["在机器学习特别是大语言模型中有着广泛应用",{"5":{"8":2}}],["在正则化中",{"5":{"9":2}}],["在正则化技术方面",{"5":{"8":2}}],["在正则条件下",{"5":{"8":2}}],["在生成模型中",{"5":{"8":2}}],["在其存在的范围内",{"5":{"8":2}}],["在其他类别上均匀分配",{"5":{"8":1}}],["在其他类别上均匀分配的概率",{"5":{"8":1}}],["在其他区域抑制",{"5":{"24":2}}],["在知识蒸馏中",{"5":{"8":2}}],["在强化学习与语言模型的结合中",{"5":{"8":2}}],["在vae中",{"5":{"8":2}}],["在高斯过程回归和二次优化中有广泛应用",{"5":{"9":2}}],["在高斯过程和贝叶斯优化中",{"5":{"8":2}}],["在高维空间中",{"5":{"7":2}}],["在高维输入空间中",{"5":{"24":2}}],["在权重初始化方面",{"5":{"8":2}}],["在统计学和机器学习中有着核心地位",{"5":{"8":2}}],["在某些正则化方法中",{"5":{"9":2}}],["在某些深度学习应用中",{"5":{"8":2}}],["在某些情况下",{"5":{"7":2}}],["在语言模型评估中",{"5":{"8":2}}],["在语言模型训练中",{"5":{"8":2}}],["在语言模型的评估中",{"5":{"8":2}}],["在语言模型的某些应用中",{"5":{"8":2}}],["在语言模型研究中",{"5":{"8":2}}],["在语言模型中",{"5":{"8":12}}],["在函数",{"5":{"7":1}}],["在函数的等值面上",{"5":{"7":1}}],["在点",{"5":{"7":3}}],["在点​附近",{"5":{"7":1}}],["在适当的噪声水平下",{"5":{"7":2}}],["在训练大语言模型中尤为重要",{"5":{"7":2}}],["在训练初期逐渐增加学习率",{"5":{"7":2}}],["在凸优化问题中",{"5":{"7":2}}],["在后期精细调优",{"5":{"7":2}}],["在贝叶斯深度学习和变分推断中",{"5":{"7":2}}],["在与梯度垂直的方向",{"5":{"7":2}}],["在与梯度相反的方向",{"5":{"7":2}}],["在大语言模型",{"5":{"9":2}}],["在大语言模型评估中",{"5":{"8":2}}],["在大语言模型的架构设计中",{"5":{"9":2}}],["在大语言模型的实践中",{"5":{"9":2}}],["在大语言模型的理论基础中占据着不可替代的核心地位",{"5":{"9":2}}],["在大语言模型的训练中",{"5":{"8":1}}],["在大语言模型的在线评估中",{"5":{"8":2}}],["在大语言模型的开发和研究中",{"5":{"8":2}}],["在大语言模型的语境下",{"5":{"7":2,"9":2}}],["在大语言模型中",{"5":{"7":2,"8":14,"9":18}}],["在深度维度上划分层",{"5":{"9":2}}],["在深度学习优化中",{"5":{"7":2}}],["在深度学习的反向传播算法中",{"5":{"7":2}}],["在深度学习中有着广泛的应用",{"5":{"9":2}}],["在深度学习中",{"5":{"7":4,"8":4,"9":6}}],["在深入数学细节之前",{"5":{"5":2}}],["在深层网络中",{"5":{"22":2,"23":2}}],["在几何上对仿射变换的结果执行了逐元素的非线性",{"5":{"13":1}}],["在几何上等价于线性变换",{"5":{"13":1}}],["在几何上等价于先进行线性变换",{"5":{"5":1}}],["在几何上称为仿射变换",{"5":{"5":2}}],["在计算图中",{"5":{"7":2}}],["在计算损失和梯度时",{"5":{"5":2}}],["在计算机图形学和计算机视觉中",{"5":{"5":2}}],["在时",{"5":{"22":2}}],["在时间反向传播",{"5":{"22":2}}],["在数值上完全不可检测",{"5":{"22":2,"23":2}}],["在极端情况下",{"5":{"22":2}}],["在自然语言处理中",{"5":{"8":2}}],["在自注意力机制中",{"5":{"22":4}}],["在自监督学习中",{"5":{"24":2}}],["在梯度传播过程中",{"5":{"22":2}}],["在最小二乘问题中特别有用",{"5":{"7":2}}],["在最小奇异值方向上可能产生相对较大的输出梯度",{"5":{"22":2}}],["在非常深的网络中",{"5":{"22":2}}],["在卷积网络中",{"5":{"22":2}}],["在线学习",{"5":{"22":2}}],["在线性代数的学习路径中",{"5":{"9":2}}],["在线性区域",{"5":{"22":2}}],["在线性模型中",{"5":{"24":2}}],["在pytorch中",{"5":{"9":2}}],["在pre",{"5":{"22":4}}],["在post",{"5":{"22":2}}],["在许多场景下",{"5":{"22":2}}],["在保持正区域良好梯度的同时",{"5":{"22":2}}],["在负无穷处趋向于饱和值0",{"5":{"23":2}}],["在处",{"5":{"23":2}}],["在处有拐点",{"5":{"23":1}}],["在处有",{"5":{"23":1}}],["在sigmoid中",{"5":{"23":2}}],["在transformer中",{"5":{"22":4,"24":2}}],["在transformer架构中扮演着关键角色",{"5":{"23":2}}],["在transformer的自注意力机制中",{"5":{"24":2}}],["在形式上非常相似",{"5":{"23":1}}],["在一侧",{"5":{"24":2}}],["在另一侧",{"5":{"24":2}}],["在每个象限内",{"5":{"13":2}}],["在每一层提供必要的非线性变换",{"5":{"24":2}}],["在伯努利分布中",{"5":{"24":2}}],["在逻辑回归中",{"5":{"24":2}}],["在需要从离散分布采样的场景中",{"5":{"24":2}}],["在任何输入下",{"5":{"24":2}}],["在参数高效微调",{"5":{"9":2}}],["在参数化的概率分布族上",{"5":{"24":1}}],["在参数化的概率分布族",{"5":{"24":1}}],["在这个框架下",{"5":{"24":2}}],["在这个度量下",{"5":{"24":2}}],["在整个输入范围内应该接近1",{"5":{"22":2}}],["在整个实数轴上都是正的",{"5":{"24":2}}],["在输出建模方面",{"5":{"8":2}}],["在输入空间中定义了一个",{"5":{"24":1}}],["在输入空间中形成一个",{"5":{"24":1}}],["在获得各层误差信号后",{"5":{"25":2}}],["在混合精度训练中",{"5":{"25":2}}],["设输入向量为",{"5":{"15":2,"24":2}}],["设输入为",{"5":{"15":2,"24":2}}],["设输入矩阵",{"5":{"5":1}}],["设输入矩阵包含个样本",{"5":{"5":1}}],["设输入的绝对值较大时",{"5":{"23":2}}],["设输入空间为",{"5":{"24":2}}],["设第",{"5":{"13":1}}],["设第层的输入为",{"5":{"13":1}}],["设计权重和偏置如下",{"5":{"15":2}}],["设计模型架构和优化计算资源至关重要",{"5":{"13":2}}],["设计优化算法和理解泛化性质时非常有用",{"5":{"8":2}}],["设计专门的激活函数",{"5":{"22":2}}],["设的伴随值为",{"5":{"13":1}}],["设的第行为",{"5":{"5":1}}],["设和为两个映射",{"5":{"13":1}}],["设单个样本的特征表示是形状为",{"5":{"9":1}}],["设单个样本的特征表示是形状为的二维张量",{"5":{"9":1}}],["设连续随机变量",{"5":{"8":1}}],["设连续随机变量的pdf为",{"5":{"8":1}}],["设参数",{"5":{"8":2}}],["设参数矩阵的奇异值分解为",{"5":{"22":1}}],["设参数矩阵",{"5":{"22":1}}],["设我们有观测数据",{"5":{"8":2}}],["设是独立同分布的随机变量",{"5":{"8":2}}],["设是一个维随机向量",{"5":{"8":1}}],["设是一个单位向量",{"5":{"7":1}}],["设是一个可微函数",{"5":{"7":1}}],["设是一个多元函数",{"5":{"7":1}}],["设是隐藏状态",{"5":{"22":1}}],["设是隐藏状态向量",{"5":{"22":1}}],["设是任意非常数的有界连续函数",{"5":{"24":1}}],["设是第个类别的logit",{"5":{"8":1}}],["设是第层的表示向量",{"5":{"24":1}}],["设是",{"5":{"24":1}}],["设是位置编码矩阵",{"5":{"24":1}}],["设随机变量在个类别上服从类别分布",{"5":{"8":1}}],["设随机变量",{"5":{"8":3}}],["设有一组神经元",{"5":{"15":2}}],["设有一个三阶张量",{"5":{"9":2}}],["设有一个",{"5":{"5":1,"13":1,"15":1,"25":1}}],["设有一个层的前馈神经网络",{"5":{"13":1,"15":1}}],["设有一个层的前馈网络",{"5":{"5":1}}],["设有一个层前馈网络",{"5":{"25":1}}],["设有两个",{"5":{"9":1}}],["设有两个维向量",{"5":{"9":1}}],["设有",{"5":{"5":1}}],["设有个神经元的层",{"5":{"5":1}}],["设为第层的输出",{"5":{"13":1}}],["设为个类别之一",{"5":{"5":1}}],["设为类别标签",{"5":{"5":1}}],["设为输入向量",{"5":{"5":1}}],["设为饱和阈值",{"5":{"22":1}}],["设为损失函数",{"5":{"22":1}}],["设为标量损失函数",{"5":{"25":1}}],["设",{"5":{"5":8,"7":7,"8":10,"13":10,"22":10,"23":2,"24":4,"25":6}}],["设权重",{"5":{"5":6}}],["设个头的输出拼接为",{"5":{"22":1}}],["设​",{"5":{"22":1}}],["设​为后向传播的中间值",{"5":{"13":1}}],["设​为第层的误差信号",{"5":{"25":1}}],["设mini",{"5":{"22":2}}],["设卷积输出的形状为",{"5":{"22":2}}],["设平均每层的梯度传递因子为",{"5":{"22":2}}],["设平均每层的梯度衰减因子为",{"5":{"23":2}}],["设真实的数据生成过程涉及两个变量的交互效应",{"5":{"24":2}}],["设隐藏维度",{"5":{"13":2}}],["设隐藏层的输出为",{"5":{"24":2}}],["设一个深度为",{"5":{"24":2}}],["设网络的一层为",{"5":{"22":2}}],["设网络的原始输出为",{"5":{"24":2}}],["设激活函数逐元素应用于向量",{"5":{"24":1}}],["设激活函数",{"5":{"24":1}}],["设标量函数",{"5":{"25":2}}],["设批量输入",{"5":{"25":2}}],["感知机",{"5":{"5":2}}],["感知机的数学定义为",{"5":{"5":2}}],["感知机模型",{"5":{"5":2}}],["感知机通常采用连续可微的激活函数",{"5":{"5":2}}],["感知机是麦肯罗皮层神经元的重要推广",{"5":{"5":2}}],["感知机与仿射变换",{"2":{"5":1},"5":{"5":1}}],["phi",{"5":{"15":2,"23":6}}],["p",{"5":{"15":2,"23":98}}],["pipeline",{"5":{"9":2}}],["pitts",{"5":{"5":4}}],["pi",{"5":{"23":2}}],["peft",{"5":{"9":2}}],["permutation",{"5":{"8":2}}],["perplexity",{"5":{"8":2}}],["perceptron",{"5":{"5":2}}],["pe",{"5":{"23":4}}],["pca",{"5":{"9":6}}],["power",{"5":{"9":2}}],["point",{"5":{"7":2,"13":2}}],["post",{"5":{"22":3}}],["pytorch",{"5":{"9":2}}],["p值受样本量影响很大",{"5":{"8":2}}],["p值",{"5":{"8":2}}],["ppo",{"5":{"8":2}}],["pdf同样满足非负性",{"5":{"8":1}}],["pdf同样满足非负性和归一性",{"5":{"8":1}}],["pdf在单点的取值不代表概率",{"5":{"8":2}}],["pdf",{"5":{"8":2}}],["pmf必须满足两个基本性质",{"5":{"8":2}}],["pmf",{"5":{"8":2}}],["png",{"5":{"5":8,"7":12,"8":12,"9":8,"13":8,"25":4}}],["partial",{"5":{"9":2,"23":68}}],["params",{"5":{"15":4}}],["parameter",{"5":{"9":2}}],["parallelism",{"5":{"9":6}}],["parafac",{"5":{"9":2}}],["path",{"5":{"22":2}}],["probability",{"5":{"8":4}}],["processing",{"5":{"5":2,"9":2}}],["propagation",{"5":{"8":2,"13":2}}],["property",{"5":{"23":2}}],["product",{"5":{"24":2}}],["pre",{"5":{"22":8}}],["prelu",{"5":{"23":2}}],["弗兰克",{"5":{"5":2}}],["它使用半精度浮点数",{"5":{"9":2}}],["它使得我们能够高效地计算复合函数对底层变量的梯度",{"5":{"13":2}}],["它使得大规模神经网络的端到端训练成为可能",{"5":{"25":2}}],["它是离散的二值模型",{"5":{"5":2}}],["它是一个复合函数的逐层求值过程",{"5":{"13":2}}],["它是一个无界的实数",{"5":{"24":2}}],["它是负对数似然的期望",{"5":{"24":2}}],["它揭示了神经网络如何通过层次化的非线性变换逼近复杂的函数关系",{"5":{"13":2}}],["它本质上实现了从输入空间到输出空间的函数映射",{"5":{"13":2}}],["它对应于从",{"5":{"9":1}}],["它对应于从到的线性变换中最大的拉伸因子",{"5":{"9":1}}],["它通过在前向传播时只保存部分层的激活值",{"5":{"9":2}}],["它通过将权重矩阵除以其谱范数来约束网络函数的lipschitz常数",{"5":{"9":2}}],["它通过匹配矩",{"5":{"8":2}}],["它通过限制梯度的范数来防止过度更新",{"5":{"22":2}}],["它通过对激活值进行归一化来稳定训练过程",{"5":{"22":2}}],["它可以看作是矩阵展开为向量后的l2范数",{"5":{"9":2}}],["它可以显著简化代码",{"5":{"9":2}}],["它可以被理解为",{"5":{"24":2}}],["它只旋转或反射空间而不改变向量的长度",{"5":{"9":2}}],["它只能处理线性可分的问题",{"5":{"5":2}}],["它用于权重归一化以稳定训练过程",{"5":{"9":2}}],["它描述了输入数据如何逐层变换",{"5":{"13":2}}],["它描述了输入中的冗余信息",{"5":{"9":2}}],["它描述了矩阵所能表示的所有输出",{"5":{"9":2}}],["它描述了当其他变量保持不变时",{"5":{"7":2}}],["它与随机变量",{"5":{"8":1}}],["它与随机变量具有相同的量纲",{"5":{"8":1}}],["它不仅告诉我们差异是否显著",{"5":{"8":2}}],["它们共享相同的输入",{"5":{"15":2}}],["它们的复合",{"5":{"13":1}}],["它们的复合定义为",{"5":{"13":1}}],["它们的kronecker积定义为",{"5":{"9":2}}],["它们的均值",{"5":{"8":2}}],["它们的联合结构使得损失景观具有特定的几何形状",{"5":{"24":2}}],["它们揭示了线性变换的本质特征",{"5":{"9":2}}],["它们描述了大量随机变量之和",{"5":{"8":2}}],["它们用于衡量两个概率分布之间的差异",{"5":{"8":2}}],["它们刻画了随机变量的集中趋势",{"5":{"8":2}}],["它们直接决定了神经网络能否有效学习",{"5":{"22":2}}],["它们通过规范化激活值或梯度的分布来稳定训练过程",{"5":{"22":2}}],["它们都可以作为通用逼近器的核心组件",{"5":{"24":2}}],["它们决定了信息如何被编码和传递",{"5":{"24":2}}],["它们定义了从输入空间到表示空间的映射",{"5":{"24":2}}],["它考虑了各变量之间的相关性",{"5":{"8":2}}],["它总是存在的",{"5":{"8":2}}],["它唯一确定了随机变量的概率分布",{"5":{"8":2}}],["它结合了动量方法和rmsprop的优点",{"5":{"7":2}}],["它累积历史梯度来平滑参数更新",{"5":{"7":2}}],["它决定了每次更新的幅度",{"5":{"7":2}}],["它利用中心极限定理的原理来稳定训练过程",{"5":{"8":2}}],["它利用负梯度方向作为搜索方向来迭代地最小化目标函数",{"5":{"7":2}}],["它利用链式法则自动计算复杂函数的导数",{"5":{"7":2}}],["它将仿射变换统一为线性变换的形式",{"5":{"15":2}}],["它将输入数据依次通过",{"5":{"13":1}}],["它将输入数据依次通过层非线性变换",{"5":{"13":1}}],["它将输入空间划分为两个半空间",{"5":{"5":2}}],["它将数学表达式显式地表示为节点和边的有向无环图",{"5":{"13":2}}],["它将一个",{"5":{"9":1}}],["它将一个的矩阵与一个的矩阵组合成一个的大矩阵",{"5":{"9":1}}],["它将矩阵分解为下三角矩阵与其转置的乘积",{"5":{"9":2}}],["它将权重矩阵的谱范数归一化为1",{"5":{"9":2}}],["它将标量",{"5":{"9":2}}],["它将某一子层的输入与该子层的输出直接相加",{"5":{"9":2}}],["它将后验分布的推断转化为优化问题",{"5":{"8":2}}],["它将参数视为随机变量并使用贝叶斯公式进行推断",{"5":{"8":2}}],["它将硬标签",{"5":{"8":2}}],["它将随机试验的结果数值化",{"5":{"8":2}}],["它将hessian矩阵近似为两个小矩阵的kronecker积",{"5":{"7":2}}],["它将实数映射到",{"5":{"23":3}}],["它将实数映射到区间",{"5":{"23":1}}],["它将概率值转换回实数域",{"5":{"23":2}}],["它将维向量映射到概率单纯形",{"5":{"24":1}}],["它将位置信息从隐式",{"5":{"24":2}}],["它将",{"5":{"24":1}}],["它指的是在训练过程中梯度值变得非常小",{"5":{"22":2}}],["它指出大量独立同分布随机变量之和",{"5":{"8":2}}],["它指出",{"5":{"24":2}}],["它减少了内部协变量偏移",{"5":{"22":2}}],["它允许同时处理多个样本以提高计算效率",{"5":{"9":2}}],["它允许使用更大的学习率",{"5":{"22":2}}],["它移除了均值归一化",{"5":{"22":2}}],["它无法表示复杂的曲线",{"5":{"24":2}}],["它从根本上改变了梯度传播的路径",{"5":{"22":2}}],["它从数学上证明了深度学习的表达能力基础",{"5":{"24":2}}],["它在更新之前先对梯度进行校正",{"5":{"7":2}}],["它在输入空间的某个区域激活",{"5":{"24":2}}],["它正是交叉熵损失的核心组成部分",{"5":{"24":2}}],["它提供了一定的正则化效果",{"5":{"22":2}}],["它提供了数值稳定性",{"5":{"24":2}}],["它保持了空间的平行性和比例关系",{"5":{"5":2}}],["它保留正信息",{"5":{"24":2}}],["它度量了和之间的依赖程度",{"5":{"24":1}}],["它度量了",{"5":{"24":1}}],["它告诉我们如何计算复合函数的导数",{"5":{"7":2}}],["它告诉我们给定当前参数下网络的输出是什么",{"5":{"25":2}}],["它告诉我们",{"5":{"25":2}}],["它表示损失函数对各层净输入的梯度",{"5":{"25":2}}],["这组神经元构成神经网络的一个层",{"5":{"15":2}}],["这有助于理解其参数效率",{"5":{"9":2}}],["这有助于提高模型的泛化能力",{"5":{"5":2}}],["这涉及到部分和",{"5":{"9":2}}],["这样",{"5":{"9":4}}],["这对训练生成对抗网络",{"5":{"9":2}}],["这对于主动学习和探索",{"5":{"8":2}}],["这对于训练深层网络和循环神经网络尤为重要",{"5":{"7":2}}],["这对于学习位置敏感的token交互模式至关重要",{"5":{"24":4}}],["这在理论上可以防止初始化时的梯度消失或爆炸问题",{"5":{"9":2}}],["这在将展平后的向量恢复为嵌入序列时非常有用",{"5":{"9":2}}],["这在近端策略优化",{"5":{"8":2}}],["这本质上就是计算查询矩阵与转置后的键矩阵的乘积",{"5":{"9":2}}],["这也是大语言模型能够实现高效训练和推理的关键技术基础",{"5":{"9":2}}],["这从参数命名就可以看出",{"5":{"8":2}}],["这使得求解正交矩阵的逆矩阵变得极其简单",{"5":{"9":2}}],["这使得贝叶斯推断在计算上更加方便",{"5":{"8":2}}],["这使得高斯分布成为最易于处理的分布之一",{"5":{"8":2}}],["这使得在保持二阶信息的同时实现了可扩展性",{"5":{"7":2}}],["这使得负输入也会进入饱和区域",{"5":{"22":2}}],["这使得它在以下场景中特别有用",{"5":{"22":2}}],["这使得",{"5":{"22":2}}],["这使得tanh在训练初期通常比sigmoid收敛更快",{"5":{"23":2}}],["这可能是因为神经网络具有大量的对称性",{"5":{"7":2}}],["这可以通过矩阵运算高效地实现",{"5":{"5":2}}],["这被认为有助于模型在早期阶段找到一个更好的参数区域",{"5":{"7":2}}],["这解决了adagrad学习率单调递减的问题",{"5":{"7":2}}],["这些超平面的组合形成了一个复杂的决策边界",{"5":{"15":2}}],["这些神经元按照层次结构组织",{"5":{"15":2}}],["这些变换的复合最终将输入数据映射到所需的输出空间",{"5":{"13":2}}],["这些变体反映了深度学习优化研究的持续进展",{"5":{"7":2}}],["这些运算的吞吐量通常是全精度运算的数倍",{"5":{"9":2}}],["这些运算在大语言模型中有着直接的应用场景",{"5":{"9":2}}],["这些范数在稀疏优化和线性规划中有重要应用",{"5":{"9":2}}],["这些方向称为主成分",{"5":{"9":2}}],["这些矩阵乘法的计算效率直接影响模型的整体性能",{"5":{"9":2}}],["这些定理为机器学习中许多方法的合理性提供了理论依据",{"5":{"8":2}}],["这些性质为mle在大样本场景下的应用提供了理论保障",{"5":{"8":2}}],["这些高阶矩在统计分析中有一定应用",{"5":{"8":2}}],["这些统计量在机器学习的理论和实践中都有广泛应用",{"5":{"8":2}}],["这些最小值通常具有更好的泛化能力",{"5":{"7":2}}],["这些调度策略的目标是在训练初期快速收敛",{"5":{"7":2}}],["这些点可能是极小值",{"5":{"7":2}}],["这些激活函数各有特点",{"5":{"5":2}}],["这些梯度可以通过链式法则推导",{"5":{"22":2}}],["这些嵌入经过层归一化处理后",{"5":{"22":2}}],["这些机制可以从数学角度进行详细分析",{"5":{"22":2}}],["这些约束意味着归一化后的表示位于一个特定的流形上",{"5":{"22":2}}],["这些原则不仅解释了现有激活函数的设计逻辑",{"5":{"22":2}}],["这些数学分析为理解大语言模型的训练过程提供了必要的理论基础",{"5":{"23":2}}],["这些分析为理解大语言模型训练的动力学过程提供了坚实的数学基础",{"5":{"22":2}}],["这些分析为后续章节",{"5":{"24":2}}],["这意味着计算一个",{"5":{"9":1}}],["这意味着计算一个权重矩阵与向量的乘积需要次标量乘法和加法",{"5":{"9":1}}],["这意味着正交变换是一种保距变换",{"5":{"9":2}}],["这意味着每个维度都不是二元的或稀疏的",{"5":{"9":2}}],["这意味着函数图上任意两点的连线位于函数图的上方",{"5":{"7":2}}],["这意味着如果我们沿着梯度方向移动",{"5":{"7":2}}],["这意味着堆叠多层网络不会带来任何额外的表达能力",{"5":{"5":2}}],["这意味着",{"5":{"23":2}}],["这意味着梯度总是非负的",{"5":{"23":2}}],["这意味着在未饱和区域",{"5":{"22":2}}],["这意味着在附近",{"5":{"23":1}}],["这意味着在",{"5":{"23":1}}],["这意味着sigmoid",{"5":{"24":2}}],["这意味着gelu不会像relu那样产生",{"5":{"24":2}}],["这一局限性的根本原因在于",{"5":{"15":2}}],["这一局限性直到多层神经网络",{"5":{"5":2}}],["这一过程可以与前向传播并行进行",{"5":{"13":2}}],["这一过程称为反向传播",{"5":{"7":2}}],["这一过程实现了损失信息从输出到输入的逆向流动",{"5":{"25":2}}],["这一观点与深度学习中的",{"5":{"13":2}}],["这一结论虽然难以给出严格的数学证明",{"5":{"13":2}}],["这一性质在变分推断和高斯过程回归中非常重要",{"5":{"8":2}}],["这一性质在反向传播算法的推导中至关重要",{"5":{"8":2}}],["这一性质被用于分析梯度分布",{"5":{"8":2}}],["这一性质被称为",{"5":{"23":2}}],["这一性质大大简化了反向传播的实现",{"5":{"25":2}}],["这一小批样本的梯度是整体数据梯度的一个良好近似",{"5":{"8":2}}],["这为损失函数的设计提供了理论基础",{"5":{"5":2}}],["这为使用梯度下降等优化算法提供了数学基础",{"5":{"5":2}}],["这与样条函数",{"5":{"13":2}}],["这与第六章讨论的交叉熵损失有着深刻的联系",{"5":{"5":2}}],["这与概率的边界行为一致",{"5":{"5":2}}],["这与多项式分布的参数空间完全一致",{"5":{"24":2}}],["这与gelu输入的范围需求相匹配",{"5":{"24":2}}],["这与反向模式自动微分的定义完全一致",{"5":{"25":2}}],["这将在第四章详细讨论",{"5":{"5":2}}],["这种连接模式可以用矩阵乘法简洁地表示",{"5":{"15":2}}],["这种表示方法不仅是深度学习框架实现高效计算的理论基础",{"5":{"15":2}}],["这种并行性是深度学习在大规模数据和模型上高效训练的基础",{"5":{"13":2}}],["这种分割不会影响计算的数学正确性",{"5":{"13":2}}],["这种分析对于诊断模型过拟合和理解模型容量都很有价值",{"5":{"9":2}}],["这种分析不仅具有理论意义",{"5":{"24":2}}],["这种转化在推导反向传播公式和优化算法时非常有用",{"5":{"9":2}}],["这种约束有助于提高模型的稳定性和泛化能力",{"5":{"9":2}}],["这种方法的数学基础正是低秩近似理论",{"5":{"9":2}}],["这种方法在模型压缩和少样本学习中有一定的应用价值",{"5":{"7":2}}],["这种密集表示使得模型能够在连续空间中学习平滑的语义关系",{"5":{"9":2}}],["这种几何表示使得我们可以利用空间中的几何关系来理解和操作语义信息",{"5":{"9":2}}],["这种运算在模型的权重初始化",{"5":{"9":2}}],["这种近似在分析神经网络的学习动态",{"5":{"8":2}}],["这种矩阵表示的优势在于它可以利用现代硬件",{"5":{"5":2}}],["这种现象称为梯度饱和",{"5":{"22":2,"23":2}}],["这种",{"5":{"22":4,"24":8}}],["这种极端分布使得反向传播时产生大的梯度",{"5":{"22":2}}],["这种稳定性对于深层网络的成功训练至关重要",{"5":{"22":2}}],["这种归一化具有以下几个重要的数学效应",{"5":{"22":2}}],["这种设计保持了卷积的空间结构",{"5":{"22":2}}],["这种结构影响着优化算法的行为",{"5":{"7":2}}],["这种结构在训练初期可能不稳定",{"5":{"22":2}}],["这种结构将层归一化置于残差分支内部",{"5":{"22":1}}],["这种结构使得梯度计算可以高效地逐元素进行",{"5":{"24":2}}],["这种结构与许多自然数据",{"5":{"24":2}}],["这种规范化意味着激活值的尺度不会随网络深度增加而累积变化",{"5":{"22":2}}],["这种渐近行为表明sigmoid函数在正无穷处趋向于饱和值1",{"5":{"23":2}}],["这种数学上的简洁性不仅在理论上令人赞叹",{"5":{"23":2}}],["这种积分",{"5":{"23":2}}],["这种递推关系揭示了sigmoid非线性变换的深层数学结构",{"5":{"23":2}}],["这种饱和效应会逐层累积放大",{"5":{"23":2}}],["这种单向梯度可能导致优化过程的",{"5":{"23":2}}],["这种相似性反映了两个函数在数学结构上的内在联系",{"5":{"23":2}}],["这种快速饱和特性仍然是tanh的主要局限性",{"5":{"23":2}}],["这种变换保持了几何结构的许多性质",{"5":{"24":2}}],["这种局限性在处理现实世界的复杂数据时尤为突出",{"5":{"24":2}}],["这种非线性的引入使得神经网络能够学习任意复杂的输入",{"5":{"24":2}}],["这种逐元素应用的方式保持了向量结构",{"5":{"24":2}}],["这种层叠结构通过矩阵运算的复合来表示",{"5":{"15":2}}],["这种层次化的注意力学习是大语言模型能力的重要来源",{"5":{"24":2}}],["这种抽象使得我们可以利用线性代数和逻辑代数的工具来分析神经网络的行为",{"5":{"5":2}}],["这种抽象化过程正是深度学习成功的关键数学机制",{"5":{"24":2}}],["这种概率解释在逻辑回归",{"5":{"5":2}}],["这种概率解释不仅加深了我们对激活函数数学本质的理解",{"5":{"24":2}}],["这种概率解释与注意力机制中的softmax概率分布形成了数学上的呼应",{"5":{"24":2}}],["这种映射关系使得sigmoid函数在概率建模中具有直接的应用价值",{"5":{"24":2}}],["这种技术在训练大语言模型时尤为重要",{"5":{"9":2}}],["这种技术使得离散随机变量的重参数化成为可能",{"5":{"24":2}}],["这种视角对于理解神经网络的输出解释和损失函数设计具有重要意义",{"5":{"5":2}}],["这种视角揭示了激活函数在信息处理中的深层作用",{"5":{"24":2}}],["这种压缩是显著的",{"5":{"9":2}}],["这种压缩本质上是信息的有损编码",{"5":{"24":1}}],["这种压缩本质上是信息的",{"5":{"24":1}}],["这种有损压缩在机器学习中是有益的",{"5":{"24":2}}],["这种选择影响着生成样本的多样性和质量",{"5":{"8":2}}],["这种选择性与生物神经元的行为类似",{"5":{"24":2}}],["这种联系揭示了为什么softmax与交叉熵的组合在优化上具有特殊的性质",{"5":{"24":2}}],["这种定义使得gelu具有概率解释",{"5":{"24":2}}],["这种双重非线性的组合使得transformer能够学习极其复杂的函数映射",{"5":{"24":2}}],["这种编码将位置信息嵌入到固定范围的正弦和余弦函数中",{"5":{"24":2}}],["这个决策边界始终是线性的",{"5":{"15":2}}],["这个求和需要",{"5":{"13":1}}],["这个求和需要次乘法和次加法",{"5":{"13":1}}],["这个问题看似简单",{"5":{"13":2}}],["这个运算被高度优化",{"5":{"9":2}}],["这个近似将原始矩阵分解为k个秩一矩阵的和",{"5":{"9":2}}],["这个近似在局部区域非常准确",{"5":{"7":2}}],["这个向量的每一个维度都编码了某种语义或语法特征",{"5":{"9":2}}],["这个估计越可靠",{"5":{"8":2}}],["这个简洁的梯度形式是交叉熵损失被广泛采用的重要原因之一",{"5":{"8":2}}],["这个公式在计算方差时更加实用",{"5":{"8":2}}],["这个公式清楚地表明",{"5":{"7":2}}],["这个公式表明",{"5":{"7":2,"24":2}}],["这个信息对于确定参数更新的方向和幅度至关重要",{"5":{"7":2}}],["这个优化过程涉及对损失函数求导",{"5":{"7":2}}],["这个特征映射可以表示为复合函数",{"5":{"5":2}}],["这个递推关系的解为",{"5":{"22":2}}],["这个初始化确保了激活值的方差在前向传播中保持稳定",{"5":{"22":2}}],["这个下界确保了即使的梯度很小",{"5":{"22":1}}],["这个下界确保了即使",{"5":{"22":1}}],["这个反函数在逻辑回归中具有重要意义",{"5":{"23":2}}],["这个证明揭示了sigmoid导数自化性质的数学根源",{"5":{"23":2}}],["这个关系表明",{"5":{"23":2}}],["这个结果表明",{"5":{"24":2}}],["这个数学事实揭示了一个深刻的原理",{"5":{"24":2}}],["这个定理虽然使用了简化的阈值激活函数",{"5":{"15":2}}],["这个定理揭示了relu网络的一个重要性质",{"5":{"13":2}}],["这个定理揭示了前向传播的数学本质",{"5":{"13":2}}],["这个定理揭示了单层神经网络的根本局限性",{"5":{"5":2}}],["这个定理揭示了一个深刻的事实",{"5":{"5":2}}],["这个定理揭示了反向传播的本质机制",{"5":{"25":2}}],["这个定理表明",{"5":{"5":2}}],["这个定理的数学意义极其深远",{"5":{"24":2}}],["这个定理是反向传播中权重梯度计算的核心公式",{"5":{"25":2}}],["这个定义包含了线性映射的两个核心性质",{"5":{"24":2}}],["这个例子直观地说明了线性模型的表达能力边界",{"5":{"24":2}}],["这个超平面将输入空间分为两个半空间",{"5":{"24":2}}],["这个恒等式可以从代数上验证",{"5":{"24":2}}],["这个表达式在分类任务的训练中至关重要",{"5":{"24":2}}],["这个分布在重参数化梯度计算中扮演关键角色",{"5":{"24":2}}],["这个不等式给出了梯度范数的精确边界",{"5":{"22":2}}],["这个不等式表明",{"5":{"24":2}}],["这个矩阵的谱结构决定了梯度下降在概率空间中的几何行为",{"5":{"24":2}}],["这个选择背后有深刻的数学考量",{"5":{"24":2}}],["这个过程与sigmoid将logits转换为概率的过程在数学上是相似的",{"5":{"24":2}}],["这个变换包含两个步骤",{"5":{"5":2}}],["这个变换的逆变换为​",{"5":{"24":1}}],["这个变换的逆变换为",{"5":{"24":1}}],["这个变量在反向传播过程中起着桥梁作用",{"5":{"25":2}}],["这个推论揭示了一个重要的简化",{"5":{"25":2}}],["这个算法清晰地展示了反向传播的执行流程",{"5":{"25":2}}],["这是线性代数中的基本定理",{"5":{"13":2}}],["这是所谓的",{"5":{"8":2}}],["这是一个关于",{"5":{"9":1}}],["这是一个关于的多项式方程",{"5":{"9":1}}],["这是一个重要的性质",{"5":{"8":2}}],["这是一个超平面",{"5":{"5":2}}],["这是一个简单的乘法交互",{"5":{"24":2}}],["这是多数实际应用中的常见假设",{"5":{"7":2}}],["这是",{"5":{"6":2}}],["这是深度学习高效训练的关键技术基础",{"5":{"5":2}}],["这是不可能拟合复杂非线性函数的原因",{"5":{"5":2}}],["这是导致深层神经网络训练困难的根本原因之一",{"5":{"22":2}}],["这是曲线的",{"5":{"23":2}}],["这是神经网络强大表达能力的关键来源",{"5":{"5":2}}],["这是神经网络理论中最深刻的结果之一",{"5":{"24":2}}],["这是微积分中的基本定理",{"5":{"13":2}}],["这是微积分基本定理的直接推论",{"5":{"25":2}}],["这是反向传播的最后一步",{"5":{"25":2}}],["这两个定义密切相关但并不等价",{"5":{"22":2}}],["这表明在给定方差的所有连续分布中",{"5":{"8":2}}],["这表明tanh比sigmoid更早进入饱和区域",{"5":{"22":2}}],["这表明拐点处的曲率为零",{"5":{"23":2}}],["这看似不会导致梯度爆炸",{"5":{"22":2}}],["这类似于dropout的正则化效果",{"5":{"22":2}}],["这具有某种",{"5":{"22":2}}],["这体现了sigmoid函数的代数闭合性",{"5":{"23":2}}],["这就是为什么transformer在处理长序列时面临计算挑战",{"5":{"9":2}}],["这就是梯度下降算法的理论基础",{"5":{"7":2}}],["这就是著名的",{"5":{"24":2}}],["这就",{"5":{"23":2}}],["这带来了两个重要的数学优势",{"5":{"23":2}}],["这正是4",{"5":{"15":2}}],["这正是深度学习",{"5":{"15":2}}],["这正是异或函数的输出",{"5":{"15":2}}],["这正是矩阵乘法的定义",{"5":{"13":2,"25":2}}],["这正是复合函数的定义",{"5":{"13":2}}],["这正是",{"5":{"5":1,"24":2}}],["这正是单样本仿射变换的定义",{"5":{"5":2}}],["这正是层网络的等效仿射变换形式",{"5":{"5":1}}],["这正好是概率值的取值范围",{"5":{"24":2}}],["这避免了sigmoid和tanh的梯度饱和问题",{"5":{"24":2}}],["这里我们关注激活函数在神经元模型中的整体角色",{"5":{"5":2}}],["这里",{"5":{"24":2}}],["这需要深入理解任务的数学特性和激活函数的数学性质",{"5":{"22":2}}],["这需要对上述单样本梯度计算进行扩展",{"5":{"25":2}}],["当批大小受限于gpu内存时",{"5":{"9":2}}],["当批量太小时",{"5":{"8":2}}],["当批量大小足够大时",{"5":{"8":4}}],["当单个样本太大以至于无法在gpu内存中容纳时",{"5":{"9":2}}],["当维度匹配时",{"5":{"9":2}}],["当两个形状不同的张量进行逐元素运算时",{"5":{"9":2}}],["当样本量",{"5":{"8":1}}],["当样本量时",{"5":{"8":1}}],["当我们设计一个神经网络架构并训练其参数时",{"5":{"13":2}}],["当我们比较多个模型或在多个测试集上评估时",{"5":{"8":2}}],["当我们计算批次中所有样本的前向传播时",{"5":{"9":2}}],["当我们计算验证集上的性能指标时",{"5":{"8":2}}],["当我们计算损失函数关于某个特定参数的偏导数时",{"5":{"7":2}}],["当独立同分布的随机变量样本量趋向无穷时",{"5":{"8":2}}],["当是真实分布而是模型预测分布时",{"5":{"8":1}}],["当使用自然对数时",{"5":{"8":2}}],["当使用以2为底的对数时",{"5":{"8":2}}],["当dropout率为",{"5":{"8":2}}],["当",{"5":{"5":6,"8":6,"9":1,"13":2,"15":4,"22":15,"23":11,"24":2}}],["当时可归入任一模式",{"5":{"13":1}}],["当时",{"5":{"5":6,"8":3,"9":1,"13":1,"15":4,"22":8,"23":5,"24":2}}],["当激活函数输出接近其渐近边界时",{"5":{"22":2}}],["当较小时",{"5":{"22":1}}],["当较大时",{"5":{"22":1}}],["当接近0或1时",{"5":{"22":1}}],["当接近1时",{"5":{"22":1}}],["当梯度信号被压缩到接近零时",{"5":{"22":2}}],["当的标准差为1时",{"5":{"22":1}}],["当softmax输出接近one",{"5":{"22":2}}],["当这些梯度在拼接处合并时",{"5":{"22":2}}],["当注意力分布趋于均匀时",{"5":{"22":2}}],["当输入信号的累积效应超过某个阈值时",{"5":{"5":2}}],["当输入的绝对值很大时",{"5":{"23":1}}],["当输入",{"5":{"23":1}}],["当且仅当",{"5":{"8":1,"24":1}}],["当且仅当和独立时互信息为0",{"5":{"8":1}}],["当且仅当和独立时互信息为零",{"5":{"24":1}}],["当且仅当对于任意向量和任意标量",{"5":{"24":1}}],["当且仅当对于任意向量",{"5":{"24":1}}],["当且时",{"5":{"23":1}}],["当上一层输出的均值不为零时",{"5":{"23":2}}],["当增大时",{"5":{"23":1}}],["当网络较深时",{"5":{"23":2}}],["当真实的数据生成过程超出这个空间时",{"5":{"24":2}}],["当多个这样的曲面进行线性组合时",{"5":{"24":2}}],["当预测分布",{"5":{"24":2}}],["只是relu网络的",{"5":{"13":2}}],["只是需要明确哪个变量被当作中间变量",{"5":{"7":2}}],["只在关键步骤使用全精度",{"5":{"9":2}}],["只适用于对称正定矩阵",{"5":{"9":2}}],["只需要按顺序执行各层的矩阵运算即可",{"5":{"13":2}}],["只需要训练",{"5":{"9":1}}],["只需要训练个参数而不是个参数",{"5":{"9":1}}],["只需要知道当前的值即可",{"5":{"23":1}}],["只需要知道当前的",{"5":{"23":1}}],["只需进行转置操作",{"5":{"9":2}}],["只改变方向",{"5":{"9":2}}],["只要样本量足够大",{"5":{"8":2}}],["只要至少有一个输入为1",{"5":{"5":2}}],["只要不使用激活函数",{"5":{"24":2}}],["只要模型结构固定",{"5":{"24":2}}],["只要激活函数满足一定的数学条件",{"5":{"24":2}}],["只要激活函数是",{"5":{"24":2}}],["只有可对角化矩阵",{"5":{"9":2}}],["只有满秩",{"5":{"9":2}}],["只有方阵才可能存在逆矩阵",{"5":{"9":2}}],["只有当",{"5":{"8":1}}],["只有当和不相关时等号才成立",{"5":{"8":1}}],["只有两个输入均为0时输出为0",{"5":{"5":2}}],["只有后层能够学习",{"5":{"22":2}}],["只有超过阈值的信号才能通过",{"5":{"24":2}}],["只能捕获输入的线性可分特征",{"5":{"13":2}}],["只能处理线性可分问题",{"5":{"5":2}}],["只能解决线性可分",{"5":{"5":2}}],["只能为正或零",{"5":{"23":1}}],["只能学习直线或超平面作为决策边界",{"5":{"24":2}}],["只缩放其大小",{"5":{"22":2}}],["只保留rms归一化",{"5":{"22":2}}],["阈值",{"5":{"5":6}}],["阈值化",{"5":{"24":2}}],["​定义为",{"5":{"15":1}}],["​表示第",{"5":{"15":1}}],["​zj",{"5":{"15":1}}],["​后合并结果",{"5":{"13":1}}],["​可以分配给不同的设备",{"5":{"13":1}}],["​个输出元素的总flops为",{"5":{"13":1}}],["​个输出元素的总flops为​",{"5":{"13":1}}],["​次flops",{"5":{"13":1}}],["​次加法",{"5":{"13":1}}],["​空间中的表示",{"5":{"13":1}}],["​为网络输出",{"5":{"15":2}}],["​为后向传播的中间值",{"5":{"13":1}}],["​为",{"5":{"13":1}}],["​为偏置向量",{"5":{"13":2,"15":2}}],["​为输出",{"5":{"25":2}}],["​为第",{"5":{"25":1}}],["​来自偏置加法",{"5":{"13":2}}],["​附近",{"5":{"7":1}}],["​​",{"5":{"7":1,"9":2,"22":1}}],["​",{"5":{"5":6,"7":8,"8":12,"9":3,"13":11,"15":3,"22":20,"23":11,"24":3,"25":15}}],["​这种结构将层归一化置于残差分支内部",{"5":{"22":1}}],["​f",{"5":{"23":1}}],["​j",{"5":{"23":1}}],["​和",{"5":{"25":1}}],["​是各层映射的复合",{"5":{"13":1}}],["​是对角矩阵",{"5":{"25":1}}],["​是概率分布",{"5":{"25":1}}],["我们主要关注全连接层",{"5":{"15":2}}],["我们建立了单个神经元的数学模型",{"5":{"15":2}}],["我们建立了仿射变换的数学框架",{"5":{"5":2}}],["我们计算每个节点对其直接后继的梯度",{"5":{"13":2}}],["我们计算两个模型的性能差异",{"5":{"8":2}}],["我们伴随值",{"5":{"13":2}}],["我们将在第2",{"5":{"13":2}}],["我们将在下一节讨论如何将多个神经元组织成层",{"5":{"5":2}}],["我们将看到如何利用链式法则推导出反向传播的梯度计算公式",{"5":{"13":2}}],["我们将建立对梯度动力学的系统性理解",{"5":{"22":2}}],["我们将详细分析损失函数的优化性质",{"5":{"24":2}}],["我们将从链式法则的矩阵形式出发",{"5":{"25":2}}],["我们将讨论反向传播与自动微分的关系",{"5":{"25":2}}],["我们将复杂的复合函数求导问题分解为误差信号的前向传播和参数梯度的局部计算两个步骤",{"5":{"25":2}}],["我们会发现它蕴含着丰富的理论内涵",{"5":{"13":2}}],["我们会以最快的速度离开当前的等值面",{"5":{"7":2}}],["我们实际上是在寻找一个能够拟合目标函数的数学表达",{"5":{"13":2}}],["我们实际上是在问",{"5":{"7":2}}],["我们已经从计算单元和结构组织的角度介绍了神经网络的数学描述",{"5":{"13":2}}],["我们先计算若干个小批次的梯度",{"5":{"9":2}}],["我们关心如何用少量的奇异值最好地近似原始矩阵",{"5":{"9":2}}],["我们在优化目标中加入正交性惩罚项",{"5":{"9":2}}],["我们可以构建任意复杂的非线性函数",{"5":{"15":2}}],["我们可以构建置信区间来量化估计的不确定性",{"5":{"8":2}}],["我们可以通过梯度累积来模拟更大的有效批大小",{"5":{"9":2}}],["我们可以了解模型使用了多少",{"5":{"9":2}}],["我们可以实现降维同时最小化信息损失",{"5":{"9":2}}],["我们可以总结出激活函数设计应遵循的数学原则",{"5":{"22":2}}],["我们可以将导数表示为",{"5":{"23":2}}],["我们可以清楚地看到损失信息如何从输出层反向流动到输入层",{"5":{"25":2}}],["我们可以计算损失函数对各层参数的梯度",{"5":{"25":2}}],["我们使用迭代算法如幂迭代",{"5":{"9":2}}],["我们使用雅可比矩阵",{"5":{"7":2}}],["我们很少直接计算矩阵的逆",{"5":{"9":2}}],["我们根据样本数据计算检验统计量",{"5":{"8":2}}],["我们同时处理这两类随机变量",{"5":{"8":2}}],["我们需要组合多个神经元",{"5":{"15":2}}],["我们需要首先明确几个基本定义",{"5":{"9":2}}],["我们需要首先理解生物神经元的基本结构及其数学抽象过程",{"5":{"5":2}}],["我们需要平衡验证集大小和计算成本",{"5":{"8":2}}],["我们需要找到一组模型参数",{"5":{"7":2}}],["我们需要从线性映射的局限性出发",{"5":{"24":2}}],["我们需要将链式法则从标量情形推广到向量和矩阵情形",{"5":{"25":2}}],["我们终将知道",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["我们必须知道",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["我们通常保持权重矩阵和偏置向量的分离形式",{"5":{"15":2}}],["我们通常需要同时计算多个神经元的输出",{"5":{"5":2}}],["我们通常使用随机梯度下降或其变体",{"5":{"8":2}}],["我们通常使用较窄但较深的网络来达到相同的表达能力",{"5":{"24":2}}],["我们通常使用小批量随机梯度下降",{"5":{"25":2}}],["我们通过构造具体的权重和阈值来说明",{"5":{"5":2}}],["我们引入齐次坐标",{"5":{"5":2}}],["我们首先需要建立饱和现象的严格数学定义",{"5":{"22":2}}],["我们首先详细推导了sigmoid和tanh函数的饱和区域和饱和深度",{"5":{"22":2}}],["我们进一步讨论了梯度流与信息保持的关系",{"5":{"22":2}}],["我们总结了激活函数设计的数学原则",{"5":{"22":2}}],["我们不需要存储或重新计算导数",{"5":{"23":2}}],["我们得到形状为",{"5":{"9":1}}],["我们得到形状为的张量",{"5":{"9":1}}],["我们得到了一个可以近似离散采样的连续分布",{"5":{"24":2}}],["我们希望用简单的近似分布",{"5":{"8":1}}],["我们希望用简单的近似分布来逼近复杂的后验分布",{"5":{"8":1}}],["我们希望最大化",{"5":{"24":2}}],["我们揭示了非线性激活函数打破表达瓶颈的数学必然性",{"5":{"24":2}}],["我们论证了激活函数作为通用逼近器核心组件的数学基础",{"5":{"24":2}}],["我们分析了梯度爆炸的数学机制",{"5":{"22":2}}],["我们分析了sigmoid",{"5":{"24":2}}],["我们分析了激活函数与transformer架构",{"5":{"24":2}}],["我们探讨了激活函数的信息压缩",{"5":{"24":2}}],["我们讨论了前向传播的数学本质",{"5":{"25":2}}],["我们定义一个关键中间变量",{"5":{"25":2}}],["我们的目标是计算和对所有层",{"5":{"25":1}}],["我们的目标是计算",{"5":{"25":1}}],["n",{"5":{"15":36}}],["nuclear",{"5":{"9":2}}],["null",{"5":{"9":2}}],["numpy",{"5":{"9":2}}],["numbers",{"5":{"8":2}}],["node",{"5":{"13":2}}],["norm",{"5":{"9":6}}],["normalization",{"5":{"8":2,"9":4,"22":8}}],["normal",{"5":{"8":2}}],["nofollow",{"5":{"6":4,"14":6,"21":10,"27":10}}],["noopener",{"5":{"6":4,"14":6,"21":10,"27":10}}],["not组合表示",{"5":{"5":2}}],["not",{"5":{"5":2}}],["nesterov动量是动量方法的一个变体",{"5":{"7":2}}],["net",{"5":{"5":2}}],["network",{"5":{"5":2,"9":2,"15":2,"24":2}}],["nervous",{"5":{"5":2}}],["neuron",{"5":{"5":4}}],["neural",{"5":{"5":2,"15":2}}],["neq",{"5":{"23":8}}],["nat",{"5":{"8":2}}],["nas",{"5":{"22":2}}],["nas被用于自动发现新的激活函数",{"5":{"22":2}}],["定义为所有元素平方和的平方根",{"5":{"9":2}}],["定义为​",{"5":{"8":1,"24":1}}],["定义为",{"5":{"8":3,"13":3,"23":6,"24":5}}],["定义为使后验概率等于0",{"5":{"5":2}}],["定义2",{"5":{"5":20,"13":18,"15":8,"25":13}}],["定义3",{"5":{"22":28,"23":8,"24":20}}],["定义饱和深度函数来量化输入值距离饱和区域的远近",{"5":{"22":2}}],["定义激活函数的饱和度量来量化其饱和程度",{"5":{"22":2}}],["定义激活函数的梯度稳定性指标",{"5":{"22":2}}],["定义了流形上的几何结构",{"5":{"24":2}}],["定理2",{"5":{"5":14,"13":26,"15":8,"25":24}}],["定理3",{"5":{"22":12,"23":8,"24":4}}],["定理的证明思路基于stone",{"5":{"24":2}}],["由多个全连接层堆叠而成",{"5":{"15":2}}],["由",{"5":{"13":2}}],["由激活模式定义",{"5":{"13":2}}],["由描述",{"5":{"13":2}}],["由导数的定义",{"5":{"13":2}}],["由jensen不等式保证",{"5":{"8":2}}],["由四阶中心矩归一化得到",{"5":{"8":2}}],["由三阶中心矩归一化得到",{"5":{"8":2}}],["由于",{"5":{"13":1,"24":1,"25":1}}],["由于本身是线性的",{"5":{"13":1}}],["由于训练数据通常被组织为序列",{"5":{"8":2}}],["由于参数数量巨大",{"5":{"7":2}}],["由于任意布尔函数都可以由and",{"5":{"5":2}}],["由于约一半的神经元输出为零",{"5":{"22":2}}],["由于归一化使用mini",{"5":{"22":2}}],["由于激活值的分布更加稳定",{"5":{"22":2}}],["由于神经网络是一个深度复合函数",{"5":{"25":2}}],["由于是非线性函数",{"5":{"24":1}}],["由于是逐元素函数",{"5":{"25":1}}],["由神经元的数量决定",{"5":{"5":1}}],["由超平面",{"5":{"5":1}}],["由超平面所界定",{"5":{"5":1}}],["由细胞体",{"5":{"5":2}}],["由softmax给出时",{"5":{"24":2}}],["由矩阵描述",{"5":{"24":1}}],["由矩阵",{"5":{"24":1}}],["由向量描述",{"5":{"24":1}}],["由向量",{"5":{"24":1}}],["由链式法则",{"5":{"25":2}}],["由链式法则和雅可比矩阵的性质",{"5":{"25":2}}],["却抓住了生物神经元信息处理的核心特征",{"5":{"5":2}}],["为网络输入",{"5":{"15":1}}],["为该层的输出向量",{"5":{"15":2}}],["为逐元素应用的激活函数",{"5":{"15":2}}],["为偏置向量",{"5":{"15":2}}],["为偏置项",{"5":{"5":2}}],["为第",{"5":{"13":1,"15":1,"22":1,"25":1}}],["为第层的激活函数",{"5":{"15":1}}],["为第层的输入",{"5":{"22":1}}],["为第层激活函数的导数",{"5":{"25":1}}],["为两个映射",{"5":{"13":1}}],["为权重矩阵",{"5":{"13":1,"15":2}}],["为权重向量",{"5":{"5":2}}],["为理解反向传播提供必要的背景知识",{"5":{"13":2}}],["为理解和优化大语言模型提供了核心的数学工具",{"5":{"7":2}}],["为理解大语言模型中非线性变换的数学本质奠定坚实基础",{"5":{"24":2}}],["为我们进一步学习反向传播算法",{"5":{"13":2}}],["为大语言模型提供了处理不确定性的理论基础",{"5":{"8":2}}],["为了突破这一限制",{"5":{"15":2}}],["为了校正偏差",{"5":{"7":2}}],["为了高效地实现这些计算",{"5":{"7":2}}],["为了将仿射变换统一表示为线性变换的形式",{"5":{"5":2}}],["为了保持方差稳定",{"5":{"22":2}}],["为了深刻理解这一数学本质",{"5":{"24":2}}],["为行向量",{"5":{"5":1}}],["为类别标签",{"5":{"5":1}}],["为",{"5":{"5":2,"8":2}}],["为变换矩阵",{"5":{"5":1}}],["为激活函数",{"5":{"5":1,"13":2,"15":2}}],["为激活阈值",{"5":{"5":2}}],["为输出数",{"5":{"13":1,"25":1}}],["为输出向量",{"5":{"5":2}}],["为输入数",{"5":{"13":1,"25":1}}],["为输入向量",{"5":{"5":1,"25":2}}],["为输入信号",{"5":{"5":1}}],["为输入",{"5":{"25":1}}],["为误差函数",{"5":{"5":2}}],["为平移向量",{"5":{"5":2}}],["为神经网络的现代发展奠定了基础",{"5":{"5":2}}],["为神经元输出",{"5":{"5":2}}],["为阶跃激活函数",{"5":{"5":2}}],["为连接权重",{"5":{"5":2}}],["为设计和改进大语言模型的训练策略提供理论指导",{"5":{"22":2}}],["为深层网络的稳定训练提供了数学保障",{"5":{"22":2}}],["为负区域提供有限的梯度传递",{"5":{"22":2}}],["为未来研究和实践提供了理论指导",{"5":{"22":2}}],["为饱和阈值",{"5":{"22":1}}],["为损失函数",{"5":{"22":1}}],["为正",{"5":{"23":1}}],["为序列长度",{"5":{"13":2}}],["为序列中的每个位置添加位置信息",{"5":{"24":2}}],["为序列中的每个位置分配数学表示",{"5":{"24":2}}],["为基准",{"5":{"24":1}}],["为什么需要反向传播",{"2":{"25":1},"5":{"25":1}}],["为克罗内克函数",{"5":{"25":2}}],["为标准正态分布的累积分布函数",{"5":{"5":1}}],["为标量损失函数",{"5":{"25":1}}],["为矩阵变量",{"5":{"25":1}}],["其表达能力远超单个超平面",{"5":{"15":2}}],["其表达能力等价于单层线性变换",{"5":{"5":2}}],["其表达能力不再受限于线性变换的复合",{"5":{"24":2}}],["其所有依赖值已经就绪",{"5":{"13":2}}],["其后继节点之前被处理",{"5":{"13":2}}],["其计算图包含以下节点",{"5":{"13":2}}],["其雅可比矩阵",{"5":{"13":2}}],["其雅可比矩阵为",{"5":{"25":2}}],["其列向量",{"5":{"9":2}}],["其协方差矩阵",{"5":{"8":1}}],["其协方差矩阵是一个的对称半正定矩阵",{"5":{"8":1}}],["其他为0",{"5":{"8":2}}],["其他情况输出为0",{"5":{"5":2}}],["其pdf为",{"5":{"8":2}}],["其pmf记为",{"5":{"8":2}}],["其期望定义为",{"5":{"8":2}}],["其元素表示生成词汇表中每个词的概率",{"5":{"8":2}}],["其上方的图构成一个凸集",{"5":{"7":2}}],["其次",{"5":{"7":4,"8":2,"9":10,"22":6,"23":2,"24":2}}],["其长度",{"5":{"7":2}}],["其数学形式为",{"5":{"5":2}}],["其数学定义为",{"5":{"5":2}}],["其数学性质更容易分析",{"5":{"22":2}}],["其数学表示可以清晰地揭示线性变换的本质局限性",{"5":{"24":2}}],["其权重矩阵为",{"5":{"5":2}}],["其损失函数为交叉熵损失",{"5":{"5":2}}],["其真值表为",{"5":{"5":2}}],["其中行索引",{"5":{"15":1}}],["其中行索引对应输出神经元的索引",{"5":{"15":1}}],["其中的计算结果与完全相同",{"5":{"15":1}}],["其中的列向量是的特征向量",{"5":{"13":1}}],["其中来自矩阵乘法",{"5":{"13":1}}],["其中矩阵乘法的顺序不可交换",{"5":{"13":2,"25":2}}],["其中和是正交矩阵",{"5":{"13":1}}],["其中和是激活函数",{"5":{"7":1}}],["其中和均为可微函数",{"5":{"13":1}}],["其中和分别是的最小和最大奇异值",{"5":{"22":1}}],["其中和分别是输入和输出的维度",{"5":{"22":1}}],["其中查询向量与键向量在特征维度上进行点积运算",{"5":{"9":2}}],["其中第",{"5":{"8":1}}],["其中第个元素为",{"5":{"8":1}}],["其中条件均值为",{"5":{"8":2}}],["其中且",{"5":{"8":1}}],["其中节点表示变量或操作",{"5":{"7":2}}],["其中​",{"5":{"7":1,"8":1,"25":1}}],["其中​是概率分布",{"5":{"25":1}}],["其中是由网络参数定义的函数映射",{"5":{"13":1}}],["其中是序列长度",{"5":{"9":1}}],["其中是批大小",{"5":{"9":1}}],["其中是批均值",{"5":{"22":1}}],["其中是批量大小",{"5":{"9":1}}],["其中是批量预测矩阵",{"5":{"25":1}}],["其中是批量隐藏激活矩阵",{"5":{"25":1}}],["其中是注意力头的数量",{"5":{"9":1}}],["其中是单位矩阵",{"5":{"9":1}}],["其中是标准正态分布的cdf",{"5":{"8":1}}],["其中是似然函数",{"5":{"8":1}}],["其中是未知参数",{"5":{"8":1}}],["其中是均值向量",{"5":{"8":1}}],["其中是均值参数",{"5":{"8":1}}],["其中是均值",{"5":{"22":1}}],["其中是率参数",{"5":{"8":1}}],["其中是成功概率",{"5":{"8":1}}],["其中是衰减系数",{"5":{"7":1}}],["其中是逐元素累积的梯度平方",{"5":{"7":1}}],["其中是速度向量",{"5":{"7":1}}],["其中是第步的参数",{"5":{"7":1}}],["其中是残差函数的雅可比矩阵",{"5":{"7":1}}],["其中是梯度与方向之间的夹角",{"5":{"7":1}}],["其中是外层函数关于中间变量的导数",{"5":{"7":1}}],["其中是全1向量",{"5":{"5":2}}],["其中是仿射变换算子",{"5":{"5":1}}],["其中是裁剪阈值",{"5":{"22":1}}],["其中是的第列",{"5":{"5":1}}],["其中是的最大奇异值",{"5":{"22":1}}],["其中是的均方根",{"5":{"22":1}}],["其中是输入神经元的数量",{"5":{"8":1}}],["其中是输入分布的支持域",{"5":{"22":1}}],["其中是输入层到隐藏层的权重矩阵",{"5":{"24":1}}],["其中是输入权重向量",{"5":{"24":1}}],["其中是双曲正割函数",{"5":{"23":1}}],["其中是两个权重矩阵的乘积",{"5":{"24":1}}],["其中是变换矩阵",{"5":{"24":1}}],["其中是非线性激活函数",{"5":{"24":1}}],["其中是gumbel噪声",{"5":{"24":1}}],["其中是随机噪声源",{"5":{"24":1}}],["其中是负样本的数量",{"5":{"24":1}}],["其中是激活函数",{"5":{"24":1}}],["其中是网络的原始输出",{"5":{"24":1}}],["其中是网络预测",{"5":{"25":1}}],["其中是行向量还是列向量需要注意维度匹配",{"5":{"25":1}}],["其中是缩放因子",{"5":{"25":1}}],["其中",{"5":{"5":13,"7":9,"8":15,"9":15,"13":8,"15":6,"22":11,"23":1,"24":10,"25":12}}],["其中为网络输入",{"5":{"15":1}}],["其中为权重矩阵",{"5":{"13":1,"15":2}}],["其中为标准正态分布的累积分布函数",{"5":{"5":1}}],["其中为的摩尔",{"5":{"5":1}}],["其中为变换矩阵",{"5":{"5":1}}],["其中为激活函数",{"5":{"5":1}}],["其中为输入信号",{"5":{"5":1}}],["其中为输入",{"5":{"25":1}}],["其中为矩阵变量",{"5":{"25":1}}],["其中包含所有奇异值",{"5":{"22":1}}],["其中每个层映射",{"5":{"15":1}}],["其中每个层映射​定义为",{"5":{"15":1}}],["其中每个词被映射为空间中的一个点",{"5":{"9":2}}],["其中每个元素",{"5":{"9":2}}],["其中每个",{"5":{"7":2}}],["其中每一层的变换和激活将前一层的表示转换为新的表示",{"5":{"24":1}}],["其中每一层的变换",{"5":{"24":1}}],["其中假设",{"5":{"25":2}}],["其中表示完全正相关",{"5":{"8":1}}],["其中表示逐元素乘法",{"5":{"25":1}}],["其核心创新在于引入了参数学习机制",{"5":{"5":2}}],["其连接强度决定了信号传递的效率",{"5":{"5":2}}],["其",{"5":{"22":2}}],["其谱半径为1",{"5":{"22":2}}],["其特征值分布在单位圆附近",{"5":{"22":2}}],["其复杂性源于归一化操作的非线性",{"5":{"22":2}}],["其导数通常也接近零",{"5":{"22":2}}],["其导数在大多数区域都接近1",{"5":{"22":2}}],["其导数的",{"5":{"23":2}}],["其解为​",{"5":{"22":1}}],["其解为",{"5":{"22":1}}],["其定义域为",{"5":{"23":4}}],["其图像呈标准的s形曲线",{"5":{"23":2}}],["其激活值对输入的微小变化不敏感",{"5":{"23":2}}],["其根本原因在于激活函数引入的非线性变换",{"5":{"24":2}}],["其维度分别代表批量大小",{"5":{"9":2}}],["其维度为",{"5":{"24":2}}],["其能表示的函数就被限制在这个有限维的函数空间中",{"5":{"24":2}}],["其参数集合为",{"5":{"13":2}}],["其参数数量为",{"5":{"24":2}}],["其注意力层和前馈层堆叠形成深层结构",{"5":{"24":2}}],["其梯度为行向量",{"5":{"25":2}}],["其对角元素为非负实数",{"5":{"13":2}}],["其对角元素为",{"5":{"25":2}}],["其基本思想是用有限差分近似计算梯度",{"5":{"25":2}}],["其第行为样本的误差信号​",{"5":{"25":1}}],["其第",{"5":{"25":1}}],["突触是神经元之间传递信息的连接点",{"5":{"5":2}}],["细胞体对这些输入信号进行整合处理",{"5":{"5":2}}],["sum",{"5":{"9":2,"15":22,"23":38}}],["svd",{"5":{"9":2}}],["spline",{"5":{"13":2}}],["spectral",{"5":{"9":8}}],["space",{"5":{"9":4}}],["system",{"5":{"9":2}}],["synapse",{"5":{"5":2}}],["sne和umap",{"5":{"9":2}}],["s",{"5":{"8":2}}],["single",{"5":{"15":2}}],["singular",{"5":{"9":2}}],["size",{"5":{"8":2,"9":2,"22":2}}],["size可能为1",{"5":{"22":2}}],["sigma",{"5":{"15":4,"23":14}}],["sigmoid",{"5":{"7":2,"22":1}}],["sigmoid和softmax激活函数的输出可以解释为分类概率",{"5":{"5":2}}],["sigmoid神经元",{"5":{"5":1}}],["sigmoid神经元的输出可以解释为给定输入时",{"5":{"5":1}}],["sigmoid神经元与伯努利分布的对应",{"5":{"5":2}}],["sigmoid函数恰好可以将神经元的净输入映射为概率值",{"5":{"5":2}}],["sigmoid函数",{"5":{"5":2,"22":1,"23":5,"24":1}}],["sigmoid函数的输出范围为",{"5":{"5":2}}],["sigmoid函数的饱和区域分析",{"2":{"22":1},"5":{"22":1}}],["sigmoid函数的梯度饱和区域定义为",{"5":{"22":2}}],["sigmoid函数的数学定义",{"2":{"23":1},"5":{"23":1}}],["sigmoid函数的反函数为对数几率函数",{"5":{"23":2}}],["sigmoid函数的导数与性质",{"2":{"23":1},"5":{"23":1}}],["sigmoid函数的导数具有一个极为优美的性质",{"5":{"23":2}}],["sigmoid函数的导数为",{"5":{"23":1}}],["sigmoid函数的导数取值范围是",{"5":{"23":2}}],["sigmoid函数的分母是分子的积分",{"5":{"23":2}}],["sigmoid函数的高阶导数可以递归地通过低阶导数表示",{"5":{"23":2}}],["sigmoid函数的二阶导数为",{"5":{"23":2}}],["sigmoid函数的逆函数正是对数几率的变换",{"5":{"24":2}}],["sigmoid函数是分析梯度饱和的经典案例",{"5":{"22":1}}],["sigmoid函数是神经网络中最经典的激活函数之一",{"5":{"23":2}}],["sigmoid函数是单调递增函数",{"5":{"23":2}}],["sigmoid函数与伯努利分布",{"2":{"24":1},"5":{"24":1}}],["sigmoid函数将整个实数轴压缩到区间",{"5":{"23":1}}],["sigmoid函数将整个实数轴压缩到",{"5":{"23":1}}],["sigmoid函数将实数域映射到区间",{"5":{"24":1}}],["sigmoid函数将线性组合转换为概率",{"5":{"24":1}}],["sigmoid函数将线性组合",{"5":{"24":1}}],["sigmoid函数可以拟合任意单调递增的概率函数",{"5":{"5":2}}],["sigmoid函数可以将任意实数解释为伯努利分布的成功概率",{"5":{"24":1}}],["sigmoid函数可以将任意实数",{"5":{"24":1}}],["sigmoid曲线在其两端进入平坦区域",{"5":{"22":2}}],["sigmoid的饱和区域",{"5":{"22":2}}],["sigmoid的二阶导数",{"5":{"23":2}}],["sigmoid以0",{"5":{"23":2}}],["sigmoid导数的自化形式",{"5":{"23":2}}],["sigmoid导数的这种",{"5":{"23":2}}],["sigmoid导数",{"5":{"23":2}}],["sigmoid输出区间",{"5":{"23":1}}],["sigmoid输出",{"5":{"23":1}}],["sigmoid输出常用于二分类任务的概率预测",{"5":{"24":2}}],["sigmoid变换可以理解为将先验信息",{"5":{"24":2}}],["sigmoid激活",{"5":{"25":2}}],["signal",{"5":{"25":2}}],["smoothing",{"5":{"8":2}}],["skewness",{"5":{"8":2}}],["standard",{"5":{"8":2}}],["step",{"5":{"7":2}}],["stone",{"5":{"24":2}}],["stochastic",{"5":{"25":2}}],["sgd倾向于收敛到平坦的局部最小值",{"5":{"7":2}}],["sgd的随机性本身就提供了一种隐式的噪声源",{"5":{"7":2}}],["softmax",{"5":{"7":2,"23":4}}],["softmax和交叉熵的组合在数学上等价于最大似然估计",{"5":{"5":2}}],["softmax函数将模型的原始输出",{"5":{"8":2}}],["softmax函数将",{"5":{"5":1}}],["softmax函数将个神经元的输出归一化为概率分布",{"5":{"5":1}}],["softmax函数具有以下重要性质",{"5":{"5":2}}],["softmax函数是大语言模型",{"5":{"5":2}}],["softmax函数是sigmoid在多类别情况下的推广",{"5":{"24":2}}],["softmax函数与多项式分布",{"2":{"24":1},"5":{"24":1}}],["softmax函数与概率单纯形",{"5":{"24":2}}],["softmax函数定义为",{"5":{"24":2}}],["softmax神经元与多项分布",{"5":{"5":2}}],["softmax的数学性质",{"5":{"5":2}}],["softmax的数学性质由温度参数控制",{"5":{"24":1}}],["softmax的数学性质由温度参数",{"5":{"24":1}}],["softmax的极端梯度",{"5":{"22":2}}],["softmax的非线性确保了注意力权重具有归一化的概率解释",{"5":{"24":2}}],["softmax的输出满足概率分布的所有公理",{"5":{"24":1}}],["softmax的输出",{"5":{"24":1}}],["softmax及其梯度特性与交叉熵损失的配合至关重要",{"5":{"22":2}}],["softmax与离散分布采样",{"2":{"24":1},"5":{"24":1}}],["softmax变换正是这种对数几率的指数化和归一化",{"5":{"24":2}}],["softmax输出为",{"5":{"8":1}}],["softmax输出为​",{"5":{"8":1}}],["softmax输出的概率表示样本属于第类的概率估计",{"5":{"24":1}}],["softmax输出的概率",{"5":{"24":1}}],["softmax分布",{"5":{"24":2}}],["softmax样本定义为",{"5":{"24":2}}],["softmax趋向于one",{"5":{"24":2}}],["softmax趋向于均匀分布",{"5":{"24":2}}],["softmax是gumbel分布和softmax分布的结合",{"5":{"24":2}}],["softmax将每一行归一化为概率分布",{"5":{"24":2}}],["softmax提供了一种可微的采样近似",{"5":{"24":2}}],["softmax提供token间的交互非线性",{"5":{"24":2}}],["softmax等激活函数与概率分布的内在联系",{"5":{"24":2}}],["softmax激活",{"5":{"25":2}}],["soma",{"5":{"5":2}}],["sequence",{"5":{"9":2}}],["self",{"5":{"5":2,"6":4,"7":3,"8":3,"9":2,"13":2,"14":6,"21":10,"23":2,"25":1,"27":10}}],["separable",{"5":{"5":2}}],["src=",{"5":{"5":2,"7":3,"8":3,"9":2,"13":2,"25":1}}],["s形",{"5":{"22":2,"24":2}}],["shift",{"5":{"22":2}}],["sqrt",{"5":{"15":4,"23":4}}],["square",{"5":{"22":2}}],["swish函数通过可学习的参数提供了自适应的激活特性",{"5":{"22":1}}],["swish函数",{"5":{"22":1}}],["schmidt正交化",{"5":{"9":2}}],["schmidt正交化可以移除表示中的冗余成分",{"5":{"9":2}}],["schmidt正交化过程是构建正交基的标准算法",{"5":{"9":2}}],["scheduling",{"5":{"7":2}}],["scaled",{"5":{"24":2}}],["scaling",{"5":{"25":2}}],["和反向模式",{"5":{"13":2}}],["和平移变换",{"5":{"13":2}}],["和注意力维度",{"5":{"9":1}}],["和上三角矩阵",{"5":{"9":2}}],["和标量",{"5":{"9":1}}],["和分别为第层的权重矩阵和偏置向量",{"5":{"15":1}}],["和分层张量分解",{"5":{"9":2}}],["和分配律",{"5":{"9":2}}],["和连接节点的边",{"5":{"9":2}}],["和低秩矩阵恢复问题",{"5":{"9":2}}],["和对角矩阵",{"5":{"9":2}}],["和防止对抗攻击都很重要",{"5":{"9":2}}],["和transformer中",{"5":{"9":2}}],["和qr算法来计算特征值和特征向量",{"5":{"9":2}}],["和张量收缩",{"5":{"9":2}}],["和矩阵",{"5":{"9":2}}],["和随机变量",{"5":{"8":1}}],["和归一性",{"5":{"8":1}}],["和归一化操作",{"5":{"7":2}}],["和相关系数",{"5":{"8":2}}],["和备择假设",{"5":{"8":2}}],["和渐近有效的",{"5":{"8":2}}],["和交叉熵",{"5":{"8":2}}],["和向量",{"5":{"8":2}}],["和中位数都等于均值",{"5":{"8":2}}],["和线性无关",{"5":{"8":1}}],["和倾向于反向变化",{"5":{"8":1}}],["和倾向于同向变化",{"5":{"8":1}}],["和任意",{"5":{"7":1,"24":1}}],["和任意标量",{"5":{"24":1}}],["和符号微分",{"5":{"7":2,"13":2}}],["和的偏导数等于偏导数的和",{"5":{"7":2}}],["和",{"5":{"5":2,"7":6,"8":10,"9":12,"13":4,"15":1,"22":7,"23":1,"24":9,"25":2}}],["和逻辑非",{"5":{"5":2}}],["和突触",{"5":{"5":2}}],["和数学家沃尔特",{"5":{"5":2}}],["和传递",{"5":{"22":2}}],["和​",{"5":{"22":1}}],["和是可学习的缩放和偏移参数",{"5":{"22":1}}],["和是可学习的逐元素缩放和偏移参数",{"5":{"22":1}}],["和输出梯度",{"5":{"22":1}}],["和表示的形式",{"5":{"23":1}}],["和奇数维度",{"5":{"24":2}}],["和激活",{"5":{"24":1}}],["和预测分布",{"5":{"24":1}}],["和其他深度学习高级主题奠定了坚实的理论基础",{"5":{"13":2}}],["和其上下文",{"5":{"24":1}}],["和罗纳德",{"5":{"25":2}}],["轴突",{"5":{"5":2}}],["树突负责接收来自其他神经元的信号",{"5":{"5":2}}],["树突",{"5":{"5":2}}],["i=1",{"5":{"15":8,"23":4}}],["ij",{"5":{"15":2,"23":4}}],["iteration",{"5":{"9":4}}],["int8",{"5":{"9":2}}],["internal",{"5":{"6":4,"14":6,"21":10,"22":2,"27":10}}],["inverse",{"5":{"9":2}}],["input",{"5":{"5":2}}],["index",{"4":{"4":1,"6":1,"12":1,"14":1,"26":1,"27":1}}],["information",{"5":{"8":2,"24":2}}],["infonce损失提供了互信息的下界估计",{"5":{"24":2}}],["immanent",{"5":{"5":2}}],["ideas",{"5":{"5":2}}],["i^2",{"5":{"23":2}}],["ik",{"5":{"23":2}}],["approx",{"5":{"15":8,"23":2}}],["approximate",{"5":{"7":2}}],["attention",{"5":{"15":2}}],["attention定义为",{"5":{"24":2}}],["adjoint",{"5":{"13":2}}],["adaptation",{"5":{"9":2}}],["adaptive",{"5":{"7":2}}],["adam的超参数",{"5":{"7":1}}],["adam的超参数和通常使用默认值",{"5":{"7":1}}],["adam维护两个指数移动平均",{"5":{"7":2}}],["adam",{"5":{"7":4}}],["adagrad特别适合处理稀疏特征和非均匀分布的梯度",{"5":{"7":2}}],["adagrad为每个参数维护一个累积梯度平方和",{"5":{"7":2}}],["additivity",{"5":{"24":2}}],["annealing",{"5":{"7":2}}],["and",{"5":{"5":2}}],["automatic",{"5":{"7":2,"13":2,"25":2}}],["a>",{"5":{"6":4,"14":6,"21":10,"27":10}}],["activation",{"5":{"5":2}}],["activity",{"5":{"5":2}}],["affine",{"5":{"5":2}}],["axon",{"5":{"5":2}}],["a",{"5":{"5":2,"8":6}}],["amsgrad是adam的一个修改版本",{"5":{"7":2}}],["amp",{"5":{"23":30}}],["alt=",{"5":{"5":2,"7":3,"8":3,"9":2,"13":2,"25":1}}],["alpha",{"5":{"23":12}}],["align",{"5":{"23":16}}],["皮茨",{"5":{"5":2}}],["麦肯罗皮层神经元的表达能力",{"5":{"5":2}}],["麦肯罗皮层神经元实际上是一个线性分类器",{"5":{"5":2}}],["麦肯罗皮层神经元",{"5":{"5":2}}],["麦肯罗皮层神经元模型存在明显的局限性",{"5":{"5":2}}],["麦肯罗皮层神经元模型虽然简单",{"5":{"5":2}}],["麦肯罗皮层神经元模型",{"5":{"5":2}}],["麦肯罗皮层",{"5":{"5":2}}],["的等式依次代入",{"5":{"15":1}}],["的等值面上",{"5":{"7":1}}],["的元素",{"5":{"15":1}}],["的所有列",{"5":{"13":1}}],["的所有模式",{"5":{"8":1}}],["的伴随值为",{"5":{"13":2}}],["的线性投影",{"5":{"13":1}}],["的线性函数",{"5":{"13":2}}],["的线性变换中最大的拉伸因子",{"5":{"9":1}}],["的伪逆",{"5":{"13":1}}],["的导数为",{"5":{"13":1,"23":2}}],["的流动",{"5":{"13":2}}],["的表示编码全局",{"5":{"13":2}}],["的表示捕获数据的局部",{"5":{"13":2}}],["的分段线性逼近有相似之处",{"5":{"13":2}}],["的分布趋向于标准正态分布",{"5":{"8":1}}],["的分布范围可能达到或更宽",{"5":{"22":1}}],["的分布范围可能达到",{"5":{"22":1}}],["的分析揭示了它们如何通过稳定激活分布来改善梯度传播",{"5":{"22":2}}],["的区域被压缩到零点",{"5":{"13":2}}],["的区域保持不变",{"5":{"13":2}}],["的区域对应类别0",{"5":{"5":2}}],["的区域对应类别1",{"5":{"5":2}}],["的复合",{"5":{"13":2}}],["的逐层求值",{"5":{"13":2}}],["的计算结果与",{"5":{"15":1}}],["的计算也是并行的",{"5":{"13":2}}],["的计算量",{"5":{"9":1}}],["的计算就隐含了条件期望的概念",{"5":{"8":2}}],["的查询",{"5":{"9":1}}],["的三阶张量表示为",{"5":{"9":1}}],["的三维张量运算",{"5":{"9":1}}],["的三维张量",{"5":{"9":3}}],["的列向量是",{"5":{"13":1}}],["的列向量",{"5":{"9":1}}],["的列向量称为右奇异向量",{"5":{"9":2}}],["的列向量称为左奇异向量",{"5":{"9":2}}],["的大矩阵",{"5":{"9":1}}],["的解等价于求解",{"5":{"9":1}}],["的权重矩阵被近似为",{"5":{"9":1}}],["的最佳近似",{"5":{"9":1}}],["的最大特征值的平方根",{"5":{"9":1}}],["的最大奇异值",{"5":{"22":1}}],["的最小和最大奇异值",{"5":{"22":1}}],["的特征值的平方根",{"5":{"9":1}}],["的特征值决定",{"5":{"9":1}}],["的特征值接近1",{"5":{"22":1}}],["的特征向量",{"5":{"9":1,"13":1}}],["的稳定性由矩阵",{"5":{"9":1}}],["的稳定性由矩阵的特征值决定",{"5":{"9":1}}],["的多项式方程",{"5":{"9":1}}],["的二维张量",{"5":{"9":1}}],["的二维张量重塑为",{"5":{"9":1}}],["的二阶泰勒展开为",{"5":{"7":1}}],["的二阶偏导数连续",{"5":{"7":1}}],["的张量",{"5":{"9":2}}],["的乘积",{"5":{"9":3,"22":1,"23":2}}],["的乘法可以看作是收缩掉两个张量的一个公共维度",{"5":{"9":2}}],["的标准",{"5":{"9":2}}],["的标准差为1时",{"5":{"22":1}}],["的对称半正定矩阵",{"5":{"8":1}}],["的对角线分类",{"5":{"15":4}}],["的对角线元素",{"5":{"9":2}}],["的对角元素",{"5":{"22":1}}],["的对数几率",{"5":{"24":1}}],["的对数几率为",{"5":{"24":1}}],["的信息",{"5":{"9":2,"24":1}}],["的信息量",{"5":{"24":1}}],["的概念推广到任意维度",{"5":{"9":2}}],["的概率为1",{"5":{"8":1}}],["的概率为",{"5":{"8":1}}],["的概率",{"5":{"8":3}}],["的方阵",{"5":{"9":1}}],["的方阵才是可逆的",{"5":{"9":2}}],["的方向导数为",{"5":{"7":1}}],["的方向",{"5":{"24":4}}],["的运算",{"5":{"9":2}}],["的支持集",{"5":{"8":1}}],["的支持集必须包含",{"5":{"8":1}}],["的梯度为",{"5":{"8":1,"13":1,"22":1,"25":1}}],["的梯度具有简洁的形式",{"5":{"8":2}}],["的梯度可以直接传播",{"5":{"22":2}}],["的梯度完全消失",{"5":{"22":1}}],["的梯度很小",{"5":{"22":1}}],["的梯度特性使得gelu特别适合训练非常深的网络",{"5":{"24":2}}],["的样本时所需要的额外信息量",{"5":{"8":1}}],["的高斯噪声",{"5":{"8":1}}],["的高斯分布",{"5":{"8":1}}],["的函数",{"5":{"8":1,"24":1}}],["的pdf为",{"5":{"8":1}}],["的语言模型",{"5":{"8":1}}],["的效果",{"5":{"8":2}}],["的渐近行为",{"5":{"8":2}}],["的先验分布为",{"5":{"8":2}}],["的one",{"5":{"8":2}}],["的定义",{"5":{"8":2}}],["的单调性来保证收敛性",{"5":{"7":1}}],["的速度收敛",{"5":{"7":1}}],["的矩阵组合成一个",{"5":{"9":1}}],["的矩阵与一个",{"5":{"9":1}}],["的矩阵",{"5":{"7":1,"9":3}}],["的偏导数的乘积之和",{"5":{"7":1}}],["的偏导数与它对",{"5":{"7":1}}],["的偏导数为",{"5":{"7":1}}],["的平面相交得到的一元函数曲线的切线斜率",{"5":{"7":1}}],["的在线阅读版本",{"5":{"6":2}}],["的第列仅依赖于的第列和的所有列",{"5":{"13":1}}],["的第行仅依赖于的第行和",{"5":{"13":1}}],["的第行为为行向量",{"5":{"5":1}}],["的第",{"5":{"5":3,"13":4,"25":2}}],["的输出可以解释为给定输入",{"5":{"5":1}}],["的变换",{"5":{"5":1}}],["的摩尔",{"5":{"5":1}}],["的仿射变换定义为",{"5":{"5":1}}],["的核心正是通过这种矩阵表示实现高效的神经网络计算",{"5":{"5":2}}],["的出现才得到解决",{"5":{"5":2}}],["的问题",{"5":{"5":2}}],["的操作",{"5":{"5":2}}],["的抽象与简化之上",{"5":{"5":2}}],["的饱和特性与sigmoid类似",{"5":{"22":1}}],["的雅可比矩阵为",{"5":{"13":1}}],["的雅可比矩阵的范数满足",{"5":{"22":1}}],["的尺度与相关",{"5":{"22":1}}],["的尺度与",{"5":{"22":1}}],["的选择是一个超参数调优问题",{"5":{"22":1}}],["的奇异值分解为",{"5":{"22":1}}],["的条件分布仍然是高斯分布",{"5":{"8":1}}],["的条件数定义为",{"5":{"22":1}}],["的各元素独立同分布且方差为",{"5":{"22":1}}],["的各元素方差为",{"5":{"22":1}}],["的谱半径",{"5":{"22":1}}],["的谱性质决定了梯度如何被",{"5":{"22":1}}],["的非主特征值很小",{"5":{"22":1}}],["的均方根",{"5":{"22":1}}],["的绝对值很大时",{"5":{"23":1}}],["的交替结构是深度学习成功的关键数学基础",{"5":{"24":2}}],["的理论基础",{"5":{"24":2}}],["的",{"5":{"24":3}}],["的能力正是通用逼近能力的几何解释",{"5":{"24":2}}],["的激活函数可能需要更少的神经元",{"5":{"24":2}}],["的设计选择背后有深刻的数学原因",{"5":{"24":2}}],["的连续调节能力",{"5":{"24":2}}],["的行为类似于信息选择器",{"5":{"24":2}}],["的显式注入",{"5":{"24":2}}],["的hessian矩阵与fisher信息矩阵密切相关",{"5":{"24":2}}],["的数学基础建立在对生物神经元",{"5":{"5":2}}],["的数学协同关系",{"5":{"24":2}}],["的形式",{"5":{"24":1}}],["的全连接层",{"5":{"13":1}}],["的全连接网络",{"5":{"24":1}}],["的网络可以等效于宽度为指数级别的浅层网络",{"5":{"24":1}}],["的后验分布与似然函数和先验的乘积成正比",{"5":{"24":1}}],["的过程",{"5":{"25":2}}],["的总影响是通过所有中间变量",{"5":{"7":1}}],["的总flops约为前向传播的4倍",{"5":{"25":2}}],["的一次试验版本",{"5":{"8":2}}],["的一种特殊情况",{"5":{"25":2}}],["的维度为",{"5":{"25":1}}],["的误差信号",{"5":{"25":1}}],["的fisher信息矩阵正好是softmax雅可比矩阵与自身转置的乘积",{"5":{"24":1}}],["的flops约为",{"5":{"25":2}}],["神经活动中内在思想的逻辑演算",{"5":{"5":2}}],["神经生理学家沃伦",{"5":{"5":2}}],["神经网络层",{"5":{"15":2}}],["神经网络可以表示为各层映射的复合",{"5":{"13":2}}],["神经网络作为复合函数",{"5":{"13":2}}],["神经网络的矩阵形式<",{"5":{"14":1,"21":1,"27":1}}],["神经网络的矩阵形式",{"0":{"15":1},"4":{"10":1,"15":1},"5":{"14":4,"15":1,"21":4,"27":4}}],["神经网络的激活值等",{"5":{"8":2}}],["神经网络的表达能力与其参数矩阵的有效秩密切相关",{"5":{"22":2}}],["神经网络的目标通常是最大化输入不同部分之间的互信息",{"5":{"24":2}}],["神经网络的训练目标是最小化某个损失函数",{"5":{"25":2}}],["神经网络本质上是一个嵌套的复合函数",{"5":{"7":2}}],["神经网络",{"5":{"5":2}}],["神经网络之所以能够拟合任意复杂的函数模式",{"5":{"24":2}}],["神经网络之所以被称为",{"5":{"24":2}}],["神经网络通用逼近能力的核心定理表明",{"5":{"24":2}}],["神经网络通过编码器和分别编码和",{"5":{"24":1}}],["神经网络通过编码器",{"5":{"24":1}}],["神经元级并行",{"5":{"13":1}}],["神经元级",{"5":{"13":2}}],["神经元",{"5":{"5":1}}],["神经元是神经网络的基本计算单元",{"5":{"5":2}}],["神经元模型不仅可以从几何角度理解",{"5":{"5":2}}],["神经元模型的矩阵表示",{"2":{"5":1},"5":{"5":1}}],["神经元可以理解为将输入向量从原始输入空间映射到新的特征空间的变换",{"5":{"5":1}}],["神经元被激活并通过轴突向其他神经元传递信号",{"5":{"5":2}}],["神经元的矩阵表示使得批量处理成为可能",{"5":{"5":2}}],["神经元的特征空间映射",{"5":{"5":2}}],["神经元的几何分离能力",{"5":{"5":2}}],["神经元的几何解释",{"2":{"5":1},"5":{"5":1}}],["神经元的数学模型<",{"5":{"6":1,"14":1,"21":1,"27":1}}],["神经元的数学模型",{"0":{"5":1},"4":{"3":1,"5":1},"5":{"5":1,"6":4,"14":4,"21":4,"27":4}}],["神经架构搜索和语言模型微调中有着应用",{"5":{"8":2}}],["神经架构搜索",{"5":{"22":2}}],["7",{"2":{"5":1,"8":1,"9":1,"13":1,"25":1},"5":{"5":5,"8":1,"9":1,"13":5,"22":2,"24":2,"25":5}}],["6",{"2":{"5":1,"8":1,"9":1,"13":1,"24":1,"25":1},"5":{"5":5,"8":1,"9":1,"13":5,"22":4,"24":3,"25":7}}],["6本节小结",{"2":{"22":1},"5":{"22":1}}],["概率密度最大的点",{"5":{"8":2}}],["概率由pdf在区间上的积分给出",{"5":{"8":2}}],["概率质量函数",{"5":{"8":2}}],["概率论是研究随机现象规律性的数学分支",{"5":{"8":2}}],["概率论与统计<",{"5":{"6":1,"14":1,"21":1,"27":1}}],["概率论与统计",{"0":{"8":1},"4":{"1":1,"8":1},"5":{"6":4,"8":1,"14":4,"21":4,"27":4}}],["概率视角下的神经元模型",{"2":{"5":1},"5":{"5":1}}],["概率视角下的激活函数",{"2":{"24":1},"5":{"24":1}}],["概率是归一化后的值",{"5":{"24":2}}],["概率解释以及信息论意义",{"5":{"24":2}}],["概率解释为损失函数的设计提供了坚实的理论基础",{"5":{"24":2}}],["概率分布被视为一个流形",{"5":{"24":2}}],["概率参数的对数几率为",{"5":{"24":1}}],["概率参数",{"5":{"24":1}}],["5的输入点集",{"5":{"5":2}}],["5",{"2":{"5":1,"8":1,"9":1,"13":1,"22":1,"24":1,"25":1},"5":{"5":5,"8":1,"9":1,"13":5,"22":9,"24":3,"25":7}}],["5附近时变化最快",{"5":{"23":2}}],["5为中心",{"5":{"23":2}}],["5x",{"5":{"23":2}}],["49152",{"5":{"15":4}}],["4n",{"5":{"15":4}}],["4的递归形式可得",{"5":{"15":2}}],["4节",{"5":{"13":2}}],["4节详细讨论",{"5":{"13":2}}],["4节中",{"5":{"13":2}}],["4",{"0":{"25":1},"2":{"5":1,"8":1,"9":1,"13":1,"22":1,"24":1,"25":10},"4":{"16":1,"25":1},"5":{"5":7,"8":1,"9":1,"13":5,"15":4,"21":5,"22":7,"23":4,"24":3,"25":69,"27":5}}],["第",{"5":{"13":1,"15":1,"22":1,"25":7}}],["第2",{"5":{"13":2}}],["第2章",{"4":{"3":1,"5":1,"10":1,"11":1,"13":1,"15":1,"16":1,"25":1},"5":{"6":2,"14":6,"21":8,"27":8}}],["第个样本的第个输出",{"5":{"13":1}}],["第四",{"5":{"5":2,"7":2,"9":2,"13":2,"25":2}}],["第四章损失函数",{"5":{"24":2}}],["第四章详细讨论的交叉熵损失正是衡量两个概率分布",{"5":{"24":1}}],["第四章详细讨论的交叉熵损失",{"5":{"24":1}}],["第三",{"5":{"5":2,"7":4,"9":8,"13":2,"15":2,"22":4,"24":2,"25":2}}],["第三章",{"2":{"21":1,"27":1},"5":{"21":1,"27":1}}],["第三项为对角矩阵",{"5":{"25":2}}],["第二层输出为1",{"5":{"15":4}}],["第二层输出为0",{"5":{"15":4}}],["第二层",{"5":{"15":2}}],["第二层有一个神经元",{"5":{"15":2}}],["第二个神经元学习",{"5":{"15":2}}],["第二章",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["第二",{"5":{"5":2,"13":2,"15":2,"25":2}}],["第二项中",{"5":{"25":2}}],["第一层输出为",{"5":{"15":8}}],["第一层神经元2",{"5":{"15":2}}],["第一层神经元1",{"5":{"15":2}}],["第一层有两个神经元",{"5":{"15":2}}],["第一个神经元学习",{"5":{"15":2}}],["第一章",{"2":{"6":1,"14":1,"21":1,"27":1},"5":{"6":1,"14":1,"21":1,"27":1}}],["第一",{"5":{"5":2,"13":2,"15":2,"25":2}}],["第一项为",{"5":{"25":2}}],["第1章",{"4":{"0":1,"1":1,"2":1,"7":1,"8":1,"9":1},"5":{"6":6,"14":6,"21":6,"27":6}}],["第3章",{"4":{"17":1,"18":1,"19":1,"22":1,"23":1,"24":1},"5":{"21":6,"27":6}}],["第六章位置编码",{"5":{"24":2}}],["第层的输入为",{"5":{"15":1}}],["第层的计算为",{"5":{"22":1}}],["第层的计算定义为",{"5":{"25":1}}],["第层的误差信号矩阵为",{"5":{"25":1}}],["第层误差信号",{"5":{"25":1}}],["第层权重的梯度为",{"5":{"25":2}}],["第层偏置的梯度为",{"5":{"25":2}}],["第五",{"5":{"25":2}}],["3规模的模型",{"5":{"13":2}}],["3",{"0":{"7":1,"13":1,"22":2,"23":1,"24":1},"2":{"5":1,"7":4,"8":1,"9":1,"13":8,"15":1,"22":13,"23":4,"24":7,"25":1},"4":{"2":1,"7":1,"11":1,"13":1,"17":1,"18":1,"19":2,"22":2,"23":1,"24":1},"5":{"5":11,"6":5,"7":5,"8":1,"9":1,"13":65,"14":10,"15":5,"21":27,"22":73,"23":9,"24":12,"25":11,"27":27}}],["3节中",{"5":{"25":2}}],["3节中证明",{"5":{"25":2}}],["3的直接应用",{"5":{"25":2}}],["2n",{"5":{"15":4}}],["2节中",{"5":{"13":2}}],["2",{"0":{"5":1,"8":1,"13":1,"15":2,"23":1,"25":1},"2":{"5":8,"7":1,"8":8,"9":1,"13":8,"15":7,"22":1,"23":4,"24":1,"25":10},"4":{"1":1,"3":1,"5":1,"8":1,"10":2,"11":1,"13":1,"15":2,"16":1,"18":1,"23":1,"25":1},"5":{"5":19,"6":9,"7":1,"8":9,"9":1,"13":17,"14":22,"15":33,"21":31,"22":9,"23":46,"24":7,"25":27,"27":31}}],["25",{"5":{"22":4,"23":4}}],["2h",{"5":{"23":1}}],["15",{"5":{"15":2}}],["1节中定义的单个神经元的数学形式",{"5":{"15":2}}],["1节中看到的",{"5":{"15":2}}],["1节和2",{"5":{"13":2}}],["13",{"2":{"9":1},"5":{"9":1,"13":2,"22":2}}],["12288",{"5":{"15":4}}],["12",{"2":{"9":1},"5":{"9":1,"13":2,"22":2,"25":2}}],["11",{"2":{"9":1},"5":{"9":1,"13":2,"15":2,"22":2,"25":2}}],["1到6",{"5":{"8":2}}],["10^",{"5":{"15":2}}],["10",{"2":{"9":1},"5":{"5":2,"9":1,"13":2,"22":2,"24":2,"25":2}}],["19",{"5":{"5":8,"7":4}}],["1958年",{"5":{"5":2}}],["1943年",{"5":{"5":2}}],["1986年",{"5":{"25":2}}],["1",{"0":{"5":1,"7":1,"8":1,"9":2,"24":1},"2":{"5":8,"7":4,"8":8,"9":27,"13":1,"15":1,"22":1,"23":1,"24":7,"25":1},"4":{"0":2,"1":1,"2":1,"3":1,"5":1,"7":1,"8":1,"9":2,"17":1,"24":1},"5":{"5":67,"6":22,"7":9,"8":9,"9":35,"13":9,"14":22,"15":27,"21":27,"22":11,"23":45,"24":40,"25":19,"27":27}}],["14",{"5":{"22":2}}],["1的简化形式",{"5":{"25":2}}],["饱和现象的数学定义",{"2":{"22":1},"5":{"22":1}}],["饱和区域可以精确计算",{"5":{"22":2}}],["饱和区域为",{"5":{"22":6}}],["饱和区域",{"5":{"25":2}}],["饱和深度的量化分析",{"5":{"22":2}}],["饱和行为呈现对称分布",{"5":{"22":2}}],["饱和效应的累积放大机制",{"2":{"22":1},"5":{"22":1}}],["饱和效应会逐层累积放大",{"5":{"22":2}}],["饱和效应与权重初始化密切相关",{"5":{"22":2}}],["饱和的网络层叠效应",{"5":{"22":2}}],["初始化策略和批量归一化的效果",{"5":{"8":2}}],["初始化与梯度流",{"2":{"22":1},"5":{"22":1}}],["保证梯度计算的可操作性",{"5":{"5":2}}],["保证网络的表达能力",{"5":{"5":2}}],["保持梯度流稳定性",{"2":{"22":1},"5":{"22":1}}],["保留关于目标的信息",{"5":{"24":1}}],["保留关于目标",{"5":{"24":1}}],["保存所有层的和",{"5":{"25":1}}],["保存所有层的",{"5":{"25":1}}],["导数趋近于零",{"5":{"22":4}}],["导数达到最大值而非最小值",{"5":{"22":2}}],["导数达到最大值0",{"5":{"22":2}}],["导数最小值接近0而非0",{"5":{"22":2}}],["导数为常数1",{"5":{"22":2}}],["导数的值取决于和的乘积",{"5":{"22":1}}],["导数的值取决于",{"5":{"22":1}}],["导数推导与梯度特性",{"0":{"23":1},"4":{"18":1,"23":1},"5":{"21":4,"23":1,"27":4}}],["导数推导与梯度特性<",{"5":{"21":1,"27":1}}],["导数推导与自化性质",{"2":{"23":1},"5":{"23":1}}],["导数推导与性质分析",{"2":{"23":1},"5":{"23":1}}],["导数可以用函数自身来表示",{"5":{"23":2}}],["导数表达式变得越来越复杂",{"5":{"23":2}}],["导致这些层无法学习",{"5":{"22":2}}],["导致训练开始时梯度就很小",{"5":{"22":2}}],["导致参数更新极其缓慢甚至停止",{"5":{"22":2}}],["导致参数更新幅度过大",{"5":{"22":2}}],["导致梯度爆炸",{"5":{"22":4}}],["导致梯度消失",{"5":{"22":6,"25":2}}],["导致softmax的输出接近one",{"5":{"22":2}}],["导致表示过度平均",{"5":{"22":2}}],["导致深层网络的训练困难",{"5":{"23":2}}],["揭示信息在网络中传递和保持的数学原理",{"5":{"22":2}}],["揭示这些技术如何从根本上改善深层网络的训练动态",{"5":{"22":2}}],["揭示其与激活函数导数特性的内在联系",{"5":{"22":2}}],["揭示其与网络架构和初始化策略的关系",{"5":{"22":2}}],["揭示其作为",{"5":{"24":2}}],["揭示它们与激活函数特性",{"5":{"22":2}}],["揭示它们与统计学和信息论之间的深层联系",{"5":{"24":2}}],["揭示了激活函数导数特性与饱和现象的内在联系",{"5":{"22":2}}],["揭示了大语言模型中非线性变换的数学本质",{"5":{"24":2}}],["斜率趋近于零",{"5":{"22":2}}],["斜率同样趋近于零",{"5":{"22":2}}],["斜率的最大值出现在处",{"5":{"23":1}}],["斜率的最大值出现在",{"5":{"23":1}}],["接着计算",{"5":{"7":1}}],["接着计算​",{"5":{"7":1}}],["接近饱和区域",{"5":{"22":2}}],["接近秩",{"5":{"22":2}}],["接近1时",{"5":{"22":1}}],["接近0或1时",{"5":{"22":1}}],["接近0或1",{"5":{"23":2}}],["接近",{"5":{"23":2}}],["接近均匀混合",{"5":{"22":1}}],["接近均匀分布",{"5":{"24":2}}],["接近one",{"5":{"24":2}}],["累积衰减模型",{"5":{"22":2}}],["僵尸状态",{"5":{"22":2}}],["循环层",{"5":{"15":2}}],["循环神经网络",{"5":{"22":2}}],["呈指数级增长",{"5":{"22":2}}],["残差连接是向量加法的典型应用",{"5":{"9":2}}],["残差连接",{"5":{"22":6,"25":1}}],["残差连接定义为",{"5":{"22":2}}],["残差连接的核心数学保证是",{"5":{"22":2}}],["残差连接的数学作用",{"2":{"22":1},"5":{"22":1}}],["残差连接的数学分析展示了它如何通过恒等路径保证梯度流的稳定",{"5":{"22":2}}],["残差的梯度流分析",{"5":{"22":2}}],["残差的梯度下界",{"5":{"22":2}}],["残差路径的恒等保证",{"5":{"22":2}}],["残差路径的梯度可以直接通过恒等连接传递",{"5":{"22":2}}],["全连接",{"5":{"15":2}}],["全连接层可以统一表示为单一的矩阵乘法",{"5":{"15":2}}],["全连接层的矩阵表示是多个神经元并行计算的紧凑写法",{"5":{"15":2}}],["全连接层的矩阵运算等价于对每个输出神经元进行独立的仿射变换计算",{"5":{"15":2}}],["全连接层的计算展开",{"5":{"15":2}}],["全连接层的数学定义为",{"5":{"15":2}}],["全连接层是神经网络中最基本的层类型",{"5":{"15":2}}],["全连接层",{"5":{"15":4}}],["全连接层就是典型的线性变换加上非线性激活函数的组合",{"5":{"9":2}}],["全局梯度裁剪",{"5":{"22":2}}],["全局梯度裁剪将梯度的范数裁剪到一个固定的上界",{"5":{"22":2}}],["裁剪阈值的选择",{"5":{"22":2}}],["裁剪阈值的选择是一个超参数调优问题",{"5":{"22":1}}],["裁剪阈值",{"5":{"22":1}}],["裁剪后的梯度方向与原梯度方向相同",{"5":{"22":2}}],["裁剪后的梯度范数始终不超过",{"5":{"25":2}}],["开始",{"5":{"22":2,"25":2}}],["放大或保持稳定",{"5":{"22":2}}],["捷径",{"5":{"22":2}}],["瓶颈",{"5":{"22":2}}],["死神经元",{"5":{"22":4,"24":2}}],["死神经元问题",{"5":{"22":1}}],["除最大特征值外其余特征值趋近于零",{"5":{"22":2}}],["除非有平移分量",{"5":{"24":2}}],["除了前面介绍的奇异值分解",{"5":{"9":2}}],["除了在极端负值区域",{"5":{"24":2}}],["归一化技术的梯度稳定机制",{"2":{"22":1},"5":{"22":1}}],["归一化技术是现代深度学习训练的核心组件",{"5":{"22":2}}],["归一化技术与梯度稳定",{"2":{"22":1},"5":{"22":1}}],["归一化技术与位置编码在transformer中共同工作",{"5":{"22":2}}],["归一化技术通过多种机制稳定梯度传播",{"5":{"22":2}}],["归一化技术",{"5":{"22":2}}],["归一化的数学效应",{"5":{"22":2}}],["归一化确保了不同位置的位置编码具有相似的数值尺度",{"5":{"22":2}}],["归一化将激活值的分布规范化到固定范围",{"5":{"22":2}}],["归一化",{"5":{"22":2}}],["归一化层的梯度计算涉及除以方差",{"5":{"22":2}}],["归一化层会将其缩放",{"5":{"22":2}}],["归一化层",{"5":{"25":1}}],["归一化维度",{"5":{"22":1}}],["减少到",{"5":{"9":1}}],["减少内部协变量偏移",{"5":{"22":2}}],["减少了层与层之间的依赖关系",{"5":{"22":2}}],["减少了梯度偏移问题",{"5":{"23":2}}],["减少了协变量偏移问题",{"5":{"23":2}}],["减小了协变量偏移post",{"5":{"22":1}}],["减小了协变量偏移",{"5":{"22":1}}],["节点的值只依赖于其前驱节点",{"5":{"13":2}}],["节点",{"5":{"13":2}}],["节省了加法和除法操作",{"5":{"22":2}}],["效应量",{"5":{"8":2}}],["效应使得非常深的网络成为可能",{"5":{"22":2}}],["效应",{"5":{"22":2}}],["效果相当",{"5":{"22":2}}],["基本运算与运算规则",{"2":{"9":1},"5":{"9":1}}],["基本思想是从原始数据中有放回地抽取与原数据等大的样本",{"5":{"8":2}}],["基本梯度下降算法的更新规则为",{"5":{"7":2}}],["基于前文的分析",{"5":{"22":2}}],["虽然自然语言处理很少直接使用傅里叶变换",{"5":{"9":2}}],["虽然训练高效",{"5":{"22":2}}],["虽然在大规模语言模型中sigmoid已被relu及其变体取代",{"5":{"23":2}}],["弯曲",{"5":{"22":2}}],["程度",{"5":{"22":2}}],["简单rnn的梯度分析",{"2":{"22":1},"5":{"22":1}}],["简单性与表达能力之间存在权衡",{"5":{"22":2}}],["简单性有时比数学上的",{"5":{"24":2}}],["发表了开创性的论文",{"5":{"5":2}}],["发现性能更优的激活函数",{"5":{"22":2}}],["针对relu激活函数进行了优化",{"5":{"22":2}}],["针对特定任务",{"5":{"22":2}}],["趋向于正态分布",{"5":{"8":2}}],["趋近于其渐近值",{"5":{"22":1}}],["很大而没有进行适当的缩放",{"5":{"22":1}}],["很小",{"5":{"22":1}}],["区间",{"5":{"22":1,"23":7,"24":1}}],["区间内",{"5":{"24":1}}],["允许梯度下降在不同局部最小值之间",{"5":{"7":2}}],["允许快速学习但可能容忍不稳定",{"5":{"22":1}}],["作为近似",{"5":{"22":1}}],["否则形状不兼容",{"5":{"9":2}}],["否则为0",{"5":{"8":2}}],["否",{"5":{"22":1}}],["困惑度",{"5":{"8":2}}],["困难",{"5":{"22":2}}],["稳定性和最终性能",{"5":{"23":2}}],["天然具有概率解释",{"5":{"23":2}}],["证明",{"5":{"5":14,"13":26,"15":8,"23":8,"25":24}}],["证明了激活函数对于突破神经网络线性瓶颈的关键作用",{"5":{"5":2}}],["证明了通过错误信号的反向传播可以有效训练多层神经网络",{"5":{"25":2}}],["证毕",{"5":{"23":8}}],["此外",{"5":{"7":2,"8":2,"9":2}}],["此时",{"5":{"23":4}}],["此时网络的输出为",{"5":{"24":2}}],["三阶及更高阶的张量则可以表示更复杂的数据结构",{"5":{"9":2}}],["三阶导数",{"5":{"23":2}}],["钟形",{"5":{"23":2}}],["落在区间",{"5":{"8":1}}],["落在区间的概率为",{"5":{"8":1}}],["落在饱和区域",{"5":{"23":2}}],["浪费计算资源",{"5":{"23":2}}],["补",{"5":{"23":6}}],["未饱和区域",{"5":{"23":2}}],["uniform",{"5":{"8":2}}],["unit",{"5":{"23":2}}],["构成标准正交基",{"5":{"9":2}}],["构成了现代神经网络激活函数的主流选择",{"5":{"23":2}}],["构成的函数族在适当条件下可以满足这些要求",{"5":{"24":1}}],["求导",{"5":{"23":2}}],["附近",{"5":{"23":1}}],["增加了函数的",{"5":{"22":2}}],["增大时",{"5":{"23":1}}],["局部最小值的等价性是大规模深度学习中的一个有趣现象",{"5":{"7":2}}],["局部基函数与函数重构",{"2":{"24":1},"5":{"24":1}}],["局部基函数",{"5":{"24":2}}],["局部探测器",{"5":{"24":2}}],["局部曲面叠加",{"5":{"24":2}}],["万有逼近定理",{"5":{"24":2}}],["整个序列或批次的损失是这些位置损失的均值或和",{"5":{"8":2}}],["整个网络仍然等价于一个简单的线性变换",{"5":{"24":2}}],["调整参数",{"5":{"13":1}}],["调整参数以使逼近目标函数",{"5":{"13":1}}],["调整位置",{"5":{"24":2}}],["调整多少超参数",{"5":{"24":2}}],["曲线两端的平坦性正是梯度饱和的几何表现",{"5":{"22":2}}],["曲线在该点由上凸转为下凸",{"5":{"23":2}}],["曲线下凸",{"5":{"23":2}}],["曲线上凸",{"5":{"23":2}}],["曲线趋近于渐近线",{"5":{"22":4}}],["曲线趋于水平",{"5":{"23":2}}],["曲线也趋于水平",{"5":{"23":2}}],["曲面或更复杂的几何结构",{"5":{"24":2}}],["曲面",{"5":{"24":2}}],["图像等数据中充满了非线性模式",{"5":{"24":2}}],["要求上述不等式在",{"5":{"7":1}}],["要求上述不等式在和时取严格小于号",{"5":{"7":1}}],["要将这两个类别分开",{"5":{"24":2}}],["折叠",{"5":{"13":2}}],["折线或更复杂的形状",{"5":{"24":2}}],["支持向量机",{"5":{"24":2}}],["没有激活函数的神经网络等价于单层线性变换",{"5":{"5":2}}],["没有激活函数的神经网络无论多深",{"5":{"5":2}}],["没有softmax的线性归一化将无法实现注意力的",{"5":{"24":2}}],["功能",{"5":{"24":2}}],["封闭有界集合",{"5":{"24":2}}],["超平面",{"5":{"24":2}}],["抑制区",{"5":{"24":2}}],["历史上",{"5":{"24":2}}],["光滑",{"5":{"24":2}}],["优化目标就是最小化",{"5":{"8":1}}],["优化目标就是最小化或",{"5":{"8":1}}],["优化过程变得更加稳定",{"5":{"8":2}}],["优化更加灵活高效",{"5":{"23":2}}],["优雅",{"5":{"24":2}}],["足够宽的单层网络可以逼近任意函数",{"5":{"24":2}}],["已成为llama等现代大语言模型的默认选择",{"5":{"22":2}}],["已有理论结果表明",{"5":{"24":2}}],["已在2",{"5":{"25":2}}],["浅层网络的线性组合较为简单",{"5":{"13":2}}],["浅层网络需要指数级数量的神经元才能逼近",{"5":{"24":2}}],["句法关系",{"5":{"24":2}}],["许多自然现象和测量误差都近似服从高斯分布",{"5":{"8":2}}],["许多激活函数可以解释为某种概率分布的参数变换",{"5":{"24":2}}],["许多激活函数",{"5":{"24":2}}],["成立",{"5":{"8":2,"9":1,"24":1}}],["成功概率的对数几率",{"5":{"24":1}}],["成功概率",{"5":{"24":1}}],["成正比",{"5":{"25":1}}],["得到样本属于正类的概率估计",{"5":{"24":2}}],["限制梯度的范数以防止梯度爆炸",{"5":{"7":2}}],["限制了输出值的范围",{"5":{"24":2}}],["限制在区间内",{"5":{"24":1}}],["限制在",{"5":{"24":1}}],["真实分布和预测分布",{"5":{"24":1}}],["真实分布",{"5":{"24":1}}],["真实标签为",{"5":{"8":2}}],["真实标签",{"5":{"25":2}}],["差异的信息论度量",{"5":{"24":2}}],["策略网络的离散动作选择",{"5":{"24":2}}],["极大值或鞍点",{"5":{"7":2}}],["极大地促进了离散分布表示学习的发展",{"5":{"24":2}}],["软选择",{"5":{"24":2}}],["软",{"5":{"24":2}}],["硬件实现",{"5":{"13":1}}],["硬",{"5":{"24":2}}],["去除关于输入的冗余信息",{"5":{"24":2}}],["生物神经元是一种复杂的细胞系统",{"5":{"5":2}}],["生物神经元在膜电位超过阈值时发放动作电位",{"5":{"24":2}}],["稀疏注意力和分层注意力",{"5":{"9":2}}],["稀疏编码通常更高效地利用参数",{"5":{"24":2}}],["稀疏表示在信息论上具有几个优势",{"5":{"24":2}}],["稀疏表示对噪声更加鲁棒",{"5":{"24":2}}],["稀疏表示提供了更好的可解释性",{"5":{"24":2}}],["少数几个特征值很大",{"5":{"7":2}}],["少量的活跃神经元可以编码大量的信息",{"5":{"24":2}}],["活跃的神经元可以被解释为检测到特定模式的",{"5":{"24":2}}],["探测器",{"5":{"24":2}}],["参数复杂度从",{"5":{"9":1}}],["参数复杂度从降低到",{"5":{"9":1}}],["参数高效微调和特定网络结构的设计",{"5":{"9":2}}],["参数高效微调以及模型蒸馏等场景中具有重要应用",{"5":{"9":2}}],["参数更新幅度过大导致模型权重溢出",{"5":{"22":2}}],["参数可以通过反向传播自动学习",{"5":{"22":1}}],["参数",{"5":{"22":1,"24":1}}],["参数的后验分布为",{"5":{"8":2}}],["参数的后验分布与似然函数和先验的乘积成正比",{"5":{"24":1}}],["参数优化和性能评估的全过程",{"5":{"8":2}}],["参数优化需要计算损失函数对各参数的梯度",{"5":{"25":2}}],["参数梯度的完整推导",{"2":{"25":1},"5":{"25":1}}],["参数梯度计算",{"5":{"25":2}}],["参数梯度可以通过误差信号与前向传播保存的激活值计算",{"5":{"25":2}}],["参见第四章",{"5":{"24":2}}],["参见第六章",{"5":{"24":4}}],["估计和最大化互信息",{"5":{"24":2}}],["间接影响fisher信息度量的几何结构",{"5":{"24":2}}],["普遍选择gelu作为",{"5":{"24":2}}],["双曲正切函数",{"5":{"5":2,"23":4}}],["双重非线性",{"5":{"24":2}}],["学习率调整和梯度裁剪中都有应用",{"5":{"9":2}}],["学习率调度",{"5":{"7":2}}],["学习率预热",{"5":{"7":2}}],["学习率按余弦曲线从初始值逐渐减小到零",{"5":{"7":2}}],["学习率按指数函数递减",{"5":{"7":2}}],["学习率太大可能导致跳过最优解甚至发散",{"5":{"7":2}}],["学习率太小会导致收敛缓慢",{"5":{"7":2}}],["学习率是一个关键的超参数",{"5":{"7":2}}],["学习token之间的注意力模式",{"5":{"24":2}}],["绝对位置编码",{"5":{"24":2}}],["偶数维度",{"5":{"24":2}}],["选择哪种kl形式取决于我们希望对近似分布施加什么样的约束",{"5":{"8":2}}],["选择和互信息最大化作用",{"5":{"24":2}}],["奠定了理论基础",{"5":{"24":2}}],["能区分任意两点",{"5":{"24":2}}],["能够在输入空间中形成",{"5":{"24":1}}],["能够捕获输入之间的复杂依赖关系",{"5":{"24":1}}],["组成",{"5":{"9":2}}],["组合",{"5":{"24":1}}],["宽度为的全连接网络",{"5":{"24":1}}],["宽度为",{"5":{"24":1}}],["捕获比更抽象",{"5":{"24":1}}],["捕获比",{"5":{"24":1}}],["代价是增加额外的计算量",{"5":{"9":2}}],["代入对数几率的表达式",{"5":{"24":1}}],["代入得",{"5":{"25":2}}],["代入并化简",{"5":{"25":1}}],["代入",{"5":{"25":1}}],["解这个不等式得到两个解",{"5":{"22":2}}],["解释为伯努利分布的成功概率",{"5":{"24":1}}],["解决方案",{"5":{"25":1}}],["控制",{"5":{"24":1}}],["温度参数提供了从",{"5":{"24":1}}],["温度参数",{"5":{"24":1}}],["良好的表示应该最大化",{"5":{"24":1}}],["良好的表示",{"5":{"24":1}}],["应用sigmoid",{"5":{"23":2}}],["应该最大化",{"5":{"24":1}}],["杰弗里",{"5":{"25":2}}],["辛顿",{"5":{"25":2}}],["鲁梅尔哈特",{"5":{"25":2}}],["威廉姆斯",{"5":{"25":2}}],["杂志上发表了关于反向传播算法的开创性论文",{"5":{"25":2}}],["系统阐述神经元从生物原型到数学抽象的演化过程",{"5":{"5":2}}],["系统阐述反向传播算法的完整理论框架",{"5":{"25":2}}],["推论2",{"5":{"13":2,"25":5}}],["推理",{"5":{"25":2}}],["回归任务",{"5":{"25":2}}],["恒等激活",{"5":{"25":2}}],["隐藏维度",{"5":{"9":1}}],["隐藏维度通常在数千量级",{"5":{"9":1}}],["隐藏层误差信号传播",{"5":{"25":2}}],["写成向量形式即",{"5":{"25":2}}],["路径归一化与深度网络稳定性",{"5":{"22":2}}],["路径归一化",{"5":{"22":2}}],["路径归一化确保所有路径的深度相近",{"5":{"22":2}}],["路径深度",{"5":{"22":2}}],["路径传递到前一层",{"5":{"22":1}}],["路由",{"5":{"25":2}}],["克罗内克函数",{"5":{"25":2}}],["算法2",{"5":{"25":2}}],["综合以上结果",{"5":{"25":2}}],["返回",{"5":{"25":2}}],["约有一半的神经元输出为零",{"5":{"24":2}}],["约为2倍前向传播",{"5":{"25":2}}],["充分利用硬件并行性",{"5":{"25":2}}],["读者应能够理解反向传播的工作原理",{"5":{"25":2}}],["度量预测与真实标签之间的差异",{"5":{"25":1}}],["各层参数的梯度​",{"5":{"25":1}}],["各层参数的梯度",{"5":{"25":1}}],["误差信号的传播",{"2":{"25":1},"5":{"25":1}}],["误差信号",{"5":{"25":4}}],["误差信号包含了损失函数对该层激活值变化的敏感程度信息",{"5":{"25":2}}],["误差信号为",{"5":{"25":2}}],["误差信号有不同的简化形式",{"5":{"25":2}}],["误差信号从输出层向输入层逐层传播",{"5":{"25":2}}],["误差信号从前一层传播到当前层",{"5":{"25":2}}],["误差信号从前向传播的最后一层",{"5":{"25":2}}],["误差信号会被衰减",{"5":{"25":2}}],["误差信号矩阵需要除以批量大小",{"5":{"25":2}}],["误差信号传播",{"5":{"25":2}}],["误差传播",{"5":{"25":1}}],["外积计算",{"5":{"25":1}}],["聚合",{"5":{"25":1}}],["范数​",{"5":{"9":1}}],["范数分别是列范数和行范数",{"5":{"9":2}}],["范数和∞",{"5":{"9":2}}],["范数",{"5":{"7":2,"9":5}}],["范围内",{"5":{"25":2}}],["症状",{"5":{"25":1}}],["损失函数梯度计算",{"5":{"13":1}}],["损失函数包含重构项和kl正则项",{"5":{"8":2}}],["损失函数",{"5":{"7":1,"22":1,"25":3}}],["损失函数的等值面在高维空间中可能比直觉上预期的更加连通",{"5":{"7":2}}],["损失函数的hessian通常具有极值的特征值分布",{"5":{"7":2}}],["损失函数衡量输出与目标之间的差异",{"5":{"7":1}}],["损失函数会以多快的速度变化",{"5":{"7":2}}],["损失函数值出现nan或inf",{"5":{"22":2}}],["损失函数关于第一层权重的梯度为",{"5":{"22":1}}],["损失函数关于网络参数的梯度需要通过激活函数的导数逐层传递",{"5":{"23":2}}],["损失函数度量预测与真实标签之间的差异",{"5":{"25":1}}],["损失函数对权重的梯度可以分解为损失函数对净输入的梯度",{"5":{"25":2}}],["损失函数为",{"5":{"25":2}}],["损失nan",{"5":{"25":1}}],["精确率等指标的置信区间",{"5":{"8":2}}],["精度下溢",{"5":{"25":1}}]],"serializationVersion":2}