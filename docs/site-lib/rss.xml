<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[math_of_llm]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>site-lib/media/favicon.png</url><title>math_of_llm</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 08 Jan 2026 07:54:29 GMT</lastBuildDate><atom:link href="site-lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 08 Jan 2026 07:54:26 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[大模型中的数学]]></title><description><![CDATA[这是《大模型中的数学》的在线阅读版本。
<a data-href="第1章 数学基础/1.1 线性代数与张量运算" href="第1章-数学基础/1.1-线性代数与张量运算.html" class="internal-link" target="_self" rel="noopener nofollow">第1章 数学基础/1.1 线性代数与张量运算</a>
<br><a data-href="第1章 数学基础/1.2 概率论与统计" href="第1章-数学基础/1.2-概率论与统计.html" class="internal-link" target="_self" rel="noopener nofollow">第1章 数学基础/1.2 概率论与统计</a>
<br><a data-href="第1章 数学基础/1.3 微积分与优化基础" href="第1章-数学基础/1.3-微积分与优化基础.html" class="internal-link" target="_self" rel="noopener nofollow">第1章 数学基础/1.3 微积分与优化基础</a>
]]></description><link>index.html</link><guid isPermaLink="false">index.md</guid><pubDate>Thu, 08 Jan 2026 07:53:40 GMT</pubDate></item><item><title><![CDATA[1.3 微积分与优化基础]]></title><description><![CDATA[微积分是研究变化率和累积效应的数学分支，为理解和优化大语言模型提供了核心的数学工具。大语言模型的训练本质上是一个优化问题：我们需要找到一组模型参数，使得模型在训练数据上的损失函数值最小化。这个优化过程涉及对损失函数求导（计算梯度），然后根据梯度方向更新参数。理解偏导数、链式法则、梯度和Hessian等概念，是深入掌握深度学习优化算法的必要基础。偏导数是多元函数对单个变量的导数，它描述了当其他变量保持不变时，函数值随某一特定变量变化的速率。设是一个多元函数，，则关于变量在点处的偏导数定义为：从几何角度来看，偏导数可以理解为多元函数图像与包含坐标轴的平面相交得到的一元函数曲线的切线斜率。在大语言模型的语境下，当我们计算损失函数关于某个特定参数的偏导数时，我们实际上是在问：如果只改变这个参数而保持其他参数不变，损失函数会以多快的速度变化。这个信息对于确定参数更新的方向和幅度至关重要。偏导数的计算遵循与单变量导数相同的规则。常数的偏导数为零，常数因子的偏导数等于该因子乘以函数的偏导数，和的偏导数等于偏导数的和。对于乘积​，商的偏导数​​，链式法则将在下文详细讨论。复合函数的偏导数法则与单变量情况类似，只是需要明确哪个变量被当作中间变量。链式法则（Chain Rule）是微积分中最重要的法则之一，它告诉我们如何计算复合函数的导数。在深度学习中，神经网络本质上是一个嵌套的复合函数，链式法则使得我们能够计算损失函数相对于任意深层参数的梯度，这一过程称为反向传播。链式法则的基本形式可以这样表述：如果，则，其中是外层函数关于中间变量的导数，是内层函数关于自变量的导数。对于多元函数的复合，情况稍微复杂一些。设，其中每个，则关于的偏导数为：这个公式表明，对的总影响是通过所有中间变量传导的，每个中间变量的贡献是它对的偏导数与它对的偏导数的乘积之和。在深度学习的反向传播算法中，链式法则的应用更加系统化。考虑一个简单的两层神经网络：，，其中和是激活函数，是权重矩阵，是偏置向量，是输入，是输出。损失函数衡量输出与目标之间的差异。反向传播从输出层开始，首先计算​，然后逐层向后传递：，接着计算​，以此类推直到输入层。
<img alt="backprop.drawio.png" src="graph/backprop.drawio.png" target="_self">矩阵形式的链式法则在深度学习中尤为重要。设，，则梯度是一个与形状相同的三阶张量，而​。为了高效地实现这些计算，我们使用雅可比矩阵（Jacobian Matrix）来组织偏导数。雅可比矩阵包含所有一阶偏导数，对于函数，雅可比矩阵 是一个的矩阵，其中​。在计算图中，链式法则的应用更加直观和系统。计算图是表示数学表达式的有向无环图，其中节点表示变量或操作，边表示依赖关系。反向传播算法沿着计算图反向遍历，对于每个操作节点，计算其输出关于输入的梯度，然后使用链式法则组合这些梯度。典型的操作包括矩阵乘法、加法、元素级函数（如ReLU、Sigmoid、Softmax）和归一化操作，每种操作都有预定义的梯度计算规则。自动微分（Automatic Differentiation）是现代深度学习框架的核心功能，它利用链式法则自动计算复杂函数的导数。与数值微分（使用有限差分近似导数）和符号微分（使用代数规则显式推导导数公式）不同，自动微分将函数分解为基本操作的序列，然后应用链式法则累加梯度。自动微分结合了数值计算的效率和符号计算的精确性，是训练神经网络的关键技术基础。梯度是多元函数最陡上升方向的向量，包含了函数关于所有自变量的偏导数信息。设是一个可微函数，则在点处的梯度定义为：梯度向量指向函数增长最快的方向，其长度（范数）反映了增长的速率。负梯度方向则是函数下降最快的方向，这就是梯度下降算法的理论基础。在大语言模型中，梯度向量指导着参数的更新方向：每个参数分量根据其在梯度中的值进行调整，使得整体损失函数朝着下降的方向移动。梯度的性质对于理解优化过程至关重要。梯度与等值面（或等值线）正交：在函数的等值面上，梯度向量与等值面垂直。这意味着如果我们沿着梯度方向移动，我们会以最快的速度离开当前的等值面，进入函数值更高的区域。驻点（Critical Point）是梯度为零向量的点，即，这些点可能是极小值、极大值或鞍点。方向导数（Directional Derivative）衡量函数沿特定方向的变化率。设是一个单位向量，则沿的方向导数为，其中是梯度与方向之间的夹角。这个公式清楚地表明，方向导数在梯度方向（）上取得最大值，在与梯度相反的方向（）上取得最小值（负的最大值），在与梯度垂直的方向（）上为零。Hessian矩阵是多元函数的二阶导数矩阵，包含了关于所有自变量的二阶偏导数信息。对于函数，Hessian矩阵定义为：如果函数的二阶偏导数连续（这是多数实际应用中的常见假设），则Hessian矩阵是对称的，即​。对称性大大简化了Hessian矩阵的分析和计算。Hessian矩阵在优化中的作用主要体现在以下几个方面。首先，通过Hessian矩阵可以判断驻点的性质：如果Hessian正定（所有特征值大于零），则该点是局部极小值；如果Hessian负定，则该点是局部极大值；如果Hessian既有正特征值又有负特征值，则该点是鞍点；如果Hessian半正定或半负定，则需要进一步分析。其次，Hessian矩阵决定了泰勒展开的二次项，影响局部优化的收敛速度。在点​附近，函数的二阶泰勒展开为：这个近似在局部区域非常准确，是理解牛顿法等二阶优化方法的基础。第三，Hessian矩阵的奇异值分解揭示了函数的局部曲率结构。Hessian的特征值表示函数在不同特征方向上的曲率（凹凸程度），特征向量表示这些曲率对应的方向。在深度学习优化中，损失函数的Hessian通常具有极值的特征值分布：少数几个特征值很大（对应"平坦"方向），多数特征值很小（对应"陡峭"方向），这种结构影响着优化算法的行为。共轭梯度法利用Hessian的信息来加速收敛，而不显式地计算和存储完整的Hessian矩阵。在深度学习中，由于参数数量巨大（可达数十亿），显式计算Hessian是不现实的，因此共轭梯度法和拟牛顿法等方法在处理大规模优化问题时具有优势。<br>
<img alt="CombinedScene_ManimCE_v0.19.1.png" src="graph/combinedscene_manimce_v0.19.1.png" target="_self">K-FAC（Kronecker-Factored Approximate Curvature）是一种用于深度学习的Hessian近似方法，它将Hessian矩阵近似为两个小矩阵的Kronecker积，从而大幅降低计算和存储成本。K-FAC基于层间独立性假设，假设每层的参数可以独立地近似其Hessian块，这使得在保持二阶信息的同时实现了可扩展性。高斯-牛顿矩阵是Hessian的一个常用近似，在最小二乘问题中特别有用。对于残差形式的问题 ，高斯-牛顿矩阵为，其中是残差函数的雅可比矩阵。与完整的Hessian相比，高斯-牛顿矩阵省略了二阶导数项，通常是正定的，并且计算成本更低。拉普拉斯近似利用Hessian矩阵在峰值附近对概率分布进行高斯近似。在贝叶斯深度学习和变分推断中，拉普拉斯近似用于构建后验分布的高斯代理，使得预测具有不确定性估计。这种方法在模型压缩和少样本学习中有一定的应用价值。梯度下降是最基本也最广泛使用的优化算法，它利用负梯度方向作为搜索方向来迭代地最小化目标函数。基本梯度下降算法的更新规则为，其中是第步的参数，是学习率（步长），是损失函数在当前参数处的梯度。学习率是一个关键的超参数，它决定了每次更新的幅度：学习率太小会导致收敛缓慢，学习率太大可能导致跳过最优解甚至发散。学习率调度（Learning Rate Scheduling）是在训练过程中动态调整学习率的策略。常见的学习率调度方法包括：阶梯衰减（Step Decay），每经过若干个epoch将学习率降低一个因子；指数衰减（Exponential Decay），学习率按指数函数递减；余弦退火（Cosine Annealing），学习率按余弦曲线从初始值逐渐减小到零；Warmup，在训练初期逐渐增加学习率，然后再衰减。这些调度策略的目标是在训练初期快速收敛，在后期精细调优，最终达到较好的收敛效果。<br><img alt="grad.png" src="graph/grad.png" target="_self">
凸优化是研究凸函数在凸集上最小化问题的数学分支，具有优美的理论性质和高效的算法。在凸优化问题中，任何局部最优解都是全局最优解，且最优解集是凸集。凸函数是一类特殊的函数，其上方的图构成一个凸集。函数是凸函数的充要条件是对于任意和任意，有。几何上，这意味着函数图上任意两点的连线位于函数图的上方。严格凸函数是凸性更强的形式，要求上述不等式在和时取严格小于号。严格凸函数至多有一个全局最小值点。可微函数是凸函数的充要条件是其梯度单调不减：。对于二次可微函数，凸性的充要条件是Hessian矩阵半正定。梯度下降在凸优化中的收敛性有完善的理论保证。对于具有L-利普希茨连续梯度的凸函数（即），标准梯度下降以的速度收敛，即经过次迭代后，目标函数值与最优值的差距为。对于强凸函数（即存在使得，梯度下降以线性速度收敛，即误差按几何级数衰减。动量方法（Momentum）是加速梯度下降的重要技术，它累积历史梯度来平滑参数更新。动量更新的公式为：，​，其中是速度向量，是动量系数。动量方法可以看作是对梯度进行指数移动平均，使得更新方向更加平滑，从而加速收敛并减少震荡。Nesterov动量是动量方法的一个变体，它在更新之前先对梯度进行校正，通常能提供更好的收敛性质。自适应学习率方法为每个参数单独调整学习率，是深度学习优化中最重要的进展之一。AdaGrad为每个参数维护一个累积梯度平方和，并相应地调整学习率：，其中是逐元素累积的梯度平方，表示逐元素乘法。AdaGrad特别适合处理稀疏特征和非均匀分布的梯度。RMSprop是AdaGrad的改进版本，使用指数移动平均来累积梯度平方：，其中是衰减系数。这解决了AdaGrad学习率单调递减的问题，使得算法能够适应非平稳的目标函数。Adam（Adaptive Moment Estimation）是最流行的自适应优化算法，它结合了动量方法和RMSprop的优点。Adam维护两个指数移动平均：（一阶矩估计，即动量）和（二阶矩估计，即未中心化的方差）。为了校正偏差，使用和。更新规则为。Adam的超参数和通常使用默认值。尽管Adam在实践中表现优异，但最近的研究表明，在某些情况下，传统的带动量SGD可能优于Adam。AMSGrad是Adam的一个修改版本，通过保持的单调性来保证收敛性。RAdam（Rectified Adam）通过引入一个校正因子来修复Adam在训练早期的方差问题。这些变体反映了深度学习优化研究的持续进展。大语言模型的优化面临独特的挑战。首先，模型参数量巨大（数十亿甚至万亿级），标准优化算法的内存开销巨大。梯度检查点技术通过在前向传播时只保存部分激活值，在反向传播时重新计算其余激活值，以计算换内存。其次，混合精度训练使用半精度浮点数进行计算以节省内存和加速，但仍需要保持某些计算的精度。第三，学习率预热（Warmup）在训练大语言模型中尤为重要。warmup策略在训练初期逐渐增加学习率，从很小的值线性增长到目标学习率，然后再进行衰减。这被认为有助于模型在早期阶段找到一个更好的参数区域，避免在随机初始化阶段受到过大梯度的干扰。第四，梯度裁剪（Gradient Clipping）限制梯度的范数以防止梯度爆炸，这对于训练深层网络和循环神经网络尤为重要。非凸优化是深度学习面临的现实挑战。与凸优化问题不同，深度神经网络的损失函数通常是非凸的，存在大量的局部最小值、鞍点和平坦区域。非凸性意味着梯度下降可能收敛到局部最小值而非全局最小值。然而，实践表明深度学习模型通常能够找到性能不错的解，即使不能保证达到全局最优。关于为什么深度神经网络能够有效优化，有多种理论解释，包括损失函数的"伪凸"性质、随机梯度下降的隐式正则化效应，以及局部最小值通常具有相似的损失值等。鞍点逃离是大规模非凸优化中的一个重要问题。在高维空间中，鞍点比局部最小值更为常见。动量方法和噪声梯度有助于算法逃离鞍点。SGD的随机性本身就提供了一种隐式的噪声源，有助于逃离平坦区域。研究表明，在适当的噪声水平下，SGD倾向于收敛到平坦的局部最小值，这些最小值通常具有更好的泛化能力。局部最小值的等价性是大规模深度学习中的一个有趣现象。研究发现，对于不同的随机初始化，深度网络往往收敛到具有相似损失值的局部最小值，即使这些最小值在参数空间中相距甚远。这可能是因为神经网络具有大量的对称性（如神经元排列的不变性），使得不同的参数化方式可以产生相同的函数。此外，损失函数的等值面在高维空间中可能比直觉上预期的更加连通，允许梯度下降在不同局部最小值之间"穿行"。]]></description><link>第1章-数学基础/1.3-微积分与优化基础.html</link><guid isPermaLink="false">第1章 数学基础/1.3 微积分与优化基础.md</guid><pubDate>Thu, 08 Jan 2026 07:52:43 GMT</pubDate><enclosure url="graph/backprop.drawio.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;graph/backprop.drawio.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[1.2 概率论与统计]]></title><description><![CDATA[概率论是研究随机现象规律性的数学分支，为大语言模型提供了处理不确定性的理论基础。在自然语言处理中，文本生成本质上是一个随机过程：给定前文的条件下，下一个词的选择遵循某种概率分布。语言模型的训练目标就是学习这个条件概率分布，使得生成的文本既流畅又符合语义逻辑。理解随机变量和概率分布的概念，是掌握语言模型概率本质的必要前提。随机变量是样本空间到实数集的映射，它将随机试验的结果数值化。根据取值方式的不同，随机变量分为离散随机变量和连续随机变量。离散随机变量只能取有限或可数无穷个值，如抛硬币的结果（正面或反面）、掷骰子的点数（1到6）、词汇表中的词索引等。连续随机变量可以取任意实数值或实数区间内的值，如词语的嵌入向量、注意力权重、神经网络的激活值等。在大语言模型中，我们同时处理这两类随机变量：词索引是离散的，而连续空间中的嵌入表示和隐藏状态则是连续的。概率质量函数（Probability Mass Function，PMF）是描述离散随机变量概率分布的函数。对于离散随机变量，其PMF记为，表示取特定值的概率。PMF必须满足两个基本性质：非负性对所有成立，以及归一性。在大语言模型中，词汇表上的概率分布就是典型的PMF。对于词汇表大小为的语言模型，输出层的预测分布是一个维的概率向量，其元素表示生成词汇表中每个词的概率。伯努利分布（Bernoulli Distribution）是最简单的离散分布，描述单次二元试验的成功概率。设随机变量，则，，其中是成功概率。伯努利分布的期望为，方差为。在语言模型中，伯努利分布可用于描述二元随机事件，如判断一个词是否属于某个类别、一个词是否被mask掉、或经过Dropout操作后某个神经元是否被激活等。二项分布（Binomial Distribution）是n次独立伯努利试验中成功次数的分布。设，则，其中。二项分布的期望为，方差为。二项分布在语言模型中的应用包括描述一批样本中被mask的词的数量、计算分类任务中的正确预测数量等。类别分布（Categorical Distribution）是伯努利分布向多值情况的推广，也称为多项分布（Multinomial Distribution）的一次试验版本。设随机变量在个类别上服从类别分布，则，其中且。语言模型的输出层正是类别分布的一个典型应用：模型为词汇表中的每个词分配一个概率，表示生成该词的可能性。Softmax函数将模型的原始输出（logits）转换为有效的类别概率分布。连续随机变量由概率密度函数（Probability Density Function，PDF）描述。与PMF不同，PDF在单点的取值不代表概率，概率由PDF在区间上的积分给出。设连续随机变量的PDF为，则 落在区间的概率为。PDF同样满足非负性和归一性。均匀分布（Uniform Distribution）是最简单的连续分布。设，则其PDF为当，否则为0。均匀分布的期望为，方差为​。在语言模型中，均匀分布可用于权重初始化、随机采样策略的设计，以及某些正则化技术的理论基础。指数分布（Exponential Distribution）描述独立事件发生的时间间隔。设，则其PDF为当，其中是率参数。指数分布的期望为​，方差为​。指数分布在语言模型中的应用包括描述某些随机过程的到达时间间隔，以及在概率图模型中作为共轭先验使用。
<img alt="dist_relation.drawio.png" src="graph/dist_relation.drawio.png" target="_self">期望、方差和协方差是概率论中最重要的一阶和二阶统计量，它们刻画了随机变量的集中趋势、离散程度和相互关系。这些统计量在机器学习的理论和实践中都有广泛应用，从损失函数的定义到模型性能的评价，从梯度计算的期望到模型不确定性的估计，都离不开这些基本概念。期望（Expectation）是随机变量取值的加权平均，描述了随机变量的"中心"位置。对于离散随机变量，其期望定义为。对于连续随机变量，期望定义为。期望具有线性性质：对任意常数和随机变量，有。这一性质在反向传播算法的推导中至关重要，因为我们可以将期望操作与梯度操作交换次序。条件期望是在给定某些信息的条件下对随机变量的期望。条件期望是的函数，记为 。条件期望的一个重要性质是全期望公式：。在语言模型中，条件期望用于描述在给定上下文的条件下，下一个词的条件概率分布的期望特性。例如，困惑度（Perplexity）的计算就隐含了条件期望的概念。方差（Variance）衡量随机变量取值的离散程度，定义为。根据期望的线性性质，方差可以展开为，这个公式在计算方差时更加实用。方差的平方根称为标准差（Standard Deviation），记为​，它与随机变量具有相同的量纲，便于解释和比较。方差的性质包括：非负性；对常数有；若和独立，则。需要注意的是，一般情况下，只有当和不相关时等号才成立。在深度学习中，方差用于分析梯度的波动、权重初始化的尺度选择，以及模型输出的不确定性估计。协方差（Covariance）衡量两个随机变量之间的线性相关程度。对于两个随机变量和，协方差定义为。协方差可以展开为。当时，和倾向于同向变化；当时，和倾向于反向变化；当时，和线性无关。协方差矩阵是多维随机变量的二阶中心矩描述。对于维随机向量，其协方差矩阵是一个的对称半正定矩阵，其中第个元素为。协方差矩阵的对角线元素是各随机变量的方差，非对角线元素是变量之间的协方差。协方差矩阵在大语言模型中的应用包括：描述词嵌入向量的分布特性、分析不同层之间隐藏状态的相关性，以及在变分自编码器中定义潜在空间的先验分布。相关系数（Correlation Coefficient）是标准化的协方差，定义为​。相关系数的取值范围为，其中表示完全正相关，表示完全负相关，表示线性无关。相关系数消除了量纲的影响，使得不同变量对之间的相关性可以直接比较。在语言模型研究中，相关系数可用于分析不同词嵌入维度之间的冗余程度，以及评估模型对不同类型输入的响应一致性。矩（Moment）是随机变量幂次期望的统称。阶原点矩定义为，阶中心矩定义为。一阶原点矩是期望，二阶中心矩是方差。偏度（Skewness）由三阶中心矩归一化得到，描述分布的对称性；峰度（Kurtosis）由四阶中心矩归一化得到，描述分布的尾部厚度。这些高阶矩在统计分析中有一定应用，但在深度学习中的直接应用较少。矩母函数（Moment Generating Function，MGF）是，它唯一确定了随机变量的概率分布（在其存在的范围内）。通过对矩母函数求导并在处求值，可以得到各阶矩：。特征函数是矩母函数的复数形式，它总是存在的，且同样唯一确定概率分布。在某些深度学习应用中，如分析神经网络输出的分布特性，特征函数提供了有用的分析工具。高斯分布（Gaussian Distribution），也称为正态分布（Normal Distribution），是概率论中最重要的连续分布，在统计学和机器学习中有着核心地位。高斯分布之所以如此重要，源于多个方面的原因：从理论上看，中心极限定理表明大量独立随机变量之和趋向于高斯分布；从应用上看，许多自然现象和测量误差都近似服从高斯分布；从数学上看，高斯分布在卷积、傅里叶变换和微分方程等运算下保持封闭形式，这使得高斯分布成为最易于处理的分布之一。一维高斯分布的概率密度函数为，记为。其中是均值参数，是方差参数，是标准差。标准高斯分布（或称单位高斯分布）是均值为0、方差为1的特殊高斯分布，记为，其PDF为。任何高斯随机变量都可以通过标准化变换转换为标准高斯随机变量。高斯分布的期望为，方差为，这从参数命名就可以看出。高斯分布的众数（概率密度最大的点）和中位数都等于均值。高斯分布的偏度为0（完全对称），峰度为3（相对于标准正态分布）。高斯分布的熵（稍后详述）为，这表明在给定方差的所有连续分布中，高斯分布具有最大的熵，即最大的不确定性。<br>
<img alt="gauss_dist.drawio.png" src="graph/gauss_dist.drawio.png" target="_self">
多元高斯分布是一维高斯分布向多维空间的推广。设是一个维随机向量，若服从多元高斯分布，则记为，其中是均值向量，是协方差矩阵（必须是对称半正定的）。多元高斯分布的PDF为：其中 是协方差矩阵的行列式，是协方差矩阵的逆矩阵（称为精度矩阵）。二次型 称为马氏距离（Mahalanobis Distance），它考虑了各变量之间的相关性。多元高斯分布具有许多重要的性质。首先，边缘分布仍然是高斯分布：若，则任意分量服从一维高斯分布。其次，线性变换保持高斯性：若，则对任意矩阵 和向量，有。这一性质在变分推断和高斯过程回归中非常重要。条件分布是多元高斯分布最优美也最实用的性质之一。考虑将划分为两个子向量和​，均值和协方差相应地分块为：则给定时的条件分布仍然是高斯分布：其中条件均值为，条件协方差为​。注意条件协方差不依赖于观测值，这是一个重要的性质。条件分布在高斯过程回归、卡尔曼滤波和贝叶斯线性回归中有着核心应用。在大语言模型中，高斯分布扮演着多种重要角色。在权重初始化方面，Xavier初始化和He初始化都假设权重服从某种高斯分布（或均匀分布），以确保信号在各层之间平稳传播。具体而言，He初始化使用均值为0、方差为的高斯分布，其中是输入神经元的数量。在正则化技术方面，Dropout可以被解释为对神经网络进行高斯近似。具体来说，当Dropout率为 时，等效的高斯Dropout在权重上引入方差为的高斯噪声。变分Dropout（Variational Dropout）进一步将Dropout解释为贝叶斯推断，噪声的参数通过变分推断学习得到。在输出建模方面，高斯输出分布用于回归任务，但在语言模型中更常用的是分类分布。某些语言模型变体使用高斯混合模型来表示输出分布，以捕获更复杂的多模态特性。在对话系统中，高斯分布可用于建模响应的不确定性，使得模型能够生成多样化的回复。在高斯过程和贝叶斯优化中，高斯过程是函数的先验分布，用于建模连续的随机过程。高斯过程回归在超参数优化、神经架构搜索和语言模型微调中有着应用。高斯过程的一个优势是它提供了预测的不确定性估计，这对于主动学习和探索-利用权衡很有价值。在变分自编码器和生成模型中，高斯分布通常作为潜在空间的先验分布。标准VAE假设潜在变量服从标准高斯分布，通过编码器学习将输入映射到该分布的参数（均值和对数方差），再通过重参数化技巧进行采样，最后通过解码器重建原始输入。大语言模型的某些扩展（如结合VAE的表示学习）也采用了类似的思想。KL散度（Kullback-Leibler Divergence）和交叉熵（Cross Entropy）是信息论中的核心概念，在机器学习特别是大语言模型中有着广泛应用。它们用于衡量两个概率分布之间的差异，是定义损失函数和分析模型行为的重要工具。信息论的基本概念始于熵（Entropy）的定义。对于离散随机变量及其概率分布，熵定义为。熵衡量了随机变量不确定性的大小，单位为比特（bit，当使用以2为底的对数时）或奈特（nat，当使用自然对数时）。直观上，熵越大，分布越"平坦"（不确定性高）；熵越小，分布越"尖锐"（确定性高）。对于确定性分布（某个的概率为1，其他为0），熵为0。联合熵（Joint Entropy）是两个随机变量的联合分布的熵：。条件熵（Conditional Entropy）是给定一个随机变量时另一个随机变量的熵：。链式法则给出了多个随机变量的联合熵分解：。互信息（Mutual Information）衡量两个随机变量之间的信息共享程度：。互信息始终非负，当且仅当和独立时互信息为0。在语言模型中，互信息可用于分析不同位置或不同层之间的信息流动，以及评估词嵌入中捕获的语义信息量。KL散度衡量两个概率分布之间的"距离"（严格来说不是真正的距离，因为它不满足对称性和三角不等式）。对于两个离散概率分布和，KL散度定义为：KL散度可以理解为使用分布来编码来自分布的样本时所需要的额外信息量。从公式可以看出，KL散度是非负的（由Jensen不等式保证），即，且当且仅当时KL散度为0。然而，KL散度不对称，即一般情况下。对于连续随机变量（分布用PDF表示），KL散度的定义为：在大语言模型中，KL散度有多种重要应用。在变分推断中，我们希望用简单的近似分布来逼近复杂的后验分布，优化目标就是最小化或，具体选择取决于变分推断的目标。在VAE中，损失函数包含重构项和KL正则项，后者就是编码器分布与标准高斯先验之间的KL散度。在强化学习与语言模型的结合中，KL散度用于限制策略更新的幅度，确保新策略不会偏离旧策略太远。这在近端策略优化（PPO）等算法中非常重要。在知识蒸馏中，KL散度用于衡量学生模型和教师模型输出分布之间的差异，使学生模型能够学习教师模型的知识。交叉熵与KL散度密切相关。对于两个分布和，交叉熵定义为。交叉熵可以分解为熵与KL散度之和：。当是真实分布而是模型预测分布时，是常数（因为真实分布固定），因此最小化交叉熵等价于最小化KL散度。<br>
<img alt="msg_relation.drawio.png" src="graph/msg_relation.drawio.png" target="_self">在大语言模型的训练中，交叉熵损失是最常用的损失函数。具体而言，对于每个位置，语言模型预测下一个词的概率分布为，而真实分布是"正确答案"的one-hot编码。该位置的交叉熵损失为，其中​ 是真实词元。整个序列或批次的损失是这些位置损失的均值或和。交叉熵损失的优势在于它直接与对数似然相关，最小化交叉熵等价于最大化数据的对数似然。从优化的角度看，交叉熵损失相对于模型输出（logits）的梯度具有简洁的形式。设是第个类别的logit，softmax输出为​，真实标签为（在多类分类中为one-hot编码），则交叉熵损失相对于的梯度为​。这个简洁的梯度形式是交叉熵损失被广泛采用的重要原因之一。标签平滑（Label Smoothing）是一种常用的正则化技术，它将硬标签（one-hot编码）替换为软标签，即在真实类别上赋予的概率，在其他类别上均匀分配的概率。标签平滑可以防止模型对训练数据过度自信，提高泛化能力。从KL散度的角度看，标签平滑相当于在训练时使用真实分布与均匀分布的混合作为目标分布。KL散度在不同场景下的不同形式反映了其灵活性。正向鼓励覆盖的所有模式（即的支持集必须包含的支持集），而反向则鼓励找到一个主要的模式并紧密地拟合它。在变分推断中，选择哪种KL形式取决于我们希望对近似分布施加什么样的约束。在生成模型中，这种选择影响着生成样本的多样性和质量。统计推断是利用样本数据对总体特征进行推断的过程，包括参数估计和假设检验两大类方法。在大语言模型中，统计推断的思想贯穿于模型训练、参数优化和性能评估的全过程。理解最大似然估计、贝叶斯估计和期望传播等统计推断方法，对于深入理解语言模型的工作原理和改进模型设计都有重要意义。最大似然估计（Maximum Likelihood Estimation，MLE）是最基本也是最重要的参数估计方法。设我们有观测数据，假设这些数据独立同分布（i.i.d.）于某个概率分布，其中是未知参数。似然函数定义为，对数似然函数为。最大似然估计就是找到使对数似然最大的参数值：。在大语言模型中，MLE是训练的标准方法。语言模型的训练目标是最大化训练语料库的对数似然，即给定前文的条件下预测下一个词的对数概率之和。由于训练数据通常被组织为序列，对数似然可以分解为序列中每个位置的条件对数概率之和。训练过程就是使用随机梯度下降等优化算法来最大化这个目标函数。最大似然估计具有一些重要的渐近性质。在正则条件下，当样本量时，MLE是相合的（收敛于真实参数值）、渐近正态的（服从以真实参数为均值、费希尔信息矩阵逆为方差的正态分布）和渐近有效的（方差达到克拉美-罗下界）。这些性质为MLE在大样本场景下的应用提供了理论保障。贝叶斯估计是另一种参数估计方法，它将参数视为随机变量并使用贝叶斯公式进行推断。设参数 的先验分布为，在观测数据后，参数的后验分布为：其中是似然函数，是边际似然（或证据）。贝叶斯估计使用后验分布进行预测，而不是点估计。在语言模型中，贝叶斯方法可用于模型选择（通过边际似然）、不确定性量化和小样本学习。共轭先验是使后验分布与先验分布同分布族的先验选择，这使得贝叶斯推断在计算上更加方便。例如，高斯分布的共轭先验还是高斯分布，二项分布的共轭先验是Beta分布，多项分布的共轭先验是Dirichlet分布。在语言模型中，Beta先验和Dirichlet先验可用于建模词汇概率的不确定性。变分推断是一类近似贝叶斯推断的方法，它将后验分布的推断转化为优化问题。变分推断假设后验分布可以用某个简单的分布族（如高斯分布）来近似，然后通过最小化近似分布与真实后验分布之间的KL散度来找到最好的近似。在大语言模型中，变分推断被用于VAE的训练、主题模型的推断和某些贝叶斯神经网络方法。期望传播（Expectation Propagation）是另一种近似推断方法，它通过匹配矩（而不是最小化全局KL散度）来近似后验分布。期望传播在某些情况下比变分推断更准确，特别是当目标分布是高度非高斯的时候。在语言模型的某些应用中，如基于贝叶斯方法的超参数优化，期望传播提供了有用的近似工具。自助法（Bootstrap）是一种通过重采样来估计统计量方差的非参数方法。基本思想是从原始数据中有放回地抽取与原数据等大的样本，重复多次，计算每次的统计量估计，然后使用这些估计的分布来表征原始统计量的不确定性。在语言模型的评估中，自助法可用于估计困惑度、精确率等指标的置信区间，帮助我们理解模型性能评估的可靠性。大数定律和中心极限定理是概率论中最重要的极限定理，它们描述了大量随机变量之和（或平均值）的渐近行为。这些定理为机器学习中许多方法的合理性提供了理论依据，也指导着我们理解和解释模型训练过程中的各种现象。大数定律（Law of Large Numbers）指出，当独立同分布的随机变量样本量趋向无穷时，样本均值趋向于随机变量的期望。设是独立同分布的随机变量，期望有限，则样本均值依概率收敛于对任意成立。弱大数定律保证样本均值以概率收敛于期望，而强大数定律保证样本均值几乎必然收敛于期望。在机器学习训练中，大数定律解释了为什么使用更大的批量（batch）进行梯度估计时，梯度的方向更加稳定。随着批量大小的增加，梯度估计的方差减小，优化过程变得更加稳定。在语言模型训练中，我们通常使用随机梯度下降或其变体，每次只使用一小批样本来估计梯度。根据大数定律，当批量大小足够大时，这一小批样本的梯度是整体数据梯度的一个良好近似。然而，当批量太小时，梯度估计的方差较大，可能导致训练过程不稳定或收敛较慢。中心极限定理（Central Limit Theorem，CLT）是概率论中最深刻的定理之一，它指出大量独立同分布随机变量之和（适当标准化后）趋向于正态分布。设是独立同分布的随机变量，期望有限，方差有限，则标准化和的分布趋向于标准正态分布：，其中是标准正态分布的CDF。中心极限定理的重要性在于它表明正态分布在自然界中无处不在，无论原始随机变量的分布是什么，只要样本量足够大，它们的均值（或和）就近似服从正态分布。在深度学习中，这一性质被用于分析梯度分布、初始化策略和批量归一化的效果。批量归一化（Batch Normalization）是深度学习中最重要的技术之一，它利用中心极限定理的原理来稳定训练过程。批量归一化对每一层的输入进行标准化，使其均值接近0、方差接近1。从中心极限定理的角度看，当批量大小足够大时，该批量内样本的均值和方差是整体数据均值和方差的良好估计，因此标准化操作是合理的。在分析神经网络梯度的分布时，中心极限定理提供了重要的洞见。假设网络中有大量独立的噪声源（如随机初始化、Dropout噪声等），则梯度的分布趋向于高斯分布。这种近似在分析神经网络的学习动态、设计优化算法和理解泛化性质时非常有用。大数定律和中心极限定理也指导着模型评估策略。当我们计算验证集上的性能指标时，该指标是总体性能的一个估计。根据大数定律，验证集越大，这个估计越可靠。根据中心极限定理，我们可以构建置信区间来量化估计的不确定性。在实际应用中，我们需要平衡验证集大小和计算成本，同时理解评估结果的统计显著性。假设检验是统计推断的重要工具，用于根据样本数据判断关于总体的假设是否成立。在大语言模型的开发和研究中，假设检验被用于比较不同模型的性能、验证改进措施的有效性以及检测数据中的系统性偏差。假设检验的基本框架包括原假设（）和备择假设（​）。原假设通常是我们想要拒绝的假设（如"两个模型没有差异"），备择假设是我们想要支持的假设（如"模型A优于模型B"）。我们根据样本数据计算检验统计量，确定在原假设下观察到该统计量或更极端情况的概率（p值）。如果p值小于预先设定的显著性水平（如0.05），则拒绝原假设。配对t检验是比较两个相关样本均值的常用方法。在语言模型评估中，如果我们使用相同的测试集评估两个模型，配对t检验可以判断两个模型性能的差异是否显著。具体来说，对于每个测试样本，我们计算两个模型的性能差异，然后检验这些差异的均值是否显著不为零。显著性检验的解读需要谨慎。"统计显著"不等于"实际显著"或"重要"。一个很大的模型可能在某些指标上有统计显著的改进，但改进幅度可能很小以至于在实际应用中可以忽略。p值受样本量影响很大：样本量很大时，即使是微小的差异也可能是统计显著的；样本量很小时，即使很大的差异也可能不显著。效应量（Effect Size）衡量差异的实际大小，独立于样本量。常用的效应量包括Cohen's d（标准化均值差异）和相关系数。在比较语言模型性能时，报告效应量可以帮助读者理解改进的实际意义。此外，置信区间比p值提供更多信息：它不仅告诉我们差异是否显著，还给出了差异大小的可能范围。多重比较问题是在进行多个假设检验时需要考虑的重要问题。如果同时检验多个假设，即使所有原假设都为真，至少有一个被拒绝的概率也会大大增加（这是所谓的"多重比较谬误"）。在大语言模型评估中，当我们比较多个模型或在多个测试集上评估时，需要使用适当的多重比较校正方法（如Bonferroni校正或False Discovery Rate控制）来控制总体错误率。在比较语言模型时，置换检验（Permutation Test）是一种非参数方法，特别适用于样本量不大或分布未知的情况。置换检验通过随机打乱标签来生成置换样本，在原假设下所有排列是等可能的。通过计算真实排列的统计量在置换分布中的位置，可以得到p值。置换检验在比较两个模型在特定测试集上的性能时很有用。A/B测试是互联网公司和研究机构常用的实验方法，用于比较两个版本（如两个模型或两种策略）的效果。A/B测试的核心是随机分组和控制变量，确保比较的公平性。在大语言模型的在线评估中，A/B测试可以用于评估模型对用户交互的实际影响，如用户满意度、任务完成率等指标。]]></description><link>第1章-数学基础/1.2-概率论与统计.html</link><guid isPermaLink="false">第1章 数学基础/1.2 概率论与统计.md</guid><pubDate>Wed, 07 Jan 2026 08:10:32 GMT</pubDate><enclosure url="graph/dist_relation.drawio.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;graph/dist_relation.drawio.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[1.1 线性代数与张量运算]]></title><description><![CDATA[线性代数作为现代数学的重要分支，在大语言模型的理论基础中占据着不可替代的核心地位。从Transformer架构的注意力机制到词嵌入的向量表示，从矩阵乘法的并行计算到张量运算的维度变换，线性代数为理解和构建深度学习模型提供了坚实的数学框架。大语言模型中的所有参数最终都以矩阵或张量的形式存储和计算，因此深入掌握线性代数的基本概念和运算规则是理解模型工作原理的必要前提。在线性代数的学习路径中，我们需要首先明确几个基本定义。向量是线性空间中最基本的元素，可以理解为一维数组中的有序数集。在大语言模型的语境下，词向量就是典型的向量表示，每个词被映射为一个高维实数向量，这个向量的每一个维度都编码了某种语义或语法特征。例如，一个300维的词向量可以看作是一个从自然语言到连续向量空间的映射，将离散的符号表示转化为连续的数值表示，从而使得机器学习算法能够在这些向量上进行运算和优化。矩阵是二维数组，是线性代数中最重要的研究对象之一。在大语言模型中，权重矩阵无处不在：嵌入层将词汇映射为向量，实际上是一个巨大的查找矩阵；注意力机制中的查询、键、值投影都是通过矩阵乘法实现的；前馈神经网络中的全连接层更是由多个权重矩阵堆叠而成。矩阵的形状（行数和列数）决定了它所代表的线性变换的类型，也决定了它在网络中的具体作用方式。张量是多维数组的自然推广，是矩阵概念在更高维度的延伸。一阶张量是向量，二阶张量是矩阵，三阶及更高阶的张量则可以表示更复杂的数据结构。在现代深度学习框架中，如PyTorch和TensorFlow中，张量是最基本的数据结构，几乎所有的计算都在张量之上进行。大语言模型中的输入通常是一个三维张量，其维度分别代表批量大小、序列长度和嵌入维度。通过张量运算，模型能够高效地并行处理大量数据，这也是大语言模型能够实现高效训练和推理的关键技术基础。线性代数的基本运算包括向量加法、标量乘法、矩阵乘法、转置和求逆等。这些运算在大语言模型中有着直接的应用场景，理解它们的数学性质和计算特性对于优化模型性能和调试模型行为至关重要。向量加法是逐分量（component-wise）的运算，两个维度相同的向量可以逐分量相加。设有两个维向量 和，则其和为向量加法满足交换律和结合律。在大语言模型（如 Transformer）中，残差连接是向量加法的典型应用。它将某一子层的输入与该子层的输出直接相加，从而在反向传播时为梯度提供了一条恒等映射路径，使梯度能够绕过复杂的非线性变换直接传播到较浅层，显著提升了深层模型的可训练性，并有效缓解梯度消失问题。标量乘法是将一个向量与一个标量（实数）相乘，结果是将向量的每个分量都乘以该标量。这种运算在模型的权重初始化、学习率调整和梯度裁剪中都有应用。例如，梯度裁剪就是将梯度向量的模长限制在某个阈值之内，具体做法是如果梯度的范数超过阈值，就将梯度向量乘以一个缩放因子使其范数等于阈值。矩阵乘法是最重要也是计算量最大的运算。两个矩阵和的乘积 是一个的矩阵，其中每个元素。矩阵乘法满足结合律 和分配律，但不满足交换律，即通常不等于。在大语言模型中，注意力机制的计算过程就包含了多个矩阵乘法操作：查询矩阵与键矩阵的乘积产生注意力分数，注意力权重与值矩阵的乘积产生加权输出。这些矩阵乘法的计算效率直接影响模型的整体性能，因此现代GPU都针对矩阵运算进行了专门的硬件优化。矩阵的转置是将矩阵的行和列互换，记作或。转置运算满足和。在大语言模型中，自注意力机制需要计算查询向量与键向量的相似度，这本质上就是计算查询矩阵与转置后的键矩阵的乘积。此外，在实现反向传播时，转置操作也经常用于梯度的反向传播计算。矩阵的求逆是找到一个矩阵使得，其中是单位矩阵。只有方阵才可能存在逆矩阵，而且并非所有方阵都有逆矩阵——只有满秩（行列式非零）的方阵才是可逆的。在深度学习中，我们很少直接计算矩阵的逆，因为数值稳定性较差且计算成本高昂。但矩阵求逆的数学思想——通过逆运算解线性方程组——在优化算法和正则化方法中有着重要的应用。例如，线性回归的闭式解就是通过求解正规方程得到的，而正规方程的求解等价于计算矩阵的伪逆。张量是多维数组在数学上的抽象表示，它将标量（零阶张量）、向量（一阶张量）和矩阵（二阶张量）的概念推广到任意维度。一个阶张量有个索引，每个索引对应张量的一个维度。设有一个三阶张量，则其元素可以表示为​，其中，，。在大语言模型的实践中，张量的维度通常具有明确的语义含义。以Transformer模型为例，输入张量的形状通常是 ，其中是批量大小（batch size），是序列长度（sequence length），是隐藏维度（hidden dimension）。在多头注意力中，经过线性投影并拆分注意力头后，张量可重排为的张量，其中是注意力头的数量。经过注意力计算后的输出张量会与输入张量形状相同，便于进行残差连接和层归一化操作。
<img alt="llm_flow.drawio.png" src="graph/llm_flow.drawio.png" target="_self">
张量运算的基本操作包括逐元素运算、重塑（reshape）、转置（transpose）、广播（broadcasting）和张量收缩（contraction）等。逐元素运算对张量中的每个独立元素应用相同的函数，如ReLU激活函数就是典型的逐元素运算。重塑操作改变张量的形状但不改变其包含的数据元素，例如将一个的二维张量重塑为的三维张量，这在将展平后的向量恢复为嵌入序列时非常有用。广播是张量逐元素运算中的一种自动对齐机制。当两个形状不同的张量进行逐元素运算时，深度学习框架会在不显式复制数据的情况下，将形状较小的张量在某些维度上视为被重复，以匹配较大的张量。广播规则通常是从最后一个维度开始逐维比较：如果对应维度相等，或其中一个维度为 1，则该维度可以广播；否则形状不兼容。若两个张量的维度数不同，则会在较小张量的左侧自动补 1。 NumPy、PyTorch 等现代深度学习框架均支持广播机制，它可以显著简化代码（例如 bias 加法），但由于广播可能在语法上合法而在语义上错误，不当使用容易引入隐蔽且难以察觉的 bug。张量收缩是矩阵乘法在高维张量上的推广，又称为张量点积或广义矩阵乘法。张量收缩指定两个张量的若干维度进行配对相乘并求和。例如，两个三阶张量和在第二和第三维度上的收缩产生一个的矩阵。在注意力机制中，注意力分数的计算可以看作是一个张量收缩操作，其中查询向量与键向量在特征维度上进行点积运算。张量分解是处理高维张量的重要技术。常见的张量分解方法包括 CP 分解（CANDECOMP/PARAFAC） 和 Tucker 分解。CP 分解将一个张量表示为若干个秩一张量的和，而 Tucker 分解则将张量表示为一个核心张量与多个因子矩阵在各模上的乘积。张量分解在模型压缩、参数高效微调以及模型蒸馏等场景中具有重要应用，通过将大型权重张量分解为若干低秩张量，可以在保持模型性能的同时显著减少参数量和计算开销。特征空间是机器学习和深度学习中描述数据表示的核心概念。从几何角度来看，一个n维特征空间可以理解为一个n维欧几里得空间，空间中的每个点代表一个数据样本在该空间中的表示。在大语言模型中，词嵌入空间就是一个典型的高维特征空间，其中每个词被映射为空间中的一个点，语义相近的词在空间中距离较近，语义不同的词则距离较远。这种几何表示使得我们可以利用空间中的几何关系来理解和操作语义信息。词嵌入空间具有一些重要的几何性质。首先，嵌入空间通常是密集的，这意味着每个维度都不是二元的或稀疏的，而是取连续的实数值。这种密集表示使得模型能够在连续空间中学习平滑的语义关系。其次，嵌入空间的维度通常远小于词表大小，但又足够高以捕获丰富的语义信息。维度的选择是一个重要的超参数，过低的维度可能导致表达能力的损失（称为欠拟合），过高的维度则可能导致过拟合和计算效率的下降。向量距离和相似度是度量特征空间中样本关系的基本工具。欧几里得距离测量两个向量之间的直线距离。余弦相似度测量两个向量方向的相似程度，与向量的大小无关。在词嵌入研究中，余弦相似度是最常用的相似度度量，因为它能够捕捉语义方向的相似性而不受词频等因素的影响。线性变换是理解特征空间中数据变换的关键概念。一个线性变换可以用一个矩阵 表示，满足和 。线性变换对特征空间的作用包括旋转、缩放、投影和剪切等。旋转保持向量的模长不变，只改变方向；缩放改变向量的模长；投影将向量映射到低维子空间；剪切则是一种保持体积但改变形状的变换。在神经网络中，全连接层就是典型的线性变换加上非线性激活函数的组合。子空间是特征空间中的重要结构。一个k维子空间是特征空间的一个k维线性子集，包含所有可以通过原点的k维平面上的点。列空间（Column Space）是矩阵所有列向量的线性组合构成的空间，它描述了矩阵所能表示的所有输出。零空间（Null Space）是所有被矩阵映射到零向量的输入向量构成的空间，它描述了输入中的冗余信息。在大语言模型中，权重矩阵的列空间决定了该层能够表示的特征空间的范围，而零空间则包含了可能被"遗忘"的信息。特征值和特征向量是线性代数中最深刻也最有应用价值的概念之一。它们揭示了线性变换的本质特征：某些特定方向（特征向量）在变换下只发生缩放而不改变方向，缩放的倍数就是特征值。数学上，给定一个方阵，如果存在非零向量和标量满足，则称是的特征向量，是对应的特征值。特征值的计算需要求解特征方程，这是一个关于的多项式方程。对于大型矩阵，直接求解特征方程通常是不切实际的，因为在数值上计算高阶多项式的根非常困难。在实践中，我们使用迭代算法如幂迭代（Power Iteration）、反幂迭代（Inverse Iteration）和QR算法来计算特征值和特征向量。深度学习框架也提供了高效的函数来计算矩阵的特征值分解和奇异值分解。在大语言模型中，特征值和特征向量有着多方面的应用。首先，在主成分分析（PCA）中，特征值分解用于提取数据中方差最大的方向，这些方向称为主成分。通过保留最大的k个特征值对应的特征向量，我们可以实现降维同时最小化信息损失。词嵌入的降维可视化（如t-SNE和UMAP）就依赖于这种思想，先通过PCA进行初步降维，再用非线性方法进行二维或三维嵌入。<br>
<img alt="pca.drawio.png" src="graph/pca.drawio.png" target="_self">
其次，特征值与网络稳定性密切相关。一个线性 dynamical system 的稳定性由矩阵的特征值决定：如果所有特征值的实部都为负，则系统是稳定的；如果有特征值的实部为正，则系统是不稳定的。在循环神经网络（RNN）和Transformer中，类似的稳定性分析有助于理解和解决梯度消失或梯度爆炸问题。例如，LSTM和GRU通过门控机制设计，使得等效的变换矩阵具有接近1的特征值，从而缓解了梯度消失问题。第三，谱范数（Spectral Norm）是矩阵的最大奇异值，等于矩阵的最大特征值的平方根。谱范数在深度学习中有重要应用：它用于权重归一化以稳定训练过程，也是证明神经网络泛化性质的关键工具。谱归一化（Spectral Normalization）是一种常用的权重归一化技术，它将权重矩阵的谱范数归一化为1，从而约束了网络函数的Lipschitz常数，这对训练生成对抗网络（GAN）和防止对抗攻击都很重要。奇异值分解（Singular Value Decomposition，SVD）是将任意矩阵分解为三个矩阵乘积的强大工具，被誉为"线性代数的瑞士军刀"。对于任意矩阵，存在正交矩阵、和对角矩阵，使得。其中，的对角线元素（r为矩阵的秩）称为奇异值，是的特征值的平方根。的列向量称为左奇异向量，的列向量称为右奇异向量。奇异值分解与特征值分解有密切关系，但又有所不同。特征值分解只适用于方阵且要求矩阵可对角化，而奇异值分解适用于任意矩阵。另外，特征值可以是负数（对于实对称矩阵，特征值是实数），但奇异值总是非负的。奇异值分解之所以强大，正是因为它对任何矩阵都成立，无论是方阵还是矩形阵，无论是否满秩。低秩近似是奇异值分解最重要的应用之一。根据Eckart-Young-Mirsky定理，对于任何整数，用秩不超过k的矩阵对的最佳近似（在Frobenius范数和谱范数意义下）就是保留前k个最大的奇异值及其对应的奇异向量，即。这个近似将原始矩阵分解为k个秩一矩阵的和，每个秩一矩阵对应一个主成分方向。在大语言模型中，低秩近似技术有着广泛的应用。首先，在模型压缩方面，权重矩阵的低秩近似可以显著减少参数量和计算量。假设一个的权重矩阵被近似为​，其中，，则参数量从减少到。当时，这种压缩是显著的。其次，在参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）中，低秩更新是一种核心技术。典型的低秩适配方法如LoRA（Low-Rank Adaptation）假设预训练模型的权重更新可以表示为低秩矩阵，即，其中，。这样，只需要训练个参数而不是个参数，就可以实现对大型语言模型的有效微调。这种方法的数学基础正是低秩近似理论。第三，奇异值分解可以用于理解模型的表达能力。通过分析权重矩阵的奇异值分布，我们可以了解模型使用了多少"有效自由度"。如果只有少数几个奇异值很大，说明模型的表达能力主要集中在少数方向上；如果奇异值分布比较均匀，则模型利用了更多的参数自由度。这种分析对于诊断模型过拟合和理解模型容量都很有价值。正交性是线性代数中一个核心概念，具有重要的几何意义和计算优势。两个向量和如果满足，则称它们是正交的。一组向量如果两两正交且都是单位向量，则称为标准正交基。正交矩阵是满足的方阵，其列向量（或行向量）构成标准正交基。正交矩阵有几个重要的性质。首先，正交矩阵保持向量的范数不变，即对所有向量成立。这意味着正交变换是一种保距变换，它只旋转或反射空间而不改变向量的长度。其次，正交矩阵的逆矩阵等于其转置矩阵，即，这使得求解正交矩阵的逆矩阵变得极其简单，只需进行转置操作。第三，正交矩阵的行列式的绝对值为1，即。在大语言模型的架构设计中，正交性有着多方面的应用。首先，正交初始化是一种常用的权重初始化方法。与随机高斯初始化或Xavier初始化不同，正交初始化将权重矩阵初始化为正交矩阵，这在理论上可以防止初始化时的梯度消失或爆炸问题，因为正交矩阵的奇异值全部为1。其次，Gram-Schmidt正交化过程是构建正交基的标准算法，在特征提取和表示学习中经常被使用。例如，在对比学习中，通过Gram-Schmidt正交化可以移除表示中的冗余成分，使得不同特征方向捕捉不同的信息。第三，正交约束在优化中被用于防止权重矩阵的秩退化。在某些正则化方法中，我们在优化目标中加入正交性惩罚项，鼓励权重矩阵的列向量相互正交。这种约束有助于提高模型的稳定性和泛化能力。第四，傅里叶变换和正交基扩展是信号处理和自然语言处理中的重要工具。虽然自然语言处理很少直接使用傅里叶变换，但在处理序列数据的某些场景下，正交变换仍然是有价值的分析工具。矩阵范数是衡量矩阵"大小"的标准，在深度学习中有着广泛的应用。矩阵范数将矩阵映射到非负实数，满足非负性、齐次性和三角不等式等性质。常用的矩阵范数包括Frobenius范数、谱范数、核范数和1-范数/∞-范数。Frobenius范数是最直观的矩阵范数，定义为所有元素平方和的平方根：​​。它可以看作是矩阵展开为向量后的L2范数，具有旋转不变性。在深度学习中，Frobenius范数常用于权重衰减（Weight Decay）正则化，通过惩罚大权重来防止过拟合。谱范数（Spectral Norm）是矩阵的最大奇异值：。它对应于从到的线性变换中最大的拉伸因子。在神经网络的稳定性分析和Lipschitz约束中，谱范数是核心概念。谱归一化（Spectral Normalization）是一种重要的权重归一化技术，它通过将权重矩阵除以其谱范数来约束网络函数的Lipschitz常数。核范数（Nuclear Norm）是矩阵所有奇异值的和：。核范数是Frobenius范数和谱范数之间的折中，常用于矩阵补全（Matrix Completion）和低秩矩阵恢复问题。在推荐系统和缺失数据插补中，核范数正则化可以鼓励解的低秩性质。1-范数和∞-范数分别是列范数和行范数：和。这些范数在稀疏优化和线性规划中有重要应用，但在深度学习中使用较少。矩阵范数的选择取决于具体问题的需求。在模型压缩和低秩近似中，我们关心如何用少量的奇异值最好地近似原始矩阵，这涉及到部分和（Partial Sum）范数​。在分析梯度流和稳定性时，谱范数最重要，因为它决定了信息放大或衰减的上界。在正则化中，Frobenius范数因其可导性好而最常用。除了前面介绍的奇异值分解，矩阵分解还包括LU分解、QR分解、特征值分解和Cholesky分解等多种方法。每种分解都有其特定的应用场景和数值性质。LU分解将矩阵分解为下三角矩阵和上三角矩阵的乘积：。对于方阵，如果主元都不为零，则LU分解存在；如果进行行交换（带置换矩阵），则PLU分解总是存在。LU分解的主要用途是高效求解线性方程组，因为前向和后向替换的复杂度是而不是高斯消元的。QR分解将矩阵分解为正交矩阵和上三角矩阵的乘积：。QR分解有多种计算方法，包括Gram-Schmidt正交化、Householder变换和Givens旋转。QR分解在线性最小二乘问题中有重要应用，因为正规方程的解等价于求解。特征值分解将方阵分解为特征向量矩阵和特征值对角矩阵的乘积：。只有可对角化矩阵（具有完整特征向量基）才能进行这种分解。对于对称矩阵，特征值分解总是可行的，且特征向量矩阵是正交的。特征值分解在动力系统分析、主成分分析和谱聚类中都有应用。Cholesky分解是LU分解的特例，只适用于对称正定矩阵。它将矩阵分解为下三角矩阵与其转置的乘积：。Cholesky分解的数值稳定性好，计算量约为LU分解的一半，在高斯过程回归和二次优化中有广泛应用。Kronecker积是两个矩阵之间的特殊二元运算，它将一个的矩阵与一个的矩阵组合成一个的大矩阵。对于矩阵和，它们的Kronecker积定义为：Kronecker积具有许多有用的代数性质，包括结合律、混合积性质 （当维度匹配时）、转置性质。在深度学习中，Kronecker积的主要应用包括权重共享、参数高效微调和特定网络结构的设计。例如，MobileNet中的深度可分离卷积可以用Kronecker积来表示，这有助于理解其参数效率。在模型并行化中，Kronecker积可以帮助我们将大矩阵运算分解为多个小矩阵运算的组合，从而在多个计算设备上分布式执行。向量化（Vectorization）是将矩阵转换为列向量的操作，记为或。如果 是的矩阵，则是一个的列向量，按列优先的顺序排列矩阵元素。向量化操作与Kronecker积结合可以简化矩阵方程的表示。具体来说，对于矩阵方程，通过向量化操作可以将其转化为线性方程。这种转化在推导反向传播公式和优化算法时非常有用，因为它将复杂的矩阵运算转化为标准的矩阵-向量乘积。张量网络是用低维张量通过网络结构表示高维张量的方法，在物理学和机器学习中都有重要应用。一个张量网络由若干个节点（每个节点是一个张量）和连接节点的边（每个边代表张量的一个维度）组成。张量网络的核心思想是用多个小张量的乘积之和来表示一个巨大的高维张量，从而实现高效的存储和计算。在大语言模型中，张量网络的思维方式有助于理解复杂的计算结构。Transformer中的注意力机制可以被看作一个三阶张量（查询、键、值的交互）被分解为多个低阶张量的组合。具体的，注意力权重矩阵可以看作是查询张量和键张量的收缩。张量网络在模型压缩和高效推理中有着重要的应用价值。通过将大型权重张量分解为张量网络的形式，可以显著减少参数量和计算量。例如，CP分解可以将一个的三阶张量表示为个秩一张量的和，参数复杂度从降低到。从张量网络的角度来看，矩阵乘法是连接两个张量的最基本操作。两个二阶张量（矩阵）的乘法可以看作是收缩掉两个张量的一个公共维度。更复杂的网络结构如树状张量网络（Tree Tensor Network）和分层张量分解（Hierarchical Tucker Decomposition）可以用于表示高阶交互关系。张量运算的高效实现是现代深度学习框架的核心能力之一。现代GPU架构专门优化了张量运算，包括矩阵乘法、卷积运算和批处理操作。理解张量运算的数学本质有助于编写高效的代码和诊断性能瓶颈。例如，在PyTorch中，通过仔细规划张量的内存布局和运算顺序，可以显著减少不必要的内存分配和数据拷贝。批处理（Batch Processing）是深度学习训练和推理的基本范式，它允许同时处理多个样本以提高计算效率。从张量的角度来看，批处理是在现有维度之外增加一个新的批处理维度。设单个样本的特征表示是形状为的二维张量（序列长度 × 嵌入维度），则一个批次的表示就是形状为 的三维张量，其中是批大小。批处理的数学优势来自于矩阵运算的并行性。当我们计算批次中所有样本的前向传播时，如果使用张量运算而非循环，可以一次性完成所有样本的计算。现代GPU的并行架构非常适合这种批处理计算，因为它可以将不同样本的计算分配到不同的处理单元上同时执行。数学上，批处理相当于将多个独立的矩阵运算堆叠成一个大的张量运算。注意力机制的批处理实现是理解这一概念的良好例子。单个样本的注意力计算涉及形状为的查询、键和值张量，其中是序列长度。对于一个批次，我们得到形状为的张量。注意力分数的计算在批处理情况下变成的三维张量运算。在实际实现中，这个运算被高度优化，GPU可以同时计算批次中所有样本的注意力分数。微批处理（Micro-batch）是批处理的一个变体，用于处理超大序列或有限GPU内存的情况。当单个样本太大以至于无法在GPU内存中容纳时，可以将批次进一步划分为更小的微批次，每个微批次独立计算。这种技术在训练大语言模型时尤为重要，因为长序列可能消耗大量内存。梯度累积是另一种与批处理相关的技术。当批大小受限于GPU内存时，我们可以通过梯度累积来模拟更大的有效批大小。具体来说，我们先计算若干个小批次的梯度，将它们累加起来，然后使用累加的梯度进行一次参数更新。这样，既保证了一次更新使用的样本数量（有效批大小），又不受单批次大小的限制。梯度累积在数学上等价于使用更大的批次进行训练，因为梯度是线性的。在大语言模型中，矩阵和张量运算的计算复杂度和内存效率是核心关注点。理解不同运算的资源需求对于优化模型设计和硬件利用至关重要。矩阵乘法的计算复杂度是，其中，，输出。这意味着计算一个权重矩阵与向量的乘积需要次标量乘法和加法。对于大语言模型中的全连接层，隐藏维度通常在数千量级，因此单个层的计算量是相当可观的。注意力机制的计算复杂度与序列长度的平方成正比。具体来说，对于序列长度和注意力维度 ，注意力分数的计算需要的计算量，注意力权重与值向量的乘积同样需要。这就是为什么Transformer在处理长序列时面临计算挑战，也是为什么许多研究致力于开发高效的注意力变体，如线性注意力、稀疏注意力和分层注意力。内存效率不仅与参数数量有关，还与激活值的存储需求有关。在前向传播过程中，每一层的输入和输出都需要被保存用于反向传播。对于深层网络和长序列，激活值的内存消耗可能超过参数本身。梯度检查点（Gradient Checkpointing）是一种常用的内存优化技术，它通过在前向传播时只保存部分层的激活值，在反向传播时重新计算被省略的激活值来节省内存，代价是增加额外的计算量。混合精度训练是另一种重要的效率优化技术。它使用半精度浮点数（FP16）或更低精度（如BF16、INT8）进行大部分计算，只在关键步骤使用全精度（FP32）以保持数值稳定性。混合精度训练可以将内存消耗减半，并将计算速度提高近一倍。数学上，混合精度利用了现代GPU对低精度运算的专门优化，这些运算的吞吐量通常是全精度运算的数倍。模型并行化将大模型分布在多个计算设备上，是训练超大规模语言模型的必要技术。张量并行（Tensor Parallelism）在模型的宽度维度上划分参数，流水线并行（Pipeline Parallelism）在深度维度上划分层，而数据并行（Data Parallelism）则复制整个模型在多个数据批次上并行训练。理解这些并行策略的数学基础——如何将矩阵运算分解为可以在不同设备上独立执行的子运算——对于设计和实现高效的大规模训练系统至关重要。]]></description><link>第1章-数学基础/1.1-线性代数与张量运算.html</link><guid isPermaLink="false">第1章 数学基础/1.1 线性代数与张量运算.md</guid><pubDate>Wed, 07 Jan 2026 08:10:09 GMT</pubDate><enclosure url="graph/llm_flow.drawio.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;graph/llm_flow.drawio.png&quot;&gt;&lt;/figure&gt;</content:encoded></item></channel></rss>