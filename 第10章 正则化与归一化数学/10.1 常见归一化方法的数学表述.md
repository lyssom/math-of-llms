归一化技术是深度学习中最具影响力的技术创新之一，它从根本上改变了神经网络的训练方式。从2015年批归一化（Batch Normalization）的提出，到2016年层归一化（Layer Normalization）的诞生，再到2019年均方根归一化（RMSNorm）的出现，归一化技术经历了持续的演进与优化。这些技术的核心价值在于：通过重参数化（Reparameterization）改变损失函数的几何景观（Landscape），使得优化过程更加稳定、收敛速度更快、梯度流动更加顺畅。本节将从数学角度严谨地推导三种主流归一化算法的定义、统计量计算方式、反向传播特性及推理时的行为差异，揭示它们在张量维度处理上的本质区别。

## 10.1.1 归一化的统一范式

在深入各种具体归一化方法之前，我们首先建立归一化技术的统一数学框架，这有助于理解不同方法之间的内在联系与本质差异。设输入张量为$\mathbf{X} \in \mathbb{R}^{N \times D}$，其中$N$为批量大小（Batch Size），$D$ 为特征维度。对于卷积神经网络，输入张量通常是四维的 $\mathbf{X} \in \mathbb{R}^{N \times C \times H \times W}$，其中$C$为通道数，$H$和 $W$分别为特征图的高度和宽度；对于自然语言处理中的Transformer模型，输入张量为$\mathbf{X} \in \mathbb{R}^{N \times L \times D}$，其中$L$为序列长度。无论输入张量的维度如何，归一化操作的核心思想是相同的：在某个特定的维度轴上计算统计量（均值和方差），然后对输入进行标准化处理。

归一化的通用数学表达式可以写为：
$$
\mathbf{y} = \frac{\mathbf{x} - \boldsymbol{\mu}}{\boldsymbol{\sigma}} \odot \boldsymbol{\gamma} + \boldsymbol{\beta}
$$
其中各符号的定义如下：$\mathbf{x}$为归一化前的输入张量，$\mathbf{y}$为归一化后的输出张量，$\boldsymbol{\mu}$为计算得到的均值向量，$\boldsymbol{\sigma}$为计算得到的标准差向量，$\boldsymbol{\gamma}$为可学习的缩放参数向量，$\boldsymbol{\beta}$为可学习的平移参数向量，$\odot$ 表示逐元素乘法（Hadamard积）。这个表达式揭示了归一化的三个核心步骤：首先是去均值化（Centering）操作$\mathbf{x} - \boldsymbol{\mu}$，消除输入分布的偏移；其次是缩放归一化（Scaling）操作 $\frac{\mathbf{x} - \boldsymbol{\mu}}{\boldsymbol{\sigma}}$​，将输入分布标准化为单位方差；最后是仿射变换（Affine Transformation）$\boldsymbol{\gamma} \odot \hat{\mathbf{x}} + \boldsymbol{\beta}$，恢复模型的表示能力。

**引理 10.1（归一化的零均值单位方差性质）** 如果忽略仿射变换参数$\boldsymbol{\gamma}$和$\boldsymbol{\beta}$，即令 $\boldsymbol{\gamma} = \mathbf{1}$，$\boldsymbol{\beta} = \mathbf{0}$，则归一化后的输出$\hat{\mathbf{x}}$ 满足零均值和单位方差的统计性质：
$$
\mathbb{E}[\hat{\mathbf{x}}] = 0, \quad \text{Var}(\hat{\mathbf{x}}) = 1
$$
**证明**：设$\mu = \mathbb{E}[\mathbf{x}]$，$\sigma^2 = \text{Var}(\mathbf{x})$，则：
$$
\mathbb{E}[\hat{\mathbf{x}}] = \mathbb{E}\left[\frac{\mathbf{x} - \mu}{\sigma}\right] = \frac{\mathbb{E}[\mathbf{x}] - \mu}{\sigma} = \frac{\mu - \mu}{\sigma} = 0
$$
$$
\text{Var}(\hat{\mathbf{x}}) = \mathbb{E}[\hat{\mathbf{x}}^2] - (\mathbb{E}[\hat{\mathbf{x}}])^2 = \mathbb{E}\left[\left(\frac{\mathbf{x} - \mu}{\sigma}\right)^2\right] = \frac{\mathbb{E}[(\mathbf{x} - \mu)^2]}{\sigma^2} = \frac{\sigma^2}{\sigma^2} = 1
$$

这个引理表明，标准归一化操作将输入分布转换为标准正态分布$\mathcal{N}(0, 1)$。然而，这一性质仅在统计量（均值和方差）是精确计算而非估计时成立。在实际应用中，由于使用小批量数据的样本均值和样本方差，存在一定的统计偏差（Bias）。

**注记 10.1（常数 $\epsilon$的作用）** 在归一化公式的分母中，通常添加一个小的正常数$\epsilon$（通常取$10^{-5}$ 或 $10^{-6}$）以确保数值稳定性。当输入方差$\sigma^2$非常小（接近零）时，$\sigma^2 + \epsilon$ 仍然保持正值，避免了除零错误。此外，$\epsilon$还起到平滑参数的作用，防止因数值精度问题导致的梯度不稳定。

归一化技术的理论基础与内部协变量偏移（Internal Covariate Shift, ICS）假设密切相关。该假设认为：在深度神经网络中，由于前一层参数的更新，当前层的输入分布会发生变化，这种分布的持续变化会增加模型训练的难度。归一化技术通过将每层的输入分布稳定在固定范围内，减少了层间的相互依赖，从而加速训练过程并提升模型性能。近年来，一些研究对ICS假设提出了质疑，认为归一化的主要好处可能来自损失景观的光滑化（Loss Landscape Smoothing）而非协变量偏移的减少。然而，无论理论解释如何，归一化技术在实践中的有效性已被广泛验证。

## 10.1.2 批归一化（Batch Normalization）

批归一化是最早提出的归一化技术，由Ioffe和Szegedy于2015年在论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》中提出。批归一化的核心思想是：在每个特征通道（Channel）维度上，对整个小批量数据计算均值和方差，然后进行归一化处理。这种设计使得每个特征通道的归一化是独立进行的，类似于对每个神经元进行独立的标准化。

### 训练阶段的数学推导

设输入张量为$\mathbf{X} \in \mathbb{R}^{N \times D}$，其中$N$为批量大小，$D$为特征维度。记$x_{ij}$为第$i$个样本的第$j$个特征。批归一化首先计算第$j$个特征在当前批量上的样本均值：
$$
\mu_j = \frac{1}{N} \sum_{i=1}^{N} x_{ij}
$$
这个均值表示第$j$个特征在当前批量中的中心位置。接下来计算第$j$个特征的样本方差：
$$
\sigma_j^2 = \frac{1}{N} \sum_{i=1}^{N} (x_{ij} - \mu_j)^2
$$
样本方差衡量了第$j$个特征在当前批量中的离散程度。需要注意的是，这里使用的是有偏估计（分母为$N$而非$N-1$），因为在深度学习实践中，使用有偏估计可以简化计算且性能差异可忽略。

**引理 10.2（批归一化的统计量计算）** 批归一化在特征维度上是独立的，即每个特征$j$的归一化只依赖于该特征在批量中的统计量，不受其他特征的影响。这意味着批归一化的计算可以并行化进行，提高了计算效率。

利用计算得到的均值和方差，可以对输入进行标准化处理：
$$
\hat{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}​​
$$
标准化后的$\hat{x}_{ij}$满足零均值和单位方差（忽略 $\epsilon$的影响）。最后，批归一化引入了两个可学习的仿射变换参数$\gamma_j$和$\beta_j$（均为标量），对标准化后的值进行缩放和平移：
$$
y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j​
$$
这两个参数的作用至关重要：如果没有仿射变换，归一化操作会将层的输出强制限制在固定范围，可能限制网络的表示能力。通过引入$\gamma$和$\beta$，网络可以学习恢复任意的均值和方差，从而保持模型的表达能力。从参数化的角度看，批归一化将原本的线性变换$\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$ 重新参数化为：
$$
\mathbf{z}_{\text{BN}} = \frac{\mathbf{W}\mathbf{x} + \mathbf{b} - \mu}{\sigma} \odot \gamma + \beta
$$
这种重参数化使得梯度可以通过$\mu$、$\sigma$、$\gamma$、$\beta$进行反向传播，而不需要显式地计算逆操作。

### 推理阶段的数学处理
批归一化在训练阶段使用当前批量的统计量（$\mu_{\mathcal{B}}$​ 和$\sigma_{\mathcal{B}}^2$​）进行归一化，但在推理阶段，输入通常是单个样本而非批量，此时无法计算有意义的批量统计量。为了解决这一问题，批归一化在训练过程中维护了全局统计量的移动平均（Moving Average）：
$$
\mu_{\text{running}} \leftarrow m \cdot \mu_{\text{running}} + (1 - m) \cdot \mu_{\mathcal{B}}​
$$$$
\sigma^2_{\text{running}} \leftarrow m \cdot \sigma^2_{\text{running}} + (1 - m) \cdot \sigma^2_{\mathcal{B}}
$$​其中$m$为动量参数（通常取$0.9$或$0.99$），$\mu_{\mathcal{B}}$​ 和$\sigma^2_{\mathcal{B}}$为当前批量的统计量。在推理阶段，使用最终得到的全局统计量$\mu_{\text{running}}$ 和$\sigma^2_{\text{running}}$替代批量统计量：
$$
y = \gamma \cdot \frac{x - \mu_{\text{running}}}{\sqrt{\sigma^2_{\text{running}} + \epsilon}} + \beta
$$
**引理 10.3（移动平均的无偏性修正）** 移动平均计算的$\mu_{\text{running}}$和 $\sigma^2_{\text{running}}$是对总体均值和方差的有偏估计。如果需要无偏估计，可以使用以下修正公式：
$$
\hat{\mu} = \frac{N}{N-1} \mu_{\text{running}}, \quad \hat{\sigma}^2 = \frac{N}{N-1} \sigma^2_{\text{running}}​
$$
但在实践中，这种修正通常被省略，因为$N$通常较大，偏差可以忽略。
### 批归一化的局限性

批归一化虽然取得了巨大成功，但也存在明显的数学和实践局限性。首先，批归一化的统计量计算依赖于批量大小$N$，当批量较小时，统计量的估计方差增大，导致归一化效果不稳定。具体而言，样本均值的方差与$1/N$成正比，样本方差的方差与$1/N$ 的平方根成正比。这意味着当批量大小减小时，批归一化的性能会显著下降。

**定理 10.1（批量大小对归一化稳定性的影响）** 设批量大小为$N$，样本均值的估计误差满足：
$$
\text{Var}(\hat{\mu}) = \frac{\sigma^2}{N}​
$$
其中$\sigma^2$为真实方差。这意味着为了将均值的估计误差减半，需要将批量大小增加四倍。

其次，批归一化在循环神经网络（RNN）和Transformer等变长序列模型中存在困难。在RNN中，不同时间步的输入序列长度可能不同，批归一化难以处理这种异构数据。在Transformer中，虽然序列长度通常固定，但批归一化在处理不同位置的归一化时也存在问题。这些局限性促使研究者开发了层归一化等替代方法。

最后，批归一化在训练和推理阶段使用不同的统计量计算方式，这导致了训练与推理之间的一致性问题。虽然这种设计在大多数情况下能够正常工作，但在某些边缘情况下（如在线学习、持续学习）可能引入性能下降。

## 10.1.3 层归一化（Layer Normalization）

层归一化由Ba等人于2016年在论文《Layer Normalization》中提出，旨在解决批归一化在循环神经网络中的不适用性问题。层归一化的核心思想是：不是对每个特征维度计算批量统计量，而是对每个样本的所有特征维度计算统计量。这种设计使得层归一化在处理变长序列和单个样本时更加稳定。

### 数学公式推导

设输入张量为$\mathbf{X} \in \mathbb{R}^{N \times D}$，其中$N$为批量大小，$D$为特征维度。层归一化首先计算第$i$个样本的均值：
$$
\mu_i = \frac{1}{D} \sum_{j=1}^{D} x_{ij}​
$$
与批归一化不同，这里是对每个样本$i$的所有特征$j$求和，而非对每个特征$j$的所有样本$i$求和。均值 $\mu_i$表示第$i$个样本的特征向量的中心位置。接下来计算第$i$个样本的方差：
$$
\sigma_i^2 = \frac{1}{D} \sum_{j=1}^{D} (x_{ij} - \mu_i)^2
$$
方差$\sigma_i^2$表示第$i$个样本的特征向量各分量相对于均值的离散程度。利用这些统计量，可以对输入进行标准化处理：
$$
\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}​​
$$
最后，层归一化同样引入了可学习的仿射变换参数：
$$
y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j​
$$
**引理 10.4（层归一化的统计量维度）** 层归一化的均值向量 $\boldsymbol{\mu} \in \mathbb{R}^N$ 和方差向量 $\boldsymbol{\sigma}^2 \in \mathbb{R}^N$，即每个样本有一个均值和一个方差值。与此相比，批归一化的均值向量$\boldsymbol{\mu} \in \mathbb{R}^D$和方差向量$\boldsymbol{\sigma}^2 \in \mathbb{R}^D$，即每个特征维度有一个均值和一个方差值。

**表 10.1.1 批归一化与层归一化的统计量维度对比**

| 统计量         | 批归一化           | 层归一化           |
| ----------- | -------------- | -------------- |
| 均值维度        | $\mathbb{R}^D$ | $\mathbb{R}^N$ |
| 方差维度        | $\mathbb{R}^D$ | $\mathbb{R}^N$ |
| γ\gammaγ 维度 | $\mathbb{R}^D$ | $\mathbb{R}^D$ |
| β\betaβ 维度  | $\mathbb{R}^D$ | $\mathbb{R}^D$ |

### 层归一化在Transformer中的应用

层归一化在Transformer架构中扮演着核心角色。在原始的Transformer论文《Attention Is All You Need》中，层归一化被放置在每个子层（Self-Attention和前馈网络）的残差连接之后：
$$
\mathbf{x}_{\text{output}} = \text{LN}(\mathbf{x} + \text{Sublayer}(\mathbf{x}))
$$
其中$\text{LN}$表示层归一化，$\text{Sublayer}(\mathbf{x})$可以是注意力机制或前馈网络的输出。这种设计被称为"Post-LN"结构。然而，后续研究发现，Post-LN结构在训练初期可能导致较大的梯度波动，影响训练的稳定性。

近年来，"Pre-LN"结构逐渐成为主流：
$$
\mathbf{x}_{\text{output}} = \mathbf{x} + \text{Sublayer}(\text{LN}(\mathbf{x}))
$$
即先进行层归一化，再通过子层，最后添加残差连接。Pre-LN结构在数学上的优势在于：层归一化位于残差分支内部，可以稳定梯度的流动，减少训练初期的波动。

**定理 10.2（Pre-LN的梯度稳定性质）** 在Pre-LN结构中，层归一化对输入$\mathbf{x}$的梯度$\frac{\partial \mathcal{L}}{\partial \mathbf{x}}$不会经过子层的复杂非线性变换，而是直接叠加到原始输入的梯度上。这种设计有效缓解了深层网络中的梯度消失问题。

### 层归一化的数学特性

层归一化具有几个重要的数学特性。首先，层归一化是**样本条件独立**的，即每个样本的归一化只依赖于该样本本身的特征，不受批量中其他样本的影响。这意味着层归一化在推理时可以处理任意大小的输入（包括单个样本），而不需要维护全局统计量。这与批归一化形成对比，后者在推理时需要使用训练阶段计算的移动平均。

**引理 10.5（层归一化的样本条件独立性）** 给定样本$\mathbf{x}_i$，其归一化输出$y_i$可以仅基于$\mathbf{x}_i$计算得到，与批量中其他样本无关：
$$
y_i = f(\mathbf{x}_i; \gamma, \beta)
$$
其中$f$是由公式 $\hat{\mathbf{x}}_i = \frac{\mathbf{x}_i - \mu_i}{\sigma_i}$和$y_i = \gamma \odot \hat{\mathbf{x}}_i + \beta$复合而成的函数。

其次，层归一化保持了个体内（Within-sample）的协方差结构，但消除了个体间（Between-sample）的差异。从几何角度看，层归一化将每个样本的特征向量投影到以原点为球心、半径为 $\sqrt{D}​$的超球面上（忽略$\epsilon$的影响），然后通过仿射变换进行缩放和平移。这种操作使得不同样本在特征空间中的相对位置关系得到保持，但整体分布被标准化。

## 10.1.4 均方根归一化（RMSNorm）

均方根归一化由Zhang等人于2019年在论文《Root Mean Square Layer Normalization》中提出，是对层归一化的重要改进。RMSNorm的核心洞见是：**层归一化中的去均值化操作（减去均值）在许多情况下是不必要的，甚至可能是有害的**。通过移除均值中心化步骤，RMSNorm在保持甚至提升性能的同时，显著降低了计算复杂度。

### 数学公式推导

RMSNorm的统计量计算与层归一化有本质区别。设输入张量为$\mathbf{X} \in \mathbb{R}^{N \times D}$，RMSNorm首先计算第$i$个样本的均方根统计量：
$$
\text{RMS}_i = \sqrt{\frac{1}{D} \sum_{j=1}^{D} x_{ij}^2}​​
$$
**引理 10.6（均方根统计量的几何意义）** 均方根统计量$\text{RMS}_i$是第$i$个样本特征向量的$\ell_2$范数除以 $\sqrt{D}$​：
$$
\text{RMS}_i = \frac{\|\mathbf{x}_i\|_2}{\sqrt{D}}​​
$$
其中$\|\mathbf{x}_i\|_2 = \sqrt{\sum_{j=1}^{D} x_{ij}^2}$是$\ell_2$范数。这意味着$\text{RMS}_i$衡量了特征向量的"尺度"（Scale）而非其相对于均值的离散程度（Variance）。
利用均方根统计量，RMSNorm对输入进行归一化：
$$
\hat{x}_{ij} = \frac{x_{ij}}{\text{RMS}_i}​​
$$
与层归一化不同，RMSNorm移除了去均值化步骤，直接使用均方根对输入进行缩放归一化。最后，RMSNorm保留了仿射变换参数（但只保留缩放参数$\gamma$，移除了平移参数$\beta$）：
$$
y_{ij} = \gamma_j \hat{x}_{ij} = \gamma_j \cdot \frac{x_{ij}}{\text{RMS}_i}​​
$$
**注记 10.2（移除平移参数的合理性）** RMSNorm移除平移参数$\beta$的数学依据是：均方根归一化具有**缩放不变性**（Scale Invariance）。具体而言，如果将输入向量$\mathbf{x}$乘以一个正数$c$，则均方根统计量也乘以$|c|$，归一化后的结果保持不变：
$$
\frac{c \cdot x_{ij}}{|c| \cdot \text{RMS}_i} = \frac{x_{ij}}{\text{RMS}_i}​​
$$
这种缩放不变性意味着平移参数$\beta$不是必需的，因为归一化操作已经处理了输入的尺度变化。

**定理 10.3（RMSNorm的缩放不变性）** 对于任意标量$c \neq 0$，RMSNorm的输出满足：
$$
\text{RMSNorm}(c \cdot \mathbf{x}; \gamma) = \text{RMSNorm}(\mathbf{x}; \gamma)
$$
即对输入进行均匀缩放不改变归一化的结果。这一性质对于深度网络的训练稳定性具有重要意义。

### 计算复杂度分析

RMSNorm相比层归一化具有更低的计算复杂度，主要体现在以下几个方面。首先，RMSNorm不需要计算均值：
$$
\mu_i = \frac{1}{D} \sum_{j=1}^{D} x_{ij}​
$$
这节省了一次求和操作。其次，RMSNorm不需要计算方差：
$$
\sigma_i^2 = \frac{1}{D} \sum_{j=1}^{D} (x_{ij} - \mu_i)^2 = \frac{1}{D} \sum_{j=1}^{D} x_{ij}^2 - \mu_i^2​
$$
这不仅节省了减法和平方操作，还避免了计算$\mu_i^2$​ 的额外开销。第三，RMSNorm不需要计算 $\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sigma_i}$中的减法和除法操作。

**表 10.1.2 层归一化与RMSNorm的计算操作对比**

| 操作类型  | 层归一化 | RMSNorm |
| ----- | ---- | ------- |
| 求均值   | 1次   | 0次      |
| 求方差   | 1次   | 0次      |
| 去均值化  | D次减法 | 0次      |
| 归一化除法 | D次除法 | D次除法    |
| 总操作数  | 约4D次 | 约2D次    |

从表中可以看出，RMSNorm的计算量约为层归一化的一半。这种计算效率的提升在大规模模型（如GPT-4、LLaMA等数十亿参数模型）中具有重要意义，可以显著减少训练时间和推理延迟。

### RMSNorm的理论基础

RMSNorm的设计基于对层归一化去均值化步骤的深入分析。在层归一化中，均值$\mu_i$ 的计算会消除特征向量中的某些信息。具体而言，考虑特征向量$\mathbf{x}_i = (x_{i1}, x_{i2}, \dots, x_{iD})$，去均值化操作 $\mathbf{x}_i - \mu_i \cdot \mathbf{1}$移除了向量在$\mathbf{1}$方向上的投影。这意味着层归一化不仅归一化了特征的尺度，还归一化了特征的方向（相对于超球面坐标系的方位角）。

**引理 10.7（去均值化操作的信息损失）** 去均值化操作相当于在特征向量上施加约束 $\sum_{j=1}^{D} (x_{ij} - \mu_i) = 0$，这丢失了原向量在$\mathbf{1}$方向上的信息。在某些任务中，这一方向的信息可能对模型性能有重要贡献。

RMSNorm通过移除去均值化步骤，保留了这一方向的信息。同时，由于仿射变换参数$\beta$被移除，RMSNorm仍然保持了某种程度的归一化性质，输出向量的尺度被约束在合理的范围内。实验表明，在许多任务中，RMSNorm可以达到与层归一化相当甚至更好的性能，同时具有更低的计算成本。

## 10.1.5 三种归一化方法的维度对比与总结

为了更清晰地理解三种归一化方法的本质区别，本节将从张量维度、处理轴和统计量计算三个角度进行系统对比。

### 张量维度处理对比

考虑输入张量$\mathbf{X} \in \mathbb{R}^{N \times D}$，三种归一化方法的统计量计算轴（Axis of Reduction）如下：

**批归一化（BatchNorm）**：在批量维度$N$上求和，统计量维度为$\mathbb{R}^D$。对于每个特征维度$j$，计算所有样本在该特征上的均值和方差。数学表达为：
$$
\mu_j = \frac{1}{N} \sum_{i=1}^{N} x_{ij}, \quad \sigma_j^2 = \frac{1}{N} \sum_{i=1}^{N} (x_{ij} - \mu_j)^2
$$
**层归一化（LayerNorm）**：在特征维度$D$上求和，统计量维度为$\mathbb{R}^N$。对于每个样本$i$，计算该样本所有特征的均值和方差。数学表达为：
$$
\mu_i = \frac{1}{D} \sum_{j=1}^{D} x_{ij}, \quad \sigma_i^2 = \frac{1}{D} \sum_{j=1}^{D} (x_{ij} - \mu_i)^2
$$
**均方根归一化（RMSNorm）**：同样在特征维度D上求和，但只计算均方根统计量，不计算均值和方差。数学表达为：
$$
\text{RMS}_i = \sqrt{\frac{1}{D} \sum_{j=1}^{D} x_{ij}^2}
$$
**表 10.1.3 三种归一化方法的维度对比**

| 属性          | BatchNorm      | LayerNorm      | RMSNorm        |
| ----------- | -------------- | -------------- | -------------- |
| 求和轴         | 批量轴N           | 特征轴D           | 特征轴D           |
| 均值维度        | $\mathbb{R}^D$ | $\mathbb{R}^N$ | 无              |
| 方差维度        | $\mathbb{R}^D$ | $\mathbb{R}^N$ | 无              |
| RMS维度       | 无              | 无              | $\mathbb{R}^N$ |
| $\gamma$ 维度 | $\mathbb{R}^D$ | $\mathbb{R}^D$ | $\mathbb{R}^D$ |
| $\beta$ 维度  | $\mathbb{R}^D$ | $\mathbb{R}^D$ | 无              |
### 公式对比汇总
为便于查阅和比较，下面给出三种归一化方法的完整公式汇总。
**批归一化（BatchNorm）**：
$$\mu_j = \frac{1}{N} \sum_{i=1}^{N} x_{ij}$$$$\sigma_j^2 = \frac{1}{N} \sum_{i=1}^{N} (x_{ij} - \mu_j)^2$$$$\hat{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}$$ $$y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j$$​
**层归一化（LayerNorm）**：
$$
\mu_i = \frac{1}{D} \sum_{j=1}^{D} x_{ij}​
$$
$$
\sigma_i^2 = \frac{1}{D} \sum_{j=1}^{D} (x_{ij} - \mu_i)^2
$$
$$\hat{x}_{ij} = \frac{x_{ij} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}$$
$$y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j$$
**均方根归一化（RMSNorm）**：
$$
\text{RMS}_i = \sqrt{\frac{1}{D} \sum_{j=1}^{D} x_{ij}^2}
$$
$$
\hat{x}_{ij} = \frac{x_{ij}}{\text{RMS}_i + \epsilon}x^ij​=RMSi​+ϵxij​​  
yij=γjx^ijy_{ij} = \gamma_j \hat{x}_{ij}​
$$
**注记 10.3（$\epsilon$的位置差异）** 在RMSNorm中，$\epsilon$通常加在分母的RMS值之后，即 $\text{RMS}_i + \epsilon$，而非作为方差的加性项。这是为了保持数值稳定性，同时避免改变RMS的定义。

### 适用场景分析

三种归一化方法各有其最佳适用场景。批归一化适用于卷积神经网络（CNN），特别是当批量大小较大（如$N \geq 32$）时。在CNN中，批归一化通常应用于卷积层之后、全连接层之前，作用于通道维度$C$。批归一化的优点是能够利用批量内的样本多样性进行归一化，缺点是对批量大小敏感，且在RNN和在线学习场景中表现不佳。
层归一化适用于循环神经网络（RNN）、Transformer和需要处理单个样本的场景。在这些场景中，批量内样本的统计量可能不够稳定或不够代表（特别是当批量大小为1时）。层归一化的优点是推理时稳定、不依赖批量大小，缺点是计算量略大于批归一化，且在小批量情况下可能不如批归一化有效。
均方根归一化适用于大规模语言模型和计算资源受限的场景。在GPT、LLaMA等大型Transformer模型中，RMSNorm已经逐渐取代层归一化，成为事实上的标准选择。RMSNorm的优点是计算效率高、性能相当，缺点是理论解释相对复杂，且在某些任务中可能不如层归一化稳定。

**表 10.1.4 归一化方法的适用场景对比**

|归一化方法|最佳适用场景|主要优势|主要局限|
|---|---|---|---|
|BatchNorm|大批量CNN|性能优异、收敛快|依赖批量大小、不适用于RNN|
|LayerNorm|Transformer、RNN|推理稳定、单样本OK|计算量较大|
|RMSNorm|大规模LLM|计算高效、性能相当|理论解释复杂|
## 10.1.6 本节小结

本节系统地介绍了深度学习中三种主流归一化方法的数学原理与公式推导。我们从归一化的统一范式出发，展示了$\mathbf{y} = \frac{\mathbf{x} - \boldsymbol{\mu}}{\boldsymbol{\sigma}} \odot \boldsymbol{\gamma} + \boldsymbol{\beta}$ 这一通用表达式，并详细推导了批归一化、层归一化和均方根归一化的具体计算公式。

批归一化通过在批量维度上计算统计量，实现了对每个特征通道的独立归一化，其训练与推理阶段使用不同的统计量计算方式（批量统计量 vs 移动平均）。层归一化通过在特征维度上计算统计量，实现了对每个样本的独立归一化，避免了批归一化对批量大小的依赖。均方根归一化通过移除去均值化步骤，进一步简化了计算，同时保持了良好的归一化效果。

三种方法的本质区别在于**统计量的计算轴不同**：批归一化在批量轴$N$上聚合信息，统计量维度为$\mathbb{R}^D$；层归一化和RMSNorm在特征轴$D$上聚合信息，统计量维度为$\mathbb{R}^N$。这种维度差异决定了它们在计算特性、适用场景和数学性质上的不同。理解这些数学本质，不仅有助于正确使用这些归一化技术，更为设计和分析新的归一化方法提供了理论基础。