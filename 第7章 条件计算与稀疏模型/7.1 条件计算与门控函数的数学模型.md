# 第7章 条件计算与稀疏模型
## 7.1 条件计算与门控函数的数学模型

条件计算（Conditional Computation）从根本上改变了模型参数与计算量之间的刚性绑定关系。传统神经网络处理任何输入都会激活全部参数，计算成本随模型规模增长呈线性甚至超线性上升。条件计算引入按需激活范式：模型可拥有海量参数，但每个输入只动态选择一小部分参与计算。本节将系统阐述条件计算的形式化定义、门控函数的概率模型，以及混合专家（Mixture of Experts，MoE）架构的数学本质。

### 7.1.1 条件计算的形式化定义

传统神经网络可形式化表示为确定性函数映射 $f: \mathcal{X} \to \mathcal{Y}$，设输入向量为 $\mathbf{x} \in \mathbb{R}^d$，输出为 $\mathbf{y} = f(\mathbf{x}; \theta)$，其中 $\theta$ 是所有参数。在这个框架下，无论输入内容如何复杂，参数 $\theta$ 的每个元素都会被用于计算，这种"一刀切"的模式存在显著的资源浪费。

条件计算的核心洞见在于：根据输入内容动态决定使用哪些参数，可以避免对无关参数的无效计算。从数学上看，条件计算将传统函数映射扩展为条件函数族 $\{f_i: \mathcal{X} \to \mathcal{Y}\}_{i=1}^{E}$，并引入路由函数 $R: \mathcal{X} \to \{1, \ldots, E\}$ 决定为每个输入选择哪个函数。模型最终输出为 $f_{R(\mathbf{x})}(\mathbf{x})$，计算量从 $O(\sum_{i=1}^{E} \text{cost}(f_i))$ 降低到 $O(\text{cost}(f_{R(\mathbf{x})}))$。

混合专家模型将条件计算具体化为专家网络集合与门控机制的协调配合。设共有 $E$ 个专家网络，记为 $\mathcal{E} = \{E_1, E_2, \ldots, E_E\}$，每个专家 $E_i$ 是参数化为 $\theta_i$ 的神经网络。门控机制接收输入 $\mathbf{x}$，输出权重向量 $G(\mathbf{x}) = (g_1(\mathbf{x}), g_2(\mathbf{x}), \ldots, g_E(\mathbf{x}))$，决定每个专家的贡献程度。模型输出为专家输出的加权组合：

$$\mathbf{y}(\mathbf{x}) = \sum_{i=1}^{E} g_i(\mathbf{x}) \cdot f_i(\mathbf{x})\tag{7.1.1}$$

这个公式精确描述了MoE模型的信息融合方式：每个专家 $f_i$ 对输入 $\mathbf{x}$ 进行独立变换，门控权重 $g_i(\mathbf{x})$ 决定各专家输出的贡献比例。虽然形式上是线性组合，但由于专家网络本身是非线性的，整个MoE模型仍高度非线性。

稀疏激活是MoE区别于其他架构的关键特征。在标准MoE设置中，只有权重最高的 $K'$ 个专家（通常 $K' \ll E$）会被实际激活并参与计算。对于权重低于阈值的专家，其参数不会被访问，计算和内存访问被完全跳过。当 $K'$ 固定而 $E$ 增长时，模型参数规模可大幅扩展，推理成本仅线性于 $K'$ 而非 $E$。数学上，设 $S(\mathbf{x})$ 是权重最大的 $K'$ 个专家的索引集合，稀疏门控权重定义为：

$$g_i^{\text{sparse}}(\mathbf{x}) = \begin{cases} \frac{g_i(\mathbf{x})}{\sum_{j \in S(\mathbf{x})} g_j(\mathbf{x})} & \text{if } i \in S(\mathbf{x}) \\ 0 & \text{otherwise} \end{cases}\tag{7.1.2}$$

此定义确保 $\sum_{i \in S(\mathbf{x})} g_i^{\text{sparse}}(\mathbf{x}) = 1$，同时将计算复杂度从 $O(E)$ 降低到 $O(K')$。

### 7.1.2 门控函数的概率模型

门控机制是MoE架构的核心决策模块，其设计直接决定稀疏激活模式和训练效率。从概率论角度看，门控函数本质上是一个条件概率分布，定义给定输入 $\mathbf{x}$ 时选择各个专家的概率。

Softmax门控函数是最基础且广泛使用的门控设计，其数学形式简洁优雅，同时具有良好的可微性。门控函数 $G: \mathbb{R}^d \to \mathbb{R}^E$ 接收输入特征 $\mathbf{x}$，输出权重向量 $G(\mathbf{x}) = (g_1(\mathbf{x}), \ldots, g_E(\mathbf{x}))$。Softmax门控定义为：

$$G(\mathbf{x}) = \text{Softmax}(\mathbf{W}\mathbf{x} + \mathbf{b})\tag{7.1.3}$$

其中 $\mathbf{W} \in \mathbb{R}^{E \times d}$ 是可学习权重矩阵，$\mathbf{b} \in \mathbb{R}^E$ 是偏置向量。逐分量形式为：

$$g_i(\mathbf{x}) = \frac{\exp((\mathbf{W}\mathbf{x} + \mathbf{b})_i)}{\sum_{j=1}^{E} \exp((\mathbf{W}\mathbf{x} + \mathbf{b})_j)}\tag{7.1.4}$$

此定义确保输出满足概率分布性质：$g_i(\mathbf{x}) \geq 0$ 且 $\sum_{i=1}^{E} g_i(\mathbf{x}) = 1$。从概率论角度看，$g_i(\mathbf{x})$ 可解释为给定输入 $\mathbf{x}$ 时，专家 $i$ 被选中的后验概率。Softmax函数的核心特性是保持原始得分的相对顺序，得分最高的专家仍获得最高权重。

Softmax门控函数完全可微。设 $s_i(\mathbf{x}) = (\mathbf{W}\mathbf{x} + \mathbf{b})_i$ 是原始门控得分，则：

$$\frac{\partial g_i}{\partial s_j} = g_i (\delta_{ij} - g_j)\tag{7.1.5}$$

其中 $\delta_{ij}$ 是克罗内克函数。这种雅可比结构表明，门控权重不仅取决于自身得分的梯度，还受到其他专家得分的间接影响。

Top-K稀疏路由在实际大规模MoE实现中扮演关键角色，其核心思想是只选择权重最大的 $K'$ 个专家参与计算。Top-K选择操作可数学化为：

$$S(\mathbf{x}) = \underset{S \subseteq \{1, \ldots, E\}, |S|=K'}{\arg\max} \sum_{i \in S} s_i(\mathbf{x})\tag{7.1.6}$$

其中 $s_i(\mathbf{x}) = (\mathbf{W}\mathbf{x} + \mathbf{b})_i$ 是专家 $i$ 的原始门控得分。设原始Softmax输出为 $\mathbf{g}(\mathbf{x}) \in \mathbb{R}^E$，Top-K掩码向量为 $\mathbf{m}(\mathbf{x}) \in \{0, 1\}^E$，则稀疏门控为：

$$\mathbf{g}^{\text{sparse}}(\mathbf{x}) = \frac{\mathbf{g}(\mathbf{x}) \odot \mathbf{m}(\mathbf{x})}{\sum_{j=1}^{E} g_j(\mathbf{x}) \cdot m_j(\mathbf{x})}\tag{7.1.7}$$

其中 $\odot$ 表示逐元素乘法。

Sparsemax门控提供替代性的概率建模框架，直接输出稀疏概率分布。Sparsemax函数定义为：

$$g_i^{\text{sparsemax}}(\mathbf{x}) = \max\left(0, s_i(\mathbf{x}) - \tau\right)\tag{7.1.8}$$

其中 $\tau$ 是阈值参数，确保 $\sum_i g_i^{\text{sparsemax}}(\mathbf{x}) = 1$。Sparsemax门控的优势在于天然产生稀疏输出，不需要额外的Top-K操作，同时保持端到端的可微性。

### 7.1.3 门控机制的离散优化视角

门控机制面临根本性挑战：Top-K选择操作本身不可微，梯度无法直接反向传播到门控网络。门控近似于随机变量选择，其优化过程涉及离散优化问题，需要特殊技巧处理。

由于Top-K操作不可微（输出相对于输入的梯度几乎处处为零），直接使用Top-K会导致梯度无法反向传播。设 $\mathbf{s} \in \mathbb{R}^E$ 是原始门控得分，$S(\mathbf{x})$ 是选中的专家集合，则 $\frac{\partial \mathbf{I}(i \in S)}{\partial s_j} = 0$ 对几乎所有 $\mathbf{s}$ 成立。这意味着门控网络无法从输出损失中获得有意义的梯度信号。

Gumbel-Softmax是常用的近似Top-K选择的技巧：

$$g_i^{\text{Gumbel}}(\mathbf{x}) = \frac{\exp((s_i(\mathbf{x}) + g_i) / \tau)}{\sum_{j=1}^{E} \exp((s_j(\mathbf{x}) + g_j) / \tau)}\tag{7.1.9}$$

其中 $g_i = -\log(-\log(u_i))$ 是从Gumbel分布采样的噪声，$u_i \sim \text{Uniform}(0,1)$，$\tau$ 是温度参数。当 $\tau \to 0$ 时，Gumbel-Softmax收敛于One-Hot分布（近似硬Top-K）；当 $\tau \to \infty$ 时，收敛于均匀分布。通过训练时使用Gumbel-Softmax（$\tau > 0$）并在推理时切换到硬Top-K（$\tau \to 0$），可实现可学习的稀疏路由。

噪声加入是稳定训练和促进专家多样性的关键技巧。门控网络训练面临独特挑战：由于路由决策的稀疏性，某些专家若初始时被选中的概率较低，就很难获得足够的梯度信号，从而长期被冷落。这种正反馈机制可能导致少数专家主导路由决策，多数专家几乎不被激活，形成"专家坍缩"现象。数学上，不平衡可用专家分配概率的方差量化：

$$\text{Var}(\mathbf{\pi}) = \frac{1}{E} \sum_{i=1}^{E} (\pi_i - \bar{\pi})^2\tag{7.1.10}$$

其中 $\pi_i = \mathbb{P}(I_i = 1)$ 是专家 $i$ 被激活的边际概率，$\bar{\pi} = K'/E$ 是平均分配概率。

噪声加入的核心思想是通过在门控得分中注入随机性打破正反馈循环。设原始门控得分为 $\mathbf{s}(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}$，则带噪声的门控得分为：

$$\tilde{\mathbf{s}}(\mathbf{x}) = \mathbf{s}(\mathbf{x}) + \boldsymbol{\epsilon}(\mathbf{x})\tag{7.1.11}$$

其中 $\boldsymbol{\epsilon}(\mathbf{x}) = ( \epsilon_1(\mathbf{x}), \ldots, \epsilon_E(\mathbf{x}) )$ 是噪声向量，常用高斯分布 $\epsilon_i(\mathbf{x}) \sim \mathcal{N}(0, \sigma^2)$。噪声加入仅在训练阶段使用，推理时需要关闭。

### 7.1.4 混合专家模型的数学本质

从更广泛的视角来看，MoE模型与多个经典统计学习方法存在深刻的数学联系。MoE与稀疏线性组合、混合模型（Mixture Model）以及期望最大化（EM）思想都有数学对应关系。

MoE模型与稀疏线性组合的关系体现在输出形式上。标准MoE输出为 $\mathbf{y}(\mathbf{x}) = \sum_{i=1}^{E} g_i(\mathbf{x}) \cdot f_i(\mathbf{x})$，这是关于专家输出的线性组合，组合系数由输入依赖的门控函数给出。当门控函数输出稀疏权重时（如Top-K门控），此线性组合退化为少数专家输出的非零组合，形成稀疏表示。

MoE模型与混合模型的联系更为深刻。在统计学中，混合模型假设观测数据来自多个潜在分布的加权混合：

$$p(\mathbf{y}) = \sum_{i=1}^{E} \pi_i \cdot \mathcal{N}(\mathbf{y} | \boldsymbol{\mu}_i, \boldsymbol{\Sigma}_i)\tag{7.1.12}$$

其中 $\pi_i$ 是混合系数。比较MoE模型的输出公式，可见形式上的相似性：MoE的输出是专家输出的加权平均，混合模型的预测分布是成分分布的加权平均。关键区别在于：混合模型的混合系数 $\pi_i$ 通常是固定的，而MoE的门控权重 $g_i(\mathbf{x})$ 是输入依赖的函数。这种将混合系数条件化的设计是MoE相对于传统混合模型的本质创新。

MoE与EM思想的联系体现在专家学习过程中。EM算法交替执行E步（估计隐变量）和M步（更新模型参数）。在MoE训练中，可将专家分配 $z_i(\mathbf{x}) = \mathbb{I}(i \in S(\mathbf{x}))$ 视为隐变量。E步中，给定当前门控网络，训练样本被分配到各个专家；M步中，每个专家独立地根据分配给自己的样本进行优化。这种交替优化的结构与EM算法有异曲同工之妙。

负载均衡不仅依赖于噪声加入，通常还需要显式的辅助损失函数。设专家 $i$ 在当前批次中的分配比例为：

$$\pi_i^{\text{batch}} = \frac{1}{|\mathcal{B}|} \sum_{\mathbf{x} \in \mathcal{B}} \mathbb{I}(i \in S(\mathbf{x}))\tag{7.1.13}$$

其中 $\mathcal{B}$ 是当前批次。辅助损失函数可定义为分配比例与均匀分布的偏离程度：

$$\mathcal{L}_{\text{balance}} = E \cdot \sum_{i=1}^{E} \pi_i^{\text{batch}} \cdot \log(E \cdot \pi_i^{\text{batch}})\tag{7.1.14}$$

此损失在 $\pi_i^{\text{batch}} = 1/E$ 时最小（值为0），在分配不均衡时增大。将 $\mathcal{L}_{\text{balance}}$ 作为辅助损失加入总损失函数：$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{main}} + \lambda \cdot \mathcal{L}_{\text{balance}}$，可引导门控网络学习更均衡的专家分配策略。

### 7.1.5 本节小结

本节系统阐述了条件计算与门控函数的数学模型。条件计算的形式化定义将传统函数映射扩展为条件函数族，通过路由函数实现输入依赖的计算选择。MoE模型的核心公式 $\mathbf{y}(\mathbf{x}) = \sum_{i=1}^{E} g_i(\mathbf{x}) \cdot f_i(\mathbf{x})$ 精确描述了专家输出与门控权重的融合方式，稀疏激活通过Top-K选择实现计算复杂度的降低。

门控函数的概率模型部分分析了Softmax门控、Top-K门控和Sparsemax门控的数学性质。Softmax门控提供优雅的概率解释，输出满足归一化约束；Top-K门控实现稀疏激活的确定性选择；Sparsemax提供端到端的稀疏概率建模。从离散优化的视角来看，门控机制面临不可微性带来的挑战，Gumbel-Softmax和噪声加入等技术通过引入随机性来近似离散选择。

最后，我们将MoE与稀疏线性组合、混合模型和EM思想进行了对比分析，揭示了MoE模型在机器学习理论体系中的位置。负载均衡损失的设计使模型能够避免专家坍缩，实现高效的参数利用。这些数学工具和理论框架为设计和训练高质量的MoE模型提供了坚实的理论基础。